[{"id":0,"href":"/keeplearning/docs/favorite/cs-learn/","title":"计算机基础书籍推荐","section":"收藏夹","content":" 计算机基础书籍推荐 # 在自学过程中，最容易踩坑的地方就是不看自己当前水平，盲目跟风买那些豆瓣高分的大而全的计算机黑皮系列的书，然后学几天，就放弃了。\n这些大而全的计算机黑皮书当然很经典，但是它们并不适合新人入门学习，因为这类书籍的内容都充满大量的专业术语，我们在看到陌生又难以理解的词汇时，就会感觉很吃力，脑子看着就会很累，就会驱使你做简单和快乐的事情，比如睡觉、刷短视频、玩游戏。\n所以，学习一门学科的时候，要从最基础的书开始学起，接着搭配视频快速入门，然后再渐渐步入到这些大而全的计算机黑皮书。\n关键的问题来了，计算机基础有哪些入门的基础书和视频呢？\n一、数据结构与算法 # 数据结构与算法是非常非常非常重要的，想要冲大厂如果这方面不过关可能连笔试都过不去，更别谈面试的手撕算法了。\n算法： 动态规划、回溯算法、查找算法、搜索算法、贪心算法、分治算法、位运算、双指针、排序、模拟、数学、…… 数据结构： 数组、栈、队列、字符串、链表、树、图、堆、哈希表、…… 数据结构学习 # 首先推荐 《大话数据结构》 这本入门级别的书，因为书里的内容都是大白话，而且还图文并茂，读起来还是很顺畅的。\n然后视频推荐 《浙江大学的数据结构》 课程，在 B 站就能搜索到，课程是老师带大家用 C 语言来实现各种常见的数据结构。\n现在大多数高级语言都会有容器，就是把一些常见的数据结构封装成了容器，使用起来比较方便，但是不利于理解底层的数据结构是怎么变换和操作的，所以这门课还是很有意义的。\n如果想要实战的话，可以去 LeetCode 官方出品的免费教程 Leetbook，网站很细心的按照各个知识点循序渐进地罗列了出来。讲解知识 + 实战演练，学习起来会比看书效率高。\n算法学习 # 算法刷题可从以下三个渠道。\n第一个， 剑指offer\n第二个， 力扣\n第三个，近期 面试中常考题\n《剑指 offer》作为大经典，是一定要刷两遍以上的，很多题都是面试时的原题，里面包含了很多笔试常用的思想方法，反复看，反复研。\nleetcode 由于题目十分之多，刷完是不太可能的。可以把 leetcode 作为弱项的专项练习。leetcode 右侧有标签分类，如下图：\n另外在巩固完弱项的情况下，建议将 leetcode 前 300 题刷熟练，国内大厂面试出的代码题 80% 都是这个范围内的。\n另外，labuladong 总结的 算法小抄 对算法的理解很有帮助。\n公司常考题有：\n链表类（链表反转类题目） 二叉树类（二叉树的遍历类型题，最大公祖先类题目） 字符串操作题目 dfs/bfs 动态规划（这个考的基本都是 leetcode 上的或者是背包问题，对于动态规划问题其实有很多种类，比较见到的就是一维动态和二维动态），另外还有区间调度类型的题目（贪心算法，也属于动态规划的一种特殊情况）。 其实也没有什么技巧，多刷题，多理解就好了。\n二、计算机组成原理 # 计算组成原理确实是分为两个方向，一个是硬件电路的，一个是软件程序的。\n入门学习 # 先极力推荐 b 站的 《计算机科学速成课》 ，这个课程是国外录制的，内容真的是好，视频的动画很精美，讲课的时候不会很死板，反正就是不看后悔、相见很晚系列。\n对于入门计算机组成，可以先看前 10 个视频，看完这 10 个视频也就不到 2 个小时，看完前 10 个视频对计算机的工作方式就有一个基本的了解了。看完前 10 个视频就可以开始看书了。\n不太建议小白一上来就看那些厚的不行的计算机组成原理的黑皮书，这些书是经典的没错，也正是由于它们是经典的，所以这些书的知识体系很全、很多、很厚。这样很容易让初学者迷失在里头，可能刚兴致勃勃看几十页就放弃了，于是这些厚的不行的书就成为了垫书神器，知识没学多少，颈椎病倒是治好了。\n对于初学者，推荐两本书 《计算机是怎么样跑起来》和 《程序是怎么跑起来的》，这两本很薄而且图文并茂，作者都是用大白话的方式来阐述知识，这点对初学者非常友好。\n这两本不用 1 个月就能看完，因为在看这两本书的时候，相比学习的心态，更多的是会带着「好奇心」的心态去读。\n其中《程序是怎么跑起来的》是一个「微缩版本」的计算机组成原理。从这本书的名字也可以知道，它是从计算机是怎么运行程序的视角来讲的，然后把涉及到的计算机硬件和它们之间是如何协作的一点一点的给带出来，让读者能瞬间明白这些计算机硬件的作用。这本仅仅是入门级别，主要的作用是让初学者明白计算机组成原理这门课是学什么的，以及梳理主要的知识体系，有了这本书的概念后，再去深入计算机组成的时候，就不会雨里雾里的。\n另外， 《编码：隐匿在计算机软硬件背后的语言》这本书也很不错，是本科普类的书，非常适合非科班的同学，主要讲是计算机工作的原理（二进制编码、加减法运算、计算机部件、浮点数定点数、处理器等），也就是跟计算机组成息息相关的知识，它的内容很有趣味性，并不像教科书那样晦涩难懂，丝毫不会让你感到生硬，读起来很畅快。\n深入学习 # 想要深入学习计算机组成原理的同学，首先推荐 《计算机组成与设计：硬件 / 软件接口》这本书，这本书确实很厚，差不多 500 多页，但是书从来没有人规定一定要从头读到尾，一页页的读的。重要的不是看完一本书，而是从书上学到多少，解决了什么问题。\n可以挑这几个章节看，跟开发者关系比较大的章节：\n第一章：计算机抽象以及相关技术，这个章节主要是介绍了计算机组成的思想，可以简单快读，不用重点读； 第二章：指令，大体上讲的是计算机是如果识别和运行指令的，以及代码到指令的过程； 第三章：计算机的算数运算，介绍的是计算机是如何进行加减乘除法的，以及浮点数的运算； 第五章：层次化存储，讲的是计算机的存储层次结构，而且重点讲的是 CPU Cache。 看书觉得很累，也可以结合视频一起看，推荐 B 站 哈工大的《计算机组成原理》视频。\n看书和看视频可以相互结合的，比如看视频看了计算机指令的内容，可以不用继续往下看，可以回到这本书上，看书上对应这个章节的内容，这是个很好的学习方法，视频和书籍相辅相成。\n要是觉得哈工大的计组课程太难，可以看 B 站 王道考研的计算机组成原理的视频课程。\n这个视频虽然是针对考研的，但是也是可以作为学习计组的资料，讲的内容不会太深，适合快速建立计算机组成原理体系和梳理计组知识的脉络。\n另外，再推荐一本 《深入理解计算系统》这本书，人称 CSAPP。\n这本书是从程序员的角度学习计算机系统是如何工作的，通过描述程序是如何映射到计算机系统上，程序是如何执行的，以及程序效率低下的原因，这样的方式可以让读者能更好的知道「程序与计算机系统」的关系。\nCSAPP 涵盖的内容非常多，有计算机组成 + 操作系统 + 汇编 + C语言 + Linux系统编程，涉猎的领域比较多，是一本综合性的书，更是一本程序员修炼内功的指引书。\nCSAPP 主要包括以下内容：\n信息表示（如何使用二进制表示整型、浮点数等）； C 和汇编语言的学习（通过汇编语言更深入地理解C语言是什么）； 计算机体系结构（存储层次结构、局部性原理、处理器体系结构）； 编译链接（C语言如何从文本变成可执行文件、静态链接、动态链接）； 操作系统的使用（异常控制流、虚拟内存、多个系统调用介绍）； 网络及并发编程（并发的基本概念、网络相关的系统调用的介绍）。 这本书有部分内容和《计算机组成与设计：硬件 / 软件接口》这本书重合了，重合的部分就是重中之重的计算机组成原理知识，而且内容都是差不多的，可以看完一本书的内容，然后跳到另外一本看相同章节的内容，多本书的结合可以让我们更加容易理解。\n这两本书有个区别：\n《计算机组成与设计：硬件 / 软件接口》讲的指令格式是 RISC 的； 《深入理解计算系统》讲的指令格式是 x86 的； 其他重合的计组知识都大同小异。\nCSAPP 的视频课程是国外老师录制的，但是在 B 站已经有好人做了中文字幕，看了这视频，相当于在国外上了一门计算机课的感觉。\nB 站地址： 【精校中英字幕】2015 CMU 15-213 CSAPP 深入理解计算机系统 课程视频\n这本书是很厚，但是并不一定要把书完整看完，每个章节的知识点还是比较独立的，有关硬件的章节其实可以选择跳过。\n三、操作系统 # 操作系统真的可以说是 Super Man，它为了我们做了非常厉害的事情，以至于我们根本察觉不到，只有通过学习它，我们才能深刻体会到它的精妙之处，甚至会被计算机科学家设计思想所震撼，有些思想实际上也是可以应用于我们工作开发中。\n操作系统比较重要的四大模块，分别是 内存管理、 进程管理、 文件系统管理、 输入输出设备管理。这是推荐的学习顺序，因为内存管理不仅是最重要、最难的模块，也是和其他模块关联性最大的模块，先把它搞定，后续的模块学起来我认为会相对轻松一些。\n学习的过程中，可能会遇到很多「虚拟」的概念，比如虚拟内存、虚拟文件系统，实际上它们的本质上都是一样的，都是向下屏蔽差异，向上提供统一的东西，以方便我们程序员使用。\n还有，你也遇到各种各样的 调度算法，在这里可以看到数据结构与算法的魅力，重要的是我们要理解为什么要提出那么多调度算法，因什么问题而因此引入新算法的这个过程，才更是我们重点学习的地方。\n到这里，你会开始明白进程与线程最大的区别在于上下文切换过程中，线程不用切换虚拟内存，因为同一个进程内的线程都是共享虚拟内存空间的，线程就单这一点不用切换，就相比进程上下文切换的性能开销减少了很多。由于虚拟内存与物理内存的映射关系需要查询页表，页表的查询是很慢的过程，因此会把常用的地址映射关系缓存在 TLB 里的，这样便可以提高页表的查询速度，如果发生了进程切换，那 TLB 缓存的地址映射关系就会失效，缓存失效就意味着命中率降低，于是虚拟地址转为物理地址这一过程就会很慢。\n你也开始不会傻傻的认为 read 或 write 之后数据就直接写到硬盘了，更不会觉得多次操作 read 或 write 方法性能会很低，因为你发现操作系统会有个「磁盘高速缓冲区」 ，它已经帮我们做了缓存的工作，它会预读数据、缓存最近访问的数据，以及使用 I/O 调度算法来合并和排队磁盘调度 I/O，这些都是为了减少操作系统对磁盘的访问频率。\n……\n还有一点需要注意，学操作系统的时候，不要误以为它是在说 Linux 操作系统，这也是初学的时候犯的一个错误，操作系统是集合大多数操作系统实现的思想，跟实际具体实现的 Linux 操作系统多少都会有点差别，如果要想 Linux 操作系统的具体实现方式，可以选择看 Linux 内核相关的资料，但是在这之前应先掌握了操作系统的基本知识，这样学起来才能事半功倍。\n入门系列 # 对于没学过操作系统的小白，建议学的时候，不要直接闷头看书。毕竟直接看书太特喵的枯燥了。\nB 站关于操作系统课程资源很多，比较好的入门级课程有 《操作系统 - 清华大学 》 ，该课程由清华大学老师向勇和陈渝授课，课程授课的顺序，就如前面推荐的学习顺序：「内存管理 -\u0026gt; 进程管理 -\u0026gt; 文件系统管理 -\u0026gt; 输入输出设备管理」。\n该清华大学的视频教学搭配的书应该是 《现代操作系统》，可以视频和书籍两者结合一起学，比如看完视频的内存管理，然后就看书上对应的章节，这样相比直接啃书相对会比较好。\n清华大学的操作系统视频课讲的比较精炼，涉及到的内容没有那么细。 《操作系统 - 哈工大》 李治军老师授课的视频课程相对就会比较细节，老师会用 Linux 内核代码的角度带你进一步理解操作系统，也会用生活小例子帮助你理解。\n深入系列 # 《现代操作系统》这本书感觉缺少比较多细节，说的还是比较笼统，而且书也比较无聊。\n这里推荐一个说的更细的操作系统书 —— 《操作系统导论》，这本书不仅告诉你 What，还会告诉你 How，书的内容都是循序渐进，层层递进的，阅读起来还是觉得挺有意思的，这本书的内存管理和并发这两个部分说的很棒。\n去年国内也出了一本不错的操作系统书 《现代操作系统-原理与实现》，这本书不怎么厚，把操作系统重要的知识都讲了一遍，而且书中配图比较多，学起来不会太费解。\n当然，少不了这本被称为神书的 《深入理解计算机系统》，豆瓣评分高达 9.8 分，这本书严格来说不算操作系统书，它是以程序员视角理解计算机系统，不只是涉及到操作系统，还涉及到了计算机组成、C 语言、汇编语言等知识，是一本综合性比较强的书。\n四、计算机网络 # 计算机网络相比操作系统好学非常多，因为计算机网络不抽象，如果想要知道网络中的细节，可以通过抓包来分析，而且不管是手机、个人电脑和服务器，它们所使用的计算网络协议是一致的。也就是说，计算机网络不会因为设备的不同而不同，大家都遵循这一套「规则」来相互通信，这套规则就是 TCP/IP 网络模型。\nTCP/IP 网络参考模型共有 4 层，其中需要熟练掌握的是应用层、传输层和网络层，至于网络接口层（数据链路层和物理层）只需要做简单的了解就可以了。\n对于应用层，当然重点要熟悉最常见的 HTTP 和 HTTPS，传输层 TCP 和 UDP 都要熟悉，网络层要熟悉 IPv4，IPv6 可以做简单点了解。\n学习一个东西，就要从我们常见的事情开始着手。\n比如， ping 命令可以说在我们判断网络环境的时候，最常使用的了，你可以先把你电脑 ping 你舍友或同事的电脑的过程中发生的事情都搞明白，这样就基本知道一个数据包是怎么转发的了，于是你就知道了网络层、数据链路层和物理层之间是如何工作，如何相互配合的了。\n搞明白了 ping 过程，就明白了两个计算机是怎么通信的了，然后你学起 HTTP 请求过程的时候，会很快就能掌握了，因为网络层以下的工作方式，在学习 ping 的时候就已经明白了，这时就只需要认真掌握传输层中的 TCP 和应用层中的 HTTP 协议，就能搞明白 访问网页的整个过程了，这也是面试常见的题目了，毕竟它能考察网络知识的全面性。\n重中之重的知识就是 TCP 了，TCP 不管是 建立连接、断开连接的过程，还是数据传输的过程，都不能放过，针对数据可靠传输的特性，又可以拆解为 超时重新、流量控制、滑动窗口、拥塞控制等等知识点，学完这些只能算对 TCP 有个感性的认识，另外还得知道 Linux 提供的 TCP 内核的参数的作用，这样才能从容地应对工作中遇到的问题。\n入门系列 # 如果要入门 HTTP，首先最好书籍就是 《图解 HTTP》 ，作者真的做到完完全全的「图解」，书籍不厚，几天就可以看完。\n如果要入门 TCP/IP 网络模型，可以看 《图解 TCP/IP》，这本书也是以大量的图文来介绍了 TCP/IP 网络模式的每一层，但是这个书籍的顺序不是从「应用层 —\u0026gt; 物理层」，而是从「物理层 -\u0026gt; 应用层」顺序开始讲的，这样一上来就把最枯燥的部分讲了，很容易就被劝退了，所以建议先跳过前面几个章节，先看网络层和传输层的章节，然后再回头看前面的这几个章节。\n另外，想了解网络是怎么传输，可以看 《网络是怎样连接的》，这本书相对比较全面的把访问一个网页的发生的过程讲解了一遍，其中关于电信等运营商是怎么传输的，这部分可以跳过，当然感兴趣也可以看，只是觉得没什么必要。\n如果觉得书籍过于枯燥，可以结合 B 站 《计算机网络微课堂》视频一起学习，这个视频是湖南科技大学老师制作的，属于一看就懂的佳作。\n深入学习 # 看完入门系列，相信对计算机网络已经有个大体的认识了。\n对于 TCP/IP 网络模型深入学习的话，推荐 《计算机网络 - 自顶向下方法》，这本书是从我们最熟悉 HTTP 开始说起，一层一层的说到最后物理层的，有种挖地洞的感觉，这样的内容编排顺序相对是比较合理的。\n但如果要深入 TCP，前面的这些书还远远不够，赋有计算机网络圣经的之说的 《TCP/IP 详解 卷一：协议》 这本书，是进一步深入学习的好资料，这本书的作者用各种实验的方式来细说各种协议，虽然这本书真的很枯燥，但是它质量是真的很高，这本书可以多看几遍 TCP 部分，因为涵盖的内容非常全且细，其他部分可以选择性看。\n如果说最好的 TCP 资料，那必定是 《The TCP/IP GUIDE》 这本书了，目前只有英文版本的。有个专门的网址可以白嫖看这本书的内容，图片都是彩色，看起来很舒服很鲜明，这本书精华部分就是把 TCP 滑动窗口和流量控制说的超级明白，很可惜拥塞控制部分说的不多。\n白嫖站点： http://www.tcpipguide.com/free/t_TCPSlidingWindowAcknowledgmentSystemForDataTranspo-6.htm\n当然，计算机网络最牛逼的资料，那必定 RFC 文档，它可以称为计算机网络世界的「法规」，也是最新、最权威和最正确的地方了，困惑大家的 TCP 为什么三次握手和四次挥手，其实在 RFC 文档几句话就说明白了。\nTCP 协议的 RFC 文档： https://datatracker.ietf.org/doc/rfc1644/\n实战系列 # 在学习书籍资料的时候，不管是 TCP、UDP、ICMP、DNS、HTTP、HTTPS 等协议，最好亲手尝试抓数据报文，可以用 Wireshark 工具看每一个数据报文的信息，这样你会觉得计算机网络没有想象中那么抽象了，因为它们被你「抓」出来了，并毫无保留地显现在你面前了，于是你就可以肆无忌惮地「扒开」它们，看清它们每一个头信息。\n这里推荐 2 本关于 Wireshark 网络分析的书：《Wireshark 网络分析就这么简单》 与 《Wireshark 网络分析的艺术》。\n这两本书都是同一个作者，书中的案例都是源于作者工作中的实际的案例，作者的文笔相当有趣，看起来堪比小说一样爽，基本不用一个星期 2 本都能看完。\n五、MySQL # MySQL 入门的话是了解 SQL 语法，进阶的话是深入底层实现原理。千万不要一上来就看《高性能 MySQL》，因为这本不是入门的书籍！\n这里先介绍下 MySQL 的重点知识，也是面试常面的知识点：\n基本语法：select/delete/insert/update、limit、join等 索引：B+树，聚族索引，二级索引，组合索引，最左匹配原则，索引失效、慢查询 事务：事务四大特性ACID，事务隔离级别，MVCC 锁：全局锁、表级锁、行级锁、快照读、当前读、乐观锁、悲观锁、死锁 日志：重做日志(redo log)、回滚日志(undo log)、二进制日志(binlog) 架构：读写分离、主从架构、分库分表、数据库和缓存双写一致性 MySQL 入门推荐《SQL必知必会》，这一本很薄的书，主要是讲数据库增删查改的 SQL 语法。\n学完 SQL 语法，要深入去了解 MySQL 底层知识。\n这里建议先看 《MySQL 是怎么运行的》，这本书含有很多图，是小白学习 MySQL 底层知识的最佳书籍。\nMySQL 用的最多的就是 InnoDB 引擎了，所以进一步学习 InnoDB 是很有必要的，这里推荐 《MySQL技术内幕》，这本书可以结合《MySQL是怎么样运行的》一起看。\n看完上面的基本对 MySQL 已经有相当多的认识了，MySQL 还有一本高性能的书 《高性能 MySQL》，非常的经典，这本书比较厚，可以当作字典，索引章节可以重点看看，看完后对索引的认识又会刷新一遍。\n六、Redis # 要入门 Redis，就要先知道这东西怎么用，说白了，最开始就是先学习操作 Redis 的相关命令，就像入门 MySQL 的时候，都是先学习 SQL 语言。\n入门 Redis 推荐 尚硅谷 Redis 视频课，在 B 站就可以看，讲的还是挺清晰的，也把 Redis 很多重点知识也讲了，比如 Redis 基本数据结构、持久化技术、主从复制、哨兵、集群等等，一套连招下来，就基本入门了。\nRedis 官网也有一整套的命令详解，遇到需要或者不会的地方可以查一下： http://doc.redisfans.com\n视频是帮助我们快速入门，但是并不能至于视频，因为一些细节的知识点视频上并没有提及，这时候就要回归书本。\n这里推荐学习 Redis 的圣经级别的书 —— 《Redis设计与实现》，因为它太经典了！\n这本书不是教你如何使用 Redis，而是跟你讲解 Redis 是怎么实现，怎么设计的，也就是源码级别的学习，但是书上并没有大段贴代码，作者是用伪代码的方式来讲解，所以读起来不会太难的。\n书本上主要围绕这几大知识点：数据结构、AOF和RDB持久化技术、网络输入输出系统、主从复制、哨兵模式、集群模式。\n到这里其实已经是入门 Redis 了，不仅会了 Redis 基本命令，还懂 Redis 的实现，剩下的就是学习如何在实战中运用 Redis。\n这里推荐 《Redis实战》 这本书，该书通过实际的例子， 展示了使用 Redis 构建多种不同的应用程序的方法。\n处于进阶阶段的 Redis 学习可以通过阅读该书来学习如何使用 Redis 去构建实际的应用，然后举一反三， 把书中介绍的程序和方法应用到自己遇到的问题上。\n七、看书心得 # 没有人规定看书一定要一页一页的全部看完，我们要知道看书的目的是什么？\n无非不就是收获知识和解决问题嘛。所以最好的看书方式是带着问题去翻阅，比如：\n带着程序是如何在计算机里跑起来的问题，去学计算机组成原理； 带着输入一条 url 到网页显示， 期间发生了什么的问题，去学习计算机网络； 带着进程、内存、磁盘是如何被操作系统管理点，去学习操作系统； 带着如何实现一个高并发网络模型，去学习网络编程； … 在原文基础上做了修改，原文地址 https://xiaolincoding.com/cs_learn/cs_learn.html\n"},{"id":1,"href":"/keeplearning/docs/system-design/overview/","title":"系统设计基础","section":"系统设计","content":" 系统设计基础 # 一、性能 # 性能指标 # 1. 响应时间 # 指某个请求从发出到接收到响应消耗的时间。\n在对响应时间进行测试时，通常采用重复请求的方式，然后计算平均响应时间。\n2. 吞吐量 # 指系统在单位时间内可以处理的请求数量，通常使用每秒的请求数来衡量。\n3. 并发用户数 # 指系统能同时处理的并发用户请求数量。\n在没有并发存在的系统中，请求被顺序执行，此时响应时间为吞吐量的倒数。例如系统支持的吞吐量为 100 req/s，那么平均响应时间应该为 0.01s。\n目前的大型系统都支持多线程来处理并发请求，多线程能够提高吞吐量以及缩短响应时间，主要有两个原因：\n多 CPU IO 等待时间 使用 IO 多路复用等方式，系统在等待一个 IO 操作完成的这段时间内不需要被阻塞，可以去处理其它请求。通过将这个等待时间利用起来，使得 CPU 利用率大大提高。\n并发用户数不是越高越好，因为如果并发用户数太高，系统来不及处理这么多的请求，会使得过多的请求需要等待，那么响应时间就会大大提高。\n性能优化 # 1. 集群 # 将多台服务器组成集群，使用负载均衡将请求转发到集群中，避免单一服务器的负载压力过大导致性能降低。\n2. 缓存 # 缓存能够提高性能的原因如下：\n缓存数据通常位于内存等介质中，这种介质对于读操作特别快； 缓存数据可以位于靠近用户的地理位置上； 可以将计算结果进行缓存，从而避免重复计算。 3. 异步 # 某些流程可以将操作转换为消息，将消息发送到消息队列之后立即返回，之后这个操作会被异步处理。\n二、伸缩性 # 指不断向集群中添加服务器来缓解不断上升的用户并发访问压力和不断增长的数据存储需求。\n伸缩性与性能 # 如果系统存在性能问题，那么单个用户的请求总是很慢的；\n如果系统存在伸缩性问题，那么单个用户的请求可能会很快，但是在并发数很高的情况下系统会很慢。\n实现伸缩性 # 应用服务器只要不具有状态，那么就可以很容易地通过负载均衡器向集群中添加新的服务器。\n关系型数据库的伸缩性通过 Sharding 来实现，将数据按一定的规则分布到不同的节点上，从而解决单台存储服务器的存储空间限制。\n对于非关系型数据库，它们天生就是为海量数据而诞生，对伸缩性的支持特别好。\n三、扩展性 # 指的是添加新功能时对现有系统的其它应用无影响，这就要求不同应用具备低耦合的特点。\n实现可扩展主要有两种方式：\n使用消息队列进行解耦，应用之间通过消息传递进行通信； 使用分布式服务将业务和可复用的服务分离开来，业务使用分布式服务框架调用可复用的服务。新增的产品可以通过调用可复用的服务来实现业务逻辑，对其它产品没有影响。 四、可用性 # 冗余 # 保证高可用的主要手段是使用冗余，当某个服务器故障时就请求其它服务器。\n应用服务器的冗余比较容易实现，只要保证应用服务器不具有状态，那么某个应用服务器故障时，负载均衡器将该应用服务器原先的用户请求转发到另一个应用服务器上，不会对用户有任何影响。\n存储服务器的冗余需要使用主从复制来实现，当主服务器故障时，需要提升从服务器为主服务器，这个过程称为切换。\n监控 # 对 CPU、内存、磁盘、网络等系统负载信息进行监控，当某个信息达到一定阈值时通知运维人员，从而在系统发生故障之前及时发现问题。\n服务降级 # 服务降级是系统为了应对大量的请求，主动关闭部分功能，从而保证核心功能可用。\n五、安全性 # 要求系统在应对各种攻击手段时能够有可靠的应对措施。\n参考资料 # 大型网站技术架构：核心原理与案例分析 "},{"id":2,"href":"/keeplearning/docs/coding-practice/regex/","title":"正则表达式","section":"编码实践","content":" 正则表达式 # 一、概述 # 正则表达式用于文本内容的查找和替换。\n正则表达式内置于其它语言或者软件产品中，它本身不是一种语言或者软件。\n正则表达式在线工具\n二、匹配单个字符 # . 可以用来匹配任何的单个字符，但是在绝大多数实现里面，不能匹配换行符；\n. 是元字符，表示它有特殊的含义，而不是字符本身的含义。如果需要匹配 . ，那么要用 \\ 进行转义，即在 . 前面加上 \\ 。\n正则表达式一般是区分大小写的，但也有些实现不区分。\n正则表达式\nC.C2018 匹配结果\nMy name is CyC2018 .\n三、匹配一组字符 # [ ] 定义一个字符集合；\n0-9、a-z 定义了一个字符区间，区间使用 ASCII 码来确定，字符区间在 [ ] 中使用。\n- 只有在 [ ] 之间才是元字符，在 [ ] 之外就是一个普通字符；\n^ 在 [ ] 中是取非操作。\n应用\n匹配以 abc 为开头，并且最后一个字母不为数字的字符串：\n正则表达式\nabc[^0-9] 匹配结果\nabcd abc1 abc2 四、使用元字符 # 匹配空白字符 # 元字符 说明 [\\b] 回退（删除）一个字符 \\f 换页符 \\n 换行符 \\r 回车符 \\t 制表符 \\v 垂直制表符 \\r\\n 是 Windows 中的文本行结束标签，在 Unix/Linux 则是 \\n。\n\\r\\n\\r\\n 可以匹配 Windows 下的空白行，因为它匹配两个连续的行尾标签，而这正是两条记录之间的空白行；\n匹配特定的字符 # 1. 数字元字符 # 元字符 说明 \\d 数字字符，等价于 [0-9] \\D 非数字字符，等价于 [^0-9] 2. 字母数字元字符 # 元字符 说明 \\w 大小写字母，下划线和数字，等价于 [a-zA-Z0-9_] \\W 对 \\w 取非 3. 空白字符元字符 # 元字符 说明 \\s 任何一个空白字符，等价于 [\\f\\n\\r\\t\\v] \\S 对 \\s 取非 \\x 匹配十六进制字符，\\0 匹配八进制，例如 \\xA 对应值为 10 的 ASCII 字符 ，即 \\n。\n五、重复匹配 # + 匹配 1 个或者多个字符 ** * 匹配 0 个或者多个字符 ? 匹配 0 个或者 1 个字符 应用\n匹配邮箱地址。\n正则表达式\n[\\w.]+@\\w+\\.\\w+ [\\w.] 匹配的是字母数字或者 . ，在其后面加上 + ，表示匹配多次。在字符集合 [ ] 里，. 不是元字符；\n匹配结果\nabc.def\u0026lt;span\u0026gt;@\u0026lt;/span\u0026gt;qq.com\n{n} 匹配 n 个字符 {m,n} 匹配 m~n 个字符 {m,} 至少匹配 m 个字符 * 和 + 都是贪婪型元字符，会匹配尽可能多的内容。在后面加 ? 可以转换为懒惰型元字符，例如 *?、+? 和 {m,n}? 。\n正则表达式\na.+c 匹配结果\nabcabcabc\n由于 + 是贪婪型的，因此 .+ 会匹配更可能多的内容，所以会把整个 abcabcabc 文本都匹配，而不是只匹配前面的 abc 文本。用懒惰型可以实现匹配前面的。\n六、位置匹配 # 单词边界 # \\b 可以匹配一个单词的边界，边界是指位于 \\w 和 \\W 之间的位置；\\B 匹配一个不是单词边界的位置。\n\\b 只匹配位置，不匹配字符，因此 \\babc\\b 匹配出来的结果为 3 个字符。\n字符串边界 # ^ 匹配整个字符串的开头，$ 匹配结尾。\n^ 元字符在字符集合中用作求非，在字符集合外用作匹配字符串的开头。\n分行匹配模式（multiline）下，换行被当做字符串的边界。\n应用\n匹配代码中以 // 开始的注释行\n正则表达式\n^\\s*\\/\\/.*$ 匹配结果\npublic void fun() { // 注释 1 int a = 1; int b = 2; // 注释 2 int c = a + b; } 七、使用子表达式 # 使用 ( ) 定义一个子表达式。子表达式的内容可以当成一个独立元素，即可以将它看成一个字符，并且使用 * 等元字符。\n子表达式可以嵌套，但是嵌套层次过深会变得很难理解。\n正则表达式\n(ab){2,} 匹配结果\nababab\n| 是或元字符，它把左边和右边所有的部分都看成单独的两个部分，两个部分只要有一个匹配就行。\n正则表达式\n(19|20)\\d{2} 匹配结果\n1900 2010 1020 应用\n匹配 IP 地址。\nIP 地址中每部分都是 0-255 的数字，用正则表达式匹配时以下情况是合法的：\n一位数字 不以 0 开头的两位数字 1 开头的三位数 2 开头，第 2 位是 0-4 的三位数 25 开头，第 3 位是 0-5 的三位数 正则表达式\n((25[0-5]|(2[0-4]\\d)|(1\\d{2})|([1-9]\\d)|(\\d))\\.){3}(25[0-5]|(2[0-4]\\d)|(1\\d{2})|([1-9]\\d)|(\\d)) 匹配结果\n192.168.0.1 00.00.00.00 555.555.555.555 八、回溯引用 # 回溯引用使用 **\\n ** 来引用某个子表达式，其中 n 代表的是子表达式的序号，从 1 开始。它和子表达式匹配的内容一致，比如子表达式匹配到 abc，那么回溯引用部分也需要匹配 abc 。\n应用\n匹配 HTML 中合法的标题元素。\n正则表达式\n\\1 将回溯引用子表达式 (h[1-6]) 匹配的内容，也就是说必须和子表达式匹配的内容一致。\n\u0026lt;(h[1-6])\u0026gt;\\w*?\u0026lt;\\/\\1\u0026gt; 匹配结果\n\u0026lt;h1\u0026gt;x\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;x\u0026lt;/h2\u0026gt; \u0026lt;h3\u0026gt;x\u0026lt;/h1\u0026gt; 替换 # 需要用到两个正则表达式。\n应用\n修改电话号码格式。\n文本\n313-555-1234\n查找正则表达式\n(\\d{3})(-)(\\d{3})(-)(\\d{4}) 替换正则表达式\n在第一个子表达式查找的结果加上 () ，然后加一个空格，在第三个和第五个字表达式查找的结果中间加上 - 进行分隔。\n($1) $3-$5 结果\n(313) 555-1234\n大小写转换 # 元字符 说明 \\l 把下个字符转换为小写 \\u 把下个字符转换为大写 \\L 把\\L 和\\E 之间的字符全部转换为小写 \\U 把\\U 和\\E 之间的字符全部转换为大写 \\E 结束\\L 或者\\U 应用\n把文本的第二个和第三个字符转换为大写。\n文本\nabcd\n查找\n(\\w)(\\w{2})(\\w) 替换\n$1\\U$2\\E$3 结果\naBCd\n九、前后查找 # 前后查找规定了匹配的内容首尾应该匹配的内容，但是又不包含首尾匹配的内容。\n向前查找使用 **?= ** 定义，它规定了尾部匹配的内容，这个匹配的内容在 ?= 之后定义。所谓向前查找，就是规定了一个匹配的内容，然后以这个内容为尾部向前面查找需要匹配的内容。向后匹配用 ?\u0026lt;= 定义（注: JavaScript 不支持向后匹配，Java 对其支持也不完善）。\n应用\n查找出邮件地址 @ 字符前面的部分。\n正则表达式\n\\w+(?=@) 结果\nabc @qq.com\n对向前和向后查找取非，只要把 = 替换成 ! 即可，比如 (?=) 替换成 (?!) 。取非操作使得匹配那些首尾不符合要求的内容。\n十、嵌入条件 # 回溯引用条件 # 条件为某个子表达式是否匹配，如果匹配则需要继续匹配条件表达式后面的内容。\n正则表达式\n子表达式 (\\() 匹配一个左括号，其后的 ? 表示匹配 0 个或者 1 个。 ?(1) 为条件，当子表达式 1 匹配时条件成立，需要执行 ) 匹配，也就是匹配右括号。\n(\\()?abc(?(1)\\)) 结果\n(abc) abc (abc 前后查找条件 # 条件为定义的首尾是否匹配，如果匹配，则继续执行后面的匹配。注意，首尾不包含在匹配的内容中。\n正则表达式\n?(?=-) 为前向查找条件，只有在以 - 为前向查找的结尾能匹配 \\d{5} ，才继续匹配 -\\d{4} 。\n\\d{5}(?(?=-)-\\d{4}) 结果\n11111 22222- 33333-4444 "},{"id":3,"href":"/keeplearning/docs/mysql/overview/","title":"总览","section":"MySQL","content":" 总览 # 一、索引 # B+ Tree 原理 # 1. 数据结构 # B Tree 指的是 Balance Tree，也就是平衡树。平衡树是一颗查找树，并且所有叶子节点位于同一层。\nB+ Tree 是基于 B Tree 和叶子节点顺序访问指针进行实现，它具有 B Tree 的平衡性，并且通过顺序访问指针来提高区间查询的性能。\n在 B+ Tree 中，一个节点中的 key 从左到右非递减排列，如果某个指针的左右相邻 key 分别是 keyi 和 keyi+1，且不为 null，则该指针指向节点的所有 key 大于等于 keyi 且小于等于 keyi+1。\n2. 操作 # 进行查找操作时，首先在根节点进行二分查找，找到一个 key 所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找出 key 所对应的 data。\n插入删除操作会破坏平衡树的平衡性，因此在进行插入删除操作之后，需要对树进行分裂、合并、旋转等操作来维护平衡性。\n3. 与红黑树的比较 # 红黑树等平衡树也可以用来实现索引，但是文件系统及数据库系统普遍采用 B+ Tree 作为索引结构，这是因为使用 B+ 树访问磁盘数据有更高的性能。\n（一）B+ 树有更低的树高\n平衡树的树高 O(h)=O(logdN)，其中 d 为每个节点的出度。红黑树的出度为 2，而 B+ Tree 的出度一般都非常大，所以红黑树的树高 h 很明显比 B+ Tree 大非常多。\n（二）磁盘访问原理\n操作系统一般将内存和磁盘分割成固定大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I/O 就能完全载入一个节点。\n如果数据不在同一个磁盘块上，那么通常需要移动制动手臂进行寻道，而制动手臂因为其物理结构导致了移动效率低下，从而增加磁盘数据读取时间。B+ 树相对于红黑树有更低的树高，进行寻道的次数与树高成正比，在同一个磁盘块上进行访问只需要很短的磁盘旋转时间，所以 B+ 树更适合磁盘数据的读取。\n（三）磁盘预读特性\n为了减少磁盘 I/O 操作，磁盘往往不是严格按需读取，而是每次都会预读。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的磁盘旋转时间，速度会非常快。并且可以利用预读特性，相邻的节点也能够被预先载入。\nMySQL 索引 # 索引是在存储引擎层实现的，而不是在服务器层实现的，所以不同存储引擎具有不同的索引类型和实现。\n1. B+Tree 索引 # 是大多数 MySQL 存储引擎的默认索引类型。\n因为不再需要进行全表扫描，只需要对树进行搜索即可，所以查找速度快很多。\n因为 B+ Tree 的有序性，所以除了用于查找，还可以用于排序和分组。\n可以指定多个列作为索引列，多个索引列共同组成键。\n适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于最左前缀查找。如果不是按照索引列的顺序进行查找，则无法使用索引。\nInnoDB 的 B+Tree 索引分为主索引和辅助索引。主索引的叶子节点 data 域记录着完整的数据记录，这种索引方式被称为聚簇索引。因为无法把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。\n辅助索引的叶子节点的 data 域记录着主键的值，因此在使用辅助索引进行查找时，需要先查找到主键值，然后再到主索引中进行查找。\n2. 哈希索引 # 哈希索引能以 O(1) 时间进行查找，但是失去了有序性：\n无法用于排序与分组； 只支持精确查找，无法用于部分查找和范围查找。 InnoDB 存储引擎有一个特殊的功能叫“自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。\n3. 全文索引 # MyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等。\n查找条件使用 MATCH AGAINST，而不是普通的 WHERE。\n全文索引使用倒排索引实现，它记录着关键词到其所在文档的映射。\nInnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引。\n4. 空间数据索引 # MyISAM 存储引擎支持空间数据索引（R-Tree），可以用于地理数据存储。空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。\n必须使用 GIS 相关的函数来维护数据。\n索引优化 # 1. 独立的列 # 在进行查询时，索引列不能是表达式的一部分，也不能是函数的参数，否则无法使用索引。\n例如下面的查询不能使用 actor_id 列的索引：\nSELECT actor_id FROM sakila.actor WHERE actor_id + 1 = 5; 2. 多列索引 # 在需要使用多个列作为条件进行查询时，使用多列索引比使用多个单列索引性能更好。例如下面的语句中，最好把 actor_id 和 film_id 设置为多列索引。\nSELECT film_id, actor_ id FROM sakila.film_actor WHERE actor_id = 1 AND film_id = 1; 3. 索引列的顺序 # 让选择性最强的索引列放在前面。\n索引的选择性是指：不重复的索引值和记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，每个记录的区分度越高，查询效率也越高。\n例如下面显示的结果中 customer_id 的选择性比 staff_id 更高，因此最好把 customer_id 列放在多列索引的前面。\nSELECT COUNT(DISTINCT staff_id)/COUNT(*) AS staff_id_selectivity, COUNT(DISTINCT customer_id)/COUNT(*) AS customer_id_selectivity, COUNT(*) FROM payment; staff_id_selectivity: 0.0001 customer_id_selectivity: 0.0373 COUNT(*): 16049 4. 前缀索引 # 对于 BLOB、TEXT 和 VARCHAR 类型的列，必须使用前缀索引，只索引开始的部分字符。\n前缀长度的选取需要根据索引选择性来确定。\n5. 覆盖索引 # 索引包含所有需要查询的字段的值。\n具有以下优点：\n索引通常远小于数据行的大小，只读取索引能大大减少数据访问量。 一些存储引擎（例如 MyISAM）在内存中只缓存索引，而数据依赖于操作系统来缓存。因此，只访问索引可以不使用系统调用（通常比较费时）。 对于 InnoDB 引擎，若辅助索引能够覆盖查询，则无需访问主索引。 索引的优点 # 大大减少了服务器需要扫描的数据行数。\n帮助服务器避免进行排序和分组，以及避免创建临时表（B+Tree 索引是有序的，可以用于 ORDER BY 和 GROUP BY 操作。临时表主要是在排序和分组过程中创建，不需要排序和分组，也就不需要创建临时表）。\n将随机 I/O 变为顺序 I/O（B+Tree 索引是有序的，会将相邻的数据都存储在一起）。\n索引的使用条件 # 对于非常小的表、大部分情况下简单的全表扫描比建立索引更高效；\n对于中到大型的表，索引就非常有效；\n但是对于特大型的表，建立和维护索引的代价将会随之增长。这种情况下，需要用到一种技术可以直接区分出需要查询的一组数据，而不是一条记录一条记录地匹配，例如可以使用分区技术。\n二、查询性能优化 # 使用 Explain 进行分析 # Explain 用来分析 SELECT 查询语句，开发人员可以通过分析 Explain 结果来优化查询语句。\n比较重要的字段有：\nselect_type : 查询类型，有简单查询、联合查询、子查询等 key : 使用的索引 rows : 扫描的行数 优化数据访问 # 1. 减少请求的数据量 # 只返回必要的列：最好不要使用 SELECT * 语句。 只返回必要的行：使用 LIMIT 语句来限制返回的数据。 缓存重复查询的数据：使用缓存可以避免在数据库中进行查询，特别在要查询的数据经常被重复查询时，缓存带来的查询性能提升将会是非常明显的。 2. 减少服务器端扫描的行数 # 最有效的方式是使用索引来覆盖查询。\n重构查询方式 # 1. 切分大查询 # 一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。\nDELETE FROM messages WHERE create \u0026lt; DATE_SUB(NOW(), INTERVAL 3 MONTH); rows_affected = 0 do { rows_affected = do_query( \u0026#34;DELETE FROM messages WHERE create \u0026lt; DATE_SUB(NOW(), INTERVAL 3 MONTH) LIMIT 10000\u0026#34;) } while rows_affected \u0026gt; 0 2. 分解大连接查询 # 将一个大连接查询分解成对每一个表进行一次单表查询，然后在应用程序中进行关联，这样做的好处有：\n让缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。 分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。 减少锁竞争； 在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可伸缩。 查询本身效率也可能会有所提升。例如下面的例子中，使用 IN() 代替连接查询，可以让 MySQL 按照 ID 顺序进行查询，这可能比随机的连接要更高效。 SELECT * FROM tag JOIN tag_post ON tag_post.tag_id=tag.id JOIN post ON tag_post.post_id=post.id WHERE tag.tag=\u0026#39;mysql\u0026#39;; SELECT * FROM tag WHERE tag=\u0026#39;mysql\u0026#39;; SELECT * FROM tag_post WHERE tag_id=1234; SELECT * FROM post WHERE post.id IN (123,456,567,9098,8904); 三、存储引擎 # InnoDB # 是 MySQL 默认的事务型存储引擎，只有在需要它不支持的特性时，才考虑使用其它存储引擎。\n实现了四个标准的隔离级别，默认级别是可重复读（REPEATABLE READ）。在可重复读隔离级别下，通过多版本并发控制（MVCC）+ Next-Key Locking 防止幻影读。\n主索引是聚簇索引，在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升。\n内部做了很多优化，包括从磁盘读取数据时采用的可预测性读、能够加快读操作并且自动创建的自适应哈希索引、能够加速插入操作的插入缓冲区等。\n支持真正的在线热备份。其它存储引擎不支持在线热备份，要获取一致性视图需要停止对所有表的写入，而在读写混合场景中，停止写入可能也意味着停止读取。\nMyISAM # 设计简单，数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，则依然可以使用它。\n提供了大量的特性，包括压缩表、空间数据索引等。\n不支持事务。\n不支持行级锁，只能对整张表加锁，读取时会对需要读到的所有表加共享锁，写入时则对表加排它锁。但在表有读取操作的同时，也可以往表中插入新的记录，这被称为并发插入（CONCURRENT INSERT）。\n可以手工或者自动执行检查和修复操作，但是和事务恢复以及崩溃恢复不同，可能导致一些数据丢失，而且修复操作是非常慢的。\n如果指定了 DELAY_KEY_WRITE 选项，在每次修改执行完成时，不会立即将修改的索引数据写入磁盘，而是会写到内存中的键缓冲区，只有在清理键缓冲区或者关闭表的时候才会将对应的索引块写入磁盘。这种方式可以极大的提升写入性能，但是在数据库或者主机崩溃时会造成索引损坏，需要执行修复操作。\n比较 # 事务：InnoDB 是事务型的，可以使用 Commit 和 Rollback 语句。\n并发：MyISAM 只支持表级锁，而 InnoDB 还支持行级锁。\n外键：InnoDB 支持外键。\n备份：InnoDB 支持在线热备份。\n崩溃恢复：MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢。\n其它特性：MyISAM 支持压缩表和空间数据索引。\n四、数据类型 # 整型 # TINYINT, SMALLINT, MEDIUMINT, INT, BIGINT 分别使用 8, 16, 24, 32, 64 位存储空间，一般情况下越小的列越好。\nINT(11) 中的数字只是规定了交互工具显示字符的个数，对于存储和计算来说是没有意义的。\n浮点数 # FLOAT 和 DOUBLE 为浮点类型，DECIMAL 为高精度小数类型。CPU 原生支持浮点运算，但是不支持 DECIMAl 类型的计算，因此 DECIMAL 的计算比浮点类型需要更高的代价。\nFLOAT、DOUBLE 和 DECIMAL 都可以指定列宽，例如 DECIMAL(18, 9) 表示总共 18 位，取 9 位存储小数部分，剩下 9 位存储整数部分。\n字符串 # 主要有 CHAR 和 VARCHAR 两种类型，一种是定长的，一种是变长的。\nVARCHAR 这种变长类型能够节省空间，因为只需要存储必要的内容。但是在执行 UPDATE 时可能会使行变得比原来长，当超出一个页所能容纳的大小时，就要执行额外的操作。MyISAM 会将行拆成不同的片段存储，而 InnoDB 则需要分裂页来使行放进页内。\n在进行存储和检索时，会保留 VARCHAR 末尾的空格，而会删除 CHAR 末尾的空格。\n时间和日期 # MySQL 提供了两种相似的日期时间类型：DATETIME 和 TIMESTAMP。\n1. DATETIME # 能够保存从 1000 年到 9999 年的日期和时间，精度为秒，使用 8 字节的存储空间。\n它与时区无关。\n默认情况下，MySQL 以一种可排序的、无歧义的格式显示 DATETIME 值，例如“2008-01-16 22:37:08”，这是 ANSI 标准定义的日期和时间表示方法。\n2. TIMESTAMP # 和 UNIX 时间戳相同，保存从 1970 年 1 月 1 日午夜（格林威治时间）以来的秒数，使用 4 个字节，只能表示从 1970 年到 2038 年。\n它和时区有关，也就是说一个时间戳在不同的时区所代表的具体时间是不同的。\nMySQL 提供了 FROM_UNIXTIME() 函数把 UNIX 时间戳转换为日期，并提供了 UNIX_TIMESTAMP() 函数把日期转换为 UNIX 时间戳。\n默认情况下，如果插入时没有指定 TIMESTAMP 列的值，会将这个值设置为当前时间。\n应该尽量使用 TIMESTAMP，因为它比 DATETIME 空间效率更高。\n五、切分 # 水平切分 # 水平切分又称为 Sharding，它是将同一个表中的记录拆分到多个结构相同的表中。\n当一个表的数据不断增多时，Sharding 是必然的选择，它可以将数据分布到集群的不同节点上，从而缓存单个数据库的压力。\n垂直切分 # 垂直切分是将一张表按列切分成多个表，通常是按照列的关系密集程度进行切分，也可以利用垂直切分将经常被使用的列和不经常被使用的列切分到不同的表中。\n在数据库的层面使用垂直切分将按数据库中表的密集程度部署到不同的库中，例如将原来的电商数据库垂直切分成商品数据库、用户数据库等。\nSharding 策略 # 哈希取模：hash(key) % N； 范围：可以是 ID 范围也可以是时间范围； 映射表：使用单独的一个数据库来存储映射关系。 Sharding 存在的问题 # 1. 事务问题 # 使用分布式事务来解决，比如 XA 接口。\n2. 连接 # 可以将原来的连接分解成多个单表查询，然后在用户程序中进行连接。\n3. ID 唯一性 # 使用全局唯一 ID（GUID） 为每个分片指定一个 ID 范围 分布式 ID 生成器 (如 Twitter 的 Snowflake 算法) 六、复制 # 主从复制 # 主要涉及三个线程：binlog 线程、I/O 线程和 SQL 线程。\nbinlog 线程 ：负责将主服务器上的数据更改写入二进制日志（Binary log）中。 I/O 线程 ：负责从主服务器上读取二进制日志，并写入从服务器的中继日志（Relay log）。 SQL 线程 ：负责读取中继日志，解析出主服务器已经执行的数据更改并在从服务器中重放（Replay）。 读写分离 # 主服务器处理写操作以及实时性要求比较高的读操作，而从服务器处理读操作。\n读写分离能提高性能的原因在于：\n主从服务器负责各自的读和写，极大程度缓解了锁的争用； 从服务器可以使用 MyISAM，提升查询性能以及节约系统开销； 增加冗余，提高可用性。 读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。\n"},{"id":4,"href":"/keeplearning/docs/redis/overview/","title":"总览","section":"Redis","content":" 总览 # 一、概述 # Redis 是速度非常快的非关系型（NoSQL）内存键值数据库，可以存储键和五种不同类型的值之间的映射。\n键的类型只能为字符串，值支持五种数据类型：字符串、列表、集合、散列表、有序集合。\nRedis 支持很多特性，例如将内存中的数据持久化到硬盘中，使用复制来扩展读性能，使用分片来扩展写性能。\n二、数据类型 # 数据类型 可以存储的值 操作 STRING 字符串、整数或者浮点数 对整个字符串或者字符串的其中一部分执行操作 对整数和浮点数执行自增或者自减操作 LIST 列表 从两端压入或者弹出元素 对单个或者多个元素进行修剪， 只保留一个范围内的元素 SET 无序集合 添加、获取、移除单个元素 检查一个元素是否存在于集合中 计算交集、并集、差集 从集合里面随机获取元素 HASH 包含键值对的无序散列表 添加、获取、移除单个键值对 获取所有键值对 检查某个键是否存在 ZSET 有序集合 添加、获取、删除元素 根据分值范围或者成员来获取元素 计算一个键的排名 What Redis data structures look like\nSTRING # \u0026gt; set hello world OK \u0026gt; get hello \u0026#34;world\u0026#34; \u0026gt; del hello (integer) 1 \u0026gt; get hello (nil) LIST # \u0026gt; rpush list-key item (integer) 1 \u0026gt; rpush list-key item2 (integer) 2 \u0026gt; rpush list-key item (integer) 3 \u0026gt; lrange list-key 0 -1 1) \u0026#34;item\u0026#34; 2) \u0026#34;item2\u0026#34; 3) \u0026#34;item\u0026#34; \u0026gt; lindex list-key 1 \u0026#34;item2\u0026#34; \u0026gt; lpop list-key \u0026#34;item\u0026#34; \u0026gt; lrange list-key 0 -1 1) \u0026#34;item2\u0026#34; 2) \u0026#34;item\u0026#34; SET # \u0026gt; sadd set-key item (integer) 1 \u0026gt; sadd set-key item2 (integer) 1 \u0026gt; sadd set-key item3 (integer) 1 \u0026gt; sadd set-key item (integer) 0 \u0026gt; smembers set-key 1) \u0026#34;item\u0026#34; 2) \u0026#34;item2\u0026#34; 3) \u0026#34;item3\u0026#34; \u0026gt; sismember set-key item4 (integer) 0 \u0026gt; sismember set-key item (integer) 1 \u0026gt; srem set-key item2 (integer) 1 \u0026gt; srem set-key item2 (integer) 0 \u0026gt; smembers set-key 1) \u0026#34;item\u0026#34; 2) \u0026#34;item3\u0026#34; HASH # \u0026gt; hset hash-key sub-key1 value1 (integer) 1 \u0026gt; hset hash-key sub-key2 value2 (integer) 1 \u0026gt; hset hash-key sub-key1 value1 (integer) 0 \u0026gt; hgetall hash-key 1) \u0026#34;sub-key1\u0026#34; 2) \u0026#34;value1\u0026#34; 3) \u0026#34;sub-key2\u0026#34; 4) \u0026#34;value2\u0026#34; \u0026gt; hdel hash-key sub-key2 (integer) 1 \u0026gt; hdel hash-key sub-key2 (integer) 0 \u0026gt; hget hash-key sub-key1 \u0026#34;value1\u0026#34; \u0026gt; hgetall hash-key 1) \u0026#34;sub-key1\u0026#34; 2) \u0026#34;value1\u0026#34; ZSET # \u0026gt; zadd zset-key 728 member1 (integer) 1 \u0026gt; zadd zset-key 982 member0 (integer) 1 \u0026gt; zadd zset-key 982 member0 (integer) 0 \u0026gt; zrange zset-key 0 -1 withscores 1) \u0026#34;member1\u0026#34; 2) \u0026#34;728\u0026#34; 3) \u0026#34;member0\u0026#34; 4) \u0026#34;982\u0026#34; \u0026gt; zrangebyscore zset-key 0 800 withscores 1) \u0026#34;member1\u0026#34; 2) \u0026#34;728\u0026#34; \u0026gt; zrem zset-key member1 (integer) 1 \u0026gt; zrem zset-key member1 (integer) 0 \u0026gt; zrange zset-key 0 -1 withscores 1) \u0026#34;member0\u0026#34; 2) \u0026#34;982\u0026#34; 三、数据结构 # 字典 # dictht 是一个散列表结构，使用拉链法解决哈希冲突。\n/* This is our hash table structure. Every dictionary has two of this as we * implement incremental rehashing, for the old to the new table. */ typedef struct dictht { dictEntry **table; unsigned long size; unsigned long sizemask; unsigned long used; } dictht; typedef struct dictEntry { void *key; union { void *val; uint64_t u64; int64_t s64; double d; } v; struct dictEntry *next; } dictEntry; Redis 的字典 dict 中包含两个哈希表 dictht，这是为了方便进行 rehash 操作。在扩容时，将其中一个 dictht 上的键值对 rehash 到另一个 dictht 上面，完成之后释放空间并交换两个 dictht 的角色。\ntypedef struct dict { dictType *type; void *privdata; dictht ht[2]; long rehashidx; /* rehashing not in progress if rehashidx == -1 */ unsigned long iterators; /* number of iterators currently running */ } dict; rehash 操作不是一次性完成，而是采用渐进方式，这是为了避免一次性执行过多的 rehash 操作给服务器带来过大的负担。\n渐进式 rehash 通过记录 dict 的 rehashidx 完成，它从 0 开始，然后每执行一次 rehash 都会递增。例如在一次 rehash 中，要把 dict[0] rehash 到 dict[1]，这一次会把 dict[0] 上 table[rehashidx] 的键值对 rehash 到 dict[1] 上，dict[0] 的 table[rehashidx] 指向 null，并令 rehashidx++。\n在 rehash 期间，每次对字典执行添加、删除、查找或者更新操作时，都会执行一次渐进式 rehash。\n采用渐进式 rehash 会导致字典中的数据分散在两个 dictht 上，因此对字典的查找操作也需要到对应的 dictht 去执行。\n/* Performs N steps of incremental rehashing. Returns 1 if there are still * keys to move from the old to the new hash table, otherwise 0 is returned. * * Note that a rehashing step consists in moving a bucket (that may have more * than one key as we use chaining) from the old to the new hash table, however * since part of the hash table may be composed of empty spaces, it is not * guaranteed that this function will rehash even a single bucket, since it * will visit at max N*10 empty buckets in total, otherwise the amount of * work it does would be unbound and the function may block for a long time. */ int dictRehash(dict *d, int n) { int empty_visits = n * 10; /* Max number of empty buckets to visit. */ if (!dictIsRehashing(d)) return 0; while (n-- \u0026amp;\u0026amp; d-\u0026gt;ht[0].used != 0) { dictEntry *de, *nextde; /* Note that rehashidx can\u0026#39;t overflow as we are sure there are more * elements because ht[0].used != 0 */ assert(d-\u0026gt;ht[0].size \u0026gt; (unsigned long) d-\u0026gt;rehashidx); while (d-\u0026gt;ht[0].table[d-\u0026gt;rehashidx] == NULL) { d-\u0026gt;rehashidx++; if (--empty_visits == 0) return 1; } de = d-\u0026gt;ht[0].table[d-\u0026gt;rehashidx]; /* Move all the keys in this bucket from the old to the new hash HT */ while (de) { uint64_t h; nextde = de-\u0026gt;next; /* Get the index in the new hash table */ h = dictHashKey(d, de-\u0026gt;key) \u0026amp; d-\u0026gt;ht[1].sizemask; de-\u0026gt;next = d-\u0026gt;ht[1].table[h]; d-\u0026gt;ht[1].table[h] = de; d-\u0026gt;ht[0].used--; d-\u0026gt;ht[1].used++; de = nextde; } d-\u0026gt;ht[0].table[d-\u0026gt;rehashidx] = NULL; d-\u0026gt;rehashidx++; } /* Check if we already rehashed the whole table... */ if (d-\u0026gt;ht[0].used == 0) { zfree(d-\u0026gt;ht[0].table); d-\u0026gt;ht[0] = d-\u0026gt;ht[1]; _dictReset(\u0026amp;d-\u0026gt;ht[1]); d-\u0026gt;rehashidx = -1; return 0; } /* More to rehash... */ return 1; } 跳跃表 # 是有序集合的底层实现之一。\n跳跃表是基于多指针有序链表实现的，可以看成多个有序链表。\n在查找时，从上层指针开始查找，找到对应的区间之后再到下一层去查找。下图演示了查找 22 的过程。\n与红黑树等平衡树相比，跳跃表具有以下优点：\n插入速度非常快速，因为不需要进行旋转等操作来维护平衡性； 更容易实现； 支持无锁操作。 四、使用场景 # 计数器 # 可以对 String 进行自增自减运算，从而实现计数器功能。\nRedis 这种内存型数据库的读写性能非常高，很适合存储频繁读写的计数量。\n缓存 # 将热点数据放到内存中，设置内存的最大使用量以及淘汰策略来保证缓存的命中率。\n查找表 # 例如 DNS 记录就很适合使用 Redis 进行存储。\n查找表和缓存类似，也是利用了 Redis 快速的查找特性。但是查找表的内容不能失效，而缓存的内容可以失效，因为缓存不作为可靠的数据来源。\n消息队列 # List 是一个双向链表，可以通过 lpush 和 rpop 写入和读取消息\n不过最好使用 Kafka、RabbitMQ 等消息中间件。\n会话缓存 # 可以使用 Redis 来统一存储多台应用服务器的会话信息。\n当应用服务器不再存储用户的会话信息，也就不再具有状态，一个用户可以请求任意一个应用服务器，从而更容易实现高可用性以及可伸缩性。\n分布式锁实现 # 在分布式场景下，无法使用单机环境下的锁来对多个节点上的进程进行同步。\n可以使用 Redis 自带的 SETNX 命令实现分布式锁，除此之外，还可以使用官方提供的 RedLock 分布式锁实现。\n其它 # Set 可以实现交集、并集等操作，从而实现共同好友等功能。\nZSet 可以实现有序性操作，从而实现排行榜等功能。\n五、Redis 与 Memcached # 两者都是非关系型内存键值数据库，主要有以下不同：\n数据类型 # Memcached 仅支持字符串类型，而 Redis 支持五种不同的数据类型，可以更灵活地解决问题。\n数据持久化 # Redis 支持两种持久化策略：RDB 快照和 AOF 日志，而 Memcached 不支持持久化。\n分布式 # Memcached 不支持分布式，只能通过在客户端使用一致性哈希来实现分布式存储，这种方式在存储和查询时都需要先在客户端计算一次数据所在的节点。\nRedis Cluster 实现了分布式的支持。\n内存管理机制 # 在 Redis 中，并不是所有数据都一直存储在内存中，可以将一些很久没用的 value 交换到磁盘，而 Memcached 的数据则会一直在内存中。\nMemcached 将内存分割成特定长度的块来存储数据，以完全解决内存碎片的问题。但是这种方式会使得内存的利用率不高，例如块的大小为 128 bytes，只存储 100 bytes 的数据，那么剩下的 28 bytes 就浪费掉了。\n六、键的过期时间 # Redis 可以为每个键设置过期时间，当键过期时，会自动删除该键。\n对于散列表这种容器，只能为整个键设置过期时间（整个散列表），而不能为键里面的单个元素设置过期时间。\n七、数据淘汰策略 # 可以设置内存最大使用量，当内存使用量超出时，会施行数据淘汰策略。\nRedis 具体有 6 种淘汰策略：\n策略 描述 volatile-lru 从已设置过期时间的数据集中挑选最近最少使用的数据淘汰 volatile-ttl 从已设置过期时间的数据集中挑选将要过期的数据淘汰 volatile-random 从已设置过期时间的数据集中任意选择数据淘汰 allkeys-lru 从所有数据集中挑选最近最少使用的数据淘汰 allkeys-random 从所有数据集中任意选择数据进行淘汰 noeviction 禁止驱逐数据 作为内存数据库，出于对性能和内存消耗的考虑，Redis 的淘汰算法实际实现上并非针对所有 key，而是抽样一小部分并且从中选出被淘汰的 key。\n使用 Redis 缓存数据时，为了提高缓存命中率，需要保证缓存数据都是热点数据。可以将内存最大使用量设置为热点数据占用的内存量，然后启用 allkeys-lru 淘汰策略，将最近最少使用的数据淘汰。\nRedis 4.0 引入了 volatile-lfu 和 allkeys-lfu 淘汰策略，LFU 策略通过统计访问频率，将访问频率最少的键值对淘汰。\n八、持久化 # Redis 是内存型数据库，为了保证数据在断电后不会丢失，需要将内存中的数据持久化到硬盘上。\nRDB 持久化 # 将某个时间点的所有数据都存放到硬盘上。\n可以将快照复制到其它服务器从而创建具有相同数据的服务器副本。\n如果系统发生故障，将会丢失最后一次创建快照之后的数据。\n如果数据量很大，保存快照的时间会很长。\nAOF 持久化 # 将写命令添加到 AOF 文件（Append Only File）的末尾。\n使用 AOF 持久化需要设置同步选项，从而确保写命令同步到磁盘文件上的时机。这是因为对文件进行写入并不会马上将内容同步到磁盘上，而是先存储到缓冲区，然后由操作系统决定什么时候同步到磁盘。有以下同步选项：\n选项 同步频率 always 每个写命令都同步 everysec 每秒同步一次 no 让操作系统来决定何时同步 always 选项会严重减低服务器的性能； everysec 选项比较合适，可以保证系统崩溃时只会丢失一秒左右的数据，并且 Redis 每秒执行一次同步对服务器性能几乎没有任何影响； no 选项并不能给服务器性能带来多大的提升，而且也会增加系统崩溃时数据丢失的数量。 随着服务器写请求的增多，AOF 文件会越来越大。Redis 提供了一种将 AOF 重写的特性，能够去除 AOF 文件中的冗余写命令。\n九、事务 # 一个事务包含了多个命令，服务器在执行事务期间，不会改去执行其它客户端的命令请求。\n事务中的多个命令被一次性发送给服务器，而不是一条一条发送，这种方式被称为流水线，它可以减少客户端与服务器之间的网络通信次数从而提升性能。\nRedis 最简单的事务实现方式是使用 MULTI 和 EXEC 命令将事务操作包围起来。\n十、事件 # Redis 服务器是一个事件驱动程序。\n文件事件 # 服务器通过套接字与客户端或者其它服务器进行通信，文件事件就是对套接字操作的抽象。\nRedis 基于 Reactor 模式开发了自己的网络事件处理器，使用 I/O 多路复用程序来同时监听多个套接字，并将到达的事件传送给文件事件分派器，分派器会根据套接字产生的事件类型调用相应的事件处理器。\n时间事件 # 服务器有一些操作需要在给定的时间点执行，时间事件是对这类定时操作的抽象。\n时间事件又分为：\n定时事件：是让一段程序在指定的时间之内执行一次； 周期性事件：是让一段程序每隔指定时间就执行一次。 Redis 将所有时间事件都放在一个无序链表中，通过遍历整个链表查找出已到达的时间事件，并调用相应的事件处理器。\n事件的调度与执行 # 服务器需要不断监听文件事件的套接字才能得到待处理的文件事件，但是不能一直监听，否则时间事件无法在规定的时间内执行，因此监听时间应该根据距离现在最近的时间事件来决定。\n事件调度与执行由 aeProcessEvents 函数负责，伪代码如下：\ndef aeProcessEvents(): # 获取到达时间离当前时间最接近的时间事件 time_event = aeSearchNearestTimer() # 计算最接近的时间事件距离到达还有多少毫秒 remaind_ms = time_event.when - unix_ts_now() # 如果事件已到达，那么 remaind_ms 的值可能为负数，将它设为 0 if remaind_ms \u0026lt; 0: remaind_ms = 0 # 根据 remaind_ms 的值，创建 timeval timeval = create_timeval_with_ms(remaind_ms) # 阻塞并等待文件事件产生，最大阻塞时间由传入的 timeval 决定 aeApiPoll(timeval) # 处理所有已产生的文件事件 procesFileEvents() # 处理所有已到达的时间事件 processTimeEvents() 将 aeProcessEvents 函数置于一个循环里面，加上初始化和清理函数，就构成了 Redis 服务器的主函数，伪代码如下：\ndef main(): # 初始化服务器 init_server() # 一直处理事件，直到服务器关闭为止 while server_is_not_shutdown(): aeProcessEvents() # 服务器关闭，执行清理操作 clean_server() 从事件处理的角度来看，服务器运行流程如下：\n十一、复制 # 通过使用 slaveof host port 命令来让一个服务器成为另一个服务器的从服务器。\n一个从服务器只能有一个主服务器，并且不支持主主复制。\n连接过程 # 主服务器创建快照文件，发送给从服务器，并在发送期间使用缓冲区记录执行的写命令。快照文件发送完毕之后，开始向从服务器发送存储在缓冲区中的写命令；\n从服务器丢弃所有旧数据，载入主服务器发来的快照文件，之后从服务器开始接受主服务器发来的写命令；\n主服务器每执行一次写命令，就向从服务器发送相同的写命令。\n主从链 # 随着负载不断上升，主服务器可能无法很快地更新所有从服务器，或者重新连接和重新同步从服务器将导致系统超载。为了解决这个问题，可以创建一个中间层来分担主服务器的复制工作。中间层的服务器是最上层服务器的从服务器，又是最下层服务器的主服务器。\n十二、Sentinel # Sentinel（哨兵）可以监听集群中的服务器，并在主服务器进入下线状态时，自动从从服务器中选举出新的主服务器。\n十三、分片 # 分片是将数据划分为多个部分的方法，可以将数据存储到多台机器里面，这种方法在解决某些问题时可以获得线性级别的性能提升。\n假设有 4 个 Redis 实例 R0，R1，R2，R3，还有很多表示用户的键 user:1，user:2，\u0026hellip; ，有不同的方式来选择一个指定的键存储在哪个实例中。\n最简单的方式是范围分片，例如用户 id 从 0~1000 的存储到实例 R0 中，用户 id 从 1001~2000 的存储到实例 R1 中，等等。但是这样需要维护一张映射范围表，维护操作代价很高。 还有一种方式是哈希分片，使用 CRC32 哈希函数将键转换为一个数字，再对实例数量求模就能知道应该存储的实例。 根据执行分片的位置，可以分为三种分片方式：\n客户端分片：客户端使用一致性哈希等算法决定键应当分布到哪个节点。 代理分片：将客户端请求发送到代理上，由代理转发请求到正确的节点上。 服务器分片：Redis Cluster。 十四、一个简单的论坛系统分析 # 该论坛系统功能如下：\n可以发布文章； 可以对文章进行点赞； 在首页可以按文章的发布时间或者文章的点赞数进行排序显示。 文章信息 # 文章包括标题、作者、赞数等信息，在关系型数据库中很容易构建一张表来存储这些信息，在 Redis 中可以使用 HASH 来存储每种信息以及其对应的值的映射。\nRedis 没有关系型数据库中的表这一概念来将同种类型的数据存放在一起，而是使用命名空间的方式来实现这一功能。键名的前面部分存储命名空间，后面部分的内容存储 ID，通常使用 : 来进行分隔。例如下面的 HASH 的键名为 article:92617，其中 article 为命名空间，ID 为 92617。\n点赞功能 # 当有用户为一篇文章点赞时，除了要对该文章的 votes 字段进行加 1 操作，还必须记录该用户已经对该文章进行了点赞，防止用户点赞次数超过 1。可以建立文章的已投票用户集合来进行记录。\n为了节约内存，规定一篇文章发布满一周之后，就不能再对它进行投票，而文章的已投票集合也会被删除，可以为文章的已投票集合设置一个一周的过期时间就能实现这个规定。\n对文章进行排序 # 为了按发布时间和点赞数进行排序，可以建立一个文章发布时间的有序集合和一个文章点赞数的有序集合。（下图中的 score 就是这里所说的点赞数；下面所示的有序集合分值并不直接是时间和点赞数，而是根据时间和点赞数间接计算出来的）\n参考资料 # Carlson J L. Redis in Action[J]. Media.johnwiley.com.au, 2013. 黄健宏. Redis 设计与实现 [M]. 机械工业出版社, 2014. REDIS IN ACTION Skip Lists: Done Right 论述 Redis 和 Memcached 的差异 Redis 3.0 中文版- 分片 Redis 应用场景 Using Redis as an LRU cache "},{"id":5,"href":"/keeplearning/docs/coding-practice/docker/","title":"Docker","section":"编码实践","content":" Docker # 一、解决的问题 # 由于不同的机器有不同的操作系统，以及不同的库和组件，在将一个应用部署到多台机器上需要进行大量的环境配置操作。\nDocker 主要解决环境配置问题，它是一种虚拟化技术，对进程进行隔离，被隔离的进程独立于宿主操作系统和其它隔离的进程。使用 Docker 可以不修改应用程序代码，不需要开发人员学习特定环境下的技术，就能够将现有的应用程序部署在其它机器上。\n二、与虚拟机的比较 # 虚拟机也是一种虚拟化技术，它与 Docker 最大的区别在于它是通过模拟硬件，并在硬件上安装操作系统来实现。\n启动速度 # 启动虚拟机需要先启动虚拟机的操作系统，再启动应用，这个过程非常慢；\n而启动 Docker 相当于启动宿主操作系统上的一个进程。\n占用资源 # 虚拟机是一个完整的操作系统，需要占用大量的磁盘、内存和 CPU 资源，一台机器只能开启几十个的虚拟机。\n而 Docker 只是一个进程，只需要将应用以及相关的组件打包，在运行时占用很少的资源，一台机器可以开启成千上万个 Docker。\n三、优势 # 除了启动速度快以及占用资源少之外，Docker 具有以下优势：\n更容易迁移 # 提供一致性的运行环境。已经打包好的应用可以在不同的机器上进行迁移，而不用担心环境变化导致无法运行。\n更容易维护 # 使用分层技术和镜像，使得应用可以更容易复用重复的部分。复用程度越高，维护工作也越容易。\n更容易扩展 # 可以使用基础镜像进一步扩展得到新的镜像，并且官方和开源社区提供了大量的镜像，通过扩展这些镜像可以非常容易得到我们想要的镜像。\n四、使用场景 # 持续集成 # 持续集成指的是频繁地将代码集成到主干上，这样能够更快地发现错误。\nDocker 具有轻量级以及隔离性的特点，在将代码集成到一个 Docker 中不会对其它 Docker 产生影响。\n提供可伸缩的云服务 # 根据应用的负载情况，可以很容易地增加或者减少 Docker。\n搭建微服务架构 # Docker 轻量级的特点使得它很适合用于部署、维护、组合微服务。\n五、镜像与容器 # 镜像是一种静态的结构，可以看成面向对象里面的类，而容器是镜像的一个实例。\n镜像包含着容器运行时所需要的代码以及其它组件，它是一种分层结构，每一层都是只读的（read-only layers）。构建镜像时，会一层一层构建，前一层是后一层的基础。镜像的这种分层存储结构很适合镜像的复用以及定制。\n构建容器时，通过在镜像的基础上添加一个可写层（writable layer），用来保存着容器运行过程中的修改。\n参考资料 # DOCKER 101: INTRODUCTION TO DOCKER WEBINAR RECAP Docker 入门教程 Docker container vs Virtual machine How to Create Docker Container using Dockerfile 理解 Docker（2）：Docker 镜像 为什么要使用 Docker？ What is Docker 持续集成是什么？ "},{"id":6,"href":"/keeplearning/docs/mysql/sql-grammar/","title":"SQL 语法","section":"MySQL","content":" SQL 语法 # 一、基础 # 模式定义了数据如何存储、存储什么样的数据以及数据如何分解等信息，数据库和表都有模式。\n主键的值不允许修改，也不允许复用（不能将已经删除的主键值赋给新数据行的主键）。\nSQL（Structured Query Language)，标准 SQL 由 ANSI 标准委员会管理，从而称为 ANSI SQL。各个 DBMS 都有自己的实现，如 PL/SQL、Transact-SQL 等。\nSQL 语句不区分大小写，但是数据库表名、列名和值是否区分依赖于具体的 DBMS 以及配置。\nSQL 支持以下三种注释：\n--- 注释 SELECT * FROM mytable; -- 注释 /* 注释1 注释2 */ 数据库创建与使用：\nCREATE DATABASE test; USE test; 二、创建表 # CREATE TABLE mytable ( --- int 类型，不为空，自增 id INT NOT NULL AUTO_INCREMENT, --- int 类型，不可为空，默认值为 1，不为空 col1 INT NOT NULL DEFAULT 1, --- 变长字符串类型，最长为 45 个字符，可以为空 col2 VARCHAR(45) NULL, --- 日期类型，可为空 col3 DATE NULL, --- 设置主键为 id PRIMARY KEY (`id`)); 三、修改表 # 添加列\nALTER TABLE mytable ADD col CHAR(20); 删除列\nALTER TABLE mytable DROP COLUMN col; 删除表\nDROP TABLE mytable; 四、插入 # 普通插入\nINSERT INTO mytable(col1, col2) VALUES(val1, val2); 插入检索出来的数据\nINSERT INTO mytable1(col1, col2) SELECT col1, col2 FROM mytable2; 将一个表的内容插入到一个新表\nCREATE TABLE newtable AS SELECT * FROM mytable; 五、更新 # UPDATE mytable SET col = val WHERE id = 1; 六、删除 # DELETE FROM mytable WHERE id = 1; TRUNCATE TABLE 可以清空表，也就是删除所有行。\nTRUNCATE TABLE mytable; 使用更新和删除操作时一定要用 WHERE 子句，不然会把整张表的数据都破坏。可以先用 SELECT 语句进行测试，防止错误删除。\n七、查询 # DISTINCT # 相同值只会出现一次。它作用于所有列，也就是说所有列的值都相同才算相同。\nSELECT DISTINCT col1, col2 FROM mytable; LIMIT # 限制返回的行数。可以有两个参数，第一个参数为起始行，从 0 开始；第二个参数为返回的总行数。\n返回前 5 行：\nSELECT * FROM mytable LIMIT 5; SELECT * FROM mytable LIMIT 0, 5; 返回第 3 ~ 5 行：\nSELECT * FROM mytable LIMIT 2, 3; 八、排序 # ASC ：升序（默认） DESC ：降序 可以按多个列进行排序，并且为每个列指定不同的排序方式：\nSELECT * FROM mytable ORDER BY col1 DESC, col2 ASC; 九、过滤 # 不进行过滤的数据非常大，导致通过网络传输了多余的数据，从而浪费了网络带宽。因此尽量使用 SQL 语句来过滤不必要的数据，而不是传输所有的数据到客户端中然后由客户端进行过滤。\nSELECT * FROM mytable WHERE col IS NULL; 下表显示了 WHERE 子句可用的操作符\n操作符 说明 = 等于 \u0026lt; 小于 \u0026gt; 大于 \u0026lt;\u0026gt; != 不等于 \u0026lt;= !\u0026gt; 小于等于 \u0026gt;= !\u0026lt; 大于等于 BETWEEN 在两个值之间 IS NULL 为 NULL 值 应该注意到，NULL 与 0、空字符串都不同。\nAND 和 OR 用于连接多个过滤条件。优先处理 AND，当一个过滤表达式涉及到多个 AND 和 OR 时，可以使用 () 来决定优先级，使得优先级关系更清晰。\nIN 操作符用于匹配一组值，其后也可以接一个 SELECT 子句，从而匹配子查询得到的一组值。\nNOT 操作符用于否定一个条件。\n十、通配符 # 通配符也是用在过滤语句中，但它只能用于文本字段。\n% 匹配 \u0026gt;=0 个任意字符；\n_ 匹配 ==1 个任意字符；\n[ ] 可以匹配集合内的字符，例如 [ab] 将匹配字符 a 或者 b。用脱字符 ^ 可以对其进行否定，也就是不匹配集合内的字符。\n使用 Like 来进行通配符匹配。\nSELECT * FROM mytable WHERE col LIKE \u0026#39;[^AB]%\u0026#39;; -- 不以 A 和 B 开头的任意文本 不要滥用通配符，通配符位于开头处匹配会非常慢。\n十一、计算字段 # 在数据库服务器上完成数据的转换和格式化的工作往往比客户端上快得多，并且转换和格式化后的数据量更少的话可以减少网络通信量。\n计算字段通常需要使用 AS 来取别名，否则输出的时候字段名为计算表达式。\nSELECT col1 * col2 AS alias FROM mytable; CONCAT() 用于连接两个字段。许多数据库会使用空格把一个值填充为列宽，因此连接的结果会出现一些不必要的空格，使用 TRIM() 可以去除首尾空格。\nSELECT CONCAT(TRIM(col1), \u0026#39;(\u0026#39;, TRIM(col2), \u0026#39;)\u0026#39;) AS concat_col FROM mytable; 十二、函数 # 各个 DBMS 的函数都是不相同的，因此不可移植，以下主要是 MySQL 的函数。\n汇总 # 函 数 说 明 AVG() 返回某列的平均值 COUNT() 返回某列的行数 MAX() 返回某列的最大值 MIN() 返回某列的最小值 SUM() 返回某列值之和 AVG() 会忽略 NULL 行。\n使用 DISTINCT 可以汇总不同的值。\nSELECT AVG(DISTINCT col1) AS avg_col FROM mytable; 文本处理 # 函数 说明 LEFT() 左边的字符 RIGHT() 右边的字符 LOWER() 转换为小写字符 UPPER() 转换为大写字符 LTRIM() 去除左边的空格 RTRIM() 去除右边的空格 LENGTH() 长度 SOUNDEX() 转换为语音值 其中， SOUNDEX() 可以将一个字符串转换为描述其语音表示的字母数字模式。\nSELECT * FROM mytable WHERE SOUNDEX(col1) = SOUNDEX(\u0026#39;apple\u0026#39;) 日期和时间处理 # 日期格式：YYYY-MM-DD 时间格式：HH:\u0026lt;zero-width space\u0026gt;MM:SS 函 数 说 明 ADDDATE() 增加一个日期（天、周等） ADDTIME() 增加一个时间（时、分等） CURDATE() 返回当前日期 CURTIME() 返回当前时间 DATE() 返回日期时间的日期部分 DATEDIFF() 计算两个日期之差 DATE_ADD() 高度灵活的日期运算函数 DATE_FORMAT() 返回一个格式化的日期或时间串 DAY() 返回一个日期的天数部分 DAYOFWEEK() 对于一个日期，返回对应的星期几 HOUR() 返回一个时间的小时部分 MINUTE() 返回一个时间的分钟部分 MONTH() 返回一个日期的月份部分 NOW() 返回当前日期和时间 SECOND() 返回一个时间的秒部分 TIME() 返回一个日期时间的时间部分 YEAR() 返回一个日期的年份部分 mysql\u0026gt; SELECT NOW(); 2018-4-14 20:25:11 数值处理 # 函数 说明 SIN() 正弦 COS() 余弦 TAN() 正切 ABS() 绝对值 SQRT() 平方根 MOD() 余数 EXP() 指数 PI() 圆周率 RAND() 随机数 十三、分组 # 把具有相同的数据值的行放在同一组中。\n可以对同一分组数据使用汇总函数进行处理，例如求分组数据的平均值等。\n指定的分组字段除了能按该字段进行分组，也会自动按该字段进行排序。\nSELECT col, COUNT(*) AS num FROM mytable GROUP BY col; GROUP BY 自动按分组字段进行排序，ORDER BY 也可以按汇总字段来进行排序。\nSELECT col, COUNT(*) AS num FROM mytable GROUP BY col ORDER BY num; WHERE 过滤行，HAVING 过滤分组，行过滤应当先于分组过滤。\nSELECT col, COUNT(*) AS num FROM mytable WHERE col \u0026gt; 2 GROUP BY col HAVING num \u0026gt;= 2; 分组规定：\nGROUP BY 子句出现在 WHERE 子句之后，ORDER BY 子句之前； 除了汇总字段外，SELECT 语句中的每一字段都必须在 GROUP BY 子句中给出； NULL 的行会单独分为一组； 大多数 SQL 实现不支持 GROUP BY 列具有可变长度的数据类型。 十四、子查询 # 子查询中只能返回一个字段的数据。\n可以将子查询的结果作为 WHRER 语句的过滤条件：\nSELECT * FROM mytable1 WHERE col1 IN (SELECT col2 FROM mytable2); 下面的语句可以检索出客户的订单数量，子查询语句会对第一个查询检索出的每个客户执行一次：\nSELECT cust_name, (SELECT COUNT(*) FROM Orders WHERE Orders.cust_id = Customers.cust_id) AS orders_num FROM Customers ORDER BY cust_name; 十五、连接 # 连接用于连接多个表，使用 JOIN 关键字，并且条件语句使用 ON 而不是 WHERE。\n连接可以替换子查询，并且比子查询的效率一般会更快。\n可以用 AS 给列名、计算字段和表名取别名，给表名取别名是为了简化 SQL 语句以及连接相同表。\n内连接 # 内连接又称等值连接，使用 INNER JOIN 关键字。\nSELECT A.value, B.value FROM tablea AS A INNER JOIN tableb AS B ON A.key = B.key; 可以不明确使用 INNER JOIN，而使用普通查询并在 WHERE 中将两个表中要连接的列用等值方法连接起来。\nSELECT A.value, B.value FROM tablea AS A, tableb AS B WHERE A.key = B.key; 自连接 # 自连接可以看成内连接的一种，只是连接的表是自身而已。\n一张员工表，包含员工姓名和员工所属部门，要找出与 Jim 处在同一部门的所有员工姓名。\n子查询版本\nSELECT name FROM employee WHERE department = ( SELECT department FROM employee WHERE name = \u0026#34;Jim\u0026#34;); 自连接版本\nSELECT e1.name FROM employee AS e1 INNER JOIN employee AS e2 ON e1.department = e2.department AND e2.name = \u0026#34;Jim\u0026#34;; 自然连接 # 自然连接是把同名列通过等值测试连接起来的，同名列可以有多个。\n内连接和自然连接的区别：内连接提供连接的列，而自然连接自动连接所有同名列。\nSELECT A.value, B.value FROM tablea AS A NATURAL JOIN tableb AS B; 外连接 # 外连接保留了没有关联的那些行。分为左外连接，右外连接以及全外连接，左外连接就是保留左表没有关联的行。\n检索所有顾客的订单信息，包括还没有订单信息的顾客。\nSELECT Customers.cust_id, Customer.cust_name, Orders.order_id FROM Customers LEFT OUTER JOIN Orders ON Customers.cust_id = Orders.cust_id; customers 表：\ncust_id cust_name 1 a 2 b 3 c orders 表：\norder_id cust_id 1 1 2 1 3 3 4 3 结果：\ncust_id cust_name order_id 1 a 1 1 a 2 3 c 3 3 c 4 2 b Null 十六、组合查询 # 使用 UNION 来组合两个查询，如果第一个查询返回 M 行，第二个查询返回 N 行，那么组合查询的结果一般为 M+N 行。\n每个查询必须包含相同的列、表达式和聚集函数。\n默认会去除相同行，如果需要保留相同行，使用 UNION ALL。\n只能包含一个 ORDER BY 子句，并且必须位于语句的最后。\nSELECT col FROM mytable WHERE col = 1 UNION SELECT col FROM mytable WHERE col =2; 十七、视图 # 视图是虚拟的表，本身不包含数据，也就不能对其进行索引操作。\n对视图的操作和对普通表的操作一样。\n视图具有如下好处：\n简化复杂的 SQL 操作，比如复杂的连接； 只使用实际表的一部分数据； 通过只给用户访问视图的权限，保证数据的安全性； 更改数据格式和表示。 CREATE VIEW myview AS SELECT Concat(col1, col2) AS concat_col, col3*col4 AS compute_col FROM mytable WHERE col5 = val; 十八、存储过程 # 存储过程可以看成是对一系列 SQL 操作的批处理。\n使用存储过程的好处：\n代码封装，保证了一定的安全性； 代码复用； 由于是预先编译，因此具有很高的性能。 命令行中创建存储过程需要自定义分隔符，因为命令行是以 ; 为结束符，而存储过程中也包含了分号，因此会错误把这部分分号当成是结束符，造成语法错误。\n包含 in、out 和 inout 三种参数。\n给变量赋值都需要用 select into 语句。\n每次只能给一个变量赋值，不支持集合的操作。\ndelimiter // create procedure myprocedure( out ret int ) begin declare y int; select sum(col1) from mytable into y; select y*y into ret; end // delimiter ; call myprocedure(@ret); select @ret; 十九、游标 # 在存储过程中使用游标可以对一个结果集进行移动遍历。\n游标主要用于交互式应用，其中用户需要对数据集中的任意行进行浏览和修改。\n使用游标的四个步骤：\n声明游标，这个过程没有实际检索出数据； 打开游标； 取出数据； 关闭游标； delimiter // create procedure myprocedure(out ret int) begin declare done boolean default 0; declare mycursor cursor for select col1 from mytable; --- 定义了一个 continue handler，当 sqlstate \u0026#39;02000\u0026#39; 这个条件出现时，会执行 set done = 1 declare continue handler for sqlstate \u0026#39;02000\u0026#39; set done = 1; open mycursor; repeat fetch mycursor into ret; select ret; until done end repeat; close mycursor; end // delimiter ; 二十、触发器 # 触发器会在某个表执行以下语句时而自动执行：DELETE、INSERT、UPDATE。\n触发器必须指定在语句执行之前还是之后自动执行，之前执行使用 BEFORE 关键字，之后执行使用 AFTER 关键字。BEFORE 用于数据验证和净化，AFTER 用于审计跟踪，将修改记录到另外一张表中。\nINSERT 触发器包含一个名为 NEW 的虚拟表。\nCREATE TRIGGER mytrigger AFTER INSERT ON mytable FOR EACH ROW SELECT NEW.col into @result; SELECT @result; -- 获取结果 DELETE 触发器包含一个名为 OLD 的虚拟表，并且是只读的。\nUPDATE 触发器包含一个名为 NEW 和一个名为 OLD 的虚拟表，其中 NEW 是可以被修改的，而 OLD 是只读的。\nMySQL 不允许在触发器中使用 CALL 语句，也就是不能调用存储过程。\n二十一、事务管理 # 基本术语：\n事务（transaction）指一组 SQL 语句； 回退（rollback）指撤销指定 SQL 语句的过程； 提交（commit）指将未存储的 SQL 语句结果写入数据库表； 保留点（savepoint）指事务处理中设置的临时占位符（placeholder），你可以对它发布回退（与回退整个事务处理不同）。 不能回退 SELECT 语句，回退 SELECT 语句也没意义；也不能回退 CREATE 和 DROP 语句。\nMySQL 的事务提交默认是隐式提交，每执行一条语句就把这条语句当成一个事务然后进行提交。当出现 START TRANSACTION 语句时，会关闭隐式提交；当 COMMIT 或 ROLLBACK 语句执行后，事务会自动关闭，重新恢复隐式提交。\n设置 autocommit 为 0 可以取消自动提交；autocommit 标记是针对每个连接而不是针对服务器的。\n如果没有设置保留点，ROLLBACK 会回退到 START TRANSACTION 语句处；如果设置了保留点，并且在 ROLLBACK 中指定该保留点，则会回退到该保留点。\nSTART TRANSACTION // ... SAVEPOINT delete1 // ... ROLLBACK TO delete1 // ... COMMIT 二十二、字符集 # 基本术语：\n字符集为字母和符号的集合； 编码为某个字符集成员的内部表示； 校对字符指定如何比较，主要用于排序和分组。 除了给表指定字符集和校对外，也可以给列指定：\nCREATE TABLE mytable (col VARCHAR(10) CHARACTER SET latin COLLATE latin1_general_ci ) DEFAULT CHARACTER SET hebrew COLLATE hebrew_general_ci; 可以在排序、分组时指定校对：\nSELECT * FROM mytable ORDER BY col COLLATE latin1_general_ci; 二十三、权限管理 # MySQL 的账户信息保存在 mysql 这个数据库中。\nUSE mysql; SELECT user FROM user; 创建账户\n新创建的账户没有任何权限。\nCREATE USER myuser IDENTIFIED BY \u0026#39;mypassword\u0026#39;; 修改账户名\nRENAME USER myuser TO newuser; 删除账户\nDROP USER myuser; 查看权限\nSHOW GRANTS FOR myuser; 授予权限\n账户用 username@host 的形式定义，username@% 使用的是默认主机名。\nGRANT SELECT, INSERT ON mydatabase.* TO myuser; 删除权限\nGRANT 和 REVOKE 可在几个层次上控制访问权限：\n整个服务器，使用 GRANT ALL 和 REVOKE ALL； 整个数据库，使用 ON database.*； 特定的表，使用 ON database.table； 特定的列； 特定的存储过程。 REVOKE SELECT, INSERT ON mydatabase.* FROM myuser; 更改密码\n必须使用 Password() 函数进行加密。\nSET PASSWROD FOR myuser = Password(\u0026#39;new_password\u0026#39;); 参考资料 # BenForta. SQL 必知必会 [M]. 人民邮电出版社, 2013. "},{"id":7,"href":"/keeplearning/docs/redis/string/","title":"STRING","section":"Redis","content":" STRING # "},{"id":8,"href":"/keeplearning/docs/system-design/distributed/","title":"分布式","section":"系统设计","content":" 分布式 # 一、分布式锁 # 在单机场景下，可以使用语言的内置锁来实现进程同步。但是在分布式场景下，需要同步的进程可能位于不同的节点上，那么就需要使用分布式锁。\n阻塞锁通常使用互斥量来实现：\n互斥量为 0 表示有其它进程在使用锁，此时处于锁定状态； 互斥量为 1 表示未锁定状态。 1 和 0 可以用一个整型值表示，也可以用某个数据是否存在表示。\n数据库的唯一索引 # 获得锁时向表中插入一条记录，释放锁时删除这条记录。唯一索引可以保证该记录只被插入一次，那么就可以用这个记录是否存在来判断是否处于锁定状态。\n存在以下几个问题：\n锁没有失效时间，解锁失败的话其它进程无法再获得该锁； 只能是非阻塞锁，插入失败直接就报错了，无法重试； 不可重入，已经获得锁的进程也必须重新获取锁。 Redis 的 SETNX 指令 # 使用 SETNX（set if not exist）指令插入一个键值对，如果 Key 已经存在，那么会返回 False，否则插入成功并返回 True。\nSETNX 指令和数据库的唯一索引类似，保证了只存在一个 Key 的键值对，那么可以用一个 Key 的键值对是否存在来判断是否存于锁定状态。\nEXPIRE 指令可以为一个键值对设置一个过期时间，从而避免了数据库唯一索引实现方式中释放锁失败的问题。\nRedis 的 RedLock 算法 # 使用了多个 Redis 实例来实现分布式锁，这是为了保证在发生单点故障时仍然可用。\n尝试从 N 个互相独立 Redis 实例获取锁； 计算获取锁消耗的时间，只有时间小于锁的过期时间，并且从大多数（N / 2 + 1）实例上获取了锁，才认为获取锁成功； 如果获取锁失败，就到每个实例上释放锁。 Zookeeper 的有序节点 # 1. Zookeeper 抽象模型 # Zookeeper 提供了一种树形结构的命名空间，/app1/p_1 节点的父节点为 /app1。\n2. 节点类型 # 永久节点：不会因为会话结束或者超时而消失； 临时节点：如果会话结束或者超时就会消失； 有序节点：会在节点名的后面加一个数字后缀，并且是有序的，例如生成的有序节点为 /lock/node-0000000000，它的下一个有序节点则为 /lock/node-0000000001，以此类推。 3. 监听器 # 为一个节点注册监听器，在节点状态发生改变时，会给客户端发送消息。\n4. 分布式锁实现 # 创建一个锁目录 /lock； 当一个客户端需要获取锁时，在 /lock 下创建临时的且有序的子节点； 客户端获取 /lock 下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁；否则监听自己的前一个子节点，获得子节点的变更通知后重复此步骤直至获得锁； 执行业务代码，完成后，删除对应的子节点。 5. 会话超时 # 如果一个已经获得锁的会话超时了，因为创建的是临时节点，所以该会话对应的临时节点会被删除，其它会话就可以获得锁了。可以看到，这种实现方式不会出现数据库的唯一索引实现方式释放锁失败的问题。\n6. 羊群效应 # 一个节点未获得锁，只需要监听自己的前一个子节点，这是因为如果监听所有的子节点，那么任意一个子节点状态改变，其它所有子节点都会收到通知（羊群效应，一只羊动起来，其它羊也会一哄而上），而我们只希望它的后一个子节点收到通知。\n二、分布式事务 # 指事务的操作位于不同的节点上，需要保证事务的 ACID 特性。\n例如在下单场景下，库存和订单如果不在同一个节点上，就涉及分布式事务。\n分布式锁和分布式事务区别：\n锁问题的关键在于进程操作的互斥关系，例如多个进程同时修改账户的余额，如果没有互斥关系则会导致该账户的余额不正确。 而事务问题的关键则在于事务涉及的一系列操作需要满足 ACID 特性，例如要满足原子性操作则需要这些操作要么都执行，要么都不执行。 2PC # 两阶段提交（Two-phase Commit，2PC），通过引入协调者（Coordinator）来协调参与者的行为，并最终决定这些参与者是否要真正执行事务。\n1. 运行过程 # 1.1 准备阶段 # 协调者询问参与者事务是否执行成功，参与者发回事务执行结果。询问可以看成一种投票，需要参与者都同意才能执行。\n1.2 提交阶段 # 如果事务在每个参与者上都执行成功，事务协调者发送通知让参与者提交事务；否则，协调者发送通知让参与者回滚事务。\n需要注意的是，在准备阶段，参与者执行了事务，但是还未提交。只有在提交阶段接收到协调者发来的通知后，才进行提交或者回滚。\n2. 存在的问题 # 2.1 同步阻塞 # 所有事务参与者在等待其它参与者响应的时候都处于同步阻塞等待状态，无法进行其它操作。\n2.2 单点问题 # 协调者在 2PC 中起到非常大的作用，发生故障将会造成很大影响。特别是在提交阶段发生故障，所有参与者会一直同步阻塞等待，无法完成其它操作。\n2.3 数据不一致 # 在提交阶段，如果协调者只发送了部分 Commit 消息，此时网络发生异常，那么只有部分参与者接收到 Commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。\n2.4 太过保守 # 任意一个节点失败就会导致整个事务失败，没有完善的容错机制。\n本地消息表 # 本地消息表与业务数据表处于同一个数据库中，这样就能利用本地事务来保证在对这两个表的操作满足事务特性，并且使用了消息队列来保证最终一致性。\n在分布式事务操作的一方完成写业务数据的操作之后向本地消息表发送一个消息，本地事务能保证这个消息一定会被写入本地消息表中。 之后将本地消息表中的消息转发到消息队列中，如果转发成功则将消息从本地消息表中删除，否则继续重新转发。 在分布式事务操作的另一方从消息队列中读取一个消息，并执行消息中的操作。 三、CAP # 分布式系统不可能同时满足一致性（C：Consistency）、可用性（A：Availability）和分区容忍性（P：Partition Tolerance），最多只能同时满足其中两项。\n一致性 # 一致性指的是多个数据副本是否能保持一致的特性，在一致性的条件下，系统在执行数据更新操作之后能够从一致性状态转移到另一个一致性状态。\n对系统的一个数据更新成功之后，如果所有用户都能够读取到最新的值，该系统就被认为具有强一致性。\n可用性 # 可用性指分布式系统在面对各种异常时可以提供正常服务的能力，可以用系统可用时间占总时间的比值来衡量，4 个 9 的可用性表示系统 99.99% 的时间是可用的。\n在可用性条件下，要求系统提供的服务一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。\n分区容忍性 # 网络分区指分布式系统中的节点被划分为多个区域，每个区域内部可以通信，但是区域之间无法通信。\n在分区容忍性条件下，分布式系统在遇到任何网络分区故障的时候，仍然需要能对外提供一致性和可用性的服务，除非是整个网络环境都发生了故障。\n权衡 # 在分布式系统中，分区容忍性必不可少，因为需要总是假设网络是不可靠的。因此，CAP 理论实际上是要在可用性和一致性之间做权衡。\n可用性和一致性往往是冲突的，很难使它们同时满足。在多个节点之间进行数据同步时，\n为了保证一致性（CP），不能访问未同步完成的节点，也就失去了部分可用性； 为了保证可用性（AP），允许读取所有节点的数据，但是数据可能不一致。 四、BASE # BASE 是基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）三个短语的缩写。\nBASE 理论是对 CAP 中一致性和可用性权衡的结果，它的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。\n基本可用 # 指分布式系统在出现故障的时候，保证核心可用，允许损失部分可用性。\n例如，电商在做促销时，为了保证购物系统的稳定性，部分消费者可能会被引导到一个降级的页面。\n软状态 # 指允许系统中的数据存在中间状态，并认为该中间状态不会影响系统整体可用性，即允许系统不同节点的数据副本之间进行同步的过程存在时延。\n最终一致性 # 最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能达到一致的状态。\nACID 要求强一致性，通常运用在传统的数据库系统上。而 BASE 要求最终一致性，通过牺牲强一致性来达到可用性，通常运用在大型分布式系统中。\n在实际的分布式场景中，不同业务单元和组件对一致性的要求是不同的，因此 ACID 和 BASE 往往会结合在一起使用。\n五、Paxos # 用于达成共识性问题，即对多个节点产生的值，该算法能保证只选出唯一一个值。\n主要有三类节点：\n提议者（Proposer）：提议一个值； 接受者（Acceptor）：对每个提议进行投票； 告知者（Learner）：被告知投票的结果，不参与投票过程。 执行过程 # 规定一个提议包含两个字段：[n, v]，其中 n 为序号（具有唯一性），v 为提议值。\n1. Prepare 阶段 # 下图演示了两个 Proposer 和三个 Acceptor 的系统中运行该算法的初始过程，每个 Proposer 都会向所有 Acceptor 发送 Prepare 请求。\n当 Acceptor 接收到一个 Prepare 请求，包含的提议为 [n1, v1]，并且之前还未接收过 Prepare 请求，那么发送一个 Prepare 响应，设置当前接收到的提议为 [n1, v1]，并且保证以后不会再接受序号小于 n1 的提议。\n如下图，Acceptor X 在收到 [n=2, v=8] 的 Prepare 请求时，由于之前没有接收过提议，因此就发送一个 [no previous] 的 Prepare 响应，设置当前接收到的提议为 [n=2, v=8]，并且保证以后不会再接受序号小于 2 的提议。其它的 Acceptor 类似。\n如果 Acceptor 接收到一个 Prepare 请求，包含的提议为 [n2, v2]，并且之前已经接收过提议 [n1, v1]。如果 n1 \u0026gt; n2，那么就丢弃该提议请求；否则，发送 Prepare 响应，该 Prepare 响应包含之前已经接收过的提议 [n1, v1]，设置当前接收到的提议为 [n2, v2]，并且保证以后不会再接受序号小于 n2 的提议。\n如下图，Acceptor Z 收到 Proposer A 发来的 [n=2, v=8] 的 Prepare 请求，由于之前已经接收过 [n=4, v=5] 的提议，并且 n \u0026gt; 2，因此就抛弃该提议请求；Acceptor X 收到 Proposer B 发来的 [n=4, v=5] 的 Prepare 请求，因为之前接收到的提议为 [n=2, v=8]，并且 2 \u0026lt;= 4，因此就发送 [n=2, v=8] 的 Prepare 响应，设置当前接收到的提议为 [n=4, v=5]，并且保证以后不会再接受序号小于 4 的提议。Acceptor Y 类似。\n2. Accept 阶段 # 当一个 Proposer 接收到超过一半 Acceptor 的 Prepare 响应时，就可以发送 Accept 请求。\nProposer A 接收到两个 Prepare 响应之后，就发送 [n=2, v=8] Accept 请求。该 Accept 请求会被所有 Acceptor 丢弃，因为此时所有 Acceptor 都保证不接受序号小于 4 的提议。\nProposer B 过后也收到了两个 Prepare 响应，因此也开始发送 Accept 请求。需要注意的是，Accept 请求的 v 需要取它收到的最大提议编号对应的 v 值，也就是 8。因此它发送 [n=4, v=8] 的 Accept 请求。\n3. Learn 阶段 # Acceptor 接收到 Accept 请求时，如果序号大于等于该 Acceptor 承诺的最小序号，那么就发送 Learn 提议给所有的 Learner。当 Learner 发现有大多数的 Acceptor 接收了某个提议，那么该提议的提议值就被 Paxos 选择出来。\n约束条件 # 1. 正确性 # 指只有一个提议值会生效。\n因为 Paxos 协议要求每个生效的提议被多数 Acceptor 接收，并且 Acceptor 不会接受两个不同的提议，因此可以保证正确性。\n2. 可终止性 # 指最后总会有一个提议生效。\nPaxos 协议能够让 Proposer 发送的提议朝着能被大多数 Acceptor 接受的那个提议靠拢，因此能够保证可终止性。\n六、Raft # Raft 也是分布式一致性协议，主要是用来竞选主节点。\nRaft: Understandable Distributed Consensus 单个 Candidate 的竞选 # 有三种节点：Follower、Candidate 和 Leader。Leader 会周期性的发送心跳包给 Follower。每个 Follower 都设置了一个随机的竞选超时时间，一般为 150ms~300ms，如果在这个时间内没有收到 Leader 的心跳包，就会变成 Candidate，进入竞选阶段。\n下图展示一个分布式系统的最初阶段，此时只有 Follower 没有 Leader。Node A 等待一个随机的竞选超时时间之后，没收到 Leader 发来的心跳包，因此进入竞选阶段。 此时 Node A 发送投票请求给其它所有节点。 其它节点会对请求进行回复，如果超过一半的节点回复了，那么该 Candidate 就会变成 Leader。 之后 Leader 会周期性地发送心跳包给 Follower，Follower 接收到心跳包，会重新开始计时。 多个 Candidate 竞选 # 如果有多个 Follower 成为 Candidate，并且所获得票数相同，那么就需要重新开始投票。例如下图中 Node B 和 Node D 都获得两票，需要重新开始投票。 由于每个节点设置的随机竞选超时时间不同，因此下一次再次出现多个 Candidate 并获得同样票数的概率很低。 数据同步 # 来自客户端的修改都会被传入 Leader。注意该修改还未被提交，只是写入日志中。 Leader 会把修改复制到所有 Follower。 Leader 会等待大多数的 Follower 也进行了修改，然后才将修改提交。 此时 Leader 会通知的所有 Follower 让它们也提交修改，此时所有节点的值达成一致。 参考 # 倪超. 从 Paxos 到 ZooKeeper : 分布式一致性原理与实践 [M]. 电子工业出版社, 2015. Distributed locks with Redis 浅谈分布式锁 基于 Zookeeper 的分布式锁 聊聊分布式事务，再说说解决方案 分布式系统的事务处理 深入理解分布式事务 What is CAP theorem in distributed database system? NEAT ALGORITHMS - PAXOS Paxos By Example "},{"id":9,"href":"/keeplearning/docs/coding-practice/git/","title":"Git","section":"编码实践","content":" Git # 集中式与分布式 # Git 属于分布式版本控制系统，而 SVN 属于集中式。\n集中式版本控制只有中心服务器拥有一份代码，而分布式版本控制每个人的电脑上就有一份完整的代码。\n集中式版本控制有安全性问题，当中心服务器挂了所有人都没办法工作了。\n集中式版本控制需要连网才能工作，如果网速过慢，那么提交一个文件会慢的无法让人忍受。而分布式版本控制不需要连网就能工作。\n分布式版本控制新建分支、合并分支操作速度非常快，而集中式版本控制新建一个分支相当于复制一份完整代码。\n中心服务器 # 中心服务器用来交换每个用户的修改，没有中心服务器也能工作，但是中心服务器能够 24 小时保持开机状态，这样就能更方便的交换修改。\nGithub 就是一个中心服务器。\n工作流 # 新建一个仓库之后，当前目录就成为了工作区，工作区下有一个隐藏目录 .git，它属于 Git 的版本库。\nGit 的版本库有一个称为 Stage 的暂存区以及最后的 History 版本库，History 存储所有分支信息，使用一个 HEAD 指针指向当前分支。\ngit add files 把文件的修改添加到暂存区 git commit 把暂存区的修改提交到当前分支，提交之后暂存区就被清空了 git reset \u0026ndash; files 使用当前分支上的修改覆盖暂存区，用来撤销最后一次 git add files git checkout \u0026ndash; files 使用暂存区的修改覆盖工作目录，用来撤销本地修改 可以跳过暂存区域直接从分支中取出修改，或者直接提交修改到分支中。\ngit commit -a 直接把所有文件的修改添加到暂存区然后执行提交 git checkout HEAD \u0026ndash; files 取出最后一次修改，可以用来进行回滚操作 分支实现 # 使用指针将每个提交连接成一条时间线，HEAD 指针指向当前分支指针。\n新建分支是新建一个指针指向时间线的最后一个节点，并让 HEAD 指针指向新分支，表示新分支成为当前分支。\n每次提交只会让当前分支指针向前移动，而其它分支指针不会移动。\n合并分支也只需要改变指针即可。\n冲突 # 当两个分支都对同一个文件的同一行进行了修改，在分支合并时就会产生冲突。\nGit 会使用 \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; ，======= ，\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; 标记出不同分支的内容，只需要把不同分支中冲突部分修改成一样就能解决冲突。\n\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD Creating a new branch is quick \u0026amp; simple. ======= Creating a new branch is quick AND simple. \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; feature1 Fast forward # \u0026ldquo;快进式合并\u0026rdquo;（fast-farward merge），会直接将 master 分支指向合并的分支，这种模式下进行分支合并会丢失分支信息，也就不能在分支历史上看出分支信息。\n可以在合并时加上 \u0026ndash;no-ff 参数来禁用 Fast forward 模式，并且加上 -m 参数让合并时产生一个新的 commit。\n$ git merge --no-ff -m \u0026#34;merge with no-ff\u0026#34; dev 储藏（Stashing） # 在一个分支上操作之后，如果还没有将修改提交到分支上，此时进行切换分支，那么另一个分支上也能看到新的修改。这是因为所有分支都共用一个工作区的缘故。\n可以使用 git stash 将当前分支的修改储藏起来，此时当前工作区的所有修改都会被存到栈中，也就是说当前工作区是干净的，没有任何未提交的修改。此时就可以安全的切换到其它分支上了。\n$ git stash Saved working directory and index state \\ \u0026#34;WIP on master: 049d078 added the index file\u0026#34; HEAD is now at 049d078 added the index file (To restore them type \u0026#34;git stash apply\u0026#34;) 该功能可以用于 bug 分支的实现。如果当前正在 dev 分支上进行开发，但是此时 master 上有个 bug 需要修复，但是 dev 分支上的开发还未完成，不想立即提交。在新建 bug 分支并切换到 bug 分支之前就需要使用 git stash 将 dev 分支的未提交修改储藏起来。\nSSH 传输设置 # Git 仓库和 Github 中心仓库之间的传输是通过 SSH 加密。\n如果工作区下没有 .ssh 目录，或者该目录下没有 id_rsa 和 id_rsa.pub 这两个文件，可以通过以下命令来创建 SSH Key：\n$ ssh-keygen -t rsa -C \u0026#34;youremail@example.com\u0026#34; 然后把公钥 id_rsa.pub 的内容复制到 Github \u0026ldquo;Account settings\u0026rdquo; 的 SSH Keys 中。\n.gitignore 文件 # 忽略以下文件：\n操作系统自动生成的文件，比如缩略图； 编译生成的中间文件，比如 Java 编译产生的 .class 文件； 自己的敏感信息，比如存放口令的配置文件。 不需要全部自己编写，可以到 https://github.com/github/gitignore 中进行查询。\nGit 命令一览 # 比较详细的地址：http://www.cheat-sheets.org/saved-copy/git-cheat-sheet.pdf\n参考资料 # Git - 简明指南 图解 Git 廖雪峰 : Git 教程 Learn Git Branching "},{"id":10,"href":"/keeplearning/docs/system-design/cluster/","title":"集群","section":"系统设计","content":" 集群 # 一、负载均衡 # 集群中的应用服务器（节点）通常被设计成无状态，用户可以请求任何一个节点。\n负载均衡器会根据集群中每个节点的负载情况，将用户请求转发到合适的节点上。\n负载均衡器可以用来实现高可用以及伸缩性：\n高可用：当某个节点故障时，负载均衡器会将用户请求转发到另外的节点上，从而保证所有服务持续可用； 伸缩性：根据系统整体负载情况，可以很容易地添加或移除节点。 负载均衡器运行过程包含两个部分：\n根据负载均衡算法得到转发的节点； 进行转发。 负载均衡算法 # 1. 轮询（Round Robin） # 轮询算法把每个请求轮流发送到每个服务器上。\n下图中，一共有 6 个客户端产生了 6 个请求，这 6 个请求按 (1, 2, 3, 4, 5, 6) 的顺序发送。(1, 3, 5) 的请求会被发送到服务器 1，(2, 4, 6) 的请求会被发送到服务器 2。\n该算法比较适合每个服务器的性能差不多的场景，如果有性能存在差异的情况下，那么性能较差的服务器可能无法承担过大的负载（下图的 Server 2）。\n2. 加权轮询（Weighted Round Robbin） # 加权轮询是在轮询的基础上，根据服务器的性能差异，为服务器赋予一定的权值，性能高的服务器分配更高的权值。\n例如下图中，服务器 1 被赋予的权值为 5，服务器 2 被赋予的权值为 1，那么 (1, 2, 3, 4, 5) 请求会被发送到服务器 1，(6) 请求会被发送到服务器 2。\n3. 最少连接（least Connections） # 由于每个请求的连接时间不一样，使用轮询或者加权轮询算法的话，可能会让一台服务器当前连接数过大，而另一台服务器的连接过小，造成负载不均衡。\n例如下图中，(1, 3, 5) 请求会被发送到服务器 1，但是 (1, 3) 很快就断开连接，此时只有 (5) 请求连接服务器 1；(2, 4, 6) 请求被发送到服务器 2，只有 (2) 的连接断开，此时 (6, 4) 请求连接服务器 2。该系统继续运行时，服务器 2 会承担过大的负载。\n最少连接算法就是将请求发送给当前最少连接数的服务器上。\n例如下图中，服务器 1 当前连接数最小，那么新到来的请求 6 就会被发送到服务器 1 上。\n4. 加权最少连接（Weighted Least Connection） # 在最少连接的基础上，根据服务器的性能为每台服务器分配权重，再根据权重计算出每台服务器能处理的连接数。\n5. 随机算法（Random） # 把请求随机发送到服务器上。\n和轮询算法类似，该算法比较适合服务器性能差不多的场景。\n6. 源地址哈希法 (IP Hash) # 源地址哈希通过对客户端 IP 计算哈希值之后，再对服务器数量取模得到目标服务器的序号。\n可以保证同一 IP 的客户端的请求会转发到同一台服务器上，用来实现会话粘滞（Sticky Session）\n转发实现 # 1. HTTP 重定向 # HTTP 重定向负载均衡服务器使用某种负载均衡算法计算得到服务器的 IP 地址之后，将该地址写入 HTTP 重定向报文中，状态码为 302。客户端收到重定向报文之后，需要重新向服务器发起请求。\n缺点：\n需要两次请求，因此访问延迟比较高； HTTP 负载均衡器处理能力有限，会限制集群的规模。 该负载均衡转发的缺点比较明显，实际场景中很少使用它。\n2. DNS 域名解析 # 在 DNS 解析域名的同时使用负载均衡算法计算服务器 IP 地址。\n优点：\nDNS 能够根据地理位置进行域名解析，返回离用户最近的服务器 IP 地址。 缺点：\n由于 DNS 具有多级结构，每一级的域名记录都可能被缓存，当下线一台服务器需要修改 DNS 记录时，需要过很长一段时间才能生效。 大型网站基本使用了 DNS 做为第一级负载均衡手段，然后在内部使用其它方式做第二级负载均衡。也就是说，域名解析的结果为内部的负载均衡服务器 IP 地址。\n3. 反向代理服务器 # 反向代理服务器位于源服务器前面，用户的请求需要先经过反向代理服务器才能到达源服务器。反向代理可以用来进行缓存、日志记录等，同时也可以用来做为负载均衡服务器。\n在这种负载均衡转发方式下，客户端不直接请求源服务器，因此源服务器不需要外部 IP 地址，而反向代理需要配置内部和外部两套 IP 地址。\n优点：\n与其它功能集成在一起，部署简单。 缺点：\n所有请求和响应都需要经过反向代理服务器，它可能会成为性能瓶颈。 4. 网络层 # 在操作系统内核进程获取网络数据包，根据负载均衡算法计算源服务器的 IP 地址，并修改请求数据包的目的 IP 地址，最后进行转发。\n源服务器返回的响应也需要经过负载均衡服务器，通常是让负载均衡服务器同时作为集群的网关服务器来实现。\n优点：\n在内核进程中进行处理，性能比较高。 缺点：\n和反向代理一样，所有的请求和响应都经过负载均衡服务器，会成为性能瓶颈。 5. 链路层 # 在链路层根据负载均衡算法计算源服务器的 MAC 地址，并修改请求数据包的目的 MAC 地址，并进行转发。\n通过配置源服务器的虚拟 IP 地址和负载均衡服务器的 IP 地址一致，从而不需要修改 IP 地址就可以进行转发。也正因为 IP 地址一样，所以源服务器的响应不需要转发回负载均衡服务器，可以直接转发给客户端，避免了负载均衡服务器的成为瓶颈。\n这是一种三角传输模式，被称为直接路由。对于提供下载和视频服务的网站来说，直接路由避免了大量的网络传输数据经过负载均衡服务器。\n这是目前大型网站使用最广负载均衡转发方式，在 Linux 平台可以使用的负载均衡服务器为 LVS（Linux Virtual Server）。\n参考：\nComparing Load Balancing Algorithms Redirection and Load Balancing 二、集群下的 Session 管理 # 一个用户的 Session 信息如果存储在一个服务器上，那么当负载均衡器把用户的下一个请求转发到另一个服务器，由于服务器没有用户的 Session 信息，那么该用户就需要重新进行登录等操作。\nSticky Session # 需要配置负载均衡器，使得一个用户的所有请求都路由到同一个服务器，这样就可以把用户的 Session 存放在该服务器中。\n缺点：\n当服务器宕机时，将丢失该服务器上的所有 Session。 Session Replication # 在服务器之间进行 Session 同步操作，每个服务器都有所有用户的 Session 信息，因此用户可以向任何一个服务器进行请求。\n缺点：\n占用过多内存； 同步过程占用网络带宽以及服务器处理器时间。 Session Server # 使用一个单独的服务器存储 Session 数据，可以使用传统的 MySQL，也使用 Redis 或者 Memcached 这种内存型数据库。\n优点：\n为了使得大型网站具有伸缩性，集群中的应用服务器通常需要保持无状态，那么应用服务器不能存储用户的会话信息。Session Server 将用户的会话信息单独进行存储，从而保证了应用服务器的无状态。 缺点：\n需要去实现存取 Session 的代码。 "},{"id":11,"href":"/keeplearning/docs/mysql/dbs-theory/","title":"数据库系统原理","section":"MySQL","content":" 数据库系统原理 # 一、事务 # 概念 # 事务指的是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。\nACID # 1. 原子性（Atomicity） # 事务被视为不可分割的最小单元，事务的所有操作要么全部提交成功，要么全部失败回滚。\n回滚可以用回滚日志（Undo Log）来实现，回滚日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。\n2. 一致性（Consistency） # 数据库在事务执行前后都保持一致性状态。在一致性状态下，所有事务对同一个数据的读取结果都是相同的。\n3. 隔离性（Isolation） # 一个事务所做的修改在最终提交以前，对其它事务是不可见的。\n4. 持久性（Durability） # 一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。\n系统发生崩溃可以用重做日志（Redo Log）进行恢复，从而实现持久性。与回滚日志记录数据的逻辑修改不同，重做日志记录的是数据页的物理修改。\n事务的 ACID 特性概念简单，但不是很好理解，主要是因为这几个特性不是一种平级关系：\n只有满足一致性，事务的执行结果才是正确的。 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时只要能满足原子性，就一定能满足一致性。 在并发的情况下，多个事务并行执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。 事务满足持久化是为了能应对系统崩溃的情况。 AUTOCOMMIT # MySQL 默认采用自动提交模式。也就是说，如果不显式使用START TRANSACTION语句来开始一个事务，那么每个查询操作都会被当做一个事务并自动提交。\n二、并发一致性问题 # 在并发环境下，事务的隔离性很难保证，因此会出现很多并发一致性问题。\n丢失修改 # 丢失修改指一个事务的更新操作被另外一个事务的更新操作替换。一般在现实生活中常会遇到，例如：T1 和 T2 两个事务都对一个数据进行修改，T1 先修改并提交生效，T2 随后修改，T2 的修改覆盖了 T1 的修改。\n读脏数据 # 读脏数据指在不同的事务下，当前事务可以读到另外事务未提交的数据。例如：T1 修改一个数据但未提交，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。\n不可重复读 # 不可重复读指在一个事务内多次读取同一数据集合。在这一事务还未结束前，另一事务也访问了该同一数据集合并做了修改，由于第二个事务的修改，第一次事务的两次读取的数据可能不一致。例如：T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。\n幻影读 # 幻读本质上也属于不可重复读的情况，T1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。\n产生并发不一致性问题的主要原因是破坏了事务的隔离性，解决方法是通过并发控制来保证隔离性。并发控制可以通过封锁来实现，但是封锁操作需要用户自己控制，相当复杂。数据库管理系统提供了事务的隔离级别，让用户以一种更轻松的方式处理并发一致性问题。\n三、封锁 # 封锁粒度 # MySQL 中提供了两种封锁粒度：行级锁以及表级锁。\n应该尽量只锁定需要修改的那部分数据，而不是所有的资源。锁定的数据量越少，发生锁争用的可能就越小，系统的并发程度就越高。\n但是加锁需要消耗资源，锁的各种操作（包括获取锁、释放锁、以及检查锁状态）都会增加系统开销。因此封锁粒度越小，系统开销就越大。\n在选择封锁粒度时，需要在锁开销和并发程度之间做一个权衡。\n封锁类型 # 1. 读写锁 # 互斥锁（Exclusive），简写为 X 锁，又称写锁。 共享锁（Shared），简写为 S 锁，又称读锁。 有以下两个规定：\n一个事务对数据对象 A 加了 X 锁，就可以对 A 进行读取和更新。加锁期间其它事务不能对 A 加任何锁。 一个事务对数据对象 A 加了 S 锁，可以对 A 进行读取操作，但是不能进行更新操作。加锁期间其它事务能对 A 加 S 锁，但是不能加 X 锁。 锁的兼容关系如下：\n2. 意向锁 # 使用意向锁（Intention Locks）可以更容易地支持多粒度封锁。\n在存在行级锁和表级锁的情况下，事务 T 想要对表 A 加 X 锁，就需要先检测是否有其它事务对表 A 或者表 A 中的任意一行加了锁，那么就需要对表 A 的每一行都检测一次，这是非常耗时的。\n意向锁在原来的 X/S 锁之上引入了 IX/IS，IX/IS 都是表锁，用来表示一个事务想要在表中的某个数据行上加 X 锁或 S 锁。有以下两个规定：\n一个事务在获得某个数据行对象的 S 锁之前，必须先获得表的 IS 锁或者更强的锁； 一个事务在获得某个数据行对象的 X 锁之前，必须先获得表的 IX 锁。 通过引入意向锁，事务 T 想要对表 A 加 X 锁，只需要先检测是否有其它事务对表 A 加了 X/IX/S/IS 锁，如果加了就表示有其它事务正在使用这个表或者表中某一行的锁，因此事务 T 加 X 锁失败。\n各种锁的兼容关系如下：\n解释如下：\n任意 IS/IX 锁之间都是兼容的，因为它们只表示想要对表加锁，而不是真正加锁； 这里兼容关系针对的是表级锁，而表级的 IX 锁和行级的 X 锁兼容，两个事务可以对两个数据行加 X 锁。（事务 T1 想要对数据行 R1 加 X 锁，事务 T2 想要对同一个表的数据行 R2 加 X 锁，两个事务都需要对该表加 IX 锁，但是 IX 锁是兼容的，并且 IX 锁与行级的 X 锁也是兼容的，因此两个事务都能加锁成功，对同一个表中的两个数据行做修改。） 封锁协议 # 1. 三级封锁协议 # 一级封锁协议\n事务 T 要修改数据 A 时必须加 X 锁，直到 T 结束才释放锁。\n可以解决丢失修改问题，因为不能同时有两个事务对同一个数据进行修改，那么事务的修改就不会被覆盖。\n二级封锁协议\n在一级的基础上，要求读取数据 A 时必须加 S 锁，读取完马上释放 S 锁。\n可以解决读脏数据问题，因为如果一个事务在对数据 A 进行修改，根据 1 级封锁协议，会加 X 锁，那么就不能再加 S 锁了，也就是不会读入数据。\n三级封锁协议\n在二级的基础上，要求读取数据 A 时必须加 S 锁，直到事务结束了才能释放 S 锁。\n可以解决不可重复读的问题，因为读 A 时，其它事务不能对 A 加 X 锁，从而避免了在读的期间数据发生改变。\n2. 两段锁协议 # 加锁和解锁分为两个阶段进行。\n可串行化调度是指，通过并发控制，使得并发执行的事务结果与某个串行执行的事务结果相同。串行执行的事务互不干扰，不会出现并发一致性问题。\n事务遵循两段锁协议是保证可串行化调度的充分条件。例如以下操作满足两段锁协议，它是可串行化调度。\nlock-x(A)...lock-s(B)...lock-s(C)...unlock(A)...unlock(C)...unlock(B) 但不是必要条件，例如以下操作不满足两段锁协议，但它还是可串行化调度。\nlock-x(A)...unlock(A)...lock-s(B)...unlock(B)...lock-s(C)...unlock(C) MySQL 隐式与显式锁定 # MySQL 的 InnoDB 存储引擎采用两段锁协议，会根据隔离级别在需要的时候自动加锁，并且所有的锁都是在同一时刻被释放，这被称为隐式锁定。\nInnoDB 也可以使用特定的语句进行显示锁定：\nSELECT ... LOCK In SHARE MODE; SELECT ... FOR UPDATE; 四、隔离级别 # 未提交读（READ UNCOMMITTED） # 事务中的修改，即使没有提交，对其它事务也是可见的。\n提交读（READ COMMITTED） # 一个事务只能读取已经提交的事务所做的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。\n可重复读（REPEATABLE READ） # 保证在同一个事务中多次读取同一数据的结果是一样的。\n可串行化（SERIALIZABLE） # 强制事务串行执行，这样多个事务互不干扰，不会出现并发一致性问题。\n该隔离级别需要加锁实现，因为要使用加锁机制保证同一时间只有一个事务执行，也就是保证事务串行执行。\n五、多版本并发控制 # 多版本并发控制（Multi-Version Concurrency Control, MVCC）是 MySQL 的 InnoDB 存储引擎实现隔离级别的一种具体方式，用于实现提交读和可重复读这两种隔离级别。而未提交读隔离级别总是读取最新的数据行，要求很低，无需使用 MVCC。可串行化隔离级别需要对所有读取的行都加锁，单纯使用 MVCC 无法实现。\n基本思想 # 在封锁一节中提到，加锁能解决多个事务同时执行时出现的并发一致性问题。在实际场景中读操作往往多于写操作，因此又引入了读写锁来避免不必要的加锁操作，例如读和读没有互斥关系。读写锁中读和写操作仍然是互斥的，而 MVCC 利用了多版本的思想，写操作更新最新的版本快照，而读操作去读旧版本快照，没有互斥关系，这一点和 CopyOnWrite 类似。\n在 MVCC 中事务的修改操作（DELETE、INSERT、UPDATE）会为数据行新增一个版本快照。\n脏读和不可重复读最根本的原因是事务读取到其它事务未提交的修改。在事务进行读取操作时，为了解决脏读和不可重复读问题，MVCC 规定只能读取已经提交的快照。当然一个事务可以读取自身未提交的快照，这不算是脏读。\n版本号 # 系统版本号 SYS_ID：是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增。 事务版本号 TRX_ID ：事务开始时的系统版本号。 Undo 日志 # MVCC 的多版本指的是多个版本的快照，快照存储在 Undo 日志中，该日志通过回滚指针 ROLL_PTR 把一个数据行的所有快照连接起来。\n例如在 MySQL 创建一个表 t，包含主键 id 和一个字段 x。我们先插入一个数据行，然后对该数据行执行两次更新操作。\nINSERT INTO t(id, x) VALUES(1, \u0026#34;a\u0026#34;); UPDATE t SET x=\u0026#34;b\u0026#34; WHERE id=1; UPDATE t SET x=\u0026#34;c\u0026#34; WHERE id=1; 因为没有使用 START TRANSACTION 将上面的操作当成一个事务来执行，根据 MySQL 的 AUTOCOMMIT 机制，每个操作都会被当成一个事务来执行，所以上面的操作总共涉及到三个事务。快照中除了记录事务版本号 TRX_ID 和操作之外，还记录了一个 bit 的 DEL 字段，用于标记是否被删除。\nINSERT、UPDATE、DELETE 操作会创建一个日志，并将事务版本号 TRX_ID 写入。DELETE 可以看成是一个特殊的 UPDATE，还会额外将 DEL 字段设置为 1。\nReadView # MVCC 维护了一个 ReadView 结构，主要包含了当前系统未提交的事务列表 TRX_IDs {TRX_ID_1, TRX_ID_2, \u0026hellip;}，还有该列表的最小值 TRX_ID_MIN 和 TRX_ID_MAX。\n在进行 SELECT 操作时，根据数据行快照的 TRX_ID 与 TRX_ID_MIN 和 TRX_ID_MAX 之间的关系，从而判断数据行快照是否可以使用：\nTRX_ID \u0026lt; TRX_ID_MIN，表示该数据行快照时在当前所有未提交事务之前进行更改的，因此可以使用。\nTRX_ID \u0026gt; TRX_ID_MAX，表示该数据行快照是在事务启动之后被更改的，因此不可使用。\nTRX_ID_MIN \u0026lt;= TRX_ID \u0026lt;= TRX_ID_MAX，需要根据隔离级别再进行判断：\n提交读：如果 TRX_ID 在 TRX_IDs 列表中，表示该数据行快照对应的事务还未提交，则该快照不可使用。否则表示已经提交，可以使用。 可重复读：都不可以使用。因为如果可以使用的话，那么其它事务也可以读到这个数据行快照并进行修改，那么当前事务再去读这个数据行得到的值就会发生改变，也就是出现了不可重复读问题。 在数据行快照不可使用的情况下，需要沿着 Undo Log 的回滚指针 ROLL_PTR 找到下一个快照，再进行上面的判断。\n快照读与当前读 # 1. 快照读 # MVCC 的 SELECT 操作是快照中的数据，不需要进行加锁操作。\nSELECT * FROM table ...; 2. 当前读 # MVCC 其它会对数据库进行修改的操作（INSERT、UPDATE、DELETE）需要进行加锁操作，从而读取最新的数据。可以看到 MVCC 并不是完全不用加锁，而只是避免了 SELECT 的加锁操作。\nINSERT; UPDATE; DELETE; 在进行 SELECT 操作时，可以强制指定进行加锁操作。以下第一个语句需要加 S 锁，第二个需要加 X 锁。\nSELECT * FROM table WHERE ? lock in share mode; SELECT * FROM table WHERE ? for update; 六、Next-Key Locks # Next-Key Locks 是 MySQL 的 InnoDB 存储引擎的一种锁实现。\nMVCC 不能解决幻影读问题，Next-Key Locks 就是为了解决这个问题而存在的。在可重复读（REPEATABLE READ）隔离级别下，使用 MVCC + Next-Key Locks 可以解决幻读问题。\nRecord Locks # 锁定一个记录上的索引，而不是记录本身。\n如果表没有设置索引，InnoDB 会自动在主键上创建隐藏的聚簇索引，因此 Record Locks 依然可以使用。\nGap Locks # 锁定索引之间的间隙，但是不包含索引本身。例如当一个事务执行以下语句，其它事务就不能在 t.c 中插入 15。\nSELECT c FROM t WHERE c BETWEEN 10 and 20 FOR UPDATE; Next-Key Locks # 它是 Record Locks 和 Gap Locks 的结合，不仅锁定一个记录上的索引，也锁定索引之间的间隙。它锁定一个前开后闭区间，例如一个索引包含以下值：10, 11, 13, and 20，那么就需要锁定以下区间：\n(-∞, 10] (10, 11] (11, 13] (13, 20] (20, +∞) 七、关系数据库设计理论 # 函数依赖 # 记 A-\u0026gt;B 表示 A 函数决定 B，也可以说 B 函数依赖于 A。\n如果 {A1，A2，\u0026hellip; ，An} 是关系的一个或多个属性的集合，该集合函数决定了关系的其它所有属性并且是最小的，那么该集合就称为键码。\n对于 A-\u0026gt;B，如果能找到 A 的真子集 A\u0026rsquo;，使得 A\u0026rsquo;-\u0026gt; B，那么 A-\u0026gt;B 就是部分函数依赖，否则就是完全函数依赖。\n对于 A-\u0026gt;B，B-\u0026gt;C，则 A-\u0026gt;C 是一个传递函数依赖。\n异常 # 以下的学生课程关系的函数依赖为 {Sno, Cname} -\u0026gt; {Sname, Sdept, Mname, Grade}，键码为 {Sno, Cname}。也就是说，确定学生和课程之后，就能确定其它信息。\nSno Sname Sdept Mname Cname Grade 1 学生-1 学院-1 院长-1 课程-1 90 2 学生-2 学院-2 院长-2 课程-2 80 2 学生-2 学院-2 院长-2 课程-1 100 3 学生-3 学院-2 院长-2 课程-2 95 不符合范式的关系，会产生很多异常，主要有以下四种异常：\n冗余数据：例如 学生-2 出现了两次。 修改异常：修改了一个记录中的信息，但是另一个记录中相同的信息却没有被修改。 删除异常：删除一个信息，那么也会丢失其它信息。例如删除了 课程-1 需要删除第一行和第三行，那么 学生-1 的信息就会丢失。 插入异常：例如想要插入一个学生的信息，如果这个学生还没选课，那么就无法插入。 范式 # 范式理论是为了解决以上提到四种异常。\n高级别范式的依赖于低级别的范式，1NF 是最低级别的范式。\n1. 第一范式 (1NF) # 属性不可分。\n2. 第二范式 (2NF) # 每个非主属性完全函数依赖于键码。\n可以通过分解来满足。\n分解前 Sno Sname Sdept Mname Cname Grade 1 学生-1 学院-1 院长-1 课程-1 90 2 学生-2 学院-2 院长-2 课程-2 80 2 学生-2 学院-2 院长-2 课程-1 100 3 学生-3 学院-2 院长-2 课程-2 95 以上学生课程关系中，{Sno, Cname} 为键码，有如下函数依赖：\nSno -\u0026gt; Sname, Sdept Sdept -\u0026gt; Mname Sno, Cname-\u0026gt; Grade Grade 完全函数依赖于键码，它没有任何冗余数据，每个学生的每门课都有特定的成绩。\nSname, Sdept 和 Mname 都部分依赖于键码，当一个学生选修了多门课时，这些数据就会出现多次，造成大量冗余数据。\n分解后 关系-1\nSno Sname Sdept Mname 1 学生-1 学院-1 院长-1 2 学生-2 学院-2 院长-2 3 学生-3 学院-2 院长-2 有以下函数依赖：\nSno -\u0026gt; Sname, Sdept Sdept -\u0026gt; Mname 关系-2\nSno Cname Grade 1 课程-1 90 2 课程-2 80 2 课程-1 100 3 课程-2 95 有以下函数依赖：\nSno, Cname -\u0026gt; Grade 3. 第三范式 (3NF) # 非主属性不传递函数依赖于键码。\n上面的 关系-1 中存在以下传递函数依赖：\nSno -\u0026gt; Sdept -\u0026gt; Mname 可以进行以下分解：\n关系-11\nSno Sname Sdept 1 学生-1 学院-1 2 学生-2 学院-2 3 学生-3 学院-2 关系-12\nSdept Mname 学院-1 院长-1 学院-2 院长-2 八、ER 图 # Entity-Relationship，有三个组成部分：实体、属性、联系。\n用来进行关系型数据库系统的概念设计。\n实体的三种联系 # 包含一对一，一对多，多对多三种。\n如果 A 到 B 是一对多关系，那么画个带箭头的线段指向 B； 如果是一对一，画两个带箭头的线段； 如果是多对多，画两个不带箭头的线段。 下图的 Course 和 Student 是一对多的关系。\n表示出现多次的关系 # 一个实体在联系出现几次，就要用几条线连接。\n下图表示一个课程的先修关系，先修关系出现两个 Course 实体，第一个是先修课程，后一个是后修课程，因此需要用两条线来表示这种关系。\n联系的多向性 # 虽然老师可以开设多门课，并且可以教授多名学生，但是对于特定的学生和课程，只有一个老师教授，这就构成了一个三元联系。\n表示子类 # 用一个三角形和两条线来连接类和子类，与子类有关的属性和联系都连到子类上，而与父类和子类都有关的连到父类上。\n参考资料 # AbrahamSilberschatz, HenryF.Korth, S.Sudarshan, 等. 数据库系统概念 [M]. 机械工业出版社, 2006. 施瓦茨. 高性能 MYSQL(第3版)[M]. 电子工业出版社, 2013. 史嘉权. 数据库系统概论[M]. 清华大学出版社有限公司, 2006. The InnoDB Storage Engine Transaction isolation levels Concurrency Control The Nightmare of Locking, Blocking and Isolation Levels! Database Normalization and Normal Forms with an Example The basics of the InnoDB undo logging and history system MySQL locking for the busy web developer 浅入浅出 MySQL 和 InnoDB Innodb 中的事务隔离级别和锁的关系 "},{"id":12,"href":"/keeplearning/docs/mysql/faq1/","title":"常见问题（一）","section":"MySQL","content":" MySQL 常见问题（一） # 1. 出现 Unknown column 错误是在哪个阶段 # MySQL 会在分析器中判断语句是否正确，表是否存在，列是否存在等，如果没有问题再提交给优化器。\n2. 为什么对权限的检查不在优化器之前做 # 有些时候，SQL 语句要操作的表不只是 SQL 字面上那些。比如有个触发器，得在执行器阶段（过程中）才能确定，优化器阶段前是无能为力的。\n客户端连接时，如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限，之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。一个用户成功建立连接后，即使用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。\n3. 一个查询语句的执行流程是什么 # 一条查询语句的执行过程一般是经过连接器、分析器、优化器、执行器等功能模块，最后到达存储引擎。\n4. bin log、redo log、undo log 的区别 # redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。\nredo log 是物理日志，记录的是「在某个数据页上做了什么修改」；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如「给 ID=2 这一行的 c 字段加 1」。\nredo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。\nInnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写，如下图所示\nwrite pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。擦除后“空着的部分”就是 write pos 到 3 号文件末尾，再加上 0 号文件开头到 checkpoint 的部分。\n当 redo log 写满了时，这时候系统会停止所有更新操作，把 checkpoint 往前推进，redo log 留出空间可以继续写。\ncheckpoint 可不是随便往前修改一下位置就可以的。如上图中，把 checkpoint 位置从 CP 推进到 CP’，就需要将两个点之间的日志（浅绿色部分），对应的所有脏页都 flush 到磁盘上。之后，图中从 write pos 到 CP’ 之间就是可以再写入的 redo log 的区域。\n当内存数据页跟磁盘数据页内容不一致的时候，称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。\n5. MySQL “N叉树”中 N 的值是否可以被人工调整 # 通过改变 key 值来调整。N 叉树中非叶子节点存放的是索引信息，索引包含 Key 和 Point 指针。Point 指针固定为 6 个字节，假如 Key 为 10 个字节，那么单个索引就是 16 个字节。如果 B + 树中页大小为 16 K，那么一个页就可以存储 1024 个索引，此时 N 就等于 1024。我们通过改变 Key 的大小，就可以改变 N 的值。\n改变页的大小。页越大，一页存放的索引就越多，N 就越大。数据页调整后，如果数据页太小层数就会太深，数据页太大，加载到内存的时间和单个数据页查询时间会提高，需要达到平衡才行。\n6. 什么是长事务，为什么尽量避免使用长事务 # 长事务就是运行时间比较长，长时间未提交的事务，也可以称之为大事务。\nMySQL 在回滚操作的时候需要用到 undo log（回滚日志），假设一个值从1被按顺序改成了2、3、4，在回滚日志里面就会有类似下面的记录。\n回滚日志不会一直保留，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。而长事务意味着系统里面会存在很老的事务视图，由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。除此之外，长事务还可能会占用锁资源。\n7. 什么是索引下推 # 如果某张表存在联合索引（name, age），现在有一个需求：检索出表中“名字第一个字是张，而且年龄是10岁的所有男孩”。那么，SQL 语句是这么写的：\nselect * from tuser where name like \u0026#39;张%\u0026#39; and age=10 and ismale=1; 由于存在前缀索引规则，所以这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足条件的记录 ID3。在 MySQL5.6 之前，只能从 ID3 开始一个个回表。到主键索引上找出数据行，再对比字段值。MySQL5.6 引入的索引下推优化（index condition pushdown)，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。\n![](https://cdn.xiaobinqt.cn/xiaobinqt.io/20221228/0652864742e542878a6c76e29a18dc08.png) ![](https://cdn.xiaobinqt.cn/xiaobinqt.io/20221228/0652864742e542878a6c76e29a18dc08.png) 如上这两个图里面，每一个虚线箭头表示回表一次。\n左图中，在（name，age）索引里面特意去掉了 age 的值，这个过程 InnoDB 并不会去看 age 的值，只是按顺序把「name第一个字是『张』」的记录一条条取出来回表。因此，需要回表 4 次。\n右图跟左图的区别是，InnoDB 在（name，age）索引内部就判断了 age 是否等于 10，对于不等于 10 的记录，直接判断并跳过。在这个例子中，只需要对 ID4、ID5 这两条记录回表取数据判断，就只需要回表 2 次。\n8. MySQL 中的锁 # 全局锁 # 全局锁就是对整个数据库实例加锁，MySQL 提供加全局读锁的方法Flush tables with read lock（FTWRL），这个命令可以使整个库处于只读状态。使用该命令之后，数据更新语句、数据定义语句和更新类事务的提交语句等操作都会被阻塞。\n全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都 select 出来存成文本。但是有如下风险：\n如果在主库备份，在备份期间不能更新，业务停摆； 如果在从库备份，备份期间不能执行主库同步的 binlog，导致主从延迟。 官方自带的逻辑备份工具 mysqldump，当 mysqldump 使用参数--single-transaction的时候，会启动一个事务，确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。一致性读是好，但是前提是引擎要支持这个隔离级别。\n如果要全库只读，为什么不使用set global readonly=true的方式？\n在有些系统中，readonly 的值会被用来做其他逻辑，比如判断主备库，所以修改 global 变量的方式影响太大。 在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。 表锁 # MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。\n表锁的语法是lock tables … read/write。可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。需要注意的是lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。\n比如，在某个线程 A 中执行lock tables t1 read, t2 write;这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行unlock tables之前，也只能执行读 t1、读写 t2 的操作，连写 t1 都不允许，自然也不能访问其他表。\n另一类表级的锁是 MDL（metadata lock)。MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。\n在 MySQL5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。读锁之间不互斥，因此可以有多个线程同时对一张表增删改查。读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。\n需要注意一点的是，事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。\n行锁 # 9. buffer pool 和 change buffer # change buffer 的前身是 insert buffer，只能对 insert 操作优化，后来升级了，增加了 update/delete 的支持，名字也改叫 change buffer。\n要理解 change buffer 还得先理解 buffer pool 是什么，顾名思义，硬盘在读写速度上相比内存有着数量级差距，如果每次读写都要从磁盘加载相应数据页，DB 的效率就上不来，因而为了化解这个困局，几乎所有的 DB 都会把缓存池当做标配（在内存中开辟的一整块空间，由引擎利用一些命中算法和淘汰算法负责维护和管理），change buffer 则更进一步，把在内存中更新就能可以立即返回执行结果并且满足一致性约束（显式或隐式定义的约束条件）的记录也暂时放在缓存池中，这样大大减少了磁盘 IO 操作的几率。\nInnoDB 用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态：\n还没有使用的； 使用了并且是干净页； 使用了并且是脏页。 InnoDB 的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。如果内存不够用了，这时候只能把最久不使用的数据页从内存中淘汰掉：如果要淘汰的是一个干净页，就直接释放出来复用；但如果是脏页呢，就必须将脏页先刷到磁盘，变成干净页后才能复用。\n10. 如何优化索引 # 一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好。可以使用show index方法，看到一个索引的基数。\n排序字段加上索引，因为索引已经是有序的了，如果选择索引的话，不需要再做排序，一版情况下只需要遍历去获取数据集就可以了。而对要排序的数据，扫描行数会影响查询时间，所以区分度高的数据，利用索引查询会快。\n11. 各种 count 的用法 # count()是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加。最后返回累计值。所以，count(*)、count(主键id)和count(1)都表示返回满足条件的结果集的总行数；而count(字段)，则表示返回满足条件的数据行里面，参数“字段”不为 NULL 的总个数。\n至于分析性能差别，可以有这么几个原则：\nserver 层要什么就给什么； InnoDB 只给必要的值； 现在的优化器只优化了count(*)的语义为“取行数”，其他“显而易见”的优化并没有做。\n对于count(主键id)来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。\n对于count(1)来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。\n单看这两个用法的差别能对比出来，count(1)执行得要比count(主键id)快。因为从引擎返回id会涉及到解析数据行，以及拷贝字段值的操作。\n对于count(字段)来说：\n如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加；\n如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。\n也就是前面的第一条原则，server 层要什么字段，InnoDB 就返回什么字段。\n但是count(*)是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(*)肯定不是 null，按行累加。\n无论使用哪种 count 方式，引擎都一定要逐行去读，只是在是否使用索引和是否返回给 server 层具体数据有区别。\n所以结论是：按照效率排序的话，count(字段)\u0026lt;count(主键id)\u0026lt;count(1)≈count(*)，所以建议尽量使用count(*)。\n12. order by 是如何工作的 # MySQL 会为每个线程分配一个内存（sort_buffer）用于排序，该内存大小为 sort_buffer_size。\n如果排序的数据量小于 sort_buffer_size，排序将会在内存中完成； 如果排序数据量很大，内存中无法存下这么多数据，则会使用磁盘临时文件来辅助排序，也称外部排序； 在使用外部排序时，MySQL 会分成好几份单独的临时文件用来存放排序后的数据，然后在将这些文件合并成一个大文件。 MySQL 会通过遍历索引将满足条件的数据读取到 sort_buffer，并且按照排序字段进行快速排序。\n如果查询的字段不包含在辅助索引中，需要按照辅助索引记录的主键返回聚集索引取出所需字段； 该方式会造成随机 IO，在 MySQL5.6 提供了 MRR 的机制，会将辅助索引匹配记录的主键取出来在内存中进行排序，然后在回表； 按照情况建立联合索引来避免排序所带来的性能损耗，允许的情况下也可以建立覆盖索引来避免回表。 全字段排序 # 通过索引将所需的字段全部读取到 sort_buffer 中； 按照排序字段进行排序； 将结果集返回给客户端。 缺点：\n造成 sort_buffer 中存放不下很多数据，因为除了排序字段还存放其他字段，对 sort_buffer 的利用效率不高； 当所需排序数据量很大时，会有很多的临时文件，排序性能也会很差。 优点：MySQL 认为内存足够大时会优先选择全字段排序，因为这种方式比 rowid 排序避免了一次回表操作\nrowid 排序 # 通过控制排序的行数据的长度来让 sort_buffer 中尽可能多的存放数据，max_length_for_sort_data 是 MySQL 中专门控制用于排序的行数据的长度的一个参数。它的意思是，如果单行的长度超过这个值，MySQL 就认为单行太大，要换一个算法。 只将需要排序的字段和主键读取到 sort_buffer 中，并按照排序字段进行排序； 按照排序后的顺序，取id进行回表取出想要获取的数据； 将结果集返回给客户端。 优点：更好的利用内存的 sort_buffer 进行排序操作，尽量减少对磁盘的访问\n缺点：回表的操作是随机 IO，会造成大量的随机读，不一定就比全字段排序减少对磁盘的访问\n"},{"id":13,"href":"/keeplearning/docs/coding-practice/code-readability/","title":"代码可读性","section":"编码实践","content":" 一、可读性的重要性 # 编程有很大一部分时间是在阅读代码，不仅要阅读自己的代码，而且要阅读别人的代码。因此，可读性良好的代码能够大大提高编程效率。\n可读性良好的代码往往会让代码架构更好，因为程序员更愿意去修改这部分代码，而且也更容易修改。\n只有在核心领域为了效率才可以放弃可读性，否则可读性是第一位。\n二、用名字表达代码含义 # 一些比较有表达力的单词：\n单词 可替代单词 send deliver、dispatch、announce、distribute、route find search、extract、locate、recover start launch、create、begin、open make create、set up、build、generate、compose、add、new 使用 i、j、k 作为循环迭代器的名字过于简单，user_i、member_i 这种名字会更有表达力。因为循环层次越多，代码越难理解，有表达力的迭代器名字可读性会更高。\n为名字添加形容词等信息能让名字更具有表达力，但是名字也会变长。名字长短的准则是：作用域越大，名字越长。因此只有在短作用域才能使用一些简单名字。\n三、名字不能带来歧义 # 起完名字要思考一下别人会对这个名字有何解读，会不会误解了原本想表达的含义。\n布尔相关的命名加上 is、can、should、has 等前缀。\n用 min、max 表示数量范围；\n用 first、last 表示访问空间的包含范围；\nbegin、end 表示访问空间的排除范围，即 end 不包含尾部。\n四、良好的代码风格 # 适当的空行和缩进。\n排列整齐的注释：\nint a = 1; // 注释 int b = 11; // 注释 int c = 111; // 注释 语句顺序不能随意，比如与 html 表单相关联的变量的赋值应该和表单在 html 中的顺序一致。\n五、为何编写注释 # 阅读代码首先会注意到注释，如果注释没太大作用，那么就会浪费代码阅读的时间。那些能直接看出含义的代码不需要写注释，特别是不需要为每个方法都加上注释，比如那些简单的 getter 和 setter 方法，为这些方法写注释反而让代码可读性更差。\n不能因为有注释就随便起个名字，而是争取起个好名字而不写注释。\n可以用注释来记录采用当前解决办法的思考过程，从而让读者更容易理解代码。\n注释用来提醒一些特殊情况。\n用 TODO 等做标记：\n标记 用法 TODO 待做 FIXME 待修复 HACK 粗糙的解决方案 XXX 危险！这里有重要的问题 六、如何编写注释 # 尽量简洁明了：\n// The first String is student\u0026#39;s name // The Second Integer is student\u0026#39;s score Map\u0026lt;String, Integer\u0026gt; scoreMap = new HashMap\u0026lt;\u0026gt;(); // Student\u0026#39;s name -\u0026gt; Student\u0026#39;s score Map\u0026lt;String, Integer\u0026gt; scoreMap = new HashMap\u0026lt;\u0026gt;(); 添加测试用例来说明：\n// ... // Example: add(1, 2), return 3 int add(int x, int y) { return x + y; } 使用专业名词来缩短概念上的解释，比如用设计模式名来说明代码。\n七、提高控制流的可读性 # 条件表达式中，左侧是变量，右侧是常数。比如下面第一个语句正确：\nif (len \u0026lt; 10) if (10 \u0026gt; len) 只有在逻辑简单的情况下使用 ? : 三目运算符来使代码更紧凑，否则应该拆分成 if / else；\ndo / while 的条件放在后面，不够简单明了，并且会有一些迷惑的地方，最好使用 while 来代替。\n如果只有一个 goto 目标，那么 goto 尚且还能接受，但是过于复杂的 goto 会让代码可读性特别差，应该避免使用 goto。\n在嵌套的循环中，用一些 return 语句往往能减少嵌套的层数。\n八、拆分长表达式 # 长表达式的可读性很差，可以引入一些解释变量从而拆分表达式：\nif line.split(\u0026#39;:\u0026#39;)[0].strip() == \u0026#34;root\u0026#34;: ... username = line.split(\u0026#39;:\u0026#39;)[0].strip() if username == \u0026#34;root\u0026#34;: ... 使用摩根定理简化一些逻辑表达式：\nif (!a \u0026amp;\u0026amp; !b) { ... } if (!(a || b)) { ... } 九、变量与可读性 # 去除控制流变量 。在循环中通过使用 break 或者 return 可以减少控制流变量的使用。\nboolean done = false; while (/* condition */ \u0026amp;\u0026amp; !done) { ... if ( ... ) { done = true; continue; } } while(/* condition */) { ... if ( ... ) { break; } } 减小变量作用域 。作用域越小，越容易定位到变量所有使用的地方。\nJavaScript 可以用闭包减小作用域。以下代码中 submit_form 是函数变量，submitted 变量控制函数不会被提交两次。第一个实现中 submitted 是全局变量，第二个实现把 submitted 放到匿名函数中，从而限制了起作用域范围。\nsubmitted = false; var submit_form = function(form_name) { if (submitted) { return; } submitted = true; }; var submit_form = (function() { var submitted = false; return function(form_name) { if(submitted) { return; } submitted = true; } }()); // () 使得外层匿名函数立即执行 JavaScript 中没有用 var 声明的变量都是全局变量，而全局变量很容易造成迷惑，因此应当总是用 var 来声明变量。\n变量定义的位置应当离它使用的位置最近。\n实例解析\n在一个网页中有以下文本输入字段：\n\u0026lt;input type = \u0026#34;text\u0026#34; id = \u0026#34;input1\u0026#34; value = \u0026#34;a\u0026#34;\u0026gt; \u0026lt;input type = \u0026#34;text\u0026#34; id = \u0026#34;input2\u0026#34; value = \u0026#34;b\u0026#34;\u0026gt; \u0026lt;input type = \u0026#34;text\u0026#34; id = \u0026#34;input3\u0026#34; value = \u0026#34;\u0026#34;\u0026gt; \u0026lt;input type = \u0026#34;text\u0026#34; id = \u0026#34;input4\u0026#34; value = \u0026#34;d\u0026#34;\u0026gt; 现在要接受一个字符串并把它放到第一个空的 input 字段中，初始实现如下：\nvar setFirstEmptyInput = function(new_alue) { var found = false; var i = 1; var elem = document.getElementById(\u0026#39;input\u0026#39; + i); while (elem != null) { if (elem.value === \u0026#39;\u0026#39;) { found = true; break; } i++; elem = document.getElementById(\u0026#39;input\u0026#39; + i); } if (found) elem.value = new_value; return elem; } 以上实现有以下问题：\nfound 可以去除； elem 作用域过大； 可以用 for 循环代替 while 循环； var setFirstEmptyInput = function(new_value) { for (var i = 1; true; i++) { var elem = document.getElementById(\u0026#39;input\u0026#39; + i); if (elem === null) { return null; } if (elem.value === \u0026#39;\u0026#39;) { elem.value = new_value; return elem; } } }; 十、抽取函数 # 工程学就是把大问题拆分成小问题再把这些问题的解决方案放回一起。\n首先应该明确一个函数的高层次目标，然后对于不是直接为了这个目标工作的代码，抽取出来放到独立的函数中。\n介绍性的代码：\nint findClostElement(int[] arr) { int clostIdx; int clostDist = Interger.MAX_VALUE; for (int i = 0; i \u0026lt; arr.length; i++) { int x = ...; int y = ...; int z = ...; int value = x * y * z; int dist = Math.sqrt(Math.pow(value, 2), Math.pow(arr[i], 2)); if (dist \u0026lt; clostDist) { clostIdx = i; clostDist = value; } } return clostIdx; } 以上代码中循环部分主要计算距离，这部分不属于代码高层次目标，高层次目标是寻找最小距离的值，因此可以把这部分代替提取到独立的函数中。这样做也带来一个额外的好处有：可以单独进行测试、可以快速找到程序错误并修改。\npublic int findClostElement(int[] arr) { int clostIdx; int clostDist = Interger.MAX_VALUE; for (int i = 0; i \u0026lt; arr.length; i++) { int dist = computDist(arr, i); if (dist \u0026lt; clostDist) { clostIdx = i; clostDist = value; } } return clostIdx; } 并不是函数抽取的越多越好，如果抽取过多，在阅读代码的时候可能需要不断跳来跳去。只有在当前函数不需要去了解某一块代码细节而能够表达其内容时，把这块代码抽取成子函数才是好的。\n函数抽取也用于减小代码的冗余。\n十一、一次只做一件事 # 只做一件事的代码很容易让人知道其要做的事；\n基本流程：列出代码所做的所有任务；把每个任务拆分到不同的函数，或者不同的段落。\n十二、用自然语言表述代码 # 先用自然语言书写代码逻辑，也就是伪代码，然后再写代码，这样代码逻辑会更清晰。\n十三、减少代码量 # 不要过度设计，编码过程会有很多变化，过度设计的内容到最后往往是无用的。\n多用标准库实现。\n"},{"id":14,"href":"/keeplearning/docs/system-design/attack/","title":"攻击技术","section":"系统设计","content":" 攻击技术 # 一、跨站脚本攻击 # 概念 # 跨站脚本攻击（Cross-Site Scripting, XSS），可以将代码注入到用户浏览的网页上，这种代码包括 HTML 和 JavaScript。\n攻击原理 # 例如有一个论坛网站，攻击者可以在上面发布以下内容：\n\u0026lt;script\u0026gt;location.href = \u0026#34;//domain.com/?c=\u0026#34; + document.cookie\u0026lt;/script\u0026gt; 之后该内容可能会被渲染成以下形式：\n\u0026lt;p\u0026gt; \u0026lt;script\u0026gt;location.href = \u0026#34;//domain.com/?c=\u0026#34; + document.cookie\u0026lt;/script\u0026gt; \u0026lt;/p\u0026gt; 另一个用户浏览了含有这个内容的页面将会跳转到 domain.com 并携带了当前作用域的 Cookie。如果这个论坛网站通过 Cookie 管理用户登录状态，那么攻击者就可以通过这个 Cookie 登录被攻击者的账号了。\n危害 # 窃取用户的 Cookie 伪造虚假的输入表单骗取个人信息 显示伪造的文章或者图片 防范手段 # 1. 设置 Cookie 为 HttpOnly # 设置了 HttpOnly 的 Cookie 可以防止 JavaScript 脚本调用，就无法通过 document.cookie 获取用户 Cookie 信息。\n2. 过滤特殊字符 # 例如将 \u0026lt; 转义为 \u0026amp;lt;，将 \u0026gt; 转义为 \u0026amp;gt;，从而避免 HTML 和 Jascript 代码的运行。\n富文本编辑器允许用户输入 HTML 代码，就不能简单地将 \u0026lt; 等字符进行过滤了，极大地提高了 XSS 攻击的可能性。\n富文本编辑器通常采用 XSS filter 来防范 XSS 攻击，通过定义一些标签白名单或者黑名单，从而不允许有攻击性的 HTML 代码的输入。\n以下例子中，form 和 script 等标签都被转义，而 h 和 p 等标签将会保留。\n\u0026lt;h1 id=\u0026#34;title\u0026#34;\u0026gt;XSS Demo\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;123\u0026lt;/p\u0026gt; \u0026lt;form\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;q\u0026#34; value=\u0026#34;test\u0026#34;\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;pre\u0026gt;hello\u0026lt;/pre\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; alert(/xss/); \u0026lt;/script\u0026gt; \u0026lt;h1\u0026gt;XSS Demo\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;123\u0026lt;/p\u0026gt; \u0026amp;lt;form\u0026amp;gt; \u0026amp;lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;q\u0026#34; value=\u0026#34;test\u0026#34;\u0026amp;gt; \u0026amp;lt;/form\u0026amp;gt; \u0026lt;pre\u0026gt;hello\u0026lt;/pre\u0026gt; \u0026amp;lt;script type=\u0026#34;text/javascript\u0026#34;\u0026amp;gt; alert(/xss/); \u0026amp;lt;/script\u0026amp;gt; XSS 过滤在线测试\n二、跨站请求伪造 # 概念 # 跨站请求伪造（Cross-site request forgery，CSRF），是攻击者通过一些技术手段欺骗用户的浏览器去访问一个自己曾经认证过的网站并执行一些操作（如发邮件，发消息，甚至财产操作如转账和购买商品）。由于浏览器曾经认证过，所以被访问的网站会认为是真正的用户操作而去执行。\nXSS 利用的是用户对指定网站的信任，CSRF 利用的是网站对用户浏览器的信任。\n攻击原理 # 假如一家银行用以执行转账操作的 URL 地址如下：\nhttp://www.examplebank.com/withdraw?account=AccoutName\u0026amp;amount=1000\u0026amp;for=PayeeName。 那么，一个恶意攻击者可以在另一个网站上放置如下代码：\n\u0026lt;img src=\u0026#34;http://www.examplebank.com/withdraw?account=Alice\u0026amp;amount=1000\u0026amp;for=Badman\u0026#34;\u0026gt;。 如果有账户名为 Alice 的用户访问了恶意站点，而她之前刚访问过银行不久，登录信息尚未过期，那么她就会损失 1000 美元。\n这种恶意的网址可以有很多种形式，藏身于网页中的许多地方。此外，攻击者也不需要控制放置恶意网址的网站。例如他可以将这种地址藏在论坛，博客等任何用户生成内容的网站中。这意味着如果服务器端没有合适的防御措施的话，用户即使访问熟悉的可信网站也有受攻击的危险。\n通过例子能够看出，攻击者并不能通过 CSRF 攻击来直接获取用户的账户控制权，也不能直接窃取用户的任何信息。他们能做到的，是欺骗用户浏览器，让其以用户的名义执行操作。\n防范手段 # 1. 检查 Referer 首部字段 # Referer 首部字段位于 HTTP 报文中，用于标识请求来源的地址。检查这个首部字段并要求请求来源的地址在同一个域名下，可以极大的防止 CSRF 攻击。\n这种办法简单易行，工作量低，仅需要在关键访问处增加一步校验。但这种办法也有其局限性，因其完全依赖浏览器发送正确的 Referer 字段。虽然 HTTP 协议对此字段的内容有明确的规定，但并无法保证来访的浏览器的具体实现，亦无法保证浏览器没有安全漏洞影响到此字段。并且也存在攻击者攻击某些浏览器，篡改其 Referer 字段的可能。\n2. 添加校验 Token # 在访问敏感数据请求时，要求用户浏览器提供不保存在 Cookie 中，并且攻击者无法伪造的数据作为校验。例如服务器生成随机数并附加在表单中，并要求客户端传回这个随机数。\n3. 输入验证码 # 因为 CSRF 攻击是在用户无意识的情况下发生的，所以要求用户输入验证码可以让用户知道自己正在做的操作。\n三、SQL 注入攻击 # 概念 # 服务器上的数据库运行非法的 SQL 语句，主要通过拼接来完成。\n攻击原理 # 例如一个网站登录验证的 SQL 查询代码为：\nstrSQL = \u0026#34;SELECT * FROM users WHERE (name = \u0026#39;\u0026#34; + userName + \u0026#34;\u0026#39;) and (pw = \u0026#39;\u0026#34;+ passWord +\u0026#34;\u0026#39;);\u0026#34; 如果填入以下内容：\nuserName = \u0026#34;1\u0026#39; OR \u0026#39;1\u0026#39;=\u0026#39;1\u0026#34;; passWord = \u0026#34;1\u0026#39; OR \u0026#39;1\u0026#39;=\u0026#39;1\u0026#34;; 那么 SQL 查询字符串为：\nstrSQL = \u0026#34;SELECT * FROM users WHERE (name = \u0026#39;1\u0026#39; OR \u0026#39;1\u0026#39;=\u0026#39;1\u0026#39;) and (pw = \u0026#39;1\u0026#39; OR \u0026#39;1\u0026#39;=\u0026#39;1\u0026#39;);\u0026#34; 此时无需验证通过就能执行以下查询：\nstrSQL = \u0026#34;SELECT * FROM users;\u0026#34; 防范手段 # 1. 使用参数化查询 # Java 中的 PreparedStatement 是预先编译的 SQL 语句，可以传入适当参数并且多次执行。由于没有拼接的过程，因此可以防止 SQL 注入的发生。\nPreparedStatement stmt = connection.prepareStatement(\u0026#34;SELECT * FROM users WHERE userid=? AND password=?\u0026#34;); stmt.setString(1, userid); stmt.setString(2, password); ResultSet rs = stmt.executeQuery(); 2. 单引号转换 # 将传入的参数中的单引号转换为连续两个单引号，PHP 中的 Magic quote 可以完成这个功能。\n四、拒绝服务攻击 # 拒绝服务攻击（denial-of-service attack，DoS），亦称洪水攻击，其目的在于使目标电脑的网络或系统资源耗尽，使服务暂时中断或停止，导致其正常用户无法访问。\n分布式拒绝服务攻击（distributed denial-of-service attack，DDoS），指攻击者使用两个或以上被攻陷的电脑作为“僵尸”向特定的目标发动“拒绝服务”式攻击。\n参考资料 # 维基百科：跨站脚本 维基百科：SQL 注入攻击 维基百科：跨站点请求伪造 维基百科：拒绝服务攻击 "},{"id":15,"href":"/keeplearning/docs/system-design/cache/","title":"缓存","section":"系统设计","content":" 缓存 # 一、缓存特征 # 命中率 # 当某个请求能够通过访问缓存而得到响应时，称为缓存命中。\n缓存命中率越高，缓存的利用率也就越高。\n最大空间 # 缓存通常位于内存中，内存的空间通常比磁盘空间小的多，因此缓存的最大空间不可能非常大。\n当缓存存放的数据量超过最大空间时，就需要淘汰部分数据来存放新到达的数据。\n淘汰策略 # FIFO（First In First Out）：先进先出策略，在实时性的场景下，需要经常访问最新的数据，那么就可以使用 FIFO，使得最先进入的数据（最晚的数据）被淘汰。\nLRU（Least Recently Used）：最近最久未使用策略，优先淘汰最久未使用的数据，也就是上次被访问时间距离现在最久的数据。该策略可以保证内存中的数据都是热点数据，也就是经常被访问的数据，从而保证缓存命中率。\nLFU（Least Frequently Used）：最不经常使用策略，优先淘汰一段时间内使用次数最少的数据。\n二、缓存位置 # 浏览器 # 当 HTTP 响应允许进行缓存时，浏览器会将 HTML、CSS、JavaScript、图片等静态资源进行缓存。\nISP # 网络服务提供商（ISP）是网络访问的第一跳，通过将数据缓存在 ISP 中能够大大提高用户的访问速度。\n反向代理 # 反向代理位于服务器之前，请求与响应都需要经过反向代理。通过将数据缓存在反向代理，在用户请求反向代理时就可以直接使用缓存进行响应。\n本地缓存 # 使用 Guava Cache 将数据缓存在服务器本地内存中，服务器代码可以直接读取本地内存中的缓存，速度非常快。\n分布式缓存 # 使用 Redis、Memcache 等分布式缓存将数据缓存在分布式缓存系统中。\n相对于本地缓存来说，分布式缓存单独部署，可以根据需求分配硬件资源。不仅如此，服务器集群都可以访问分布式缓存，而本地缓存需要在服务器集群之间进行同步，实现难度和性能开销上都非常大。\n数据库缓存 # MySQL 等数据库管理系统具有自己的查询缓存机制来提高查询效率。\nJava 内部的缓存 # Java 为了优化空间，提高字符串、基本数据类型包装类的创建效率，设计了字符串常量池及 Byte、Short、Character、Integer、Long、Boolean 这六种包装类缓冲池。\nCPU 多级缓存 # CPU 为了解决运算速度与主存 IO 速度不匹配的问题，引入了多级缓存结构，同时使用 MESI 等缓存一致性协议来解决多核 CPU 缓存数据一致性的问题。\n三、CDN # 内容分发网络（Content distribution network，CDN）是一种互连的网络系统，它利用更靠近用户的服务器从而更快更可靠地将 HTML、CSS、JavaScript、音乐、图片、视频等静态资源分发给用户。\nCDN 主要有以下优点：\n更快地将数据分发给用户； 通过部署多台服务器，从而提高系统整体的带宽性能； 多台服务器可以看成是一种冗余机制，从而具有高可用性。 四、缓存问题 # 缓存穿透 # 指的是对某个一定不存在的数据进行请求，该请求将会穿透缓存到达数据库。\n解决方案：\n对这些不存在的数据缓存一个空数据； 对这类请求进行过滤。 缓存雪崩 # 指的是由于数据没有被加载到缓存中，或者缓存数据在同一时间大面积失效（过期），又或者缓存服务器宕机，导致大量的请求都到达数据库。\n在有缓存的系统中，系统非常依赖于缓存，缓存分担了很大一部分的数据请求。当发生缓存雪崩时，数据库无法处理这么大的请求，导致数据库崩溃。\n解决方案：\n为了防止缓存在同一时间大面积过期导致的缓存雪崩，可以通过观察用户行为，合理设置缓存过期时间来实现； 为了防止缓存服务器宕机出现的缓存雪崩，可以使用分布式缓存，分布式缓存中每一个节点只缓存部分的数据，当某个节点宕机时可以保证其它节点的缓存仍然可用。 也可以进行缓存预热，避免在系统刚启动不久由于还未将大量数据进行缓存而导致缓存雪崩。 缓存一致性 # 缓存一致性要求数据更新的同时缓存数据也能够实时更新。\n解决方案：\n在数据更新的同时立即去更新缓存； 在读缓存之前先判断缓存是否是最新的，如果不是最新的先进行更新。 要保证缓存一致性需要付出很大的代价，缓存数据最好是那些对一致性要求不高的数据，允许缓存数据存在一些脏数据。\n缓存 “无底洞” 现象 # 指的是为了满足业务要求添加了大量缓存节点，但是性能不但没有好转反而下降了的现象。\n产生原因：缓存系统通常采用 hash 函数将 key 映射到对应的缓存节点，随着缓存节点数目的增加，键值分布到更多的节点上，导致客户端一次批量操作会涉及多次网络操作，这意味着批量操作的耗时会随着节点数目的增加而不断增大。此外，网络连接数变多，对节点的性能也有一定影响。\n解决方案：\n优化批量数据操作命令； 减少网络通信次数； 降低接入成本，使用长连接 / 连接池，NIO 等。 五、数据分布 # 哈希分布 # 哈希分布就是将数据计算哈希值之后，按照哈希值分配到不同的节点上。例如有 N 个节点，数据的主键为 key，则将该数据分配的节点序号为：hash(key)%N。\n传统的哈希分布算法存在一个问题：当节点数量变化时，也就是 N 值变化，那么几乎所有的数据都需要重新分布，将导致大量的数据迁移。\n顺序分布 # 将数据划分为多个连续的部分，按数据的 ID 或者时间分布到不同节点上。例如 User 表的 ID 范围为 1 ~ 7000，使用顺序分布可以将其划分成多个子表，对应的主键范围为 1 ~ 1000，1001 ~ 2000，\u0026hellip;，6001 ~ 7000。\n顺序分布相比于哈希分布的主要优点如下：\n能保持数据原有的顺序； 并且能够准确控制每台服务器存储的数据量，从而使得存储空间的利用率最大。 六、一致性哈希 # Distributed Hash Table（DHT） 是一种哈希分布方式，其目的是为了克服传统哈希分布在服务器节点数量变化时大量数据迁移的问题。\n基本原理 # 将哈希空间 [0, 2n-1] 看成一个哈希环，每个服务器节点都配置到哈希环上。每个数据对象通过哈希取模得到哈希值之后，存放到哈希环中顺时针方向第一个大于等于该哈希值的节点上。\n一致性哈希在增加或者删除节点时只会影响到哈希环中相邻的节点，例如下图中新增节点 X，只需要将它前一个节点 C 上的数据重新进行分布即可，对于节点 A、B、D 都没有影响。\n虚拟节点 # 上面描述的一致性哈希存在数据分布不均匀的问题，节点存储的数据量有可能会存在很大的不同。\n数据不均匀主要是因为节点在哈希环上分布的不均匀，这种情况在节点数量很少的情况下尤其明显。\n解决方式是通过增加虚拟节点，然后将虚拟节点映射到真实节点上。虚拟节点的数量比真实节点来得多，那么虚拟节点在哈希环上分布的均匀性就会比原来的真实节点好，从而使得数据分布也更加均匀。\n七、LRU # 以下是基于 双向链表 + HashMap 的 LRU 算法实现，对算法的解释如下：\n访问某个节点时，将其从原来的位置删除，并重新插入到链表头部。这样就能保证链表尾部存储的就是最近最久未使用的节点，当节点数量大于缓存最大空间时就淘汰链表尾部的节点。 为了使删除操作时间复杂度为 O(1)，就不能采用遍历的方式找到某个节点。HashMap 存储着 Key 到节点的映射，通过 Key 就能以 O(1) 的时间得到节点，然后再以 O(1) 的时间将其从双向队列中删除。 public class LRU\u0026lt;K, V\u0026gt; implements Iterable\u0026lt;K\u0026gt; { private Node head; private Node tail; private HashMap\u0026lt;K, Node\u0026gt; map; private int maxSize; private class Node { Node pre; Node next; K k; V v; public Node(K k, V v) { this.k = k; this.v = v; } } public LRU(int maxSize) { this.maxSize = maxSize; this.map = new HashMap\u0026lt;\u0026gt;(maxSize * 4 / 3); head = new Node(null, null); tail = new Node(null, null); head.next = tail; tail.pre = head; } public V get(K key) { if (!map.containsKey(key)) { return null; } Node node = map.get(key); unlink(node); appendHead(node); return node.v; } public void put(K key, V value) { if (map.containsKey(key)) { Node node = map.get(key); unlink(node); } Node node = new Node(key, value); map.put(key, node); appendHead(node); if (map.size() \u0026gt; maxSize) { Node toRemove = removeTail(); map.remove(toRemove.k); } } private void unlink(Node node) { Node pre = node.pre; Node next = node.next; pre.next = next; next.pre = pre; node.pre = null; node.next = null; } private void appendHead(Node node) { Node next = head.next; node.next = next; next.pre = node; node.pre = head; head.next = node; } private Node removeTail() { Node node = tail.pre; Node pre = node.pre; tail.pre = pre; pre.next = tail; node.pre = null; node.next = null; return node; } @Override public Iterator\u0026lt;K\u0026gt; iterator() { return new Iterator\u0026lt;K\u0026gt;() { private Node cur = head.next; @Override public boolean hasNext() { return cur != tail; } @Override public K next() { Node node = cur; cur = cur.next; return node.k; } }; } } 参考资料 # 大规模分布式存储系统 缓存那些事 一致性哈希算法 内容分发网络 How Aspiration CDN helps to improve your website loading speed? "},{"id":16,"href":"/keeplearning/docs/system-design/queue-message/","title":"消息队列","section":"系统设计","content":" 消息队列 # 一、消息模型 # 点对点 # 消息生产者向消息队列中发送了一个消息之后，只能被一个消费者消费一次。\n发布/订阅 # 消息生产者向频道发送一个消息之后，多个消费者可以从该频道订阅到这条消息并消费。\n发布与订阅模式和观察者模式有以下不同：\n观察者模式中，观察者和主题都知道对方的存在；而在发布与订阅模式中，生产者与消费者不知道对方的存在，它们之间通过频道进行通信。 观察者模式是同步的，当事件触发时，主题会调用观察者的方法，然后等待方法返回；而发布与订阅模式是异步的，生产者向频道发送一个消息之后，就不需要关心消费者何时去订阅这个消息，可以立即返回。 二、使用场景 # 异步处理 # 发送者将消息发送给消息队列之后，不需要同步等待消息接收者处理完毕，而是立即返回进行其它操作。消息接收者从消息队列中订阅消息之后异步处理。\n例如在注册流程中通常需要发送验证邮件来确保注册用户身份的合法性，可以使用消息队列使发送验证邮件的操作异步处理，用户在填写完注册信息之后就可以完成注册，而将发送验证邮件这一消息发送到消息队列中。\n只有在业务流程允许异步处理的情况下才能这么做，例如上面的注册流程中，如果要求用户对验证邮件进行点击之后才能完成注册的话，就不能再使用消息队列。\n流量削锋 # 在高并发的场景下，如果短时间有大量的请求到达会压垮服务器。\n可以将请求发送到消息队列中，服务器按照其处理能力从消息队列中订阅消息进行处理。\n应用解耦 # 如果模块之间不直接进行调用，模块之间耦合度就会很低，那么修改一个模块或者新增一个模块对其它模块的影响会很小，从而实现可扩展性。\n通过使用消息队列，一个模块只需要向消息队列中发送消息，其它模块可以选择性地从消息队列中订阅消息从而完成调用。\n三、可靠性 # 发送端的可靠性 # 发送端完成操作后一定能将消息成功发送到消息队列中。\n实现方法：在本地数据库建一张消息表，将消息数据与业务数据保存在同一数据库实例里，这样就可以利用本地数据库的事务机制。事务提交成功后，将消息表中的消息转移到消息队列中，若转移消息成功则删除消息表中的数据，否则继续重传。\n接收端的可靠性 # 接收端能够从消息队列成功消费一次消息。\n两种实现方法：\n保证接收端处理消息的业务逻辑具有幂等性：只要具有幂等性，那么消费多少次消息，最后处理的结果都是一样的。 保证消息具有唯一编号，并使用一张日志表来记录已经消费的消息编号。 参考资料 # Observer vs Pub-Sub 消息队列中点对点与发布订阅区别 "},{"id":17,"href":"/keeplearning/docs/favorite/","title":"收藏夹","section":"Docs","content":" 收藏夹 # 面试题 # 对线面试官面试系列 命中率极高的 Go 面试题，赶紧收藏！ Redis面试题（总结最全面的面试题） 很用心的为你写了 9 道 MySQL 面试题 史上最详细的一线大厂Mysql面试题详解 面试BAT前先搞定这18道MySQL经典面试题（含答案解析） MySQL面试题（总结最全面的面试题） mysql索引相关面试题 Golang 常见面试题目解析 化身一个请求感受浏览器输入URL后奇妙的网络之旅 TCP和UDP协议的区别以及原理 视频 # 黑马程序员 MySQL数据库入门到精通，从mysql安装到mysql高级、mysql优化全囊括 李敖對佛教徒迎佛骨之批判《李敖大哥大》 21.07.05~22.02.27各大厂面试出现过的算法题都在这了，总共62题，认真看完一半算法关不再是绊脚石 在线文档 # 计算机教育中缺失的一课 interview-go Docker-从入门到实战 区块链技术指南 build-web-application-with-golang Mastering_Go Go语言101 《GO专家编程》 PHP扩展开发及内核应用 JavaScript 标准参考教程 ES6 入门教程 JavaScript 教程 网道 PHP编程之道 Kubernetes 文档 Uber Go 语言编码规范 地鼠文档 go Standard library 床长人工智能教程 Golang开发手记 Git飞行规则(Flight Rules) Pro Git（中文版） 南京大学 计算机科学与技术系 计算机系统基础 课程实验 2019 Mysql 实战45讲 一份不太简短的 LATEX介绍 A Programmer\u0026rsquo;s Guide to English 麦晓杰-专栏分享 技术文章摘抄 鳥哥的首頁 www.topgoer.com Go语言圣经 数据结构和算法（Golang实现） JavaScript Promise迷你书（中文版） 博主 # 刘丹冰aceld 代码随想录 gairuo 面向信仰编程 小林coding 煎鱼 xargin 竹子爱熊猫 技术印记 abcdocker运维博客 Java充电社 go.nsddd.top mojotv NoahNyy 大都督的技术博客 欧长坤 LABULADONG 的算法网站 CS-Notes Hello 算法 小惡魔 - AppleBOY PassJava 博客连载中 tsing\u0026rsquo;s Blog 工具 # HTTP Cats loading.io Data Structure Visualizations learn git branching navicat premium15破解教程 Bit Calculator YouTube Video Download 冷熊简历 身份证号码生成器 PlantUML简述 在线PS-Photopea / 在线PS-toolwa json2yaml crontab.guru tree.nathanfriend.io markdown book HonKit www.boce.com qwerty-learner VMware Workstation Pro 16 官方正式版下载（含密钥） Xmind 下载 XMind 2022中文破解版 v22.11.3656 最新版 思维导图工具-XMind 2022 破解版 Typora 破解教程（2022最新方法，亲测有效） demo.unlock-music.dev 资料库 # 国内多家权威出版社：10000多本电子书合集 ahhhhfs｜A姐分享 首发整理！【编程开发全套资料】 百度网盘资源 IT_book 站长免费证书 可能是最用心的「Go学习建议」 编程书籍的整理和收集 Go 书籍 Concurrency in Go 大鹏教育所有课程/提取码：q21b 万门大学（目前最全的合集） cheap-vps 全国图书馆参考咨询联盟 technical_books 电子书查找 云盘 # MEGA "},{"id":18,"href":"/keeplearning/docs/hidden/","title":"😪xxxxxx","section":"Docs","content":" Go "}]