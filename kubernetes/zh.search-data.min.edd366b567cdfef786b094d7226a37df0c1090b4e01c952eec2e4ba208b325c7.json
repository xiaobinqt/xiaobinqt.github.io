[{"id":0,"href":"/kubernetes/docs/part1-primary/1.1-docker-brief/","title":"1.1 Docker 简介","section":"第一部分 Docker","content":" 1.1 Docker 简介 # 目前使用 Docker 基本上有两个选择：Docker Desktop 和 Docker Engine。\nDocker Desktop 是专门针对个人使用而设计的，支持 Mac 和 Windows 快速安装，具有直观的图形界面，还集成了许多周边工具，方便易用。Docker Engine 完全免费，但只能在 Linux 上运行，只能使用命令行操作，缺乏辅助工具，需要我们自己动手安装运行环境，是现在各个公司在生产环境中实际使用的 Docker 产品，毕竟机房里 99% 的服务器跑的都是 Linux。\n1.1.1 Docker 安装 # Docker 的安装可以参看官网 https://docs.docker.com/engine/install/\nDocker Engine 不像 Docker Desktop 那样可以安装后就直接使用，必须要做一些手工调整才能用起来，所以在安装完毕后需要执行下面的两条命令：\nsudo service docker start #启动docker服务 sudo usermod -aG docker ${USER} #当前用户加入docker组 第一个 service docker start 是启动 Docker 的后台服务，第二个 usermod -aG 是把当前的用户加入 Docker 的用户组。这是因为操作 Docker 必须要有 root 权限，而直接使用 root 用户不够安全，加入 Docker 用户组是一个比较好的选择，这也是 Docker 官方推荐的做法。当然，如果为了图省事，也可以直接切换到 root 用户来操作 Docker。\n1.1.2 Docker 架构 # 这张图来自 Docker 官网 https://docs.docker.com/get-started/overview/，描述了 Docker Engine 的内部角色和工作流程。\n命令行 docker 实际上是一个客户端 client ，它会与 Docker Engine 里的后台服务 Docker daemon 通信，而镜像则存储在远端的仓库 Registry 里，客户端并不能直接访问镜像仓库。\n在 Docker Engine 里，真正干活的其实是默默运行在后台的 Docker daemon。Docker client 可以通过 build、pull、run 等命令向 Docker daemon 发送请求，而 Docker daemon 则是容器和镜像的 “大管家”，负责从远端拉取镜像、在本地存储镜像，还有从镜像生成容器、管理容器等所有功能。\n"},{"id":1,"href":"/kubernetes/docs/part2-break-ice/2.1-k8s-overview/","title":"2.1 简介","section":"第二部分 入门","content":" 2.1 k8s 简介 # 2.1.1 容器编排 # 容器技术其实只是解决了运维部署工作中一个很小的问题，在现实中的生产环境复杂程度特别高，除了最基本的安装，还会有各式各样的需求，比如服务发现、负载均衡、状态监控、健康检查、扩容缩容、应用迁移、高可用等等。\n这些问题已经不再是隔离一两个进程的普通问题，而是要隔离数不清的进程，还有它们之间互相通信、互相协作的超级问题，困难程度可以说是指数级别的上升。这些容器之上的管理、调度工作，就是 “容器编排”（Container Orchestration）。\n2.1.2 什么是 k8s # 简单来说，Kubernetes 就是一个生产级别的容器编排平台和集群管理系统，不仅能够创建、调度容器，还能够监控、管理服务器，它凝聚了 Google 等大公司和开源社区的集体智慧，从而让中小型公司也可以具备轻松运维海量计算节点 —— 也就是 “云计算” 的能力。\nk8s 脱胎与 Google 内部代号为 Borg 的集群应用管理系统。在 2015 年，Google 又联合 Linux 基金会成立了 CNCF（Cloud Native Computing Foundation，云原生基金会），并把 Kubernetes 捐献出来作为种子项目。\n有了 Google 和 Linux 两大家族的保驾护航，再加上宽容开放的社区，作为 CNCF 的 “头把交椅”，Kubernetes 旗下很快就汇集了众多行业精英，仅用几年的时间就打败了同期的竞争对手 Apache Mesos 和 Docker Swarm，成为了容器编排和集群管理这个领域的唯一霸主。\n"},{"id":2,"href":"/kubernetes/docs/part1-primary/","title":"第一部分 Docker","section":"Docs","content":""},{"id":3,"href":"/kubernetes/docs/part1-primary/1.2-docker-cmd/","title":"1.2 Docker 常用命令","section":"第一部分 Docker","content":" 1.2 Docker 常用命令 # docker version # docker version 会输出 Docker 客户端和服务器各自的版本信息：\ndocker info # docker info 会显示当前 Docker 系统相关的信息，例如 CPU、内存、容器数量、镜像数量、容器运行时、存储文件系统等等：\nServer: Containers: 1 Running: 0 Paused: 0 Stopped: 1 Images: 8 Server Version: 20.10.12 Storage Driver: overlay2 Backing Filesystem: extfs Cgroup Driver: systemd Default Runtime: runc Kernel Version: 5.13.0-19-generic Operating System: Ubuntu Jammy Jellyfish (development branch) OSType: linux Architecture: aarch64 CPUs: 2 Total Memory: 3.822GiB Docker Root Dir: /var/lib/docker docker info 显示的信息，对了解 Docker 的内部运行状态非常有用，比如可以很直观的能够看到当前有一个容器处于停止状态，有 8 个镜像，存储用的文件系统是 overlay2，Linux 内核是 5.13，操作系统是 Ubuntu 22.04 Jammy Jellyfish，硬件是 aarch64，两个 CPU，内存 4G。\n"},{"id":4,"href":"/kubernetes/docs/part2-break-ice/2.2-minikube/","title":"2.2 minikube","section":"第二部分 入门","content":" 2.2 minikube # minikube 是一个 “迷你” 版本的 Kubernetes，自从 2016 年发布以来一直在积极地开发维护，紧跟 Kubernetes 的版本更新，同时也兼容较旧的版本（最多可以到之前的 6 个小版本）。\nminikube 最大特点就是 “小而美”，可执行文件仅有不到 100MB，运行镜像也不过 1GB。minikube 集成了 Kubernetes 的绝大多数功能特性，不仅有核心的容器编排功能，还有丰富的插件，例如 Dashboard、GPU、Ingress、Istio、Kong、Registry 等。\n2.2.1 安装 Docker # 我的系统是 debian 11，本文所有的操作都是在 debian 11 的环境下进行。\nDocker 的安装可以参考官网 Install Docker Engine on Debian，其他系统的安装方式都可以在官网找到。debian 11 的安装步骤大致如下。\napt-get remove docker docker-engine docker.io containerd runc # 卸载旧版本 apt-get update apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\ $(lsb_release -cs) stable\u0026#34; | tee /etc/apt/sources.list.d/docker.list \u0026gt;/dev/null apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin 2.2.2 安装 minikube # 可以去官网 https://minikube.sigs.k8s.io/docs/start/ 下载对应的版本。\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 install minikube-linux-amd64 /usr/local/bin/minikube 2.2.3 安装 kubectl # minikube 只能够搭建 Kubernetes 环境，要操作 Kubernetes，还需要另一个专门的客户端工具 kubectl。\nkubectl 是一个命令行工具，通过它可以与 Kubernetes 后台服务通信，把我们的命令转发给 Kubernetes，实现容器和集群的管理功能。\nkubectl 是一个与 Kubernetes、minikube 彼此独立的项目，不包含在 minikube 里，但 minikube 提供了安装它的简化方式，只需执行下面的这条命令：\nminikube kubectl 以上这条命令会把与当前 Kubernetes 版本匹配的 kubectl 下载下来，存放在内部目录（例如.minikube/cache/linux/amd64/v1.23.3），然后就可以使用它来操作 Kubernetes 了。\n在 minikube 环境里会用到两个客户端，minikube 管理 Kubernetes 集群环境，kubectl 操作实际的 Kubernetes 功能。\n2.2.4 启动环境 # 安装了 minikube 和 kubectl 就可以在本机上运行 minikube，创建 Kubernetes 实验环境了。\n最好先关闭 swap 分区，不然会 WARNING 提示：\nswap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet\n# 关闭 swap分区 swapoff -a 使用命令 minikube start 会从 Docker Hub 上拉取镜像，以当前最新版本的 Kubernetes 启动集群，也可以在后面再加上一个参数--kubernetes-version 明确指定要使用 Kubernetes 版本。这里使用 “1.23.3”，启动命令是：\nminikube start --kubernetes-version=v1.23.3 如果出现类似以下的问题：\n* minikube v1.28.0 on Centos 7.9.2009 (lxc/amd64) * Automatically selected the docker driver. Other choices: none, ssh * The \u0026#34;docker\u0026#34; driver should not be used with root privileges. If you wish to continue as root, use --force. * If you are running minikube within a VM, consider using --driver=none: * https://minikube.sigs.k8s.io/docs/reference/drivers/none/ X Exiting due to DRV_AS_ROOT: The \u0026#34;docker\u0026#34; driver should not be used with root privileges. 可以使用加上 --force\nminikube start --kubernetes-version=v1.23.3 --force 国内网络环境复杂，一般访问外网比较慢，也可以使用加上--image-mirror-country='cn'参数：\nminikube start --image-mirror-country=\u0026#39;cn\u0026#39; --kubernetes-version=v1.23.3 --force Kubernetes 安装成功后，可以使用 minikube status、minikube node list这两个命令来查看集群的状态：\n可以看到 Kubernetes 集群里现在只有一个节点，名字就叫 “minikube”，类型是 “Control Plane”，里面有 host、kubelet、apiserver 三个服务，IP 地址是 192.168.49.2。\n2.2.5 简单使用 # minikube 自带的 kubectl 有一点限制，必须要在前面加上 minikube 的前缀，后面再加上 -- 才能使用，像这样：\nminikube kubectl -- version 可以使用 “alias” 功能，为它创建一个别名，写到当前用户目录下的 .bashrc 里，或是 /etc/profile.d 下的某个 .sh 文件中，也就是这样：\nalias kubectl=\u0026#34;minikube kubectl --\u0026#34; kubectl 还提供了命令自动补全的功能，还可以再加上 “kubectl completion”：\nsource \u0026lt;(kubectl completion bash) 如果在 Kubernetes 里运行一个 Nginx 应用，命令与 Docker 类似，也是 run，但是需要用--image指定镜像，然后 Kubernetes 会自动拉取镜像并运行：\nminikube kubectl -- run ngx --image=nginx:alpine kubectl run 只能创建 Pod，要创建 Pod 以外的其他 API 对象，需要使用命令 kubectl create 再加上对象的类型名，比如 kubectl create job。\n命令执行之后可以看到，在 Kubernetes 集群里就有了一个名字叫 ngx 的 Pod 正在运行，表示这个单节点 minikube 环境已经搭建成功。\n2.5.6 云原生 # 所谓的 “云”，现在就指的是 Kubernetes，那么 “云原生” 的意思就是应用的开发、部署、运维等一系列工作都要向 Kubernetes 看齐，使用容器、微服务、声明式 API 等技术，保证应用的整个生命周期都能够在 Kubernetes 环境里顺利实施，不需要附加额外的条件。\n“云原生” 就是 Kubernetes 里的 “原住民”，而不是从其他环境迁过来的 “移民”。\n2.2.7 K8s 和 Docker 的区别 # Docker 应用于打包、测试、交付，Kubernetes 是基于 Docker 的产物，进行容器编排、运行。例如有 1 个集群，3 个节点，这些节点都以 Docker 作为容器运行时，Docker 是更偏向底层的技术。Kubernetes 更偏向上层的技术 ，它实现了对容器运行时的抽象，抽象的目的是兼容底层容器运行时（容器进行时技术不仅有 Docker，还有 containerd、kata 等，无论哪种容器运行时，Kubernetes 层面的操作都是一样的）以及解耦，同时还提供了一套容器运行时的标准。抽象的产物是容器运行时接口 CRI（Container Runtime Interface）。\n参考 # minikube start docker docs "},{"id":5,"href":"/kubernetes/docs/part2-break-ice/","title":"第二部分 入门","section":"Docs","content":""},{"id":6,"href":"/kubernetes/docs/part1-primary/1.3-container/","title":"1.3 容器","section":"第一部分 Docker","content":" 1.3 容器 # 可以使用 docker pull 命令，拉取一个新的镜像——操作系统 Alpine：\ndocker pull alpine 然后使用 docker run 命令运行它的 Shell 程序：\ndocker run -it alpine sh -it参数，可以离开当前的操作系统，进入容器内部。\n容器，就是一个特殊的隔离环境，它能够让进程只看到这个环境里的有限信息，不能对外界环境施加影响。\n1.3.1 为什么需要隔离 # 对于操作系统来说，一个不受任何限制的应用程序是十分危险的。这个进程能够看到系统里所有的文件、所有的进程、所有的网络流量，访问内存里的任何数据，那么恶意程序很容易就会把系统搞瘫痪，正常程序也可能会因为无意的 Bug 导致信息泄漏或者其他安全事故。\n使用容器技术，就可以让应用程序运行在一个有严密防护的 “沙盒”（Sandbox）环境之内，它可以在这个环境里自由活动，但绝不允许 “越界”，从而保证了容器外系统的安全。\n在计算机里有各种各样的资源，CPU、内存、硬盘、网卡，虽然目前的高性能服务器都是几十核 CPU、上百 GB 的内存、数 TB 的硬盘、万兆网卡，但这些资源终究是有限的，而且考虑到成本，也不允许某个应用程序无限制地占用。容器技术的另一个本领就是为应用程序加上资源隔离，在系统里切分出一部分资源，让它只能使用指定的配额，比如只能使用一个 CPU，只能使用 1GB 内存等等，这样就可以避免容器内进程的过度系统消耗，充分利用计算机硬件，让有限的资源能够提供稳定可靠的服务。\n1.3.2 容器和虚拟机的区别 # 容器和虚拟机面对的都是相同的问题，使用的也都是虚拟化技术，只是所在的层次不同。\n容器和虚拟机的目的都是隔离资源，保证系统安全，尽量提高资源的利用率。\n从实现的角度来看，虚拟机虚拟化出来的是硬件，需要在上面再安装一个操作系统后才能够运行应用程序，而硬件虚拟化和操作系统都比较 “重”，会消耗大量的 CPU、内存、硬盘等系统资源，但这些消耗其实并没有带来什么价值，属于 “重复劳动” 和 “无用功”，不过好处就是隔离程度非常高，每个虚拟机之间可以做到完全无干扰。\n容器直接利用了下层的计算机硬件和操作系统，因为比虚拟机少了一层，所以自然就会节约 CPU 和内存，显得非常轻量级，能够更高效地利用硬件资源。不过，因为多个容器共用操作系统内核，应用程序的隔离程度就没有虚拟机那么高。\n1.3.3 隔离的实现 # Linux 操作系统内核为资源隔离提供了三种技术：namespace、cgroup、chroot，虽然这三种技术的初衷并不是为了实现容器，但它们三个结合在一起就会发生奇妙的 “化学反应”。\nnamespace 是 2002 年从 Linux 2.4.19 开始出现的，和编程语言里的 namespace 有点类似，它可以创建出独立的文件系统、主机名、进程号、网络等资源空间，相当于给进程盖了一间小板房，这样就实现了系统全局资源和进程局部资源的隔离。\ncgroup 是 2008 年从 Linux 2.6.24 开始出现的，它的全称是 Linux Control Group，用来实现对进程的 CPU、内存等资源的优先级和配额限制，相当于给进程的小板房加了一个天花板。\nchroot 的历史则要比前面的 namespace、cgroup 要古老得多，早在 1979 年的 UNIX V7 就已经出现了，它可以更改进程的根目录，也就是限制访问文件系统，相当于给进程的小板房铺上了地砖。目前的容器基本不再使用过于古老的 chroot 了，而是改用 pivot_root。\n综合运用这三种技术，一个四四方方、具有完善的隔离特性的容器就此出现了，进程就可以搬进这个小房间，过它的 “快乐生活” 了。\n1.3.4 常见容器操作 # 基本的格式是 “docker run 设置参数”，再跟上 “镜像名或 ID”，后面可能还会有附加的 “运行命令”。\ndocker run -h srv alpine hostname 这里的 -h srv 就是容器的运行参数，alpine 是镜像名，它后面的 hostname 表示要在容器里运行的 “hostname” 这个程序，输出主机名。\ndocker run 是最复杂的一个容器操作命令，有非常多的额外参数用来调整容器的运行状态，可以加上 \u0026ndash;help 来看它的帮助信息。\n-it 表示开启一个交互式操作的 Shell，这样可以直接进入容器内部，就好像是登录虚拟机一样，这个命令实际上是-i和-t两个参数的组合形式。\n-d 表示让容器在后台运行，这在启动例入 Nginx、Redis 等服务器程序的时候非常有用。\n--name 可以为容器起一个名字，方便查看，不过它不是必须的，如果不用这个参数，Docker 会分配一个随机的名字。\n--rm 可以让容器运行结束后自动删除。\n对于正在运行中的容器，可以使用 docker exec 命令在里面执行另一个程序，效果和 docker run 很类似，但因为容器已经存在，所以不会创建新的容器。最常见的用法是使用 -it 参数打开一个 Shell，从而进入容器内部，例如：\ndocker exec -it 容器名称/容器ID sh/bash 参考 # Use containers to Build, Share and Run your applications Reference documentation "},{"id":7,"href":"/kubernetes/docs/part2-break-ice/2.3-working-mechanism/","title":"2.3 工作机制","section":"第二部分 入门","content":" 2.3 工作机制 # 2.3.1 基本架构 # Kubernetes 采用了 “控制面 / 数据面”（Control Plane / Data Plane）架构，集群里的计算机被称为 “节点”（Node），可以是物理机也可以是虚拟机，少量的节点用作控制面来执行集群的管理维护工作，其他的大部分节点都被划归数据面，用来跑业务应用。\n控制面的节点在 Kubernetes 里叫做 Master Node，一般简称为 Master，它是整个集群里最重要的部分，可以说是 Kubernetes 的大脑和心脏。数据面的节点叫做 Worker Node，一般就简称为 Worker 或者 Node，相当于 Kubernetes 的手和脚，在 Master 的指挥下干活。Node 的数量非常多，构成了一个资源池，Kubernetes 就在这个池里分配资源，调度应用。因为资源被 “池化”了，所以管理也就变得比较简单，可以在集群中任意添加或者删除节点。\nMaster 和 Node 的划分不是绝对的。当集群的规模较小，工作负载较少的时候，Master 也可以承担 Node 的工作，就像 minikube 环境，它就只有一个节点，这个节点既是 Master 又是 Node。\n在下面这张架构图中，可以看到有一个 kubectl，它是 Kubernetes 的客户端工具，用来操作 Kubernetes，但它位于集群之外，理论上不属于集群。\n2.3.2 节点内部结构 # Kubernetes 的节点内部具有非常复杂的结构，由很多的模块构成的，这些模块又可以分成组件（Component）和插件（Addon）两类。\n组件实现了 Kubernetes 的核心功能特性，没有这些组件 Kubernetes 就无法启动，而插件则是 Kubernetes 的一些附加功能，属于 “锦上添花”，不安装也不会影响 Kubernetes 的正常运行。\n组件 # Master 有 4 个组件，分别是 apiserver、etcd、scheduler、controller-manager。\napiserver 是 Master 节点，同时也是整个 Kubernetes 系统的唯一入口，它对外公开了一系列的 RESTful API，并且加上了验证、授权等功能，所有其他组件都只能和它直接通信，可以说是 Kubernetes 里的联络员。\netcd 是一个高可用的分布式 Key-Value 数据库，用来持久化存储系统里的各种资源对象和状态，相当于 Kubernetes 里的配置管理员。它只与 apiserver 有直接联系，也就是说任何其他组件想要读写 etcd 里的数据都必须经过 apiserver。\nscheduler 负责容器的编排工作，检查节点的资源状态，把 Pod 调度到最适合的节点上运行，相当于部署人员。因为节点状态和 Pod 信息都存储在 etcd 里，所以 scheduler 必须通过 apiserver 才能获得。\ncontroller-manager 负责维护容器和节点等资源的状态，实现故障检测、服务迁移、应用伸缩等功能，相当于监控运维人员。同样地，它也必须通过 apiserver 获得存储在 etcd 里的信息，才能够实现对资源的各种操作。\n这 4 个组件也都被容器化运行在集群的 Pod 里，可以用 kubectl 来查看它们的状态：\n-n kube-system 参数，表示检查 “kube-system” 名字空间里的 Pod。\nNode 里有 3 个组件，分别是 kubelet、kube-proxy、container-runtime。\nkubelet 是 Node 的代理，负责管理 Node 相关的绝大部分操作，Node 上只有它能够与 apiserver 通信，实现状态报告、命令下发、启停容器等功能。\nkube-proxy 的作用有点特别，它是 Node 的网络代理，只负责管理容器的网络通信，简单来说就是为 Pod 转发 TCP/UDP 数据包。\ncontainer-runtime 是容器和镜像的实际使用者，在 kubelet 的指挥下创建容器，管理 Pod 的生命周期。\nKubernetes 的定位是容器编排平台，所以它没有限定 container-runtime 必须是 Docker，完全可以替换成任何符合标准的其他容器运行时，例如 containerd、CRI-O 等。\n工作流程 # 每个 Node 上的 kubelet 会定期向 apiserver 上报节点状态，apiserver 再存到 etcd 里。\n每个 Node 上的 kube-proxy 实现了 TCP/UDP 反向代理，让容器对外提供稳定的服务。\nscheduler 通过 apiserver 得到当前的节点状态，调度 Pod，然后 apiserver 下发命令给某个 Node 的 kubelet，kubelet 调用 container-runtime 启动容器。\ncontroller-manager 也通过 apiserver 得到实时的节点状态，监控可能的异常情况，再使用相应的手段去调节恢复。\n2.3.3 API 对象 # 作为一个集群操作系统，Kubernetes 归纳总结了 Google 多年的经验，在理论层面抽象出了很多个概念，用来描述系统的管理运维工作，这些概念就叫做 “API 对象”。\n因为 apiserver 是 Kubernetes 系统的唯一入口，外部用户和内部组件都必须和它通信，而它采用了 HTTP 协议的 URL 资源理念，API 风格也用 RESTful 的 GET/POST/DELETE 等等，所以，这些概念很自然地就被称为是 “API 对象”。\n可以使用 kubectl api-resources 来查看当前 Kubernetes 版本支持的所有对象：\n“NAME” 一栏，就是对象的名字，比如 ConfigMap、Pod、Service 等等，第二栏 “SHORTNAMES” 则是这种资源的简写。\n在使用 kubectl 命令的时候，你还可以加上一个参数 \u0026ndash;v=9，可以显示出详细的命令执行过程，清楚地看到发出的 HTTP 请求，比如：\nroot@debian-wb3:~# minikube kubectl -- get pod --v=9 I0511 16:16:12.470389 52352 loader.go:372] Config loaded from file: /root/.kube/config I0511 16:16:12.471093 52352 cert_rotation.go:137] Starting client certificate rotation controller I0511 16:16:12.474656 52352 round_trippers.go:466] curl -v -XGET -H \u0026#34;Accept: application/json;as=Table;v=v1;g=meta.k8s.io,application/json;as=Table;v=v1beta1;g=meta.k8s.io,application/json\u0026#34; -H \u0026#34;User-Agent: kubectl/v1.23.3 (linux/amd64) kubernetes/816c97a\u0026#34; \u0026#39;https://192.168.49.2:8443/api/v1/namespaces/default/pods?limit=500\u0026#39; I0511 16:16:12.475040 52352 round_trippers.go:510] HTTP Trace: Dial to tcp:192.168.49.2:8443 succeed I0511 16:16:12.481955 52352 round_trippers.go:570] HTTP Statistics: DNSLookup 0 ms Dial 0 ms TLSHandshake 4 ms ServerProcessing 2 ms Duration 7 ms I0511 16:16:12.482003 52352 round_trippers.go:577] Response Headers: I0511 16:16:12.482030 52352 round_trippers.go:580] Cache-Control: no-cache, private I0511 16:16:12.482074 52352 round_trippers.go:580] Content-Type: application/json I0511 16:16:12.482102 52352 round_trippers.go:580] X-Kubernetes-Pf-Flowschema-Uid: 0bc68592-ae0d-443e-bdc8-020149821437 I0511 16:16:12.482155 52352 round_trippers.go:580] X-Kubernetes-Pf-Prioritylevel-Uid: baa7014e-a109-46a4-bf9b-1d2bc2998aac I0511 16:16:12.482199 52352 round_trippers.go:580] Date: Thu, 11 May 2023 08:16:12 GMT I0511 16:16:12.482236 52352 round_trippers.go:580] Audit-Id: 07a6c7e2-86fb-4d6b-b30c-ae132e098e9d I0511 16:16:12.482346 52352 request.go:1181] Response Body: {\u0026#34;kind\u0026#34;:\u0026#34;Table\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;meta.k8s.io/v1\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;resourceVersion\u0026#34;:\u0026#34;760475\u0026#34;},\u0026#34;columnDefinitions\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;Name\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;,\u0026#34;format\u0026#34;:\u0026#34;name\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;Name must be unique within a namespace. Is required when creating resources, although some resources may allow a client to request the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/identifiers#names\u0026#34;,\u0026#34;priority\u0026#34;:0},{\u0026#34;name\u0026#34;:\u0026#34;Ready\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;,\u0026#34;format\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;The aggregate readiness state of this pod for accepting traffic.\u0026#34;,\u0026#34;priority\u0026#34;:0},{\u0026#34;name\u0026#34;:\u0026#34;Status\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;,\u0026#34;format\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;The aggregate status of the containers in this pod.\u0026#34;,\u0026#34;priority\u0026#34;:0},{\u0026#34;name\u0026#34;:\u0026#34;Restarts\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;,\u0026#34;format\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;The number of times the containers in this pod have been restarted and when the last container in this pod has restarted.\u0026#34;,\u0026#34;priority\u0026#34;:0},{\u0026#34;name\u0026#34;:\u0026#34;Age\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;,\u0026#34;format\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC.\\n\\nPopulated by the system. Read-only. Null for lists. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\u0026#34;,\u0026#34;priority\u0026#34;:0},{\u0026#34;name\u0026#34;:\u0026#34;IP\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;,\u0026#34;format\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;IP address allocated to the pod. Routable at least within the cluster. Empty if not yet allocated.\u0026#34;,\u0026#34;priority\u0026#34;:1},{\u0026#34;name\u0026#34;:\u0026#34;Node\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;,\u0026#34;format\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;NodeName is a request to schedule this pod onto a specific node. If it is non-empty, the scheduler simply schedules this pod onto that node, assuming that it fits resource requirements.\u0026#34;,\u0026#34;priority\u0026#34;:1},{\u0026#34;name\u0026#34;:\u0026#34;Nominated Node\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;,\u0026#34;format\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;nominatedNodeName is set only when this pod preempts other pods on the node, but it cannot be scheduled right away as preemption victims receive their graceful termination periods. This field does not guarantee that the pod will be scheduled on this node. Scheduler may decide to place the pod elsewhere if other nodes become available sooner. Scheduler may also decide to give the resources on this node to a higher priority pod that is created after preemption. As a result, this field may be different than PodSpec.nodeName when the pod is scheduled.\u0026#34;,\u0026#34;priority\u0026#34;:1},{\u0026#34;name\u0026#34;:\u0026#34;Readiness Gates\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;,\u0026#34;format\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;If specified, all readiness gates will be evaluated for pod readiness. A pod is ready when all its containers are ready AND all conditions specified in the readiness gates have status equal to \\\u0026#34;True\\\u0026#34; More info: https://git.k8s.io/enhancements/keps/sig-network/580-pod-readiness-gates\u0026#34;,\u0026#34;priority\u0026#34;:1}],\u0026#34;rows\u0026#34;:[{\u0026#34;cells\u0026#34;:[\u0026#34;ngx\u0026#34;,\u0026#34;1/1\u0026#34;,\u0026#34;Running\u0026#34;,\u0026#34;1 (4h30m ago)\u0026#34;,\u0026#34;113d\u0026#34;,\u0026#34;172.17.0.2\u0026#34;,\u0026#34;minikube\u0026#34;,\u0026#34;\\u003cnone\\u003e\u0026#34;,\u0026#34;\\u003cnone\\u003e\u0026#34;],\u0026#34;object\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;PartialObjectMetadata\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;meta.k8s.io/v1\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;ngx\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;uid\u0026#34;:\u0026#34;9141b021-f735-4d4f-97d7-a16ba477312b\u0026#34;,\u0026#34;resourceVersion\u0026#34;:\u0026#34;749131\u0026#34;,\u0026#34;creationTimestamp\u0026#34;:\u0026#34;2023-01-17T14:20:09Z\u0026#34;,\u0026#34;labels\u0026#34;:{\u0026#34;run\u0026#34;:\u0026#34;ngx\u0026#34;},\u0026#34;managedFields\u0026#34;:[{\u0026#34;manager\u0026#34;:\u0026#34;kubectl-run\u0026#34;,\u0026#34;operation\u0026#34;:\u0026#34;Update\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2023-01-17T14:20:09Z\u0026#34;,\u0026#34;fieldsType\u0026#34;:\u0026#34;FieldsV1\u0026#34;,\u0026#34;fieldsV1\u0026#34;:{\u0026#34;f:metadata\u0026#34;:{\u0026#34;f:labels\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:run\u0026#34;:{}}},\u0026#34;f:spec\u0026#34;:{\u0026#34;f:containers\u0026#34;:{\u0026#34;k:{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;ngx\\\u0026#34;}\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:image\u0026#34;:{},\u0026#34;f:imagePullPolicy\u0026#34;:{},\u0026#34;f:name\u0026#34;:{},\u0026#34;f:resources\u0026#34;:{},\u0026#34;f:terminationMessagePath\u0026#34;:{},\u0026#34;f:terminationMessagePolicy\u0026#34;:{}}},\u0026#34;f:dnsPolicy\u0026#34;:{},\u0026#34;f:enableServiceLinks\u0026#34;:{},\u0026#34;f:restartPolicy\u0026#34;:{},\u0026#34;f:schedulerName\u0026#34;:{},\u0026#34;f:securityContext\u0026#34;:{},\u0026#34;f:terminationGracePeriodSeconds\u0026#34;:{}}}},{\u0026#34;manager\u0026#34;:\u0026#34;Go-http-client\u0026#34;,\u0026#34;operation\u0026#34;:\u0026#34;Update\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2023-05-11T03:46:17Z\u0026#34;,\u0026#34;fieldsType\u0026#34;:\u0026#34;FieldsV1\u0026#34;,\u0026#34;fieldsV1\u0026#34;:{\u0026#34;f:status\u0026#34;:{\u0026#34;f:conditions\u0026#34;:{\u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;ContainersReady\\\u0026#34;}\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:lastProbeTime\u0026#34;:{},\u0026#34;f:lastTransitionTime\u0026#34;:{},\u0026#34;f:status\u0026#34;:{},\u0026#34;f:type\u0026#34;:{}},\u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;Initialized\\\u0026#34;}\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:lastProbeTime\u0026#34;:{},\u0026#34;f:lastTransitionTime\u0026#34;:{},\u0026#34;f:status\u0026#34;:{},\u0026#34;f:type\u0026#34;:{}},\u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;Ready\\\u0026#34;}\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:lastProbeTime\u0026#34;:{},\u0026#34;f:lastTransitionTime\u0026#34;:{},\u0026#34;f:status\u0026#34;:{},\u0026#34;f:type\u0026#34;:{}}},\u0026#34;f:containerStatuses\u0026#34;:{},\u0026#34;f:hostIP\u0026#34;:{},\u0026#34;f:phase\u0026#34;:{},\u0026#34;f:podIP\u0026#34;:{},\u0026#34;f:podIPs\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;k:{\\\u0026#34;ip\\\u0026#34;:\\\u0026#34;172.17.0.2\\\u0026#34;}\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:ip\u0026#34;:{}}},\u0026#34;f:startTime\u0026#34;:{}}},\u0026#34;subresource\u0026#34;:\u0026#34;status\u0026#34;}]}}}]} NAME READY STATUS RESTARTS AGE ngx 1/1 Running 1 (4h30m ago) 113d 可以看到，kubectl 客户端等价于调用了 curl，向 8443 端口发送了 HTTP GET 请求，URL 是 /api/v1/namespaces/default/pods。\n如何描述 # 可以使用 YAML 描述并创建 API 对象。\napiVersion: v1 kind: Pod metadata: name: ngx-pod labels: env: demo owner: xiaobinqt spec: containers: - image: nginx:alpine name: ngx ports: - containerPort: 80 apiVersion 表示操作这种资源的 API 版本号，由于 Kubernetes 的迭代速度很快，不同的版本创建的对象会有差异，为了区分这些版本就需要使用 apiVersion 这个字段，比如 v1、v1alpha1、v1beta1 等等。\nkind 表示资源对象的类型，这个应该很好理解，比如 Pod、Node、Job、Service 等等。\nmetadata 表示的是资源的一些 “元信息”，也就是用来标记对象，方便 Kubernetes 管理的一些信息。一般来说，“metadata” 里应该有 name 和 labels 这两个字段。\napiVersion、kind、metadata 这三个字段是任何对象都必须有的，由于每种对象会有不同的规格定义，在 YAML 里就表现为 spec 字段（即 specification），表示我们对对象的 “期望状态”（desired status）。\n以下的这个 spec 里是一个 containers 数组，里面的每个元素又是一个对象，指定了名字、镜像、端口等信息：\nspec: containers: - image: nginx:alpine name: ngx ports: - containerPort: 80 如何使用 # 使用 kubectl apply、kubectl delete，加上参数 -f 指定描述的 YAML 文件，就可以创建或者删除对象了：\nkubectl apply -f ngx-pod.yml kubectl delete -f ngx-pod.yml Kubernetes 收到 YAML 声明的数据，再根据 HTTP 请求里的 POST/DELETE 等方法，就会自动操作这个资源对象，至于对象在哪个节点上、怎么创建、怎么删除完全不用使用者操心。\n编写 YAML 描述 # kubectl explain，相当于是 Kubernetes 自带的 API 文档，会给出对象字段的详细说明。比如想要看 Pod 里的字段该怎么写，就可以这样：\nkubectl explain pod kubectl explain pod.metadata kubectl explain pod.spec kubectl explain pod.spec.containers kubectl 的两个特殊参数 --dry-run=client 和 -o yaml，前者是空运行，后者是生成 YAML 格式，结合起来使用就会让 kubectl 不会有实际的创建动作，而只生成 YAML 文件。\n例如，想要生成一个 Pod 的 YAML 样板示例，可以在 kubectl run 后面加上这两个参数：\nkubectl run ngx --image=nginx:alpine --dry-run=client -o yaml 参考 # K8S Architecture Kubernetes API "},{"id":8,"href":"/kubernetes/docs/part1-primary/1.4-image/","title":"1.4 镜像","section":"第一部分 Docker","content":" 1.4 镜像 # 1.4.1 什么是镜像 # 镜像和常见的 tar、rpm、deb 等安装包一样，都打包了应用程序，但最大的不同点在于它里面不仅有基本的可执行文件，还有应用运行时的整个系统环境。这就让镜像具有了非常好的跨平台便携性和兼容性，能够让开发者在一个系统上开发（例如 Ubuntu），然后打包成镜像，再去另一个系统上运行（例如 CentOS），完全不需要考虑环境依赖的问题，是一种更高级的应用打包方式。\n1.4.2 镜像的内部机制 # 容器镜像内部并不是一个平坦的结构，而是由许多的镜像层组成的，每层都是只读不可修改的一组文件，相同的层可以在镜像之间共享，然后多个层像搭积木一样堆叠起来，再使用一种叫 “Union FS 联合文件系统” 的技术把它们合并在一起，就形成了容器最终看到的文件系统。\nDocker 会检查是否有重复的层，如果本地已经存在就不会重复下载，如果层被其他镜像共享就不会删除，这样就可以节约磁盘和网络成本。\n1.4.3 容器化应用 # “容器化的应用” 或 “应用的容器化”，就是指应用程序不再直接和操作系统打交道，而是封装成镜像，再交给容器环境去运行。镜像就是静态的应用容器，容器就是动态的应用镜像，两者互相依存，互相转化，密不可分。\n1.4.4 镜像的命名规则 # 镜像的完整名字由两个部分组成，名字和标签，中间用:连接起来。\n名字表明了应用的身份，比如 busybox、Alpine、Nginx、Redis 等等。\n标签（tag）可以理解成是为了区分不同版本的应用而做的额外标记，任何字符串都可以，比如 3.15 是纯数字的版本号、jammy 是项目代号、1.21-alpine 是版本号加操作系统名等等。其中有一个比较特殊的标签叫 “latest”，它是默认的标签，如果只提供名字没有附带标签，那么就会使用这个默认的 “latest” 标签。\n通常来说，镜像标签的格式是应用的版本号加上操作系统。版本号基本上都是主版本号 + 次版本号 + 补丁号的形式，有的还会在正式发布前出 rc 版（候选版本，release candidate）。而操作系统的情况略微复杂，因为各个 Linux 发行版的命名方式 “花样” 太多。Alpine、CentOS 的命名比较简单明了，就是数字的版本号，像 alpine3.15 ，而 Ubuntu、Debian 则采用了代号的形式。比如 Ubuntu 18.04 是 bionic，Ubuntu 20.04 是 focal，Debian 9 是 stretch，Debian 10 是 buster，Debian 11 是 bullseye。另外，有的标签还会加上 slim、fat，来进一步表示这个镜像的内容是经过精简的，还是包含了较多的辅助工具。通常 slim 镜像会比较小，运行效率高，而 fat 镜像会比较大，适合用来开发调试。\n如上图，REPOSITORY 列就是镜像的名字，TAG 就是这个镜像的标签。IMAGE ID 是镜像唯一的标识，就好像是身份证号一样。这里用 REPOSITORY 而不是 IMAGE 是因为 Docker 认为一系列同名但不同版本的镜像构成了一个集合，就好像是一个 “镜像存储库”，用 REPOSITORY 来表述更加恰当，相当于 GitHub 上的 Repository。\n同一个镜像也可以打上不同的标签，也就是说 IMAGE ID 一样，但是 TAG 可以不一样。如果一个镜像同时具有多个标签就不能直接使用 IMAGE ID 来删除，Docker 会提示镜像存在多个引用（即标签），拒绝删除。\nIMAGE ID 还有一个好处，因为它是十六进制形式且唯一，Docker 特意为它提供了 “短路” 操作，在本地使用镜像的时候，我们不用像名字那样要完全写出来这一长串数字，通常只需要写出前三位就能够快速定位，在镜像数量比较少的时候用两位甚至一位数字也许就可以了。\n1.4.5 Dockerfile # Dockerfile 是一个纯文本，里面记录了一系列的构建指令，比如选择基础镜像、拷贝文件、运行脚本等等，每个指令都会生成一个 Layer，而 Docker 顺序执行这个文件里的所有步骤，最后就会创建出一个新的镜像出来。比如：\n# Dockerfile.busybox FROM busybox # 选择基础镜像 CMD echo \u0026#34;hello world\u0026#34; # 启动容器时默认运行的命令 第一条指令是 FROM，所有的 Dockerfile 都要从它开始，表示选择构建使用的基础镜像，\n第二条指令是 CMD，它指定 docker run 启动容器时默认运行的命令，这里使用了 echo 命令，输出 “hello world” 字符串。\n利用 docker build 命令可以根据 Dockerfile 文件创建出镜像。-f 参数可以指定 Dockerfile 文件名，后面必须跟一个文件路径，叫做 “构建上下文”（build’s context），这里只是一个简单的点号，表示当前路径。\n新的镜像暂时还没有名字，用 docker images 会看到是 创建镜像的时候应当尽量使用 -t 参数，为镜像起一个有意义的名字，方便管理。名字必须要符合命名规范，用 : 分隔名字和标签，如果不提供标签默认就是 “latest”。\n只有名字 my1234 没有 tag，会默认加上 latest tag：\n编写规范 # Dockerfile 中的指令，RUN, COPY, ADD 会生成新的镜像层（其它指令只会产生临时层，不影响构建大小），所以在 Dockerfile 里最好不要滥用指令，尽量精简合并，否则太多的层会导致镜像非常臃肿。\n构建镜像的第一条指令必须是 FROM，所以基础镜像的选择非常关键。如果关注的是镜像的安全和大小，那么一般会选择 Alpine；如果关注的是应用的运行稳定性，那么可能会选择 Ubuntu、Debian、CentOS。\n如果需要将源码、配置文件等打包进镜像里，可以使用 COPY 命令，它的用法和 Linux 的 cp 差不多，不过拷贝的源文件必须是 “构建上下文” 路径里的，不能随意指定文件。\nCOPY ./a.txt /tmp/a.txt # 把构建上下文里的 a.txt 拷贝到镜像的 /tmp 目录 COPY /etc/hosts /tmp # 错误！不能使用构建上下文之外的文件 RUN 命令可以执行任意的 Shell 命令，比如更新系统、安装应用、下载文件、创建目录、编译程序等等，实现任意的镜像构建步骤。\nDockerfile 里一条指令只能是一行，所以有的 RUN 指令会在每行的末尾使用续行符 \\，命令之间也会用 \u0026amp;\u0026amp; 来连接，这样保证在逻辑上是一行。\nRUN apt-get update \\ \u0026amp;\u0026amp; apt-get install -y \\ build-essential \\ \u0026amp;\u0026amp; cd /tmp \\ 有的时候在 Dockerfile 里写超长的 RUN 指令很不美观，而且一旦写错了，每次调试都要重新构建也很麻烦，可以采用一种变通的技巧：把这些 Shell 命令集中到一个脚本文件里，用 COPY 命令拷贝进去再用 RUN 来执行：\nCOPY setup.sh /tmp/ # 拷贝脚本到/tmp目录 RUN cd /tmp \u0026amp;\u0026amp; chmod +x setup.sh \\ # 添加执行权限 \u0026amp;\u0026amp; ./setup.sh \u0026amp;\u0026amp; rm setup.sh # 运行脚本然后再删除 参数化 # RUN 指令实际上就是 Shell 编程，在 Shell 编程中有变量的概念，可以实现参数化运行，这在 Dockerfile 里也可以做到，需要使用两个指令 ARG 和 ENV。它们区别在于：\nARG 创建的变量只在镜像构建过程中可见，容器运行时不可见，而 ENV 创建的变量不仅能够在构建镜像的过程中使用，在容器运行时也能够以环境变量的形式被应用程序使用。\n下面是一个简单的例子，使用 ARG 定义了基础镜像的名字（可以用在 “FROM” 指令里），使用 ENV 定义了两个环境变量：\nARG IMAGE_BASE=\u0026#34;node\u0026#34; ARG IMAGE_TAG=\u0026#34;alpine\u0026#34; ENV PATH=$PATH:/tmp ENV DEBUG=OFF 参考 # Dockerfile reference "},{"id":9,"href":"/kubernetes/docs/part2-break-ice/2.4-pod/","title":"2.4 Pod","section":"第二部分 入门","content":" 2.4 Pod # 2.4.1 什么是 Pod # 当容器进入到现实的生产环境中时，容器的隔离性就带来了一些麻烦。因为很少有应用是完全独立运行的，经常需要几个进程互相协作才能完成任务。比如可能有多个应用结合得非常紧密以至于无法把它们拆开，但是将它们都放在同一个容器中又不是一种好的做法，因为容器的理念是对应用的独立封装，它里面就应该是一个进程、一个应用，如果里面有多个应用，不仅违背了容器的初衷，也会让容器更难以管理。\n为了解决多应用联合运行的问题，同时还要不破坏容器的隔离，就需要在容器外面再建立一个 “收纳舱”，让多个容器既保持相对独立，又能够小范围共享网络、存储等资源，而且永远是 “绑在一起” 的状态。这就是 Pod 的初衷，实际上，“spec.containers” 字段其实是一个数组，里面允许定义多个容器。\nPod 是对容器的 “打包”，里面的容器是一个整体，总是能够一起调度、一起运行，绝不会出现分离的情况。Pod 属于 Kubernetes，可以在不触碰下层容器的情况下任意定制修改。Kubernetes 让 Pod 去编排处理容器，然后把 Pod 作为应用调度部署的最小单位，Pod 也因此成为了 Kubernetes 世界里的 “原子”，基于 Pod 就可以构建出更多更复杂的业务形态了。\n2.4.2 YAML 描述 Pod # 可以理解为所有的 API 对象都天然具有 apiVersion、kind、metadata、spec 这四个基本组成部分，当然也包括 Pod。\n在使用 Docker 创建容器的时候，可以不给容器起名字，但在 Kubernetes 里，Pod 必须要有一个名字，这也是 Kubernetes 里所有资源对象的一个约定。通常会为 Pod 名字统一加上 pod 后缀，这样可以和其他类型的资源区分开。\nname 只是一个基本的标识，信息有限，所以 labels 字段就很有用，它可以添加任意数量的 Key-Value，给 Pod “贴” 上归类的标签，结合 name 就更方便识别和管理。比如：\napiVersion: v1 kind: Pod metadata: name: busy-pod labels: owner: xiaobinqt env: demo region: north tier: back “metadata” 一般写上 name 和 labels 就足够了，但是 “spec” 字段由于需要管理、维护 Pod 这个基本调度单元，里面有非常多的关键信息，厂常见的有 containers、hostname、restartPolicy 等字段。\ncontainers # containers 是一个数组，里面的每一个元素又是一个 container 对象，也就是容器。container 对象必须要有一个 name 表示名字，还要有一个 image 字段来说明它使用的镜像，这两个字段是必须要有的，否则会报数据验证错误。\ncontainer 对象的其他字段还有：\nports：列出容器对外暴露的端口，和 Docker 的 -p 参数有点像。\nimagePullPolicy：指定镜像的拉取策略，可以是 Always/Never/IfNotPresent，一般默认是 IfNotPresent，也就是说只有本地不存在才会远程拉取镜像，可以减少网络消耗。\nenv：定义 container 的环境变量，和 Dockerfile 里的 ENV 指令有点类似，但它是运行时指定的，更加灵活可配置。\ncommand：定义容器启动时要执行的命令，相当于 Dockerfile 里的 ENTRYPOINT 指令。\nargs：它是 command 运行时的参数，相当于 Dockerfile 里的 CMD 指令。\nhostname # TODO\nrestartPolicy # TODO\n下面的 spec 部分，添加 env、command、args 等字段：\nspec: containers: - image: busybox:latest name: busy imagePullPolicy: IfNotPresent env: - name: os value: \u0026#34;ubuntu\u0026#34; - name: debug value: \u0026#34;on\u0026#34; command: - /bin/echo args: - \u0026#34;$(os), $(debug)\u0026#34; 指定使用镜像 busybox:latest，拉取策略是 IfNotPresent ，然后定义了 os 和 debug 两个环境变量，启动命令是 /bin/echo，参数里输出刚才定义的环境变量。\n2.4.3 kubectl 操作 Pod # apply，delete # kubectl apply、kubectl delete 这两个命令可以使用 -f 参数指定 YAML 文件创建或者删除 Pod，例如：\nkubectl apply -f busy-pod.yml kubectl delete -f busy-pod.yml 在 delete 删除 pod 时也可以使用 YAML 里定义了 “metadata.name” 字段：\nkubectl delete pod busy-pod logs # 可以使用 kubectl logs 可以查看 Pod 日志\nkubectl logs busy-pod 运行状态 # 使用命令 kubectl get pod 可以查看 Pod 列表和运行状态：\nkubectl get pod READY 栏显示的是 Pod 内部的容器状态，格式是 x/y,表示 Pod 里总共定义了 y 个容器，其中 x 个是正常的（ready）。\ndescribe pod # 如果某个 Pod 运行有点不正常，比如状态是 “CrashLoopBackOff”，可以使用命令 kubectl describe 来检查它的详细状态：\nkubectl describe pod busybox-pod 比如有 yaml 描述：\napiVersion: v1 kind: Pod metadata: name: busybox-pod spec: containers: - image: busybox:latest name: busy imagePullPolicy: IfNotPresent env: - name: os value: \u0026#34;ubuntu\u0026#34; - name: debug value: \u0026#34;on\u0026#34; command: - /bin/echo args: - \u0026#34;$(os), $(debug)\u0026#34; 对于这个 busybox-pod，因为它只执行了一条 echo 命令就退出了，而 Kubernetes 默认会重启 Pod，所以就会进入一个反复停止 - 启动的循环错误状态。\ncp，exec # kubectl 也提供与 docker 类似的 cp 和 exec 命令，kubectl cp 可以把本地文件拷贝进 Pod，kubectl exec 是进入 Pod 内部执行 Shell 命令：\necho \u0026#39;aaa\u0026#39; \u0026gt; a.txt kubectl cp a.txt ngx-pod:/tmp 准确地说，“kubectl cp”、“kubectl exec” 操作的应该是 Pod 里的容器，需要用 “-c” 参数指定容器名，不过因为大多数 Pod 里只有一个容器，所以就省略了。\nkubectl exec 的命令格式与 Docker 有一点小差异，需要在 Pod 后面加上 --，把 kubectl 的命令与 Shell 命令分隔开：\nkubectl exec -it ngx-pod -- sh "},{"id":10,"href":"/kubernetes/docs/part1-primary/1.5-network/","title":"1.5 网络互通","section":"第一部分 Docker","content":" 1.5 网络互通 # 1.5.1 容器网络 # 在 Docker 网络中，有三个比较核心的概念，分别是：沙盒（Sandbox）、网络（Network）、端点（Endpoint）。\n沙盒提供了容器的虚拟网络栈，也就是端口套接字、IP 路由表、防火墙等内容。实现隔离容器网络与宿主机网络，形成了完全独立的容器网络环境。\n网络可以理解为 Docker 内部的虚拟子网，网络内的参与者相互可见并能够进行通讯。Docker 的这种虚拟网络也是与宿主机网络存在隔离关系的，其目的主要是形成容器间的安全通讯环境。\n端点是位于容器或网络隔离墙之上的 “洞”，其主要目的是形成一个可以控制的突破封闭的网络环境的出入口。当容器的端点与网络的端点形成配对后，就如同在这两者之间搭建了桥梁，便能够进行数据传输了。\n这三者形成了 Docker 网络的核心模型，也就是容器网络模型（Container Network Model）。\n1.5.2 网络驱动 # Docker 官方提供了五种基础的 Docker 网络驱动：Bridge Driver、Host Driver、Overlay Driver、MacLan Driver、None Driver，并基于这些网络驱动又衍生了一些其他的网络驱动，如 IPvlan。\nBridge # Bridge（桥接）网络是默认的网络驱动程序，它提供了容器之间的基本网络通信功能。Docker 桥接网络通过在主机上创建一个虚拟网桥并将容器连接到该网桥来实现容器之间的通信。\n当创建一个桥接网络时，Docker 会在主机上创建一个虚拟网桥（默认为 docker0），并为该网桥分配一个 IP 地址172.17.0.1。每个容器连接到这个桥接网络时，都会分配一个唯一的 B 类私 IP 地址，如172.17.0.2，并通过网络地址转换（NAT）实现与主机和其他容器之间的通信。\n端口号映射需要使用 bridge 模式，并且在 docker run 启动容器时使用 -p 参数，用:分隔本机端口和容器端口。\n使用 Docker 桥接网络有以下特点：\n默认网络驱动程序：桥接网络是 Docker 的默认网络驱动程序，因此当创建容器时，如果没有显式指定网络驱动程序，则会自动使用桥接网络。\n内部网络隔离：每个桥接网络都有自己的 IP 地址范围（默认为172.17.0.0/16），容器之间在网络上是相互隔离的，它们可以使用相同的 IP 地址范围而不会发生冲突。\n网络地址转换（NAT）：通过桥接网络，容器可以与主机和其他容器进行通信。桥接网络使用网络地址转换（NAT）将容器的私有 IP 地址转换为主机的公共 IP 地址。\n主机网络连接：桥接网络允许容器与主机网络连接，容器可以访问主机上的网络服务。\n创建和管理桥接网络的命令包括：\n创建一个桥接网络： docker network create --driver bridge \u0026lt;network_name\u0026gt; 运行容器并连接到桥接网络： docker run --network=\u0026lt;network_name\u0026gt; --name \u0026lt;container_name\u0026gt; -d \u0026lt;image_name\u0026gt; 桥接网络在单主机上提供了容器之间的基本网络通信功能。如果需要在多个主机之间建立容器网络通信，可以考虑使用 Overlay 网络驱动程序或其他支持跨主机通信的网络驱动程序。\nHost # host 是一种特殊的网络模式，它使用宿主机的网络命名空间，将容器直接连接到宿主机的网络上。在使用 host 网络模式时，容器与宿主机共享相同的网络接口，可以直接访问宿主机上的网络服务。\n如果在本机和容器里分别执行 ip addr 命令：\nip addr # 本机查看网卡 docker exec xxx ip addr # 容器查看网卡 可以看到这两个 ip addr 命令的输出信息是完全一样的，这就证明容器确实与本机共享了网络栈。\n使用 host 网络模式有以下特点：\n容器与宿主机网络共享：容器不会创建自己的网络命名空间，而是与宿主机共享相同的网络命名空间，因此容器可以直接使用宿主机的网络接口和 IP 地址。\n无网络隔离：容器与宿主机共享网络命名空间，它们之间的网络隔离较低。容器可以直接访问宿主机上的网络服务，并且容器之间也可以直接进行网络通信，不需要通过端口映射或网络转发。\n更高的网络性能：由于容器直接连接到宿主机网络，无需进行网络地址转换（NAT）或端口映射，因此在 host 网络模式下，容器与网络之间的通信性能更高。\nhost 网络模式适用于以下情况：\n需要容器与宿主机共享相同的网络命名空间，以便容器可以直接访问宿主机上的网络服务。\n需要容器之间进行高性能的网络通信，而不需要经过网络地址转换或端口映射。\n要将容器设置为使用 host 网络模式，可以在Docker命令中指定--network=host，例如：\ndocker run --network=host \u0026lt;image_name\u0026gt; 由于 host 网络模式没有网络隔离，容器中运行的应用程序可以直接访问宿主机上的所有网络服务，这可能会带来一些安全风险。因此，在使用 host 网络模式时，需要特别注意网络安全和权限管理。\nOverlay # Overlay 网络是一种用于构建跨主机容器网络的网络驱动程序。它使用了 VXLAN（Virtual Extensible LAN）技术，通过在宿主机之间创建覆盖网络来实现容器之间的通信。\nOverlay 网络驱动程序在多个 Docker 守护进程之间创建一个逻辑网络，使得容器可以在不同的宿主机上运行，并且它们之间可以透明地进行通信，就好像它们在同一个本地主机上运行一样。这为构建分布式应用程序和服务提供了便利。\n在 Overlay 网络中，每个 Docker 主机上的容器都分配了一个唯一的 IP 地址。这些 IP 地址是从一个预定义的网络地址范围中动态分配的，称为 Overlay 网络的子网。每个容器的 IP 地址来自 Overlay 网络子网的地址空间。\n以使用以下命令查看每个容器的IP地址：\ndocker network inspect my-overlay-network 上述命令将返回 Overlay 网络的详细信息，包括每个容器的IP地址。在输出中，你会看到类似以下的内容：\n[ { \u0026#34;Name\u0026#34;: \u0026#34;my-overlay-network\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;abcde1234567890\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;swarm\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;overlay\u0026#34;, \u0026#34;Containers\u0026#34;: { \u0026#34;container1\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;container1\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;10.0.0.2/24\u0026#34;, ... }, \u0026#34;container2\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;container2\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;10.0.0.3/24\u0026#34;, ... } }, ... } ] 使用 Overlay 网络时，可以按照以下步骤进行配置：\n创建 Overlay 网络： docker network create --driver overlay --subnet=\u0026lt;subnet\u0026gt; --gateway=\u0026lt;gateway\u0026gt; \u0026lt;network_name\u0026gt; \u0026lt;subnet\u0026gt;：指定网络的子网，如192.168.0.0/24。 \u0026lt;gateway\u0026gt;：指定网络的网关地址，如192.168.0.1。 \u0026lt;network_name\u0026gt;：指定要创建的 Overlay 网络的名称。 在不同的主机上运行容器并连接到 Overlay 网络：\ndocker run --network=\u0026lt;network_name\u0026gt; --name \u0026lt;container_name\u0026gt; -d \u0026lt;image_name\u0026gt; \u0026lt;network_name\u0026gt;：指定要连接的 Overlay 网络。 \u0026lt;container_name\u0026gt;：指定容器的名称。 \u0026lt;image_name\u0026gt;：指定容器所使用的镜像。 在这个配置下，Docker 会自动管理 Overlay 网络的路由和连接，使得跨主机的容器可以直接进行通信。Overlay 网络使用 VXLAN 技术将数据包封装在 UDP 包中，并通过宿主机之间的隧道进行传输。\n使用 Overlay 网络驱动程序需要满足一些要求，包括宿主机的内核版本支持 VXLAN 和网络互联的设置。此外，Overlay 网络驱动程序还支持使用 Swarm 模式来创建容器集群，并提供内置的负载均衡和服务发现功能，使得分布式应用程序的部署和管理更加简便。\nMacvlan # Macvlan 网络驱动程序允许将容器连接到宿主机网络上的物理网络接口，使得容器可以直接与宿主机网络上的其他设备进行通信，而不需要通过 NAT 或端口映射。它提供了更高级别的网络功能和更好的性能。\n使用 Macvlan 网络驱动程序时，每个容器都有一个唯一的 MAC 地址和 IP 地址。容器的 IP 地址是通过动态主机配置协议（DHCP）或手动配置（静态分配）来获取的，并可以直接与宿主机网络上的其他设备通信，就像它是一个独立的物理设备一样。\n[ { \u0026#34;Name\u0026#34;: \u0026#34;my-macvlan-network\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;abcde1234567890\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;macvlan\u0026#34;, \u0026#34;IPAM\u0026#34;: { \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;192.168.1.0/24\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;192.168.1.1\u0026#34; } ] }, \u0026#34;Containers\u0026#34;: { \u0026#34;container1\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;container1\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;192.168.1.2/24\u0026#34;, ... }, \u0026#34;container2\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;container2\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;192.168.1.3/24\u0026#34;, ... } }, ... } ] 以下是使用 Macvlan 网络驱动程序的一般步骤：\n创建Macvlan网络： docker network create -d macvlan --subnet=\u0026lt;subnet\u0026gt; --gateway=\u0026lt;gateway\u0026gt; -o parent=\u0026lt;parent_interface\u0026gt; \u0026lt;network_name\u0026gt; \u0026lt;subnet\u0026gt;：指定网络的子网，如192.168.0.0/24。 \u0026lt;gateway\u0026gt;：指定网络的网关地址，如192.168.0.1。 \u0026lt;parent_interface\u0026gt;：指定宿主机上的父接口，即物理网络接口。 运行容器并连接到 Macvlan 网络： docker run --network=\u0026lt;network_name\u0026gt; --name \u0026lt;container_name\u0026gt; -d \u0026lt;image_name\u0026gt; \u0026lt;network_name\u0026gt;：指定要连接的 Macvlan 网络。 \u0026lt;container_name\u0026gt;：指定容器的名称。 \u0026lt;image_name\u0026gt;：指定容器所使用的镜像。 通过这样的配置，容器将具有独立的 MAC 地址，并且可以直接与宿主机网络上的其他设备进行通信。使用 Macvlan 网络驱动程序需要宿主机的网络设备支持将虚拟 MAC 地址传递到物理网络上。\nIPvlan # 基于 MacVLAN 的网络驱动程序还有一种新的驱动程序 IPvlan，它提供了更高级别的功能和灵活性。IPvlan 驱动程序允许创建具有独立 MAC 地址的虚拟网络接口，容器可以拥有自己的唯一 IP 地址，并直接与物理网络进行通信，而无需进行NAT转换。\n[ { \u0026#34;Name\u0026#34;: \u0026#34;my-ipvlan-network\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;abcde1234567890\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;ipvlan\u0026#34;, \u0026#34;IPAM\u0026#34;: { \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;192.168.1.0/24\u0026#34; } ] }, \u0026#34;Containers\u0026#34;: { \u0026#34;container1\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;container1\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;192.168.1.2/24\u0026#34;, ... }, \u0026#34;container2\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;container2\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;192.168.1.3/24\u0026#34;, ... } }, ... } ] IPvlan 提供了三种模式：L2（二层）模式、L3（三层）模式和 L3s（二层和三层混合）模式，可以根据需要选择合适的模式。\nL2 模式下，容器可以直接与主机网络上的其他设备进行通信，但容器之间无法直接通信。\nL3 模式下，容器可以在容器网络内直接通信，容器之间和主机之间的通信需要通过路由器进行。\nL3s 模式结合了 L2 和 L3 模式的特点，容器既可以直接与主机网络上的其他设备通信，也可以在容器网络内直接通信。\n使用 ipvlan 网络驱动程序，可以按照以下步骤配置容器网络：\n创建 ipvlan 网络： docker network create -d ipvlan --subnet=\u0026lt;subnet\u0026gt; --gateway=\u0026lt;gateway\u0026gt; --ip-range=\u0026lt;ip-range\u0026gt; -o ipvlan_mode=\u0026lt;mode\u0026gt; -o parent=\u0026lt;parent_interface\u0026gt; \u0026lt;network_name\u0026gt; \u0026lt;subnet\u0026gt;：指定网络的子网，如192.168.0.0/24。 \u0026lt;gateway\u0026gt;：指定网络的网关地址，如192.168.0.1。 \u0026lt;ip-range\u0026gt;：指定网络的IP地址范围，如192.168.0.2/28。 \u0026lt;mode\u0026gt;：指定 ipvlan 模式，可以是 l2、l3 或 l3s。 \u0026lt;parent_interface\u0026gt;：指定宿主机上的父接口，即物理网络接口。 运行容器并连接到 ipvlan 网络： docker run --network=\u0026lt;network_name\u0026gt; --name \u0026lt;container_name\u0026gt; -d \u0026lt;image_name\u0026gt; \u0026lt;network_name\u0026gt;：指定要连接的 ipvlan 网络。 \u0026lt;container_name\u0026gt;：指定容器的名称。 \u0026lt;image_name\u0026gt;：指定容器所使用的镜像。 通过这样的配置，容器将具有独立的 MAC 地址，并可以直接与物理网络上的其他设备进行通信。\nipvlan 网络驱动程序需要宿主机的内核版本支持，并且一些网络功能可能受到宿主机网络设备的限制。此外，ipvlan 网络驱动程序在多宿主机的分布式环境下也可以使用，以构建具有高度可扩展性和灵活性的容器网络。\nNone # None 驱动程序禁用容器的网络功能，使其完全与外部网络隔离。使用 None 驱动程序时，容器将无法进行网络通信，包括与宿主主机或其他容器的通信。 这种网络模式适用于某些安全要求较高的容器，或者仅用于计算任务而不需要网络访问的容器。\n1.5.3 NAT 网络地址转换 # 网络地址转换（Network Address Translation，NAT）是一种网络技术，用于在不同网络之间转换 IP 地址。NAT 常用于连接私有网络（如家庭网络或企业内部网络）与公共网络（如 Internet）之间，以实现多个设备共享单个公共 IP 地址的功能。\nNAT 的主要目的是解决 IPv4 地址短缺的问题。由于 IPv4 地址资源有限，当一个网络中有多个设备需要连接到 Internet 时，不可能为每个设备都分配一个唯一的公共 IP 地址。这就引入了 NAT 作为一种解决方案。\nNAT 通过在网络边界的设备（通常是路由器或防火墙）上执行地址转换，将内部私有 IP 地址转换为外部公共 IP 地址。这样，内部网络中的多个设备可以共享一个或一组公共 IP 地址来访问 Internet。\nNAT有几种常见的模式：\n静态 NAT（Static NAT）：一对一映射，将内部私有 IP 地址与外部公共 IP 地址进行静态映射，一般用于将特定的内部服务（如 Web 服务器）暴露给外部网络。\n动态 NAT（Dynamic NAT）：多对多映射，内部私有 IP 地址动态地映射到一组可用的公共 IP 地址。\nPAT（Port Address Translation）或 NAT Overload：将多个内部私有 IP 地址映射到单个公共 IP 地址，通过使用不同的端口号来区分不同的内部连接。\nNAT 提供了以下几个好处：\n节约公共 IP 地址：通过 NAT，多个设备可以使用相同的公共 IP 地址访问 Internet，节约了 IPv4 地址资源。\n增加网络安全性：由于 NAT 会隐藏内部网络的细节，对外部网络而言，只能看到公共 IP 地址，内部网络结构对外部网络来说是不可见的，提高了网络安全性。\n简化网络配置：NAT 可以简化内部网络的配置，因为内部设备不需要直接与外部网络进行通信，只需要使用内部私有 IP 地址即可。\n虽然 IPv6 地址的广泛采用可以缓解 IPv4 地址短缺问题，因为 IPv6 地址空间更大。但在过渡期间，NAT 仍然是一个常用的网络技术。\n"},{"id":11,"href":"/kubernetes/docs/part2-break-ice/2.5-job/","title":"2.5 Job","section":"第二部分 入门","content":" 2.5 Job # 2.5.1 业务分类 # Kubernetes 里的有两大业务类型。一类是像 Nginx、MySQL 这样长时间运行的 “在线业务”，一旦运行起来基本上不会停，也就是永远在线。另一类是像日志分析这样短时间运行的 “离线业务”，“离线业务” 的特点是必定会退出，不会无期限地运行下去。\n“离线业务” 可以分为两种。一种是 “临时任务”，跑完就完事了，下次有需求再重新安排；另一种是 “定时任务”，可以按时按点周期运行，不需要过多干预。在 Kubernetes 里，“临时任务” 是 API 对象 Job，“定时任务” 是 API 对象 CronJob，使用这两个对象就能够在 Kubernetes 里调度管理任意的离线业务。\n2.5.2 Job # 比如用 busybox 创建一个 “echo-job”，命令就是这样的：\nexport out=\u0026#34;--dry-run=client -o yaml\u0026#34; # 定义Shell变量 kubectl create job echo-job --image=busybox $out 会生成一个基本的 YAML 文件，保存之后做点修改，就有了一个 Job 对象：\napiVersion: batch/v1 kind: Job metadata: name: echo-job spec: template: spec: restartPolicy: OnFailure containers: - image: busybox name: echo-job imagePullPolicy: IfNotPresent command: [ \u0026#34;/bin/echo\u0026#34; ] args: [ \u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34; ] Job 的描述与 Pod 很像，但又有些不一样，主要的区别在 “spec” 字段里多了一个 template 字段，然后又是一个 “spec”，显得很奇怪。这主要是在 Job 对象里应用了组合模式，template 字段定义了一个 “应用模板”，里面嵌入了一个 Pod，这样 Job 就可以从这个模板来创建出 Pod。而这个 Pod 因为受 Job 的管理控制，不直接和 apiserver 打交道，也就没必要重复 apiVersion 等 “头字段”，只需要定义好关键的 spec，描述清楚容器相关的信息就可以了，可以说是一个 “无头” 的 Pod 对象。\napply 创建 Job 对象：\n可以看到 Pod 被自动关联了一个名字，用的是 Job 的名字（echo-job）再加上一个随机字符串。\n常用字段 # activeDeadlineSeconds，设置 Job 运行的超时时间，单位是秒。该值适用于 Job 的整个生命期，无论 Job 创建了多少个 Pod。 一旦 Job 运行时间达到 activeDeadlineSeconds 秒，其所有运行中的 Pod 都会被终止， 并且 Job 的状态更新为type: Failed及 reason: DeadlineExceeded。\nbackoffLimit，设置 Pod 的失败重试次数。\ncompletions，Job 完成需要运行多少个 Pod，默认是 1 个。\nparallelism，它与 completions 相关，表示允许并发运行的 Pod 数量，避免过多占用资源。\n这几个字段并不在 template 字段下，而是在 spec 字段下，它们是属于 Job 级别的，用来控制模板里的 Pod 对象。\n2.5.3 CronJob # 因为 CronJob 需要定时运行，所以在用 create 时需要在命令行里指定参数 --schedule。\nexport out=\u0026#34;--dry-run=client -o yaml\u0026#34; # 定义Shell变量 kubectl create cj echo-cj --image=busybox --schedule=\u0026#34;\u0026#34; $out apiVersion: batch/v1 kind: CronJob metadata: name: echo-cj spec: schedule: \u0026#39;*/1 * * * *\u0026#39; jobTemplate: spec: template: spec: restartPolicy: OnFailure containers: - image: busybox name: echo-cj imagePullPolicy: IfNotPresent command: [ \u0026#34;/bin/echo\u0026#34; ] args: [ \u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34; ] CronJob 有三个 spec 嵌套层次：\n第一个 spec 是 CronJob 自己的对象规格声明。\n第二个 spec 从属于 “jobTemplate”，它定义了一个 Job 对象，是必需的。。\n第三个 spec 从属于 “template”，它定义了 Job 里运行的 Pod。\n关于 spec.schedule 的 cron 语法，可以参看 Cron 时间表语法 https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/cron-jobs/#cron-schedule-syntax。\n# ┌───────────── 分钟 (0 - 59) # │ ┌───────────── 小时 (0 - 23) # │ │ ┌───────────── 月的某天 (1 - 31) # │ │ │ ┌───────────── 月份 (1 - 12) # │ │ │ │ ┌───────────── 周的某天 (0 - 6)（周日到周一；在某些系统上，7 也是星期日） # │ │ │ │ │ 或者是 sun，mon，tue，web，thu，fri，sat # │ │ │ │ │ # │ │ │ │ │ # * * * * * 例如0 0 13 * 5表示此任务必须在每个星期五的午夜以及每个月的 13 日的午夜开始。\n有一个非常好的网站 crontab.guru 可以很直观地解释 cron 表达式的含义。\n2.5.4 其他设置 # Job 在运行结束后不会立即删除，这是为了方便获取计算结果，但如果积累过多的已完成 Job 也会消耗系统资源，可以使用字段 .spec.ttlSecondsAfterFinished 单位是秒，设置一个保留的时限。\n出于节约资源的考虑，CronJob 不会无限地保留已经运行的 Job,它默认只保留 3 个最近的执行结果，但可 以用字段 successfulJobsHistoryLimit 改变。\napiVersion: batch/v1 kind: Job metadata: name: my-job spec: successfulJobsHistoryLimit: 5 template: # Job的模板定义 在上面的示例中，successfulJobsHistoryLimit 被设置为5，这意味着只会保留最近 5 个成功的 Job 记录。当成功的 Job 记录超过限制时，最早的记录将被删除。\nsuccessfulJobsHistoryLimit 仅适用于成功的 Job 记录。如果要限制失败的 Job 记录数量，可以使用 failedJobsHistoryLimit 参数。\n参考 # Job CronJob "},{"id":12,"href":"/kubernetes/docs/part1-primary/1.6-docker-compose/","title":"1.6 Docker Compose","section":"第一部分 Docker","content":" 1.6 Docker Compose # docker-compose 是一个在单机环境里轻量级的容器编排工具。\n在 Docker 把容器技术大众化之后，Docker 周边涌现出了数不胜数的扩展、增强产品，其中有一个名字叫 Fig 的项目。Fig 为 Docker 引入了 “容器编排” 的概念，使用 YAML 来定义容器的启动参数、先后顺序和依赖关系，让用户不再有 Docker 冗长命令行的烦恼，第一次见识到了 “声明式” 的威力。Docker 公司在 2014 年 7 月把 Fig 买了下来，集成进 Docker 内部，然后改名成了 docker-compose。\n1.6.1 安装 # docker-compose 的安装比较简单，它在 GitHub https://github.com/docker/compose 上提供了多种形式的二进制可执行文件，支持 Windows、macOS、Linux 等操作系统，也支持 x86_64、arm64 等硬件架构，可以直接下载。docker-compose 还可以安装成 docker 的插件，以子命令的形式使用，也就是docker compose（没有中间的横线），具体可以参看文档 Install the Compose plugin。建议使用传统的 docker-compose 的形式，这样兼容性更强。\nsudo curl -SL https://github.com/docker/compose/releases/download/v2.17.2/docker-compose-linux-x86_64 \\ -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose docker-compose version 1.6.2 使用 # docker-compose 里管理容器的核心概念是 service。service 就是一个容器化的应用程序，通常是一个后台服务，用 YAML 定义这些容器的参数和相互之间的关系。下面的这个就是私有镜像仓库 Registry 的 YAML 文件：\nservices: registry: image: registry container_name: registry restart: always ports: - 5000:5000 具体的 docker-compose 字段定义可以在官网 https://docs.docker.com/compose/compose-file/ 上查看。在 docker-compose 里，每个 service 都有一个自己的名字，它同时也是这个容器的唯一网络标识。可以使用命令 docker-compose up -d，同时还要用 -f 参数来指定 YAML 文件：\ndocker-compose -f reg-compose.yml up -d docker-compose 在底层还是调用的 Docker，所以它启动的容器用 docker ps 也是能够看到的。不过，用 docker-compose ps 能够看到更多的信息：\ndocker-compose -f reg-compose.yml ps 如果想要停止应用，可以使用 docker-compose down 命令：\ndocker-compose -f reg-compose.yml down 1.6.3 搭建 wordpress # 第一步，定义数据库 MariaDB，环境变量可以使用字段 environment，直接定义：\nservices: mariadb: image: mariadb:10 container_name: mariadb restart: always environment: MARIADB_DATABASE: db MARIADB_USER: wp MARIADB_PASSWORD: 123 MARIADB_ROOT_PASSWORD: 123 第二步，定义 WordPress 网站，也使用 environment 来设置环境变量：\nservices: ... wordpress: image: wordpress:5 container_name: wordpress restart: always environment: WORDPRESS_DB_HOST: mariadb #注意这里，数据库的网络标识 WORDPRESS_DB_USER: wp WORDPRESS_DB_PASSWORD: 123 WORDPRESS_DB_NAME: db depends_on: - mariadb 因为 docker-compose 会自动把 MariaDB 的名字用做网络标识，所以在连接数据库的时候（字段 WORDPRESS_DB_HOST）就不需要手动指定 IP 地址了，直接用 service 的名字 mariadb 就可以了。\nWordPress 定义里还有一个值得注意的是字段 depends_on，它用来设置容器的依赖关系，指定容器启动的先后顺序，这在编排由多个容器组成的应用的时候是一个非常便利且重要的特性。\n第三步，定义 Nginx 反向代理了，docker-compose 里要加载配置必须用外部文件，无法集成进 YAML。Nginx 的配置文件在 proxy_pass 指令里不需要写 IP 地址了，直接用 WordPress 的名字就行：\nserver { listen 80; default_type text/html; location / { proxy_http_version 1.1; proxy_set_header Host $host; proxy_pass http://wordpress; #注意这里，网站的网络标识 } } 然后就可以在 YAML 里定义 Nginx 了，加载配置文件用的是 volumes 字段：\nservices: ... nginx: image: nginx:alpine container_name: nginx hostname: nginx restart: always ports: - 80:80 volumes: - ./wp.conf:/etc/nginx/conf.d/default.conf depends_on: - wordpress 完整的 wp-compose.yml 文件如下：\nservices: mariadb: image: mariadb:10 container_name: mariadb restart: always environment: MARIADB_DATABASE: db MARIADB_USER: wp MARIADB_PASSWORD: 123 MARIADB_ROOT_PASSWORD: 123 wordpress: image: wordpress:5 container_name: wordpress restart: always environment: WORDPRESS_DB_HOST: mariadb #注意这里，数据库的网络标识 WORDPRESS_DB_USER: wp WORDPRESS_DB_PASSWORD: 123 WORDPRESS_DB_NAME: db depends_on: - mariadb nginx: image: nginx:alpine container_name: nginx hostname: nginx restart: always ports: - 80:80 volumes: - ./wp.conf:/etc/nginx/conf.d/default.conf depends_on: - wordpress 用 docker-compose up -d 启动网站：\ndocker-compose -f wp-compose.yml up -d 启动之后，可以用 docker-compose ps 来查看状态：\n可以用 docker-compose exec 来进入容器内部，验证一下这几个容器的网络标识是否工作正常：\ndocker-compose -f wp-compose.yml exec -it nginx sh 可以看到，分别 ping 了 mariadb 和 wordpress 这两个服务，网络都是通的，不过它的 IP 地址段用的是172.20.0.0/16，和 Docker 默认的172.17.0.0/16不一样。\n当在浏览器中，直接访问 IP 地址，也是可以访问到 wordpress 服务的：\n"},{"id":13,"href":"/kubernetes/docs/part2-break-ice/2.6-config-manage/","title":"2.6 配置管理","section":"第二部分 入门","content":" 2.6 配置管理 # 服务中的配置信息，从数据安全的角度来看可以分成两类：一类是明文配置，可以任意查询修改，比如服务端口、运行参数、文件路径等等。另一类则是机密配置，由于涉及敏感信息需要保密，不能随便查看，比如密码、密钥、证书等等。这两类配置信息本质上都是字符串，只是由于安全性的原因，在存放和使用方面有些差异。\nKubernetes 中的 ConfigMap API 用来保存明文配置，Secret API 用来保存秘密配置。\n2.6.1 ConfigMap # export out=\u0026#34;--dry-run=client -o yaml\u0026#34; # 定义Shell变量 kubectl create cm info --from-literal=k=v $out ConfigMap 里的数据都是 Key-Value 结构，所以 --from-literal 参数使用 k=v 的形式生成数据。ConfigMap 的 YAML 描述大概如下：\napiVersion: v1 kind: ConfigMap metadata: name: info data: count: \u0026#39;10\u0026#39; debug: \u0026#39;on\u0026#39; path: \u0026#39;/etc/systemd\u0026#39; greeting: | say hello to kubernetes. 由上图可知，现在 ConfigMap 的 Key-Value 信息就已经存入了 etcd 数据库，后续就可以被其他 API 对象使用。\n2.6.2 Secret # Secret 和 ConfigMap 的结构和用法很类似，不过 Secret 对象又细分出很多类，比如：\n访问私有镜像仓库的认证信息 身份识别的凭证信息 HTTPS 通信的证书和私钥 一般的机密信息（格式由用户自行解释） 最后一种比较常见。创建 YAML 样板的命令是 kubectl create secret generic ，可以使用参数 --from-literal 给出 Key-Value 值：\nkubectl create secret generic user --from-literal=name=root $out echo 的 -n 参数是为了去掉字符串里隐含的换行符\n比如有如下的一个 Secret YAML 描述：\napiVersion: v1 kind: Secret metadata: name: user data: name: cm9vdA== # root pwd: MTIzNDU2 # 123456 db: bXlzcWw= # mysql Secret 对配置参数做了 base64 加密。kubectl describe 不能直接看到内容，只能看到数据的大小。\n2.6.3 使用配置 # ConfigMap 和 Secret 只是一些存储在 etcd 里的字符串，如果想要在运行时产生效果，就必须要以某种方式 “注入” 到 Pod 里，让应用去读取。有两种实现途径：环境变量和加载文件。\n环境变量 # 在 Pod 里描述容器的字段 “containers” 里有一个 “env”，它定义了 Pod 里容器能够看到的环境变量。可以使用 “value” 的形式，把环境变量的值写死在了 YAML 里，比如：\nspec: containers: - image: busybox:latest name: busy env: - name: os value: \u0026#34;ubuntu\u0026#34; 这里的 os 环境变量就固定写死是 ubuntu 了。\n实际上 env 还可以使用 “valueFrom” 字段，从 ConfigMap 或者 Secret 对象里获取值，这样就实现了把配置信息以环境变量的形式注入进 Pod，也就是配置与应用的解耦。\n可以使用 kubectl explain 查看对 valueFrom 的说明：\nvalueFrom 字段指定了环境变量值的来源，可以是 configMapKeyRef 或者 secretKeyRef，然后需要再进一步指定应用的 ConfigMap/Secret 的 name 和它里面的 key，需要注意的是这个 name 字段是 API 对象的名字，也就是 ConfigMap 或是 Secret 对象里的metadata.name的值，而不是 Key-Value 的名字。\n如下的例子，ConfigMap YAML：\napiVersion: v1 kind: ConfigMap metadata: name: info data: count: \u0026#39;10\u0026#39; debug: \u0026#39;on\u0026#39; path: \u0026#39;/etc/systemd\u0026#39; greeting: | say hello to kubernetes. Secret YAML：\napiVersion: v1 kind: Secret metadata: name: user data: name: cm9vdA== # root pwd: MTIzNDU2 # 123456 db: bXlzcWw= # mysql env-pod YAML 引用了 ConfigMap 和 Secret 对象里的配置：\napiVersion: v1 kind: Pod metadata: name: env-pod spec: containers: - env: - name: COUNT valueFrom: configMapKeyRef: name: info key: count - name: GREETING valueFrom: configMapKeyRef: name: info key: greeting - name: USERNAME valueFrom: secretKeyRef: name: user key: name - name: PASSWORD valueFrom: secretKeyRef: name: user key: pwd image: busybox name: busy imagePullPolicy: IfNotPresent command: [ \u0026#34;/bin/sleep\u0026#34;, \u0026#34;300\u0026#34; ] 对于 env-pod，apply 运行起来之后，可以通过 kubectl exec 进到 pod 中：\nenvFrom # 如果 ConfigMap 里的信息比较多，用 env.valueFrom 一个个地写会非常麻烦，容易出错，而 envFrom 可以一次性地把 ConfigMap 里的字段全导入进 Pod，并且还能够指定变量名的前缀，非常方便。比如下面的这个示例：\ncm.yaml\napiVersion: v1 kind: ConfigMap metadata: name: maria-cm data: DATABASE: \u0026#39;db\u0026#39; USER: \u0026#39;wp\u0026#39; PASSWORD: \u0026#39;123\u0026#39; ROOT_PASSWORD: \u0026#39;123\u0026#39; pod.yaml\napiVersion: v1 kind: Pod metadata: name: maria-pod labels: app: wordpress role: database spec: containers: - image: mariadb:10 name: maria imagePullPolicy: IfNotPresent ports: - containerPort: 3306 envFrom: - prefix: \u0026#39;MARIADB_\u0026#39; configMapRef: name: maria-cm 这里使用 envFrom 将 maria-cm 的数据全部加载进来到 pod 里，并设置了一个 prefix 前缀。\n加载文件 # Pod 有 Volume 存储卷的概念，如果把 Pod 理解成是一个虚拟机，那么 Volume 就相当于是虚拟机里的磁盘。可以为 Pod “挂载（mount）” 多个 Volume，里面存放供 Pod 访问的数据，这种方式有点类似 docker run -v。\n在 Pod 里挂载 Volume 只需要在 “spec” 里增加一个 “volumes” 字段，然后再定义卷的名字和引用的 ConfigMap/Secret 就可以。Volume 属于 Pod，不属于容器，所以这个字段和 “containers” 字段是同级的，都属于 “spec”。\n比如，分别引用 ConfigMap 和 Secret，名字是 cm-vol 和 sec-vol：\nspec: volumes: - name: cm-vol configMap: name: info - name: sec-vol secret: secretName: user 在容器里挂载需用到 volumeMounts 字段，该字段可以把定义好的 Volume 挂载到容器里的某个路径下，还需要在里面用 mountPath，name 明确地指定挂载路径和 Volume 的名字，配置信息就可以加载成文件。\ncontainers: - volumeMounts: - mountPath: /tmp/cm-items name: cm-vol - mountPath: /tmp/sec-items name: sec-vol 比如如下的 vol-pod YAML：\napiVersion: v1 kind: Pod metadata: name: vol-pod spec: volumes: - name: cm-vol configMap: name: info - name: sec-vol secret: secretName: user containers: - volumeMounts: - mountPath: /tmp/cm-items name: cm-vol - mountPath: /tmp/sec-items name: sec-vol image: busybox name: busy imagePullPolicy: IfNotPresent command: [ \u0026#34;/bin/sleep\u0026#34;, \u0026#34;300\u0026#34; ] apply 创建后可以通过 exec 进入 pod 查看：\nConfigMap 和 Secret 都变成了目录的形式，而里面的 Key-Value 变成了一个个的文件，而文件名就是 Key。\nConfigMap 在设计上不是用来保存大量数据的。在 ConfigMap 中保存的数据不可超过 1 MiB。如果需要保存超出此尺寸限制的数据，可以考虑挂载存储卷。\n环境变量用法简单，更适合存放简短的字符串，而 Volume 更适合存放大数据量的配置文件，在 Pod 里加载成文件后让应用直接读取使用。\n参考 # ConfigMap "},{"id":14,"href":"/kubernetes/docs/part1-primary/1.7-private-registry/","title":"1.7 私有镜像仓库","section":"第一部分 Docker","content":" 1.7 私有镜像仓库 # 在离线环境里，可以自己搭建私有仓库。私有镜像仓库有很多现成的解决方案，最简单的是 Docker Registry，也有功能更完善的 CNCF Harbor。\n1.7.1 registry # 可以在 Docker Hub 网站上搜索 “registry”，找到官方页面 https://registry.hub.docker.com/_/registry/：\n首先，需要使用 docker pull 命令拉取镜像：\ndocker pull registry 然后，需要做一个端口映射，对外暴露端口，这样 Docker Registry 才能提供服务。它的容器内端口是 5000，可以再容器外也使用同样的 5000 端口，运行命令是\ndocker run -d -p 5000:5000 registry ：docker run -d -p 5000:5000 registry 启动 Docker Registry 之后，可以使用 docker ps 查看运行状态，可以看到它确实把本机的 5000 端口映射到了容器内的 5000 端口。\n可以使用 docker tag 命令给镜像打标签再上传了。因为上传的目标不是默认的 Docker Hub，而是本地的私有仓库，所以镜像的名字前面还必须再加上仓库的地址（域名或者 IP 地址都行），形式上和 HTTP 的 URL 相似。\n下面示例中，把 “nginx:alpine” 改成了 “127.0.0.1:5000/nginx:alpine”：\ndocker tag nginx:alpine 127.0.0.1:5000/nginx:alpine 现在，这个镜像有了一个附加仓库地址的完整名字，就可以用 docker push 推上去了：\ndocker push 127.0.0.1:5000/nginx:alpine 为了验证是否已经成功推送，可以把刚才打标签的镜像删掉，再重新下载：\ndocker rmi 127.0.0.1:5000/nginx:alpine docker pull 127.0.0.1:5000/nginx:alpine API # Docker Registry 没有图形界面，但提供了 RESTful API，可以发送 HTTP 请求来查看仓库里的镜像，具体的信息可以参考官方文档 https://docs.docker.com/registry/spec/api/，下面的这两条 curl 命令就分别获取了镜像列表和 Nginx 镜像的标签列表：\ncurl 127.0.0.1:5000/v2/_catalog curl 127.0.0.1:5000/v2/nginx/tags/list https 服务 # 要搭建 https 协议的 docker registry 首先需要证书。\n要生成免费的 SSL/TLS 证书，可以使用 OpenSSL 工具。下面是使用 OpenSSL 生成自签名证书的基本步骤：\n安装 OpenSSL： 根据你的操作系统，安装适用于 OpenSSL 的软件包或使用系统自带的 OpenSSL。\n在终端或命令提示符中，运行 openssl version 命令，确认 OpenSSL 是否成功安装。\n生成自签名证书： 使用以下命令生成自签名证书：\nopenssl req -x509 -days 365 -out registry.test.crt -keyout registry.test.key \\ -newkey rsa:2048 -nodes -sha256 \\ -subj \u0026#39;/CN=registry.test\u0026#39; -extensions EXT -config \u0026lt;( \\ printf \u0026#34;[dn]\\nCN=registry.test\\n[req]\\ndistinguished_name = dn\\n[EXT]\\nsubjectAltName=DNS:registry.test\\nkeyUsage=digitalSignature\\nextendedKeyUsage=serverAuth\u0026#34;) openssl req: 使用 OpenSSL 工具中的 req 子命令，用于生成证书请求或自签名证书。 -x509: 指定生成自签名证书而不是证书请求。 -days 365: 指定证书的有效期为 365 天。 -out registry.test.crt: 指定生成的证书文件名为 registry.test.crt，并将其保存在当前目录下。 -keyout registry.test.key: 指定生成的私钥文件名为 registry.test.key，并将其保存在当前目录下。 -newkey rsa:2048: 创建一个新的 RSA 密钥对，密钥长度为 2048 位。 -nodes: 在生成私钥时不加密密钥文件，这样私钥文件不会有密码保护。 -sha256: 使用 SHA-256 哈希算法签名证书。 -subj '/CN=registry.test': 设置证书的主题字段，这里指定 Common Name (CN) 为 registry.test，即证书的通用名称。 -extensions EXT -config \u0026lt;(printf \u0026quot;[dn]\\nCN=k8s.test\\n[req]\\ndistinguished_name = dn\\n[EXT]\\nsubjectAltName=DNS:k8s.test\\nkeyUsage=digitalSignature\\nextendedKeyUsage=serverAuth\u0026quot;): 配置扩展字段，用于指定主题备用名称 (Subject Alternative Name, SAN)、密钥用途和扩展密钥用途。这里将 SAN 设置为 DNS:registry.test，表示证书可以用于 registry.test 主机名的验证。 对于开发、测试或个人用途，自签名证书是一个方便且免费的选项。自签名证书不会被信任的证书颁发机构（CA）认可，因此在生产环境中可能会被浏览器或其他客户端视为不受信任。在生产环境中，可能需要使用受信任的 CA 颁发的证书。\n当有了证书，就可以使用 docker run 命令启动 registry 容器，并将配置文件和证书、私钥挂载到容器中：\ndocker run -d \\ --name registry \\ -p 5000:5000 \\ --restart always \\ -v /home/weibin/cert.d:/etc/cert.d \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/etc/cert.d/registry.test.crt \\ -e REGISTRY_HTTP_TLS_KEY=/etc/cert.d/registry.test.key \\ registry /home/weibin/cert.d 目录是宿主机的目录，里面有证书文件：\n可以看到，容器可以正常启动：\n测试请求正常：\n权限 # TODO ... 参考 # 私有仓库 Docker私有仓库Registry服务器配置SSL证书以支持HTTPS协议 "},{"id":15,"href":"/kubernetes/docs/part2-break-ice/2.7-general-cmd/","title":"2.7 常用命令","section":"第二部分 入门","content":" 2.7 常用命令 # port-forward 端口映射\n因为 Pod 都是运行在 Kubernetes 内部的私有网段里的，外界无法直接访问，想要对外暴露服务，需要使用一个专门的 kubectl port-forward 命令，它专门负责把本机的端口映射到在目标对象的端口号，有点类似 Docker 的参数 -p，经常用于 Kubernetes 的临时调试和测试。\n比如将本地的 8080 映射到 a-pod 的 80 端口，kubectl 会把这个端口的所有数据都转发给集群内部的 Pod：\nkubectl port-forward a-pod 8080:80 \u0026amp; 命令的末尾使用了一个 \u0026amp; 符号，让端口转发工作在后台进行，这样就不会阻碍我们后续的操作。如果想关闭端口转发，需要敲命令 fg ，它会把后台的任务带回到前台，然后就可以简单地用 “Ctrl + C” 来停止转发了。\n"},{"id":16,"href":"/kubernetes/docs/part2-break-ice/2.8-kubeadm/","title":"2.8 kubeadm 搭建","section":"第二部分 入门","content":" 2.8 kubeadm 搭建 # kubeadm 和 minikube 类似，也是用容器和镜像来封装 Kubernetes 的各种组件，但它的目标不是单机部署，而是要能够轻松地在集群环境里部署 Kubernetes，并且让这个集群接近甚至达到生产级别质量。\nkubeadm 具有了和 minikube 一样的易用性，只要很少的几条命令，如 init、join、upgrade、reset 就能够完成 Kubernetes 集群的管理维护工作，让它不仅适用于集群管理员，也适用于开发、测试人员。\n2.8.1 准备工作 # 所谓的多节点集群，要求服务器应该有两台或者更多，其实最小可用的 Kubernetes 集群就只有两台主机，一台是 Master 节点，另一台是 Worker 节点。Master 节点需要运行 apiserver、etcd、scheduler、controller-manager 等组件，管理整个集群，Worker 节点只运行业务应用。\n因为 Kubernetes 对系统有一些特殊要求，所以要先在 Master 和 Worker 节点上做一些准备，包括改主机名、改 Docker 配置、改网络设置、改交换分区这四步。\n第一，由于 Kubernetes 使用主机名来区分集群里的节点，所以每个节点的 hostname 必须不能重名。需要修改 /etc/hostname 这个文件，把它改成容易辨识的名字，比如 Master 节点就叫 master，Worker 节点就叫 worker。\n第二，虽然 Kubernetes 目前支持多种容器运行时，但 Docker 还是最方便最易用的一种，所以使用 Docker 作为 Kubernetes 的底层支持，这里可以参考 Docker 官网 安装 Docker Engine。安装完成后需要再对 Docker 的配置做一点修改，在 /etc/docker/daemon.json 里把 cgroup 的驱动程序改成 systemd ，然后重启 Docker 的守护进程，具体的操作如下：\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/docker/daemon.json { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;100m\u0026#34; }, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34; } EOF sudo systemctl enable docker sudo systemctl daemon-reload sudo systemctl restart docker 在 Docker 中，cgroup 驱动程序是用来管理和限制容器资源的一种机制。默认情况下，Docker 使用 cgroupfs 作为 cgroup 驱动程序。然而，将 Docker 的 cgroup 驱动程序更改为 systemd 可以带来一些好处：\n与系统一致性：将 Docker 的 cgroup 驱动程序设置为 systemd 可以使 Docker 与系统中的其他进程一致。这样，Docker 容器的资源管理将与其他系统服务使用的 cgroup 驱动程序一致，简化了资源管理和监控。\n安全性和隔离性：systemd-cgroups 提供了更强大的资源隔离和安全性特性。使用 systemd 作为 cgroup 驱动程序可以更好地利用 systemd 提供的资源控制和隔离功能，进一步增强容器的安全性和资源限制能力。\n性能改进：systemd-cgroups 在某些情况下可能会提供更好的性能。systemd-cgroups 使用一个单一的 cgroup 作为容器的父级 cgroup，而 cgroupfs 则在每个层级都使用单独的 cgroup。这种改变可能会减少 cgroup 操作的数量，从而提高性能。\n需要注意的是，将 Docker 的 cgroup 驱动程序更改为 systemd 需要系统已经安装和配置了 systemd，并且与 Docker 版本兼容。\n第三，为了让 Kubernetes 能够检查、转发网络流量，你需要修改 iptables 的配置，启用 br_netfilter 模块：\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward=1 # better than modify /etc/sysctl.conf EOF sudo sysctl --system br_netfilter 是 Linux 内核中的一个模块，用于在 Linux 桥接（bridge）网络中实现网络过滤功能。\nLinux 桥接是一种网络技术，用于将多个网络接口连接在一起，形成一个逻辑上的网络。桥接器可以根据 MAC 地址来转发网络数据包，使得连接在不同网桥接口上的设备可以互相通信。\nbr_netfilter 模块提供了桥接网络中的网络过滤功能，主要用于实现网络层（IP）和传输层（TCP、UDP）的数据包过滤。具体来说，br_netfilter 模块使得可以在 Linux 桥接器上执行以下操作：\n桥接器上的 IP 数据包过滤：通过在桥接器上启用 iptables 规则，可以对进出桥接器的 IP 数据包进行过滤、修改和重定向操作。这对于实施网络安全策略、防火墙规则和网络地址转换（NAT）等非常有用。\n桥接器上的传输层数据包过滤：通过在桥接器上启用 ebtables 规则，可以对进出桥接器的传输层（如 TCP、UDP）数据包进行过滤、修改和重定向操作。这使得可以在桥接器级别上执行更细粒度的网络流量控制。\nbr_netfilter 模块扩展了 Linux 桥接网络的功能，使得可以在桥接器上实现更多的网络过滤和控制策略，提高网络的安全性和可配置性。\n第四，你需要修改 /etc/fstab ，关闭 Linux 的 swap 分区，提升 Kubernetes 的性能：\nsudo swapoff -a sudo sed -ri \u0026#39;/\\sswap\\s/s/^#?/#/\u0026#39; /etc/fstab 完成之后，重启一下系统。\n2.8.2 安装 kubeadm # 在 Master 节点和 Worker 节点上都要做有安装 kubeadm 这一操作。kubeadm 可以直接从 Google 自己的软件仓库下载安装，但国内的网络不稳定，很难下载成功，需要改用其他的软件源，可以选择国内的某云厂商：\nsudo apt install -y apt-transport-https ca-certificates curl curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF sudo apt update 更新了软件仓库之后，可以用 apt install 获取 kubeadm、kubelet 和 kubectl 这三个安装必备工具。apt 默认会下载最新版本，也可以指定版本号，比如 1.23.3：\nsudo apt install -y kubeadm=1.23.3-00 kubelet=1.23.3-00 kubectl=1.23.3-00 安装完成之后，可以用 kubeadm version、kubectl version 来验证版本是否正确：\nkubeadm version kubectl version --client 最后可以使用命令 apt-mark hold ，锁定这三个软件的版本，避免意外升级导致版本错误：\nsudo apt-mark hold kubeadm kubelet kubectl 2.8.3 下载 Kubernetes 组件镜像 # kubeadm 把 apiserver、etcd、scheduler 等组件都打包成了镜像，以容器的方式启动 Kubernetes，但这些镜像不是放在 Docker Hub 上，而是放在 Google 自己的镜像仓库网站 gcr.io，在国内的访问很困难。可以采取一些变通措施，提前把镜像下载到本地。使用命令 kubeadm config images list 可以查看安装 Kubernetes 所需的镜像列表，参数 --kubernetes-version 可以指定版本号：\nkubeadm config images list --kubernetes-version v1.23.3 k8s.gcr.io/kube-apiserver:v1.23.3 k8s.gcr.io/kube-controller-manager:v1.23.3 k8s.gcr.io/kube-scheduler:v1.23.3 k8s.gcr.io/kube-proxy:v1.23.3 k8s.gcr.io/pause:3.6 k8s.gcr.io/etcd:3.5.1-0 k8s.gcr.io/coredns/coredns:v1.8.6 可以从国内的镜像网站下载然后再用 docker tag 改名，shell 脚本如下：\nrepo=registry.aliyuncs.com/google_containers for name in `kubeadm config images list --kubernetes-version v1.23.3`; do src_name=${name#k8s.gcr.io/} src_name=${src_name#coredns/} docker pull $repo/$src_name docker tag $repo/$src_name $name docker rmi $repo/$src_name done 2.8.4 安装 Master 节点 # 只需要一个命令 kubeadm init 就可以把组件在 Master 节点上运行起来，不过它还有很多参数用来调整集群的配置，可以使用 -h 查看。常见的有 3 个参数：\n--pod-network-cidr，设置集群里 Pod 的 IP 地址段。\n--apiserver-advertise-address，设置 apiserver 的 IP 地址，对于多网卡服务器来说很重要（比如 VirtualBox 虚拟机就用了两块网卡），可以指定 apiserver 在哪个网卡上对外提供服务。\n--kubernetes-version，指定 Kubernetes 的版本号。下面的这个安装命令里，指定了 Pod 的地址段是10.10.0.0/16，apiserver 的服务地址是 192.168.10.210（主机的 IP 地址），Kubernetes 的版本号是 1.23.3。\nsudo kubeadm init \\ --pod-network-cidr=10.10.0.0/16 \\ --apiserver-advertise-address=192.168.10.210 \\ --kubernetes-version=v1.23.3 kubeadm 安装完成后，会提示出接下来要做的工作：\nTo start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 另外还有一个很重要的 kubeadm join 提示，其他节点要加入集群必须要用指令里的 token 和 ca 证书，所以这条命令务必拷贝后保存好：\nThen you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.10.210:6443 --token tv9mkx.tw7it9vphe158e74 \\ --discovery-token-ca-cert-hash sha256:e8721b8630d5b562e23c010c70559a6d3084f629abad6a2920e87855f8fb96f3 安装 Flannel 网络插件 # CNI（Container Networking Interface）标准是一个用于容器网络的开放标准和接口规范。它定义了容器运行时（如 Docker、Kubernetes 等）与网络插件之间的通信协议和数据格式，以实现容器网络的配置和管理。\nCNI 标准的设计目标是提供一个统一的、可互操作的容器网络接口，使得不同的容器运行时可以使用各种网络插件，并与宿主机上的网络配置进行无缝集成。通过遵循 CNI 标准，容器运行时可以动态地创建、配置和删除容器网络，而不依赖于特定的容器运行时或网络插件实现。\nKubernetes 定义了 CNI 标准，有很多网络插件，可以使用常用的 Flannel。只需要使用如下的 kube-flannel.yml 文件（或者去仓库 https://github.com/flannel-io/flannel/）找相关文档）在 Kubernetes 里部署一下就好了。因为它应用了 Kubernetes 的网段地址，需要修改 net-conf.json 字段，把 Network 改成刚才 kubeadm 的参数 --pod-network-cidr 设置的地址段。比如在这里，就要修改成 10.10.0.0/16 ：\napiVersion: v1 kind: Namespace metadata: labels: k8s-app: flannel pod-security.kubernetes.io/enforce: privileged name: kube-flannel --- apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: flannel name: flannel namespace: kube-flannel --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: k8s-app: flannel name: flannel rules: - apiGroups: - \u0026#34;\u0026#34; resources: - pods verbs: - get - apiGroups: - \u0026#34;\u0026#34; resources: - nodes verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - nodes/status verbs: - patch - apiGroups: - networking.k8s.io resources: - clustercidrs verbs: - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: k8s-app: flannel name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-flannel --- apiVersion: v1 data: cni-conf.json: | { \u0026#34;name\u0026#34;: \u0026#34;cbr0\u0026#34;, \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;flannel\u0026#34;, \u0026#34;delegate\u0026#34;: { \u0026#34;hairpinMode\u0026#34;: true, \u0026#34;isDefaultGateway\u0026#34;: true } }, { \u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34;, \u0026#34;capabilities\u0026#34;: { \u0026#34;portMappings\u0026#34;: true } } ] } net-conf.json: | { \u0026#34;Network\u0026#34;: \u0026#34;10.10.0.0/16\u0026#34;, \u0026#34;Backend\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;vxlan\u0026#34; } } kind: ConfigMap metadata: labels: app: flannel k8s-app: flannel tier: node name: kube-flannel-cfg namespace: kube-flannel --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: flannel k8s-app: flannel tier: node name: kube-flannel-ds namespace: kube-flannel spec: selector: matchLabels: app: flannel k8s-app: flannel template: metadata: labels: app: flannel k8s-app: flannel tier: node spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux containers: - args: - --ip-masq - --kube-subnet-mgr command: - /opt/bin/flanneld env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: EVENT_QUEUE_DEPTH value: \u0026#34;5000\u0026#34; image: docker.io/flannel/flannel:v0.22.0 name: kube-flannel resources: requests: cpu: 100m memory: 50Mi securityContext: capabilities: add: - NET_ADMIN - NET_RAW privileged: false volumeMounts: - mountPath: /run/flannel name: run - mountPath: /etc/kube-flannel/ name: flannel-cfg - mountPath: /run/xtables.lock name: xtables-lock hostNetwork: true initContainers: - args: - -f - /flannel - /opt/cni/bin/flannel command: - cp image: docker.io/flannel/flannel-cni-plugin:v1.1.2 name: install-cni-plugin volumeMounts: - mountPath: /opt/cni/bin name: cni-plugin - args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist command: - cp image: docker.io/flannel/flannel:v0.22.0 name: install-cni volumeMounts: - mountPath: /etc/cni/net.d name: cni - mountPath: /etc/kube-flannel/ name: flannel-cfg priorityClassName: system-node-critical serviceAccountName: flannel tolerations: - effect: NoSchedule operator: Exists volumes: - hostPath: path: /run/flannel name: run - hostPath: path: /opt/cni/bin name: cni-plugin - hostPath: path: /etc/cni/net.d name: cni - configMap: name: kube-flannel-cfg name: flannel-cfg - hostPath: path: /run/xtables.lock type: FileOrCreate name: xtables-lock 改好后，可以用 kubectl apply 来安装 Flannel 网络：\nkubectl apply -f kube-flannel.yml 安装完成后，如果执行 kubectl get node 能够看到 Master 节点的状态是 Ready，则表明节点网络工作正常了。\n2.8.5 安装 Worker 节点 # 如果已经成功安装了 Master 节点，那么 Worker 节点就执行执行之前拷贝的那条 kubeadm join 命令就可以了：\nsudo kubeadm join 192.168.10.210:6443 --token tv9mkx.tw7it9vphe158e74 \\ --discovery-token-ca-cert-hash sha256:e8721b8630d5b562e23c010c70559a6d3084f629abad6a2920e87855f8fb96f3 kubeadm join 命令里的 token 有时效性，默认是 24 小时，如果失效了可以用 kubeadm token create 创建一个新的 token。\n这条命令会连接 Master 节点，然后拉取镜像，安装网络插件，最后把节点加入集群。在这个过程中可能也会遇到拉取镜像的问题，可以按照安装 Master 的方式解决。Worker 节点安装完毕后，执行 kubectl get node ，就会看到两个节点都是 Ready 状态：\n在 Master 和 Worker 都安装成功后，可以使用 kubectl run ，运行 Nginx 测试一下：\nkubectl run ngx --image=nginx:alpine kubectl get pod -o wide 2.8.6 Console 节点 # 在生产环境中，在 Kubernetes 集群之外还需要有一台起辅助作用的服务器，也就是 Console 控制台，Console 服务器上只需要安装一个 kubectl，所有对 Kubernetes 集群的管理命令都是从这台主机发出去的，因为出于因为安全的原因，集群里的主机部署好之后应该尽量少直接登录上去操作。\n由于 Console 节点的部署只需要安装一个 kubectl，然后复制 config 文件就行，可以直接在 Master 节点上用 scp 远程拷贝，例如：\nscp $(which kubectl) root@192.168.10.208:~/ scp ~/.kube/config root@192.168.10.208:~/.kube "},{"id":17,"href":"/kubernetes/docs/part2-break-ice/2.9-deployment/","title":"2.9 Deployment","section":"第二部分 入门","content":" 2.9 Deployment # 在线业务远不是单纯启动一个 Pod 这么简单，还有多实例、高可用、版本更新等许多复杂的操作。比如多实例需求，为了提高系统的服务能力，应对突发的流量和压力，需要创建多个应用的副本，还要即时监控它们的状态。如果只使用 Pod，但有人不小心用 kubectl delete 误删了 Pod，又或者 Pod 运行的节点发生了断电故障，那么 Pod 就会在集群里彻底消失，Pod 容器里运行的服务也会消息，这样就会导致业务出现异常。\n处理这种问题的思路就是 “单一职责” 和 “对象组合”。既然 Pod 管理不了自己，那么就再创建一个新的对象，由它来管理 Pod，采用 “对象套对象” 的形式。这个用来管理 Pod，实现在线业务应用的新 API 对象，就是 Deployment。\n2.9.1 创建 # Deployment 的简称是 deploy，它的 apiVersion 是 apps/v1，kind 是 Deployment。\nDeployment 的 YAML 描述大致如下：\napiVersion: apps/v1 kind: Deployment metadata: labels: app: ngx-dep name: ngx-dep spec: replicas: 2 selector: matchLabels: app: ngx-dep template: metadata: labels: app: ngx-dep spec: containers: - image: nginx:alpine name: nginx replicas 字段 # replicas 字段的含义比较简单明了，就是 “副本数量” 的意思，也就是说，指定要在 Kubernetes 集群里运行多少个 Pod 实例。有了这个字段，就相当于为 Kubernetes 明确了应用部署的 “期望状态”，Deployment 对象就可以扮演运维监控人员的角色，自动地在集群里调整 Pod 的数量。\n比如，Deployment 对象刚创建出来的时候，Pod 数量肯定是 0，那么它就会根据 YAML 文件里的 Pod 模板，逐个创建出要求数量的 Pod。接下来 Kubernetes 还会持续地监控 Pod 的运行状态，万一有 Pod 发生意外消失了，数量不满足 “期望状态”，它就会通过 apiserver、scheduler 等核心组件去选择新的节点，创建出新的 Pod，直至数量与 “期望状态” 一致。这里面的工作流程复杂，但对于外部用户来说，只需要一个 replicas 字段就可以了，不需要再用人工监控管理，整个过程完全自动化。\nselector 字段 # selector 的作用是 “筛选” 出要被 Deployment 管理的 Pod 对象，下属字段 “matchLabels” 定义了 Pod 对象应该携带的 label，它必须和 “template” 里 Pod 定义的 “labels” 完全相同，否则 Deployment 就会找不到要控制的 Pod 对象，apiserver 也会告诉你 YAML 格式校验错误无法创建。\n这个 selector 字段的用法看起来好像是有点多余，但为了保证 Deployment 成功创建，必须在 YAML 里把 label 重复写两次：一次是在 selector.matchLabels，另一次是在 template.matadata。比如上面的例子中就要在这两个地方连续写 app: ngx-dep ：\n... spec: replicas: 2 selector: matchLabels: app: ngx-dep template: metadata: labels: app: ngx-dep ... Deployment 和 Pod 实际上是一种松散的组合关系，Deployment 实际上并不 “持有” Pod 对象，它只是帮助 Pod 对象能够有足够的副本数量运行，仅此而已。\nKubernetes 采用的是这种 “贴标签” 的方式，通过在 API 对象的 metadata 元信息里加各种标签（labels），就可以使用类似关系数据库里查询语句的方式，筛选出具有特定标识的那些对象。通过标签这种设计，Kubernetes 就解除了 Deployment 和模板里 Pod 的强绑定，把组合关系变成了 “弱引用”。\n2.9.2 部署 # 把 Deployment 的 YAML 写好之后，可以用 kubectl apply 来创建对象了：\nkubectl apply -f deploy.yml 要查看 Deployment 的状态，可以使用 kubectl get 命令：\nkubectl get deploy READY 表示运行的 Pod 数量，前面的数字是当前数量，后面的数字是期望数量，“2/2” 的意思就是要求有两个 Pod 运行，现在已经启动了两个 Pod。\nUP-TO-DATE 指的是当前已经更新到最新状态的 Pod 数量。因为如果要部署的 Pod 数量很多或者 Pod 启动比较慢，Deployment 完全生效需要一个过程，UP-TO-DATE 就表示现在有多少个 Pod 已经完成了部署，达成了模板里的 “期望状态”。\nAVAILABLE 要比 READY、UP-TO-DATE 更进一步，不仅要求已经运行，还必须是健康状态，能够正常对外提供服务。\nAGE 表示 Deployment 从创建到现在所经过的时间，也就是运行的时间。\n因为 Deployment 管理的是 Pod，最终使用的也是 Pod，还可以用 kubectl get pod 命令来看看 Pod 的状态：\n被 Deployment 管理的 Pod 自动带上了名字，命名的规则是 Deployment 的名字加上两串随机数（其实是 Pod 模板的 Hash 值）。\n现在可以模拟一下 Pod 发生故障的情景，用 kubectl delete 删除一个 Pod，然后再查看 Pod 的状态：\n可以看到，被删除的 Pod 确实是消失了，但 Kubernetes 在 Deployment 的管理之下，很快又创建出了一个新的 Pod，保证了应用实例的数量始终是在 YAML 里定义的数量。\n2.9.3 扩容伸缩 # 在 Deployment 部署成功之后，可以随时调整 Pod 的数量，实现所谓的 “应用伸缩”。kubectl scale 是专门用于实现 “扩容” 和 “缩容” 的命令，只要用参数 --replicas 指定需要的副本数量，Kubernetes 就会自动增加或者删除 Pod，让最终的 Pod 数量达到 “期望状态”。\n比如下面的这条命令，就把 ngx-dep 应用扩容到了 5 个：\nkubectl scale --replicas=5 deploy ngx-dep kubectl scale 是命令式操作，扩容和缩容只是临时的措施，如果应用需要长时间保持一个确定的 Pod 数量，还是需要编辑 Deployment 的 YAML 文件，改动 replicas，再以声明式的 kubectl apply 修改对象的状态。\n2.9.5 常用命令 # kubectl get deploy kubectl get deploy -o wide kubectl delete deploy xxx 2.9.6 labels 筛选 # labels 为对象 “贴” 了各种 “标签”，在使用 kubectl get 命令的时候，加上参数 -l，使用 ==、!=、in、notin 的表达式，就能够很容易地用“标签”筛选、过滤出所要查找的对象。\n以下两个例子，第一条命令找出 app 标签是 nginx 的所有 Pod，第二条命令找出 app 标签是 ngx、nginx、ngx-dep 的所有 Pod：\nkubectl get pod -l app=nginx kubectl get pod -l \u0026#39;app in (ngx, nginx, ngx-dep)\u0026#39; "},{"id":18,"href":"/kubernetes/docs/part2-break-ice/2.10-daemonset/","title":"2.10 DaemonSet","section":"第二部分 入门","content":" 2.10 DaemonSet # DaemonSet 会在 Kubernetes 集群的每个节点上都运行一个 Pod，就好像是 Linux 系统里的 “守护进程”（Daemon）一样。\nDaemonSet 和 Deployment 有很大区别，Deployment 能够创建任意多个的 Pod 实例，并且维护这些 Pod 的正常运行，保证应用始终处于可用状态。但是，Deployment 并不关心这些 Pod 会在集群的哪些节点上运行，在它看来，Pod 的运行环境与功能是无关的，只要 Pod 的数量足够，应用程序应该会正常工作。但是对一些业务比较特殊服务，它们不是完全独立于系统运行的，而是与主机存在 “绑定” 关系，必须要依附于节点才能产生价值，比如：\n网络应用（如 kube-proxy），必须每个节点都运行一个 Pod，否则节点就无法加入 Kubernetes 网络。\n监控应用（如 Prometheus），必须每个节点都有一个 Pod 用来监控节点的状态，实时上报信息。\n日志应用（如 Fluentd），必须在每个节点上运行一个 Pod，才能够搜集容器运行时产生的日志数据。\n安全应用，每个节点都要有一个 Pod 来执行安全审计、入侵检查、漏洞扫描等工作。\n以上这些业务如果用 Deployment 来部署就不太合适了，因为 Deployment 所管理的 Pod 数量是固定的，而且可能会在集群里 “漂移”，但，实际的需求却是要在集群里的每个节点上都运行 Pod，也就是说 Pod 的数量与节点数量保持同步。\nDaemonSet，它在形式上和 Deployment 类似，都是管理控制 Pod，但管理调度策略却不同。DaemonSet 的目标是在集群的每个节点上运行且仅运行一个 Pod。\n2.10.1 描述 DaemonSet # kubectl 不提供自动创建 DaemonSet YAML 样板的功能，不过可以在 Kubernetes 的官网 https://kubernetes.io/zh/docs/concepts/workloads/controllers/daemonset/上找到 DaemonSet 的 YAML 示例，以下是一个 DaemonSet 的 YAML 描述：\napiVersion: apps/v1 kind: DaemonSet metadata: name: redis-ds labels: app: redis-ds spec: selector: matchLabels: name: redis-ds template: metadata: labels: name: redis-ds spec: containers: - image: redis:5-alpine name: redis ports: - containerPort: 6379 DaemonSet 仅仅是在 Pod 的部署调度策略上和 Deployment 不同，其他的都是相同的，某种程度上可以把 DaemonSet 看做是 Deployment 的一个特例。\n2.10.2 使用 DaemonSet # Master 默认是不跑应用的，所以 DaemonSet 就只生成了一个 Pod，运行在了 worker 节点上，但是按照 DaemonSet 的本意，应该在每个节点上都运行一个 Pod 实例才对，但 Master 节点却被排除在外了，这是由 Kubernetes 节点的两个属性：污点（taint）和容忍度（toleration）导致的。\n污点和容忍度 # “污点” 是 Kubernetes 节点的一个属性，它的作用也是给节点 “贴标签”，但为了不和已有的 labels 字段混淆，就改成了 taint。和 “污点” 相对的，就是 Pod 的“ 容忍度”，也就是 Pod 能否 “容忍” 污点。\n集群里的节点各式各样，有的节点 “纯洁无瑕”，没有 “污点”；而有的节点因为某种原因粘上了 “泥巴”，也就有了 “污点”。Pod 也脾气各异，有的 “洁癖” 很严重，不能容忍 “污点”，只能挑选 “干净” 的节点；而有的 Pod 则要求不那么高，可以适当地容忍一些小 “污点”。\nKubernetes 在创建集群的时候会自动给节点 Node 加上一些 “污点”，方便 Pod 的调度和部署。可以用 kubectl describe node 来查看 Master 和 Worker 的状态：\n在 Kubernetes v1.24 中，master 节点将不再使用污点 node-role.kubernetes.io/master，而是改成 node-role.kubernetes.io/control\u0026ndash;plane。\nMaster 节点默认有一个 taint，名字是 node-role.kubernetes.io/master，它的效果是 NoSchedule，也就是说这个污点会拒绝 Pod 调度到本节点上运行，而 Worker 节点的 taint 字段则是空的。通常来说 Pod 都不能容忍任何 “污点”，所以加上了 taint 属性的 Master 节点也就会无缘 Pod 了。\n有 2 种方法可以让让 DaemonSet 在 Master 节点（或者任意其他节点）上运行了。\n第一种方法是去掉 Master 节点上的 taint，让 Master 变得和 Worker 一样 “纯洁无瑕”，DaemonSet 自然就不需要再区分 Master/Worker。\n操作 Node 上的 “污点” 属性需要使用命令 kubectl taint，然后指定节点名、污点名和污点的效果，去掉污点要额外加上一个 -。比如要去掉 Master 节点的 “NoSchedule” 效果，可以使用这条命令：\nkubectl taint node master node-role.kubernetes.io/master:NoSchedule- 这种方法修改的是 Node 的状态，影响面比较大，可能会导致很多 Pod 都跑到这个节点上运行，可以选择保留 Node 的 “污点”，为需要的 Pod 添加 “容忍度”，实现精细化调度。\n第二种方法是为 Pod 添加字段 tolerations，让它能够 “容忍” 某些 “污点”，就可以在任意的节点上运行。\ntolerations 是一个数组，里面可以列出多个被 “容忍” 的 “污点”，需要写清楚 “污点” 的名字、效果。比较特别是要用 operator 字段指定如何匹配 “污点”，一般使用 Exists，也就是说存在这个名字和效果的 “污点”。\n如果想让 DaemonSet 里的 Pod 能够在 Master 节点上运行，就要写出这样的一个 tolerations，容忍节点的 node-role.kubernetes.io/master:NoSchedule 这个污点：\ntolerations: - key: node-role.kubernetes.io/master effect: NoSchedule operator: Exists 所以现在的 DaemonSet YAML 描述文件就变成了这样：\napiVersion: apps/v1 kind: DaemonSet metadata: name: redis-ds labels: app: redis-ds spec: selector: matchLabels: name: redis-ds template: metadata: labels: name: redis-ds spec: containers: - image: redis:5-alpine name: redis ports: - containerPort: 6379 tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule operator: Exists 重新 apply 部署后可以看到有两个 Pod，分别运行在 Master 和 Worker 节点上。\n2.10.3 静态 Pod # DaemonSet 是在 Kubernetes 里运行节点专属 Pod 最常用的方式，但不是唯一的方式，Kubernetes 还支持另外一种叫 “静态 Pod” 的应用部署手段。\n“静态 Pod” 非常特殊，不受 Kubernetes 系统的管控，不与 apiserver、scheduler 发生关系。但既然是 Pod，也必然会 “跑” 在容器运行时上，也会有 YAML 文件来描述它，而唯一能够管理它的 Kubernetes 组件也就只有在每个节点上运行的 kubelet 。\n“静态 Pod” 的 YAML 文件默认都存放在节点的 /etc/kubernetes/manifests 目录下，它是 Kubernetes 的专用目录。\n如果有一些 DaemonSet 无法满足的特殊的需求，可以考虑使用静态 Pod，编写一个 YAML 文件放到这个目录里，节点的 kubelet 会定期检查目录里的文件，发现变化就会调用容器运行时创建或者删除静态 Pod。\n参考 # 污点和容忍度 "},{"id":19,"href":"/kubernetes/docs/part2-break-ice/2.11-service/","title":"2.11 Service","section":"第二部分 入门","content":" 2.11 Service # 2.11.1 什么是 Service # Service 是集群内部的负载均衡机制，用来解决服务发现的关键问题。在 Kubernetes 集群里 Pod 的生命周期是比较 “短暂” 的，虽然 Deployment 和 DaemonSet 可以维持 Pod 总体数量的稳定，但在运行过程中，难免会有 Pod 销毁又重建，这就会导致 Pod 集合处于动态的变化之中。这种 “动态稳定” 对于现在流行的微服务架构来说是非常致命的，如果后台 Pod 的 IP 地址老是变来变去，客户端该怎么访问呢？\n对于这种 “不稳定” 的后端服务问题，业内的解决方案是 “负载均衡”，典型的应用有 LVS、Nginx 等，它们在前端与后端之间加入了一个 “中间层”，屏蔽后端的变化，为前端提供一个稳定的服务。Service 的工作原理和 LVS、Nginx 差不多，Kubernetes 会给它分配一个静态 IP 地址，然后它再去自动管理、维护后面动态变化的 Pod 集合，当客户端访问 Service，它就根据某种策略，把流量转发给后面的某个 Pod。\nLVS 即 Linux Virtual Server，是由章文嵩发起的一个开源项目，后来被集成进 Linux 内核。\nService 使用了 iptables 技术，每个节点上的 kube-proxy 组件自动维护 iptables 规则，客户不再关心 Pod 的具体地址，只要访问 Service 的固定 IP 地址，Service 就会根据 iptables 规则转发请求给它管理的多个 Pod，是典型的负载均衡架构。\niptables 基于 Linux 内核里的 netfilter 模块，用来处理网络数据包，实现修改、过滤、地址转换等功能。\nService 并不是只能使用 iptables 来实现负载均衡，它还有另外两种实现技术：性能更差的 userspace 和性能更好的 ipvs。\n2.11.2 YAML 描述 Service # Service YAML 描述样本可以使用命令 kubectl expose 进行创建。因为在 Kubernetes 里提供服务的是 Pod，而 Pod 又可以用 Deployment/DaemonSet 对象来部署，所以 kubectl expose 支持从多种对象创建服务，得先有对象，才有服务。Pod、Deployment、DaemonSet 都可以。\n使用 kubectl expose 指令时还需要用参数 --port 和 --target-port 分别指定映射端口和容器端口，而 Service 自己的 IP 地址和后端 Pod 的 IP 地址可以自动生成，用法上和 Docker 的命令行参数 -p 很类似。\n比如，用以下的 YAML 描述生产 Deployment 对象：\napiVersion: apps/v1 kind: Deployment metadata: labels: app: ngx-dep name: ngx-dep spec: replicas: 2 selector: matchLabels: app: ngx-dep template: metadata: labels: app: ngx-dep spec: containers: - image: nginx:alpine name: nginx 用 ngx-dep 对象生成 Service，命令可以这么写：\nexport out=\u0026#34;--dry-run=client -o yaml\u0026#34; kubectl expose deploy ngx-dep --port=80 --target-port=80 $out 生成的 Service YAML 大致如下：\napiVersion: v1 kind: Service metadata: name: ngx-svc spec: selector: app: ngx-dep ports: - port: 80 targetPort: 80 protocol: TCP selector 用来过滤出要代理的那些 Pod，因为已经指定了要代理 Deployment，所以 Kubernetes 就自动填上了 ngx-dep 的标签，会选择这个 Deployment 对象部署的所有 Pod。\nports 里面的三个字段分别表示外部端口、内部端口和使用的协议，在这里就是内外部都使用 80 端口，协议是 TCP。\n2.11.3 使用 Service # 在使用 YAML 创建 Service 对象之前，可以先对 Deployment 做一点改造，方便观察 Service 的效果。\n首先，创建一个 ConfigMap，定义一个 Nginx 的配置片段，它会输出服务器的地址、主机名、请求的 URI 等基本信息：\napiVersion: v1 kind: ConfigMap metadata: name: ngx-conf data: default.conf: | server { listen 80; location / { default_type text/plain; return 200 \u0026#39;srv : $server_addr:$server_port\\nhost: $hostname\\nuri : $request_method $host $request_uri\\ndate: $time_iso8601\\n\u0026#39;; } } 然后在 Deployment 的 template.volumes 里定义存储卷，再用 volumeMounts 把配置文件加载进 Nginx 容器里：\napiVersion: apps/v1 kind: Deployment metadata: name: ngx-dep spec: replicas: 2 selector: matchLabels: app: ngx-dep template: metadata: labels: app: ngx-dep spec: volumes: - name: ngx-conf-vol configMap: name: ngx-conf containers: - image: nginx:alpine name: nginx ports: - containerPort: 80 volumeMounts: - mountPath: /etc/nginx/conf.d name: ngx-conf-vol 先 apply 部署 ConfigMap 和 改造过的 Deployment，然后通过以下的 Service YAML 部署 Service：\napiVersion: v1 kind: Service metadata: name: ngx-svc spec: selector: app: ngx-dep ports: - port: 80 targetPort: 80 protocol: TCP 可以看到，Kubernetes 为 Service 对象自动分配了一个 IP 地址 10.102.179.255。Service 对象的 IP 地址还有一个特点，它是一个 “虚地址”，不存在实体，只能用来转发流量。\n如果想要看 Service 代理了哪些后端的 Pod，你可以用 kubectl describe 命令：\nkubectl describe svc ngx-svc 可以看到 Service 对象管理了两个 endpoint，分别是 10.10.1.145:80和10.10.1.146:80，如何知道这两个 IP 地址是不是 Nginx Pod 的实际地址呢？可以使用 kubectl get pod 来看一下，加上参数 -o wide：\nkubectl get pod -o wide 把 Pod 的地址与 Service 的信息做个对比，就能够验证 Service 确实用一个静态 IP 地址代理了两个 Pod 的动态 IP 地址。\nService 负载均衡效果 # 因为 Service、 Pod 的 IP 地址都是 Kubernetes 集群的内部网段，所以需要用 kubectl exec 进入到 Pod 内部（或者 ssh 登录集群节点），再用 curl 等工具来访问 Service：\n在 Pod 里，用 curl 访问 Service 的 IP 地址，就会看到它把数据转发给后端的 Pod，输出信息会显示具体是哪个 Pod 响应了请求，就表明 Service 确实完成了对 Pod 的负载均衡任务。\n2.11.4 域名的方式使用 Service # Service 对象的 IP 地址是静态的，保持稳定，这在微服务里确实很重要，不过数字形式的 IP 地址用起来不太方便。Kubernetes 的 DNS 插件可以为 Service 创建易写易记的域名，让 Service 更容易使用。\n名字空间 # namespace 名字空间用来在集群里实现对 API 对象的隔离和分组。namespace 的简写是 ns，可以使用命令 kubectl get ns 来查看当前集群里都有哪些名字空间，也就是说 API 对象有哪些分组：\nKubernetes 有一个默认的名字空间，叫 default，如果不显式指定，API 对象都会在这个 default 名字空间里。而其他的名字空间都有各自的用途，比如 kube-system 就包含了 apiserver、etcd 等核心组件的 Pod。\nDNS 是一种层次结构，为了避免太多的域名导致冲突，Kubernetes 就把名字空间作为域名的一部分，减少了重名的可能性。Service 对象的域名完全形式是 “对象.名字空间.svc.cluster.local”，但很多时候也可以省略后面的部分，直接写 “对象.名字空间” 甚至 “对象名” 就足够了，默认会使用对象所在的名字空间。\n可以试验一下 DNS 域名的用法，以 kubectl exec 命令进入 Pod，然后用 curl 访问 ngx-svc、ngx-svc.default 等域名：\n可以看到，不再关心 Service 对象的 IP 地址，只需要知道它的名字，就可以用 DNS 的方式去访问后端服务。\n其实 Kubernetes 也为每个 Pod 分配了域名，形式是 “IP 地址.名字空间.pod.cluster.local”，但需要把 IP 地址里的.改成-。比如地址 10.10.1.87，它对应的域名就是 10-10-1-87.default.pod。\n2.11.5 对外暴露服务 # Service 是一种负载均衡技术，它不仅能够管理 Kubernetes 集群内部的服务，还能够担任向集群外部暴露服务的重任。\nService 对象有一个关键字段 type，表示 Service 是哪种类型的负载均衡。前面我们看到的用法都是对集群内部 Pod 的负载均衡，默认为 ClusterIP，Service 的静态 IP 地址只能在集群内访问。除了 ClusterIP，Service 还支持其他三种类型，分别是 ExternalName，LoadBalancer，NodePort。前两种类型一般由云服务商提供。\n在使用命令 kubectl expose 的时候如果加上参数 --type=NodePort，或者在 YAML 里添加字段 type:NodePort，那么 Service 除了会对后端的 Pod 做负载均衡之外，还会在集群里的每个节点上创建一个独立的端口，用这个端口对外提供服务，这也正是 NodePort 这个名字的由来。\n加上 type 的 Service YAML 描述文件如下：\napiVersion: v1 kind: Service metadata: name: ngx-svc spec: type: NodePort selector: app: ngx-dep ports: - port: 80 targetPort: 80 protocol: TCP apply 部署后查看 Service 的状态：\n可以看到 TYPE 变成了 NodePort，而在 PORT 列里的端口信息也不一样，除了集群内部使用的 80 端口，还多出了一个 32096 端口，这就是 Kubernetes 在节点上为 Service 创建的专用映射端口。\n因为这个端口号属于节点，外部能够直接访问，所以现在就可以不用登录集群节点或者进入 Pod 内部的情况下，直接在集群外使用任意一个节点的 IP 地址，就能够访问 Service 和它代理的后端服务了。\n比如我现在所在的服务器是 192.168.14.70，在这台主机上用 curl 访问 Kubernetes 集群的两个节点 192.168.14.142，192.168.14.143，就可以得到 Nginx Pod 的响应数据：\nNodePort 与 Service、Deployment 的工作原理大致如下：\n其实 NodePort 类型的 Service 虽然方便，但是也有一些缺点。\n端口数量有限，Kubernetes 为了避免端口冲突，默认只在 30000~32767 这个范围内随机分配，只有 2000 多个，而且都不是标准端口号，这对于具有大量业务应用的系统来说可能会不够用。\n会在每个节点上都开端口，然后使用 kube-proxy 路由到真正的后端 Service，这对于有很多计算节点的大集群来说就带来了一些网络通信成本，不是特别经济。\n要求向外界暴露节点的 IP 地址，这在很多时候是不可行的，为了安全还需要在集群外再搭一个反向代理，增加了方案的复杂度。\n"},{"id":20,"href":"/kubernetes/docs/part2-break-ice/2.12-ingress/","title":"2.12 Ingress","section":"第二部分 入门","content":" 2.12 Ingress # Service 的负载均衡功能有限，只能够依据 IP 地址和端口号做一些简单的判断和组合，而更多的高级路由条件，比如主机名、URI、请求头、证书等 Service 无法实现。Service 还有一个缺点，它比较适合代理集群内部的服务。如果想要把服务暴露到集群外部，就只能使用 NodePort 或者 LoadBalancer 这两种方式，而它们缺乏足够的灵活性，难以管控。\nIngress 对象可以作为流量的总入口，统管集群的进出口数据，“扇入” “扇出” 流量（也就是常说的 “南北向”），让外部用户能够安全、顺畅、便捷地访问内部服务。\n2.12.1 Ingress Controller \u0026amp; Class # Service 本身是没有服务能力的，它只是一些 iptables 规则，真正配置、应用这些规则的实际上是节点里的 kube-proxy 组件。如果没有 kube-proxy，Service 定义得再完善也没有用。\nIngress 只是一些 HTTP 路由规则的集合，相当于一份静态的描述文件，真正要把这些规则在集群里实施运行，需要的是 Ingress Controller，它的作用就相当于 Service 的 kube-proxy，能够读取、应用 Ingress 规则，处理、调度流量。\n由于 Ingress Controller 与上层业务联系密切，所以 Kubernetes 把 Ingress Controller 的实现交给了社区，只要遵守 Ingress 规则，任何人都可以开发 Ingress Controller。在众多 Ingress Controller 中，Nginx 公司开发实现 Ingress Controller 是最多使用的。\n最初 Kubernetes 的构想是，一个集群里有一个 Ingress Controller，再给它配上许多不同的 Ingress 规则，应该就可以解决请求的路由和分发问题了。但随着 Ingress 在实践中的大量应用，有很多问题逐渐显现出来，比如：\n由于某些原因，项目组需要引入不同的 Ingress Controller，但 Kubernetes 不允许这样做；\nIngress 规则太多，都交给一个 Ingress Controller 处理会让它不堪重负；\n多个 Ingress 对象没有很好的逻辑分组方式，管理和维护成本很高；\n集群里有不同的租户，他们对 Ingress 的需求差异很大甚至有冲突，无法部署在同一个 Ingress Controller 上。\n基于以上的这些问题，又有了 Ingress Class 的概念，让它插在 Ingress 和 Ingress Controller 中间，作为流量规则和控制器的协调人，解除了 Ingress 和 Ingress Controller 的强绑定关系。\nKubernetes 用户可以转向管理 Ingress Class，用它来定义不同的业务逻辑分组，简化 Ingress 规则的复杂度。比如，可以用 Class A 处理博客流量、Class B 处理短视频流量、Class C 处理购物流量。\n2.12.2 描述 Ingress # Ingress 和 Ingress Class 的 apiVersion 都是 networking.k8s.io/v1，而且 Ingress 有一个简写 ing。Ingress Controller 和 Ingress，Ingress Class 两个对象不太一样，它不只是描述文件，是一个要实际干活、处理流量的应用程序，而应用程序在 Kubernetes 里早就有对象来管理了，那就是 Deployment 和 DaemonSet。\nIngress 也是可以使用 kubectl create 来创建，它需要用两个附加参数：\n--class，指定 Ingress 从属的 Ingress Class 对象。\n--rule，指定路由规则，基本形式是URI=Service。也就是说是访问 HTTP 路径就转发到对应的 Service 对象，再由 Service 对象转发给后端的 Pod。\nexport out=\u0026#34;--dry-run=client -o yaml\u0026#34; kubectl create ing ngx-ing --rule=\u0026#34;ngx.test/=ngx-svc:80\u0026#34; --class=ngx-ink $out apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ngx-ing spec: ingressClassName: ngx-ink rules: - host: ngx.test http: paths: - path: / pathType: Exact backend: service: name: ngx-svc port: number: 80 以上的 YAML 描述文件，ingressClassName 和 rules，分别对应了命令行参数。rules 字段嵌套层次比较深，其实只是把路由规则拆散了，有 host 和 http path，在 path 里又指定了路径的匹配方式，可以是精确匹配（Exact）或者是前缀匹配（Prefix），再用 backend 来指定转发的目标 Service 对象。\n其实 Ingress Class 本身并没有什么实际的功能，只是起到联系 Ingress 和 Ingress Controller 的作用，在 spec 里只有一个必需的字段 controller，表示要使用哪个 Ingress Controller，具体的名字就要看实现文档了。比如，要用 Nginx 开发的 Ingress Controller，那么就要用名字 nginx.org/ingress-controller：\napiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: ngx-ink spec: controller: nginx.org/ingress-controller 2.12.3 使用 Ingress # 因为 Ingress Class 的 YAML 描述很小，可以把 Ingress Class 与 Ingress 合成了一个 YAML 文件，也就是下面的这个 ingress.yaml：\napiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: ngx-ink spec: controller: nginx.org/ingress-controller --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ngx-ing spec: ingressClassName: ngx-ink rules: - host: ngx.test http: paths: - path: / pathType: Exact backend: service: name: ngx-svc port: number: 80 在使用 kubectl create 创建 Ingress 的 YAML 描述时，通过--rule指定了路由形式，Server 的 name 是 ngx-svc，所以创建 Service 的 YAML 描述可以是 ngx-svc.yml：\napiVersion: v1 kind: Service metadata: name: ngx-svc spec: type: NodePort selector: app: ngx-dep ports: - port: 80 targetPort: 80 protocol: TCP 对应创建 Deployment 的描述是 nginx.yml：\napiVersion: apps/v1 kind: Deployment metadata: name: ngx-dep spec: replicas: 2 selector: matchLabels: app: ngx-dep template: metadata: labels: app: ngx-dep spec: volumes: - name: ngx-conf-vol configMap: name: ngx-conf containers: - image: nginx:alpine name: nginx ports: - containerPort: 80 volumeMounts: - mountPath: /etc/nginx/conf.d name: ngx-conf-vol nginx.yml 需要的 ConfigMap 的描述 ngx-conf.yaml 为：\napiVersion: v1 kind: ConfigMap metadata: name: ngx-conf data: default.conf: | server { listen 80; location / { default_type text/plain; return 200 \u0026#39;srv : $server_addr:$server_port\\nhost: $hostname\\nuri : $request_method $host $request_uri\\ndate: $time_iso8601\\n\u0026#39;; } } 具体命令如下：\n命令 kubectl describe 可以看到更详细的 Ingress 信息：\nkubectl describe ing ngx-ing Ingress 对象的路由规则 Host/Path 就是在 YAML 里设置的域名ngx.test/，而且已经关联了我们创建的 Service 对象，还有 Service 中的两个 Pod。\nIngress 里 Default backend 的错误，在找不到路由的时候，它被设计用来提供一个默认的后端服务，不设置也不会有什么问题，在大多数时候可以忽略这个错误。\n2.12.4 使用 Ingress Controller # 有了 Ingress 和 Ingress Class 对象，就可以部署真正处理路由规则的 Ingress Controller 了。这里使用 Nginx Ingress Controller 项目 https://github.com/nginxinc/kubernetes-ingress，它以 Pod 的形式运行在 Kubernetes 里，所以同时支持 Deployment 和 DaemonSet 两种部署方式，这里选择 Deployment 的部署方式 https://github.com/nginxinc/kubernetes-ingress/blob/main/deployments/deployment/nginx-ingress.yaml。\nNginx Ingress Controller 的安装有点麻烦，有很多个 YAML 需要执行，但如果只是做简单的试验，只需要用到以下的 4 个 YAML：\nkubectl apply -f ns-and-sa.yaml kubectl apply -f rbac.yaml kubectl apply -f nginx-config.yaml kubectl apply -f default-server-secret.yaml 前两条命令为 Ingress Controller 创建了一个独立的名字空间 nginx-ingress，还有相应的账号和权限，这是为了访问 apiserver 获取 Service、Endpoint 信息用的；后两条则是创建了一个 ConfigMap 和 Secret，用来配置 HTTP/HTTPS 服务。\nns-and-sa.yaml 如下：\napiVersion: v1 kind: Namespace metadata: name: nginx-ingress --- apiVersion: v1 kind: ServiceAccount metadata: name: nginx-ingress namespace: nginx-ingress rbac.yaml 如下：\nkind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nginx-ingress rules: - apiGroups: - \u0026#34;\u0026#34; resources: - services - endpoints verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - secrets verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps verbs: - get - list - watch - update - create - apiGroups: - \u0026#34;\u0026#34; resources: - pods verbs: - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - events verbs: - create - patch - list - apiGroups: - networking.k8s.io resources: - ingresses verbs: - list - watch - get - apiGroups: - networking.k8s.io resources: - ingresses/status verbs: - update - apiGroups: - k8s.nginx.org resources: - virtualservers - virtualserverroutes - globalconfigurations - transportservers - policies verbs: - list - watch - get - apiGroups: - k8s.nginx.org resources: - virtualservers/status - virtualserverroutes/status - policies/status - transportservers/status verbs: - update - apiGroups: - networking.k8s.io resources: - ingressclasses verbs: - get - apiGroups: - cis.f5.com resources: - ingresslinks verbs: - list - watch - get - apiGroups: - cert-manager.io resources: - certificates verbs: - list - watch - get - update - create - delete --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nginx-ingress subjects: - kind: ServiceAccount name: nginx-ingress namespace: nginx-ingress roleRef: kind: ClusterRole name: nginx-ingress apiGroup: rbac.authorization.k8s.io nginx-config.yaml 如下：\nkind: ConfigMap apiVersion: v1 metadata: name: nginx-config namespace: nginx-ingress data: default-server-secret.yaml 如下：\napiVersion: v1 kind: Secret metadata: name: default-server-secret namespace: nginx-ingress type: kubernetes.io/tls data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN2akNDQWFZQ0NRREFPRjl0THNhWFhEQU5CZ2txaGtpRzl3MEJBUXNGQURBaE1SOHdIUVlEVlFRRERCWk8KUjBsT1dFbHVaM0psYzNORGIyNTBjbTlzYkdWeU1CNFhEVEU0TURreE1qRTRNRE16TlZvWERUSXpNRGt4TVRFNApNRE16TlZvd0lURWZNQjBHQTFVRUF3d1dUa2RKVGxoSmJtZHlaWE56UTI5dWRISnZiR3hsY2pDQ0FTSXdEUVlKCktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQUwvN2hIUEtFWGRMdjNyaUM3QlBrMTNpWkt5eTlyQ08KR2xZUXYyK2EzUDF0azIrS3YwVGF5aGRCbDRrcnNUcTZzZm8vWUk1Y2Vhbkw4WGM3U1pyQkVRYm9EN2REbWs1Qgo4eDZLS2xHWU5IWlg0Rm5UZ0VPaStlM2ptTFFxRlBSY1kzVnNPazFFeUZBL0JnWlJVbkNHZUtGeERSN0tQdGhyCmtqSXVuektURXUyaDU4Tlp0S21ScUJHdDEwcTNRYzhZT3ExM2FnbmovUWRjc0ZYYTJnMjB1K1lYZDdoZ3krZksKWk4vVUkxQUQ0YzZyM1lma1ZWUmVHd1lxQVp1WXN2V0RKbW1GNWRwdEMzN011cDBPRUxVTExSakZJOTZXNXIwSAo1TmdPc25NWFJNV1hYVlpiNWRxT3R0SmRtS3FhZ25TZ1JQQVpQN2MwQjFQU2FqYzZjNGZRVXpNQ0F3RUFBVEFOCkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWpLb2tRdGRPcEsrTzhibWVPc3lySmdJSXJycVFVY2ZOUitjb0hZVUoKdGhrYnhITFMzR3VBTWI5dm15VExPY2xxeC9aYzJPblEwMEJCLzlTb0swcitFZ1U2UlVrRWtWcitTTFA3NTdUWgozZWI4dmdPdEduMS9ienM3bzNBaS9kclkrcUI5Q2k1S3lPc3FHTG1US2xFaUtOYkcyR1ZyTWxjS0ZYQU80YTY3Cklnc1hzYktNbTQwV1U3cG9mcGltU1ZmaXFSdkV5YmN3N0NYODF6cFErUyt1eHRYK2VBZ3V0NHh3VlI5d2IyVXYKelhuZk9HbWhWNThDd1dIQnNKa0kxNXhaa2VUWXdSN0diaEFMSkZUUkk3dkhvQXprTWIzbjAxQjQyWjNrN3RXNQpJUDFmTlpIOFUvOWxiUHNoT21FRFZkdjF5ZytVRVJxbStGSis2R0oxeFJGcGZnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= tls.key: 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 部署 Ingress Controller 不需要从头编写 Deployment，Nginx 已经为我们提供了 示例 YAML，创建之前为了适配我们上面自己的应用还必须要做几处小改动：\nmetadata 里的 name 要改成自己的名字，比如 ngx-kic-dep。\nspec.selector 和 template.metadata.labels 也要修改成自己的名字，比如还是用 ngx-kic-dep。\ncontainers.image 可以改用 alpine 版本，加快下载速度，比如 nginx/nginx-ingress:2.2-alpine。\n最下面的 args 要加上 -ingress-class=ngx-ink，也就是前面创建的 Ingress Class 的名字，这是让 Ingress Controller 管理 Ingress 的关键。\n修改完之后，Ingress Controller 的 YAML 如下：\napiVersion: apps/v1 kind: Deployment metadata: name: ngx-kic-dep namespace: nginx-ingress spec: replicas: 1 selector: matchLabels: app: ngx-kic-dep template: metadata: labels: app: ngx-kic-dep app.kubernetes.io/name: nginx-ingress #annotations: #prometheus.io/scrape: \u0026#34;true\u0026#34; #prometheus.io/port: \u0026#34;9113\u0026#34; #prometheus.io/scheme: http spec: serviceAccountName: nginx-ingress automountServiceAccountToken: true securityContext: seccompProfile: type: RuntimeDefault # volumes: # - name: nginx-etc # emptyDir: {} # - name: nginx-cache # emptyDir: {} # - name: nginx-lib # emptyDir: {} # - name: nginx-log # emptyDir: {} containers: - image: nginx/nginx-ingress:2.2-alpine imagePullPolicy: IfNotPresent name: nginx-ingress ports: - name: http containerPort: 80 - name: https containerPort: 443 - name: readiness-port containerPort: 8081 - name: prometheus containerPort: 9113 readinessProbe: httpGet: path: /nginx-ready port: readiness-port periodSeconds: 1 resources: requests: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;128Mi\u0026#34; #limits: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;1Gi\u0026#34; securityContext: allowPrivilegeEscalation: false # readOnlyRootFilesystem: true runAsUser: 101 #nginx runAsNonRoot: true capabilities: drop: - ALL add: - NET_BIND_SERVICE # volumeMounts: # - mountPath: /etc/nginx # name: nginx-etc # - mountPath: /var/cache/nginx # name: nginx-cache # - mountPath: /var/lib/nginx # name: nginx-lib # - mountPath: /var/log/nginx # name: nginx-log env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name args: - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config - -ingress-class=ngx-ink - -enable-custom-resources=false #- -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret #- -include-year #- -enable-cert-manager #- -enable-external-dns #- -v=3 # Enables extensive logging. Useful for troubleshooting. #- -report-ingress-status #- -external-service=nginx-ingress #- -enable-prometheus-metrics #- -global-configuration=$(POD_NAMESPACE)/nginx-configuration # initContainers: # - image: nginx/nginx-ingress:3.1.1 # imagePullPolicy: IfNotPresent # name: init-nginx-ingress # command: [\u0026#39;cp\u0026#39;, \u0026#39;-vdR\u0026#39;, \u0026#39;/etc/nginx/.\u0026#39;, \u0026#39;/mnt/etc\u0026#39;] # securityContext: # allowPrivilegeEscalation: false # readOnlyRootFilesystem: true # runAsUser: 101 #nginx # runAsNonRoot: true # capabilities: # drop: # - ALL # volumeMounts: # - mountPath: /mnt/etc # name: nginx-etc 有了 Ingress Controller，这些 API 对象的关联就更复杂，下面这张图示可以看出它们是如何使用对象名字联系起来的：\n执行命令如下：\nIngress Controller 位于名字空间 nginx-ingress，查看状态需要用 -n 参数显式指定，否则我们只能看到 default 名字空间里的 Pod：\nkubectl get deploy -n nginx-ingress kubectl get pod -n nginx-ingress 如截图所示，Ingress Controller 已经运行起来了。因为 Ingress Controller 本身也是一个 Pod，想要向外提供服务还是要依赖于 Service 对象。所以至少还要再为它定义一个 Service，使用 NodePort 或者 LoadBalancer 暴露端口，才能真正把集群的内外流量打通。\n可以简单使用命令 kubectl port-forward，它可以直接把本地的端口映射到 Kubernetes 集群的某个 Pod 里，在测试验证的时候非常方便。\n下面这条命令就把本地的 8080 端口映射到了 Ingress Controller Pod 的 80 端口：\nkubectl port-forward -n nginx-ingress ngx-kic-dep-59bdb44896-2bgvm 8080:80 \u0026amp; 因为 Ingress 的路由规则是 HTTP 协议，所以就不能用 IP 地址的方式访问，必须要用域名、URI。可以修改 /etc/hosts 来手工添加域名解析，也可以在 curl 时使用 \u0026ndash;resolve 参数，指定域名的解析规则，比如在这里把 ngx.test 强制解析到 127.0.0.1，也就是被 kubectl port-forward 转发的本地地址：\ncurl --resolve ngx.test:8080:127.0.0.1 http://ngx.test:8080 可以看到已经成功的把请求转发到了集群内部的 Pod。\n参考 # NGINX Ingress Controller "},{"id":21,"href":"/kubernetes/docs/part2-break-ice/2.13-persistent-volume/","title":"2.13 PersistentVolume","section":"第二部分 入门","content":" 2.13 PersistentVolume # Pod 里的容器是由镜像产生的，而镜像文件本身是只读的，进程要读写磁盘只能用一个临时的存储空间，一旦 Pod 销毁，临时存储也就会立即回收释放，数据也就丢失了。\nKubernetes 的 Volume 对数据存储已经给出了一个很好的抽象，它只是定义了有这么一个 “存储卷”，而这个 “存储卷” 是什么类型、有多大容量、怎么存储，可以自由发挥。Pod 不需要关心那些专业、复杂的细节，只要设置好 volumeMounts，就可以把 Volume 加载进容器里使用。所以，由 Volume 的概念，延伸出了 PersistentVolume 对象，它专门用来表示持久存储设备，但隐藏了存储的底层实现，使用者只需要知道它能安全可靠地保管数据就可以了（由于 PersistentVolume 这个词很长，一般把它简称为 PV）。\n作为存储的抽象，PV 实际上就是一些存储设备、文件系统，比如 Ceph、GlusterFS、NFS，甚至是本地磁盘，管理它们已经超出了 Kubernetes 的能力范围，所以，一般会由系统管理员单独维护，然后再在 Kubernetes 里创建对应的 PV。PV 属于集群的系统资源，是和 Node 平级的一种对象，Pod 对它没有管理权，只有使用权。\n2.13.1 PersistentVolumeClaim/StorageClass # 由于不同存储设备的差异实在是太大了：有的速度快，有的速度慢；有的可以共享读写，有的只能独占读写；有的容量小，只有几百 MB，有的容量大到 TB、PB 级别等，这么多种存储设备，只用一个 PV 对象来管理不符合 “单一职责” 的原则，让 Pod 直接去选择 PV 也不灵活。所以 Kubernetes 就又增加了两个新对象，PersistentVolumeClaim 和 StorageClass，这种 “中间层” 的思想，把存储卷的分配管理过程再次细化。\nPersistentVolumeClaim，简称 PVC，用来向 Kubernetes 申请存储资源。PVC 是给 Pod 使用的对象，它相当于是 Pod 的代理，代表 Pod 向系统申请 PV。一旦资源申请成功，Kubernetes 就会把 PV 和 PVC 关联在一起，这个动作叫做 “绑定”（bind）。\n由于系统里的存储资源非常多，如果要 PVC 去直接遍历查找合适的 PV 也很麻烦，这里就用到了 StorageClass。StorageClass 抽象了特定类型的存储系统（比如 Ceph、NFS），在 PVC 和 PV 之间充当 “协调人” 的角色，帮助 PVC 找到合适的 PV。\n2.13.2 描述 PersistentVolume # Kubernetes 里有很多种类型的 PV，最容易的是本机存储 HostPath，它和 Docker 里挂载本地目录的 -v 参数非常类似。因为 Pod 会在集群的任意节点上运行，所以系统管理员需在每个节点上创建一个目录，它将会作为本地存储卷挂载到 Pod 里。\n比如在 /tmp 里建一个名字是 host-10m-pv 的目录，表示一个只有 10MB 容量的存储设备。有了存储，就可以使用 YAML 来描述这个 PV 对象了。这里不能用 kubectl create 直接创建 PV 对象，只能用 kubectl api-resources、kubectl explain 查看 PV 的字段说明，手动编写 PV 的 YAML 描述文件😂。以下是一个 YAML 描述示例：\napiVersion: v1 kind: PersistentVolume metadata: name: host-10m-pv spec: storageClassName: host-test accessModes: - ReadWriteOnce capacity: storage: 10Mi hostPath: path: /tmp/host-10m-pv/ storageClassName 表示对存储类型的抽象 StorageClass。这个 PV 是我们手动管理的，名字可以任意起，这里写的是 host-test。\naccessModes 定义了存储设备的访问模式，也就是虚拟盘的读写权限，和 Linux 的文件访问模式差不多，目前 Kubernetes 有 3 种：\nReadWriteOnce：存储卷可读可写，但只能被一个节点上的 Pod 挂载。\nReadOnlyMany：存储卷只读不可写，可以被任意节点上的 Pod 多次挂载。\nReadWriteMany：存储卷可读可写，也可以被任意节点上的 Pod 多次挂载。\n这 3 种访问模式限制的对象是节点而不是 Pod，因为存储是系统级别的概念，不属于 Pod 里的进程。因为本地目录只能是在本机使用，所以这个 PV 使用了 ReadWriteOnce 访问模式。\ncapacity 表示存储设备的容量，这里设置为 10MB。\nKubernetes 里定义存储容量使用的是国际标准，日常习惯使用的 KB/MB/GB 的基数是 1024，要写成 Ki/Mi/Gi，如果写错了，单位不一致实际容量就会对不上。\nhostPath 指定了存储卷的本地路径，也就是在节点上创建的目录。\n2.13.3 描述 PersistentVolumeClaim # 有了 PV 就表示集群里有了一个持久化存储可以供 Pod 使用，需要再定义 PVC 对象，向 Kubernetes 申请存储。下面这个 YAML PVC 描述，要求使用一个 5MB 的存储设备，访问模式是 ReadWriteOnce：\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: host-5m-pvc spec: storageClassName: host-test accessModes: - ReadWriteOnce resources: requests: storage: 5Mi PVC 的 YAML 内容与 PV 很像，但它不表示实际的存储，而是一个 “申请” 或者 “声明”，spec 里的字段描述的是对存储的 “期望状态”。PVC 里的 storageClassName、accessModes 和 PV 是一样的，但不会有字段 capacity，而是要用 resources.request 表示希望要有多大的容量。\nKubernetes 会根据 PVC 里的描述，去找能够匹配 StorageClass 和容量的 PV，然后把 PV 和 PVC “绑定” 在一起，实现存储的分配。\n2.13.4 使用 PersistentVolume # 当已经准备好了 PV 和 PVC，就可以让 Pod 实现持久化存储了。\nhost-path-pv.yml\napiVersion: v1 kind: PersistentVolume metadata: name: host-10m-pv spec: storageClassName: host-test accessModes: - ReadWriteOnce capacity: storage: 10Mi hostPath: path: /tmp/host-10m-pv/ host-path-pvc.yml\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: host-5m-pvc spec: storageClassName: host-test accessModes: - ReadWriteOnce resources: requests: storage: 5Mi 从上面的截图可以看到，这个 PV 的容量是 10MB，访问模式是 RWO（ReadWriteOnce），StorageClass 是我们定义的 host-test，状态显示的是 Available，也就是处于可用状态，可以随时分配给 Pod 使用。\n一旦 PVC 对象创建成功，Kubernetes 就会立即通过 StorageClass、resources 等条件在集群里查找符合要求的 PV，如果找到合适的存储对象就会把它俩 “绑定” 在一起。\nPVC 对象申请的是 5MB，但现在系统里只有一个 10MB 的 PV，没有更合适的对象，所以 Kubernetes 也只能把这个 PV 分配出去，多出的容量就算是 “福利” 了。这两个对象的状态都是 Bound，也就是说存储申请成功，PVC 的实际容量就是 PV 的容量 10MB，而不是最初申请的容量 5MB。\n如果把 PVC 的申请容量改大一些，比如改成 100MB，apply 后会看到 PVC 会一直处于 Pending 状态，意味着 Kubernetes 在系统里没有找到符合要求的存储，无法分配资源，只能等有满足要求的 PV 才能完成绑定。\n2.13.5 Pod 挂载 PersistentVolume # 当 PV 和 PVC 绑定好了，有了持久化存储，就可以为 Pod 挂载存储卷。先要在 spec.volumes 定义存储卷，然后在 containers.volumeMounts 挂载进容器。因为用的是 PVC，所以要在 volumes 里用字段 persistentVolumeClaim 指定 PVC 的名字。\n以下的 Pod 的 YAML 描述文件，把存储卷挂载到了 Nginx 容器的 /tmp 目录：\napiVersion: v1 kind: Pod metadata: name: host-pvc-pod spec: volumes: - name: host-pvc-vol persistentVolumeClaim: claimName: host-5m-pvc containers: - name: ngx-pvc-pod image: nginx:alpine ports: - containerPort: 80 volumeMounts: - name: host-pvc-vol mountPath: /tmp 创建 Pod 并查看它的状态：\nkubectl apply -f host-path-pod.yml kubectl get pod -o wide 可以看到，这个 Pod 被 Kubernetes 调到了 worker 节点上。可以使用 kubectl exec 进入容器，查看 PV 是否确实挂载成功：\n在容器的 /tmp 目录里生成了一个 a.txt 的文件，根据 PV 的定义，这个文件就应该落在 worker 节点的磁盘上，登录到 worker 节点上查看，在 worker 节点这台机器的 /tmp/host-10m-pv 目录下，里面有个 a.txt 文件，文件内容跟我们在容器中创建的一样，可以确认确实是刚才在 Pod 里生成的文件。其实并没有提前在 worker 节点上创建 /tmp/host-10m-pv 目录，但是当 Pod 运行起来时，/tmp/host-10m-pv 目录在 worker 节点上被自动创建了。\n因为 Pod 产生的数据已经通过 PV 存在了磁盘上，如果 Pod 删除后再重新创建，挂载存储卷时会依然使用这个目录，数据保持不变，也就实现了持久化存储。但是，由于这个 PV 是 HostPath 类型，只在本节点存储，如果 Pod 重建时被调度到了其他节点上，那么即使加载了本地目录，也不会是之前的存储位置，持久化功能也就失效了。\n所以，HostPath 类型的 PV 一般用来做测试，或者是用于 DaemonSet 这样与节点关系比较密切的应用。\n"},{"id":22,"href":"/kubernetes/docs/part2-break-ice/2.14-persistentvolume-nfs/","title":"2.14 网络共享存储","section":"第二部分 入门","content":" 2.14 网络共享存储 # 由于 Kubernetes 里的 Pod 经常会在集群里 “漂移”，要想让存储卷真正能被 Pod 任意挂载，就不能限定在本地磁盘，而是要改成网络存储，这样 Pod 无论在哪里运行，只要知道 IP 地址或者域名，就可以通过网络通信访问存储设备。\n在网络存储中有比较简单的 NFS 系统（Network File System），可以通过 NFS 理解在 Kubernetes 里使用网络存储，以及静态存储卷和动态存储卷的概念。\n2.14.1 安装 NFS 服务器 # NFS 采用的是 Client/Server 架构，需要选定一台主机作为 Server，安装 NFS 服务端；其他要使用存储的主机作为 Client，安装 NFS 客户端工具。\n可以在 Kubernetes 集群里增添一台名字叫 Storage 的服务器，在上面安装 NFS，实现网络存储、共享网盘的功能。这台 Storage 只是一个逻辑概念，在实际安装部署的时候完全可以把它合并到集群里的某台主机里。\n在 Ubuntu/Debian 系统里安装 NFS 服务端很容易，使用 apt 即可：\nsudo apt -y install nfs-kernel-server 安装好之后，需要给 NFS 指定一个存储位置，也就是网络共享目录。一般来说，应该建立一个专门的 /data 目录，这里使用了临时目录 /tmp/nfs：\nmkdir -p /tmp/nfs 接下来需要配置 NFS 访问共享目录，修改 /etc/exports，指定目录名、允许访问的网段，还有权限等参数。把下面这行加上就行，注意目录名和 IP 地址要改成和自己的环境一致：\n/tmp/nfs 192.168.14.0/24(rw,sync,no_subtree_check,no_root_squash,insecure) 改好之后，需要用 exportfs -ra 通知 NFS，让配置生效，再用 exportfs -v 验证效果：\nsudo exportfs -ra sudo exportfs -v 以上的步骤完成之后，就可以使用 systemctl 来启动 NFS 服务器了：\nsudo systemctl start nfs-server sudo systemctl enable nfs-server sudo systemctl status nfs-server 可以使用命令 showmount 来检查 NFS 的网络挂载情况：\nshowmount -e 127.0.0.1 2.14.2 安装 NFS 客户端 # NFS 服务器安装完成之后，为了让 Kubernetes 集群能够访问 NFS 存储服务，还需要在每个节点上都安装 NFS 客户端。\nsudo apt -y install nfs-common 可以在节点上可以用 showmount 检查 NFS 能否正常挂载，这里的 IP 地址要写成 NFS 服务器的地址：\n挂载测试 # 现在可以尝试手动挂载一下 NFS 网络存储，可以在 worker 节点创建一个目录 /tmp/test 作为挂载点：\nmkdir -p /tmp/test 然后用命令 mount 把 NFS 服务器的共享目录挂载到刚才创建的本地目录上：\nsudo mount -t nfs 192.168.14.142:/tmp/nfs /tmp/test 在 worker 节点 /tmp/test 目录里随便创建一个文件，比如 x.yml：\ntouch /tmp/test/x.yml 回到 NFS 服务器，检查共享目录 /tmp/nfs，应该会看到也出现了一个同样的文件 x.yml，这就说明 NFS 安装成功了。之后集群里的任意节点，只要通过 NFS 客户端，就能把数据写入 NFS 服务器，实现网络存储。\n2.14.3 使用 NFS 存储卷 # 在配置好 NFS 存储系统后，就可以使用它来创建 PV 存储对象了。可以先手工分配一个存储卷，需要指定 storageClassName 是 nfs，而 accessModes 可以设置成 ReadWriteMany，这是由 NFS 的特性决定的，因为它支持多个节点同时访问一个共享目录。\n因为存储卷是 NFS 系统，所以需要在 YAML 里添加 nfs 字段，指定 NFS 服务器的 IP 地址和共享目录名。\n在 NFS 服务器的 /tmp/nfs 目录里创建了一个新的目录 1g-pv，表示分配了 1GB 的可用存储空间，相应的，PV 里的 capacity 也要设置成同样的数值，也就是 1Gi。\n以下是一个使用 NFS 网络存储的 YAML 描述文件：\napiVersion: v1 kind: PersistentVolume metadata: name: nfs-1g-pv spec: storageClassName: nfs accessModes: - ReadWriteMany capacity: storage: 1Gi nfs: path: /tmp/nfs/1g-pv server: 192.168.14.142 spec.nfs 里的 IP 地址一定要正确，路径也要事先建好目录，不然在 Pod 使用 NFS 时会报 No such file or directory 的错误。使用命令 kubectl apply 创建 PV 对象，可以使用 kubectl get pv 查看它的状态：\n有了 PV，就可以定义申请存储的 PVC 对象了，内容和 PV 差不多，但不涉及 NFS 存储的细节，只需要用 resources.request 来表示希望要有多大的容量，这里写成 1GB，和 PV 的容量相同：\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-static-pvc spec: storageClassName: nfs accessModes: - ReadWriteMany resources: requests: storage: 1Gi 创建 PVC 对象之后，Kubernetes 就会根据 PVC 的描述，找到最合适的 PV，把它们 “绑定” 在一起，也就是存储分配成功：\n此时可以再创建一个 Pod，把 PVC 挂载成它的一个 volume，用 persistentVolumeClaim 指定 PVC 的名字就可以了：\napiVersion: v1 kind: Pod metadata: name: nfs-static-pod spec: volumes: - name: nfs-pvc-vol persistentVolumeClaim: claimName: nfs-static-pvc containers: - name: nfs-pvc-test image: nginx:alpine ports: - containerPort: 80 volumeMounts: - name: nfs-pvc-vol mountPath: /tmp Pod、PVC、PV 和 NFS 存储的关系可以用下图来表示：\n因为在 PV/PVC 里指定了 storageClassName 是 nfs，节点上也安装了 NFS 客户端，所以 Kubernetes 就会自动执行 NFS 挂载动作，把 NFS 的共享目录 /tmp/nfs/1g-pv 挂载到 Pod 里的 /tmp，完全不需要去手动管理。\nspec.nfs 里的路径一定要事先存在，不然在创建 Pod 时会报错\nOutput: mount.nfs: mounting 192.168.14.142:/tmp/nfs/1g-pv failed, reason given by server: No such file or directory 在用 kubectl apply 创建 Pod 之后，可以使用 kubectl exec 进入 Pod，再试着操作 NFS 共享目录：\n可以看到，NFS 服务器的 /tmp/nfs/1g-pv 目录里有在 Pod 里创建的文件，说明文件确实写入了共享目录。\nNFS 是一个网络服务，不会受 Pod 调度位置的影响，所以只要网络通畅，这个 PV 对象就会一直可用，数据也就实现了真正的持久化存储。\n2.14.4 部署 NFS Provisoner # 网络存储系统确实能够让集群里的 Pod 任意访问，数据在 Pod 销毁后仍然存在，新创建的 Pod 可以再次挂载，然后读取之前写入的数据，整个过程完全是自动化的。但因为 PV 还是需要人工管理，必须要由系统管理员手动维护各种存储设备，再根据开发需求逐个创建 PV，而且 PV 的大小也很难精确控制，容易出现空间不足或者空间浪费的情况。\n在一个大集群里，每天可能会有几百几千个应用需要 PV 存储，如果仍然用人力来管理分配存储，管理员很可能会忙得焦头烂额，导致分配存储的工作大量积压。\n在 Kubernetes 里有一个 “动态存储卷” 的概念，它可以用 StorageClass 绑定一个 Provisioner /prəˈvɪʒənə(r)/ 对象，而这个 Provisioner 就是一个能够自动管理存储、创建 PV 的应用，代替了原来系统管理员的手工劳动。有了 “动态存储卷” 的概念，手工创建的 PV 可以称为 “静态存储卷”。\nKubernetes 里每类存储设备都有相应的 Provisioner 对象，对于 NFS 来说，它的 Provisioner 就是 “NFS subdir external provisioner”，可以在 GitHub 上找到这个项目 https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner。\nNFS Provisioner 也是以 Pod 的形式运行在 Kubernetes 里的，在 GitHub 的 deploy 目录里是部署它所需的 YAML 文件，一共有三个，分别是 rbac.yaml、class.yaml 和 deployment.yaml。但是这三个文件只是示例，想要在集群里真正运行起来还要修改其中的两个文件。\n第一个要修改的是 rbac.yaml，它使用的是默认的 default 名字空间，应该把它改成其他的名字空间，避免与普通应用混在一起，可以用 “查找替换” 的方式把它统一改成 kube-system。\n第二个要修改的是 deployment.yaml，首先要把名字空间改成和 rbac.yaml 一样，比如是 kube-system，然后要修改 volumes 和 env 里的 IP 地址和共享目录名，必须和集群里的 NFS 服务器配置一样。\n按照现行环境修改后的 rbac.yaml、class.yaml 和 deployment.yaml 分别如下：\nrbac.yaml\napiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: kube-system --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [ \u0026#34;\u0026#34; ] resources: [ \u0026#34;nodes\u0026#34; ] verbs: [ \u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34; ] - apiGroups: [ \u0026#34;\u0026#34; ] resources: [ \u0026#34;persistentvolumes\u0026#34; ] verbs: [ \u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;delete\u0026#34; ] - apiGroups: [ \u0026#34;\u0026#34; ] resources: [ \u0026#34;persistentvolumeclaims\u0026#34; ] verbs: [ \u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;update\u0026#34; ] - apiGroups: [ \u0026#34;storage.k8s.io\u0026#34; ] resources: [ \u0026#34;storageclasses\u0026#34; ] verbs: [ \u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34; ] - apiGroups: [ \u0026#34;\u0026#34; ] resources: [ \u0026#34;events\u0026#34; ] verbs: [ \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34; ] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: kube-system roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: kube-system rules: - apiGroups: [ \u0026#34;\u0026#34; ] resources: [ \u0026#34;endpoints\u0026#34; ] verbs: [ \u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34; ] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: kube-system subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: kube-system roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io class.yaml\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-client provisioner: k8s-sigs.io/nfs-subdir-external-provisioner # or choose another name, must match deployment\u0026#39;s env PROVISIONER_NAME\u0026#39; parameters: archiveOnDelete: \u0026#34;false\u0026#34; deployment.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner labels: app: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: kube-system spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2 # image: xiaobinqt/nfs-subdir-external-provisioner:v4.0.2 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: k8s-sigs.io/nfs-subdir-external-provisioner - name: NFS_SERVER value: 192.168.14.142 #改IP地址 - name: NFS_PATH value: /tmp/nfs #改共享目录名 volumes: - name: nfs-client-root nfs: server: 192.168.14.142 #改IP地址 path: /tmp/nfs #改共享目录名 deployment.yaml 的镜像仓库用的是 gcr.io，拉取很困难，可以使用 Docker Hub 的镜像 xiaobinqt/nfs-subdir-external-provisioner:v4.0.2。\n或是直接把镜像打成 tar 包上传到服务器上再 load 解包。\nYAML 修改好之后，就可以在 Kubernetes 里创建 NFS Provisioner 了：\nkubectl apply -f rbac.yaml kubectl apply -f class.yaml kubectl apply -f deployment.yaml 使用命令 kubectl get，再加上名字空间限定 -n kube-system，就可以看到 NFS Provisioner 在 Kubernetes 里运行起来了。\n2.14.5 使用 NFS 动态存储卷 # 因为有了 Provisioner，就不再需要手工定义 PV 对象了，只需要在 PVC 里指定 StorageClass 对象，它再关联到 Provisioner。\nNFS 默认的 StorageClass 定义：\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-client provisioner: k8s-sigs.io/nfs-subdir-external-provisioner parameters: archiveOnDelete: \u0026#34;false\u0026#34; YAML 里的关键字段是 provisioner，它指定了应该使用哪个 Provisioner。另一个字段 parameters 是调节 Provisioner 运行的参数，需要参考文档来确定具体值，在这里的 archiveOnDelete: \u0026quot;false\u0026quot; 就是自动回收存储空间。\n定义一个 PVC，向系统申请 10MB 的存储空间，使用的 StorageClass 是默认的 nfs-client：\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-dyn-10m-pvc spec: storageClassName: nfs-client accessModes: - ReadWriteMany resources: requests: storage: 10Mi 有了 PVC，还需要在 Pod 里用 volumes 和 volumeMounts 挂载，然后 Kubernetes 就会自动找到 NFS Provisioner，在 NFS 的共享目录上创建出合适的 PV 对象：\napiVersion: v1 kind: Pod metadata: name: nfs-dyn-pod spec: volumes: - name: nfs-dyn-10m-vol persistentVolumeClaim: claimName: nfs-dyn-10m-pvc containers: - name: nfs-dyn-test image: nginx:alpine ports: - containerPort: 80 volumeMounts: - name: nfs-dyn-10m-vol mountPath: /tmp 使用 kubectl apply 创建好 PVC 和 Pod，可以查看一下集群里的 PV 状态：\n从上图可以看到，虽然没有直接定义 PV 对象，但由于有 NFS Provisioner，它就自动创建一个 PV，大小刚好是在 PVC 里申请的 10MB。\n这个时候如果去 NFS 服务器上查看共享目录，会发现多出了一个目录，名字与这个自动创建的 PV 一样，但加上了名字空间和 PVC 的前缀：\nPod、PVC、StorageClass 和 Provisioner 的关联关系，可以通过下图有一个大致的了解：\n"},{"id":23,"href":"/kubernetes/docs/part2-break-ice/2.15-statefulset/","title":"2.15 StatefulSet","section":"第二部分 入门","content":" 2.15 StatefulSet # 2.15.1 状态和应用 # 理论上任何应用都是有状态的，只是有的应用的状态信息不是很重要，即使不恢复状态也能够正常运行，这就是 “无状态应用”。“无状态应用” 典型的例子就是 Nginx 这样的 Web 服务器，它只是处理 HTTP 请求，本身不生产数据（日志除外），不需要特意保存状态，无论以什么状态重启都能很好地对外提供服务。\n还有一些应用，运行状态信息很重要，如果因为重启而丢失了状态是绝对无法接受的，这样的应用是 “有状态应用”。比如 Redis、MySQL 这样的数据库，它们的 “状态” 就是在内存或者磁盘上产生的数据，是应用的核心价值所在，如果不能够把这些数据及时保存再恢复，那绝对会是灾难性的后果。\n对于 Deployment 来说，多个实例之间是无关的，启动的顺序不固定，Pod 的名字、IP 地址、域名也都是完全随机的，这正是 “无状态应用” 的特点。对于 “有状态应用”，多个实例之间可能存在依赖关系，比如 master/slave、active/passive，需要依次启动才能保证应用正常运行，外界的客户端也可能要使用固定的网络标识来访问实例，而且这些信息还必须要保证在 Pod 重启后不变。\nKubernetes 定义了一个新的 API 对象 StatefulSet，专门用来管理有状态的应用。\n2.15.2 描述 StatefulSet # StatefulSet 也可以看做是 Deployment 的一个特例，它不能直接用 kubectl create 创建样板文件，它的对象描述和 Deployment 差不多，可以把 Deployment 适当修改一下，就变成了 StatefulSet 对象。以下是一个使用 Redis 的 StatefulSet 描述文件：\napiVersion: apps/v1 kind: StatefulSet metadata: name: redis-sts spec: serviceName: redis-svc replicas: 2 selector: matchLabels: app: redis-sts template: metadata: labels: app: redis-sts spec: containers: - image: redis:5-alpine name: redis ports: - containerPort: 6379 YAML 文件里除了 kind 必须是 “StatefulSet”，在 spec 里还多出了一个 “serviceName” 字段外，其余的部分和 Deployment 是一模一样的，比如 replicas、selector、template。\n2.15.3 使用 StatefulSet # 当使用 apply 创建 StatefulSet 对象，可以看到：\nStatefulSet 所管理的 Pod 不再是随机的名字了，而是有了顺序编号，从 0 开始分别被命名为 redis-sts-0、redis-sts-1，Kubernetes 也会按照这个顺序依次创建，这其实就解决了 “有状态应用” 的第一个问题：启动顺序。\n当有了启动的先后顺序，应用该怎么确定互相之间的依赖关系呢？\nKubernetes 给出的方法是使用 hostname，也就是每个 Pod 里的主机名，可以使用 kubectl exec 命令登录 Pod 内部看看：\n在 Pod 里查看环境变量 $HOSTNAME 或者是执行命令 hostname，都可以得到这个 Pod 的名字 redis-sts-0。有了这个唯一的名字，应用就可以自行决定依赖关系了，比如在这个 Redis 例子里，就可以让先启动的 0 号 Pod 是主实例，后启动的 1 号 Pod 是从实例。\n解决了启动顺序和依赖关系，网络标识就需要用到 Service 对象。不能用命令 kubectl expose 直接为 StatefulSet 生成 Service，只能手动编写 YAML。\n在写 Service 对象的时候要注意，metadata.name 必须和 StatefulSet 里的 serviceName 相同，selector 里的标签也必须和 StatefulSet 里的一致：\napiVersion: v1 kind: Service metadata: name: redis-svc spec: selector: app: redis-sts ports: - port: 6379 protocol: TCP targetPort: 6379 Service 自己会有一个域名，格式是 对象名.名字空间，每个 Pod 也会有一个域名，形式是 IP 地址.名字空间。但因为 IP 地址不稳定，所以 Pod 的域名并不实用，一般使用稳定的 Service 域名。\n当把 Service 对象应用于 StatefulSet 的时候，情况会发生变化。Service 发现这些 Pod 不是一般的应用，而是有状态应用，需要有稳定的网络标识，所以就会为 Pod 再多创建出一个新的域名，格式是 Pod 名.服务名.名字空间.svc.cluster.local。这个域名也可以简写成 Pod 名.服务名。\n可以使用 kubectl exec 进入 Pod 内部，用 ping 命令来验证一下：\n在 StatefulSet 里的两个 Pod 都有各自的域名，也就是稳定的网络标识。外部的客户端只要知道了 StatefulSet 对象，就可以用固定的编号去访问某个具体的实例，虽然 Pod 的 IP 地址可能会变，但这个有编号的域名由 Service 对象维护，是稳定不变的。\nService 原本的目的是负载均衡，应该由它在 Pod 前面来转发流量，但是对 StatefulSet 来说，这项功能反而是不必要的，因为 Pod 已经有了稳定的域名，外界访问服务就不应该再通过 Service 这一层了。从安全和节约系统资源的角度考虑，可以在 Service 里添加一个字段 clusterIP: None ，告诉 Kubernetes 不必再为这个对象分配 IP 地址。\napiVersion: v1 kind: Service metadata: name: redis-svc spec: clusterIP: None selector: app: redis-sts ports: - port: 6379 protocol: TCP targetPort: 6379 使用了 “clusterlP:None”，没有集群 IP 地址的 Service 对象，也被称为是 “Headless Service”。\n下面这张图展示了 StatefulSet 与 Service 对象的关系：\n2.15.4 数据持久化 # 为了强调持久化存储与 StatefulSet 的一对一绑定关系，Kubernetes 为 StatefulSet 专门定义了一个字段 “volumeClaimTemplates”，直接把 PVC 定义嵌入 StatefulSet 的 YAML 文件里。这样能保证创建 StatefulSet 的同时，就会为每个 Pod 自动创建 PVC，让 StatefulSet 的可用性更高。\napiVersion: apps/v1 kind: StatefulSet metadata: name: redis-pv-sts spec: serviceName: redis-pv-svc volumeClaimTemplates: - metadata: name: redis-100m-pvc spec: storageClassName: nfs-client accessModes: - ReadWriteMany resources: requests: storage: 100Mi replicas: 2 selector: matchLabels: app: redis-pv-sts template: metadata: labels: app: redis-pv-sts spec: containers: - image: redis:5-alpine name: redis ports: - containerPort: 6379 volumeMounts: - name: redis-100m-pvc mountPath: /data 这个描述文件中 StatefulSet 对象的名字是 redis-pv-sts，表示使用了 PV 存储。“volumeClaimTemplates” 里定义了一个 PVC，名字是 redis-100m-pvc，申请了 100MB 的 NFS 存储。在 Pod 模板里用 volumeMounts 引用了这个 PVC，把网盘挂载到了 /data 目录，也就是 Redis 的数据目录。\napply 创建后，一个带持久化功能的 “有状态应用” 就算是运行起来了。可以使用命令 kubectl get pvc 来查看 StatefulSet 关联的存储卷状态：\n这两个 PVC 的命名，是有规律的，用的是 PVC 名字加上 StatefulSet 的名字组合而成，所以即使 Pod 被销毁，因为它的名字不变，还能够找到这个 PVC，再次绑定使用之前存储的数据。\n用 kubectl exec 运行 Redis 的客户端，在里面添加一些 KV 数据：\nkubectl exec -it redis-pv-sts-0 -- redis-cli 模拟意外事故，删除这个 Pod：\nkubectl delete pod redis-pv-sts-0 由于 StatefulSet 和 Deployment 一样会监控 Pod 的实例，发现 Pod 数量少了就会很快创建出新的 Pod，并且名字、网络标识也都会和之前的 Pod 一模一样：\n可以再用 Redis 客户端登录去检查一下数据是否存在：\nkubectl exec -it redis-pv-sts-0 -- redis-cli 因为把 NFS 网络存储挂载到了 Pod 的 /data 目录，Redis 就会定期把数据落盘保存，所以新创建的 Pod 再次挂载目录的时候会从备份文件里恢复数据，内存里的数据就恢复原状了。\n"},{"id":24,"href":"/kubernetes/docs/part2-break-ice/2.16-rolling-update/","title":"2.16 滚动更新","section":"第二部分 入门","content":" 2.16 滚动更新 # 在实际生产环境中，只是把应用发布到集群里是远远不够的，要让应用稳定可靠地运行，还需要有持续的运维工作。比如 Deployment 的 “应用伸缩” 功能就是一种常见的运维操作，在 Kubernetes 里，使用命令 kubectl scale，可以轻松调整 Deployment 下属的 Pod 数量。除了 “应用伸缩”，其他的运维操作比如应用更新、版本回退等工作也是日常运维中经常会遇到的问题。\n2.16.1 应用版本 # 版本更新实际做起来是一个相当棘手的事。因为系统已经上线运行，必须要保证不间断地对外提供服务。尤其在特殊时候可能需要开发、测试、运维、监控、网络等各个部门的一大堆人来协同工作，费时又费力。\n在 Kubernetes 里，版本更新使用的不是 API 对象，而是两个命令：kubectl apply 和 kubectl rollout，需要搭配部署应用所需要的 Deployment、DaemonSet 等 YAML 文件。\n在 Kubernetes 里应用都是以 Pod 的形式运行的，而 Pod 通常又会被 Deployment 等对象来管理，所以应用的 “版本更新” 实际上更新的是整个 Pod。Pod 是由 YAML 描述文件来确定的，是 Deployment 等对象里的字段 template。所以，在 Kubernetes 里应用的版本变化就是 template 里 Pod 的变化，哪怕 template 里只变动了一个字段，那也会形成一个新的版本，也算是版本变化。但在 template 里的内容太多了，拿这么长的字符串来当做 “版本号” 不太现实，所以 Kubernetes 就使用了 “摘要” 功能，用摘要算法计算 template 的 Hash 值作为 “版本号”。\n比如通过以下的 YAML 描述文件生成的 Deployment：\napiVersion: apps/v1 kind: Deployment metadata: labels: app: ngx-dep name: ngx-dep spec: replicas: 2 selector: matchLabels: app: ngx-dep template: metadata: labels: app: ngx-dep spec: containers: - image: nginx:alpine name: nginx Pod 名字里的那串随机数 “bfbb5f64b” 就是 Pod 模板的 Hash 值，也就是 Pod 的 “版本号”。\n如果变动了 Pod YAML 描述，比如把镜像改成 nginx:stable-alpine，或者把容器名字改成 nginx-test，都会生成一个新的应用版本，kubectl apply 后就会重新创建 Pod：\n可以看到，Pod 名字里的 Hash 值变成了 “c98cdf864”，这就表示 Pod 的版本更新了。\n2.16.2 如何实现应用更新 # 可以用一个 Nginx Deployment 对象，看看 Kubernetes 到底是怎么实现版本更新的。\n以下是一个 ConfigMap，让它输出 Nginx 的版本号，方便用 curl 查看版本：\napiVersion: v1 kind: ConfigMap metadata: name: ngx-conf data: default.conf: | server { listen 80; location / { default_type text/plain; return 200 \u0026#39;ver : $nginx_version\\nsrv : $server_addr:$server_port\\nhost: $hostname\\n\u0026#39;; } } 以下是一个 Deployment YAML 描述，指定 Nginx 版本号是 1.21-alpine，实例数设置为 4 个：\napiVersion: apps/v1 kind: Deployment metadata: name: ngx-dep spec: replicas: 4 selector: matchLabels: app: ngx-dep template: metadata: labels: app: ngx-dep spec: volumes: - name: ngx-conf-vol configMap: name: ngx-conf containers: - image: nginx:1.21-alpine name: nginx ports: - containerPort: 80 volumeMounts: - mountPath: /etc/nginx/conf.d name: ngx-conf-vol 把这个 YAML 命名为 ngx-v1.yml，然后执行命令 kubectl apply 部署这个应用：\n为这个 Deployment 创建 Service 对象，用 kubectl port-forward 转发请求来查看状态，以下是 Service 的 YAML 描述：\napiVersion: v1 kind: Service metadata: name: ngx-svc spec: selector: app: ngx-dep ports: - port: 80 targetPort: 80 protocol: TCP kubectl port-forward svc/ngx-svc 8080:80 \u0026amp; curl 127.1:8080 从 curl 命令的输出中可以看到，现在应用的版本是 1.21.x。\n现在，编写一个新版本对象 ngx-v2.yml，把镜像升级到 nginx:1.22-alpine，其他的都不变。\n因为 Kubernetes 的动作太快了，为了能够观察到应用更新的过程，还需要添加一个字段 minReadySeconds，让 Kubernetes 在更新过程中等待一点时间，确认 Pod 没问题才继续其余 Pod 的创建工作。以下是新版本的 YAML 描述：\napiVersion: apps/v1 kind: Deployment metadata: name: ngx-dep spec: minReadySeconds: 15 # 确认Pod就绪的等待时间 replicas: 4 selector: matchLabels: app: ngx-dep template: metadata: labels: app: ngx-dep spec: volumes: - name: ngx-conf-vol configMap: name: ngx-conf containers: - image: nginx:1.22-alpine name: nginx ports: - containerPort: 80 volumeMounts: - mountPath: /etc/nginx/conf.d name: ngx-conf-vol 现在执行命令 kubectl apply 来更新应用，因为改动了镜像名，Pod 模板变了，就会触发 “版本更新”，然后用一个新命令：kubectl rollout status，来查看应用更新的状态：\nkubectl apply -f ngx-v2.yml kubectl rollout status deployment ngx-dep 从 kubectl rollout status 的输出信息，可以发现，Kubernetes 不是把旧 Pod 全部销毁再一次性创建出新 Pod，而是在逐个地创建新 Pod，同时也在销毁旧 Pod，保证系统里始终有足够数量的 Pod 在运行。新 Pod 数量增加的过程有点像是 “滚雪球”，从零开始，越滚越大，也就是所谓的 “滚动更新”（rolling update）。\n其实 “滚动更新” 就是由 Deployment 控制的两个同步进行的 “应用伸缩” 操作，老版本缩容到 0，同时新版本扩容到指定值，是一个 “此消彼长” 的过程。\n更新完成后，再执行 kubectl get pod，就会看到 Pod 已经全部替换成了新版本 “d575d5776”，用 curl 访问 Nginx，输出的版本信息也变成了 1.22.x：\n滚动更新的过程可以用下面的这张图体下：\n2.16.3 管理应用更新 # 如果更新过程中发生了错误或者更新后发现有 Bug，可以使用 kubectl rollout 命令。\n在应用更新的过程中，可以随时使用 kubectl rollout pause 来暂停更新，检查、修改 Pod，或者测试验证，如果确认没问题，再用 kubectl rollout resume 来继续更新。\n对于更新后出现的问题，可以查看之前的每次更新记录，并且回退到任何位置，和 Git 等版本控制软件非常类似。查看更新历史使用的命令是 kubectl rollout history，如：\nkubectl rollout history deploy ngx-dep kubectl rollout history 的列表输出的有用信息太少，可以在命令后加上参数 --revision 来查看每个版本的详细信息，包括标签、镜像名、环境变量、存储卷等等，通过这些就可以大致了解每次都变动了哪些关键字段：\nkubectl rollout history deploy ngx-dep --revision=1 假设认为刚刚更新的 nginx:1.22-alpine 不好，想要回退到上一个版本，可以使用命令 kubectl rollout undo，也可以加上参数 --to-revision 回退到任意一个历史版本：\nkubectl rollout undo 的操作过程其实和 kubectl apply 是一样的，执行的仍然是 “滚动更新”，只不过使用的是旧版本 Pod 模板，把新版本 Pod 数量收缩到 0，同时把老版本 Pod 扩展到指定值。\n这个 V2 到 V1 的 “版本降级” 的变化过程如下图所示：\n1.16.4 添加更新描述 # kubectl rollout history 的版本列表 CHANGE-CAUSE 列可以添加说明信息，当没有添加说明信息时显示 。\n当需要添加说明信息时，需要在 Deployment 的 metadata 里加上一个新的字段 annotations。annotations 字段的含义是 “注解” “注释”，形式上和 labels 一样，都是 Key-Value，也都是给 API 对象附加一些额外的信息，但是用途上区别很大。\nannotations 添加的信息一般是给 Kubernetes 内部的各种对象使用的，有点像是 “扩展属性”；\nlabels 主要面对的是 Kubernetes 外部的用户，用来筛选、过滤对象的。\n借助 annotations，Kubernetes 既不破坏对象的结构，也不用新增字段，就能够给 API 对象添加任意的附加信息，这就是面向对象设计中典型的 OCP “开闭原则”，让对象更具扩展性和灵活性。\nannotations 里的值可以任意写，Kubernetes 会自动忽略不理解的 Key-Value，但要编写更新说明就需要使用特定的字段 kubernetes.io/change-cause。\n这里有 3 个版本的 Nginx 应用，同时添加更新说明：\nngx1.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: ngx-dep annotations: kubernetes.io/change-cause: v1, ngx=1.21 spec: minReadySeconds: 15 # 确认Pod就绪的等待时间 replicas: 4 selector: matchLabels: app: ngx-dep template: metadata: labels: app: ngx-dep spec: volumes: - name: ngx-conf-vol configMap: name: ngx-conf containers: - image: nginx:1.21-alpine name: nginx ports: - containerPort: 80 volumeMounts: - mountPath: /etc/nginx/conf.d name: ngx-conf-vol ngx2.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: ngx-dep annotations: kubernetes.io/change-cause: update to v2, ngx=1.22 spec: minReadySeconds: 15 # 确认Pod就绪的等待时间 replicas: 4 selector: matchLabels: app: ngx-dep template: metadata: labels: app: ngx-dep spec: volumes: - name: ngx-conf-vol configMap: name: ngx-conf containers: - image: nginx:1.22-alpine name: nginx ports: - containerPort: 80 volumeMounts: - mountPath: /etc/nginx/conf.d name: ngx-conf-vol ngx3.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: ngx-dep annotations: kubernetes.io/change-cause: update to v3, change name spec: minReadySeconds: 15 # 确认Pod就绪的等待时间 replicas: 4 selector: matchLabels: app: ngx-dep template: metadata: labels: app: ngx-dep spec: volumes: - name: ngx-conf-vol configMap: name: ngx-conf containers: - image: nginx:1.22-alpine name: nginx ports: - containerPort: 80 volumeMounts: - mountPath: /etc/nginx/conf.d name: ngx-conf-vol 依次使用 kubectl apply 创建并更新对象之后，再用 kubectl rollout history 来看一下更新历史：\n这次显示的列表信息，每个版本的主要变动情况列得非常清楚，和 Git 版本管理的感觉很像。\nKubernetes 不会记录所有的更新历史，那样太浪费资源，默认它只会保留最近的 10 次操作，但这个值可以用字段 “revisionHistoryLimit” 调整。\n参考 # 滚动更新 Deploymen "},{"id":25,"href":"/kubernetes/docs/part2-break-ice/2.17-app-assurance/","title":"2.17 应用保障","section":"第二部分 入门","content":" 2.17 应用保障 # 2.17.1 容器资源配额 # 创建容器有三大隔离技术：namespace、cgroup、chroot。其中的 namespace 实现了独立的进程空间，chroot 实现了独立的文件系统，cgroup 的作用是管控 CPU、内存，保证容器不会无节制地占用基础资源，进而影响到系统里的其他应用。\n因为 CPU、内存与存储卷有明显的不同，它是直接 “内置” 在系统里的，不像硬盘那样需要 “外挂”，所以申请和管理的过程会简单很多。Kubernetes 在管控容器使用 CPU 和内存的做法是，只要在 Pod 容器的描述部分添加一个新字段 resources 就可以了，它就相当于申请资源的 Claim。\n以下是一个 YAML 描述示例：\napiVersion: v1 kind: Pod metadata: name: ngx-pod-resources spec: containers: - image: nginx:alpine name: ngx resources: requests: cpu: 10m memory: 100Mi limits: cpu: 20m memory: 200Mi requests 意思是容器要申请的资源，也就是说要求 Kubernetes 在创建 Pod 的时候必须分配这里列出的资源，否则容器就无法运行。\nlimits 意思是容器使用资源的上限，不能超过设定值，否则就有可能被强制停止运行。\n内存的写法和磁盘容量一样，使用 Ki、Mi、Gi 来表示 KB、MB、GB，比如 512Ki、100Mi、0.5Gi 等。\n因为 CPU 因为在计算机中数量有限，非常宝贵，所以 Kubernetes 允许容器精细分割 CPU，既可以 1 个、2 个地完整使用 CPU，也可以用小数 0.1、0.2 的方式来部分使用 CPU。这其实是效仿了 UNIX “时间片” 的用法，意思是进程最多可以占用多少 CPU 时间。CPU 时间也不能无限分割，Kubernetes 里 CPU 的最小使用单位是 0.001，为了方便表示用了一个特别的单位 m，就是 “milli” “毫” 的意思，比如说 500m 就相当于 0.5。\n上面的示例 YAML 描述向系统申请的是 1% 的 CPU 时间和 100MB 的内存，运行时的资源上限是 2%CPU 时间和 200MB 内存。有了这个申请，Kubernetes 就会在集群中查找最符合这个资源要求的节点去运行 Pod。\nKubernetes 会根据每个 Pod 声明的需求，像搭积木或者玩俄罗斯方块一样，把节点尽量 “塞满”，充分利用每个节点的资源，让集群的效益最大化。\n如果 Pod 不写 resources 字段，就意味着 Pod 对运行的资源要求 “既没有下限，也没有上限”，Kubernetes 不用管 CPU 和内存是否足够，可以把 Pod 调度到任意的节点上，而且后续 Pod 运行时也可以无限制地使用 CPU 和内存。如果是生产环境就很危险了，Pod 可能会因为资源不足而运行缓慢，或者是占用太多资源而影响其他应用，所以应当合理评估 Pod 的资源使用情况，尽量为 Pod 加上限制。\n如果预估错误，Pod 申请的资源太多，系统无法满足，比如申请 10 个 CPU，但是系统里没有节点能满足这个要求，Kubernetes 会调度失败，当前集群里的所有节点都无法运行这个 Pod。\n1.17.2 容器状态探针 # 使用 resources 字段加上资源配额之后，Pod 在 Kubernetes 里的运行就有了初步保障，Kubernetes 会监控 Pod 的资源使用情况，让它既不会 “饿死” 也不会 “撑死”。 如果还希望 Kubernetes 能够更细致地监控 Pod 的状态，除了保证崩溃重启，还必须要能够探查到 Pod 的内部运行状态，定时给应用做 “体检”，让应用时刻保持 “健康”，能够满负荷稳定工作，这就需要用到 “探针”（Probe）。\nKubernetes 为检查应用状态定义了三种探针，分别对应容器不同的状态：\nStartup 启动探针，用来检查应用是否已经启动成功，适合那些有大量初始化工作要做，启动很慢的应用。\nLiveness 存活探针，用来检查应用是否正常运行，是否存在死锁、死循环。\nReadiness 就绪探针，用来检查应用是否可以接收流量，是否能够对外提供服务。\n这三种探针是递进的关系：应用程序先启动，加载完配置文件等基本的初始化数据就进入了 Startup 状态，之后如果没有什么异常就是 Liveness 存活状态，但可能有一些准备工作没有完成，还不一定能对外提供服务，只有到最后的 Readiness 状态才是一个容器最健康可用的状态。\n如果一个 Pod 里的容器配置了探针，Kubernetes 在启动容器后就会不断地调用探针来检查容器的状态：\n如果 Startup 探针失败，Kubernetes 会认为容器没有正常启动，就会尝试反复重启，其后面的 Liveness 探针和 Readiness 探针也不会启动。\n如果 Liveness 探针失败，Kubernetes 就会认为容器发生了异常，也会重启容器。\n如果 Readiness 探针失败，Kubernetes 会认为容器虽然在运行，但内部有错误，不能正常提供服务，就会把容器从 Service 对象的负载均衡集合中排除，不会给它分配流量。\n2.17.3 使用容器状态探针 # startupProbe、livenessProbe、readinessProbe 三种探针的配置方式都是一样的，关键字段有：\nperiodSeconds 执行探测动作的时间间隔，默认是 10 秒探测一次。\ntimeoutSeconds 探测动作的超时时间，如果超时就认为探测失败，默认是 1 秒。\nsuccessThreshold 连续几次探测成功才认为是正常，对于 startupProbe 和 livenessProbe 来说它只能是 1。\nfailureThreshold 连续探测失败几次才认为是真正发生了异常，默认是 3 次。\nKubernetes 支持 3 种探测方式，分别是：Shell、TCP Socket、HTTP GET，需要在探针里配置：\nexec，执行一个 Linux 命令，比如 ps、cat 等等，和 container 的 command 字段很类似。\ntcpSocket，使用 TCP 协议尝试连接容器的指定端口。\nhttpGet，连接端口并发送 HTTP GET 请求。\n要使用这些探针，就必须要在开发应用时预留出 “检查口”，这样 Kubernetes 才能调用探针获取信息。这里以 Nginx 作为示例，用 ConfigMap 编写一个配置文件：\napiVersion: v1 kind: ConfigMap metadata: name: ngx-conf data: default.conf: | server { listen 80; location = /ready { return 200 \u0026#39;I am ready\u0026#39;; } } 在这个配置文件里，启用 80 端口，然后用 location 指令定义了 HTTP 路径 /ready，把它作为对外暴露的 “检查口”，用来检测就绪状态，返回简单的 200 状态码和一个字符串表示工作正常。\n接下来是 Pod 里三种探针的具体定义：\napiVersion: v1 kind: Pod metadata: name: ngx-pod-probe spec: volumes: - name: ngx-conf-vol configMap: name: ngx-conf containers: - image: nginx:alpine name: ngx ports: - containerPort: 80 volumeMounts: - mountPath: /etc/nginx/conf.d name: ngx-conf-vol startupProbe: periodSeconds: 1 exec: command: [ \u0026#34;cat\u0026#34;, \u0026#34;/var/run/nginx.pid\u0026#34; ] livenessProbe: periodSeconds: 10 tcpSocket: port: 80 readinessProbe: periodSeconds: 5 httpGet: path: /ready port: 80 StartupProbe 使用了 Shell 方式，使用 cat 命令检查 Nginx 存在磁盘上的进程号文件（/var/run/nginx.pid），如果存在就认为是启动成功，它的执行频率是每秒探测一次。\nLivenessProbe 使用了 TCP Socket 方式，尝试连接 Nginx 的 80 端口，每 10 秒探测一次。\nReadinessProbe 使用的是 HTTP GET 方式，访问容器的 /ready 路径，每 5 秒发一次请求。\napply 创建 Pod 后，然后查看它的状态，也可以使用 kubectl logs 来查看 Nginx 的访问日志，里面会记录 HTTP GET 探针的执行情况：\n可以看到 Kubernetes 正是以大约 5 秒一次的频率，向 URI /ready 发送 HTTP 请求，不断地检查容器是否处于就绪状态。\n探针可以配置 “initialDelaySeconds” 字段，表示容器启动后多久才执行探针动作，适用于某些启动比较慢的应用，默认值是 0。\n在容器里还可以配置 “lifecycle” 字段，在启动后和终止前安装两个钩子 “postStart” “preStop”，执行 Shell 命令或者发送 HTTP 请求做一些初始化和收尾工作。\n"},{"id":26,"href":"/kubernetes/docs/part2-break-ice/2.18-cluster-management/","title":"2.18 集群管理","section":"第二部分 入门","content":" 2.18 集群管理 # 2.18.1 名字空间 # Kubernetes 的名字空间并不是一个实体对象，只是一个逻辑上的概念。它可以把集群切分成一个个彼此独立的区域，然后把对象放到这些区域里，就实现了类似容器技术里 namespace 的隔离效果，应用只能在自己的名字空间里分配资源和运行，不会干扰到其他名字空间里的应用。\n在 Master/Node 架构里引入名字空间，是因为集群很大、计算资源充足，会有非常多的用户在 Kubernetes 里创建各式各样的应用，可能会有百万数量级别的 Pod，这就使得资源争抢和命名冲突的概率大大增加了，情形和单机 Linux 系统里是非常相似的。\n比如，现在有一个 Kubernetes 集群，前端组、后端组、测试组都在使用它。这个时候就很容易命名冲突，比如后端组先创建了一个 Pod 叫 Web，这个名字就被 “占用” 了，之后前端组和测试组就只能绞尽脑汁再新起一个不冲突的名字。资源争抢也容易出现，比如，测试组不小心部署了有 Bug 的应用，在节点上把资源都给 “吃” 完了，就会导致其他组的同事根本无法工作。\n当多团队、多项目共用 Kubernetes 的时候，就需要把集群给适当地 “局部化”，为每一类用户创建出只属于它自己的 “工作空间”。\n2.18.2 使用名字空间 # 名字空间也是一种 API 对象，使用命令 kubectl api-resources 可以看到它的简称是 “ns”，命令 kubectl create 不需要额外的参数，可以很容易地创建一个名字空间，比如：\nkubectl create ns test-ns kubectl get ns Kubernetes 初始化集群的时会预设 4 个名字空间：default、kube-system、kube-public、kube-node-lease。default 是用户对象默认的名字空间，kube-system 是系统组件所在的名字空间。\n想要把一个对象放入特定的名字空间，需要在它的 metadata 里添加一个 namespace 字段，比如要在 “test-ns” 名字空间里创建一个简单的 Nginx Pod，就要这样写：\napiVersion: v1 kind: Pod metadata: name: ngx namespace: test-ns spec: containers: - image: nginx:alpine name: ngx apply 创建对象之后，使用 kubectl get 是看不到它的，因为默认查看的是 “default” 名字空间，想要操作其他名字空间的对象必须要用 -n 参数明确指定：\n名字空间里的对象都从属于名字空间，所以在删除名字空间的时候一定要小心，一旦名字空间被删除，它里面的所有对象也都会消失。可以执行一下 kubectl delete，尝试删除刚才创建的名字空间 “test-ns”：\n会发现删除名字空间后，它里面的 Pod 也删除了。\n2.18.3 资源配额 # 有了名字空间后，可以像管理容器一样，给名字空间设定配额，把整个集群的计算资源分割成不同的大小，按需分配给团队或项目使用。不过集群和单机不一样，除了限制最基本的 CPU 和内存，还必须限制各种对象的数量，否则对象之间也会互相挤占资源。\n名字空间的资源配额需要使用一个专门的 API 对象 ResourceQuota，简称是 quota，可以使用命令 kubectl create 创建一个它的样板文件：\nexport out=\u0026#34;--dry-run=client -o yaml\u0026#34; kubectl create quota dev-qt $out 因为资源配额对象必须依附在某个名字空间上，所以在它的 metadata 字段里必须明确写出 namespace，否则就会应用到 default 名字空间。\nResourceQuota 对象的使用方式比较灵活，既可以限制整个名字空间的配额，也可以只限制某些类型的对象（使用 scopeSelector），比如限制整个名字空间的配额，需要在 spec 里使用 hard 字段，意思就是 “硬性全局限制”。在 ResourceQuota 里可以设置各类资源配额，字段非常多：\nCPU 和内存配额，使用 request.、limits.，这是和容器资源限制是一样的。\n存储容量配额，使 requests.storage 限制的是 PVC 的存储总量，也可以用 persistentvolumeclaims 限制 PVC 的个数。\n核心对象配额，使用对象的名字（英语复数形式），比如 pods、configmaps、secrets、services。\n其他 API 对象配额，使用 count/name.group 的形式，比如 count/jobs.batch、count/deployments.apps。\n2.18.4 使用资源配额 # 以下的 YAML 描述是创建一个名字空间 “dev-ns”，再创建一个资源配额对象 “dev-qt”：\napiVersion: v1 kind: Namespace metadata: name: dev-ns --- apiVersion: v1 kind: ResourceQuota metadata: name: dev-qt namespace: dev-ns spec: hard: requests.cpu: 10 requests.memory: 10Gi limits.cpu: 10 limits.memory: 20Gi requests.storage: 100Gi persistentvolumeclaims: 100 pods: 100 configmaps: 100 secrets: 100 services: 10 count/jobs.batch: 1 count/cronjobs.batch: 1 count/deployments.apps: 1 所有 Pod 的需求总量最多是 10 个 CPU 和 10GB 的内存，上限总量是 10 个 CPU 和 20GB 的内存。\n只能创建 100 个 PVC 对象，使用 100GB 的持久化存储空间。\n只能创建 100 个 Pod，100 个 ConfigMap，100 个 Secret，10 个 Service。\n只能创建 1 个 Job，1 个 CronJob，1 个 Deployment。\napply 创建资源配额对象后，用 kubectl get 加上 -n 指定名字空间查看：\nkubectl apply -f quota-ns.yml kubectl get quota -n dev-ns 可以用命令 kubectl describe 查看对象，会有一个清晰的表格：\nkubectl describe quota -n dev-ns 可以尝试在这个名字空间里运行两个 busybox Job，要加上 -n 参数：\nkubectl create job echo1 -n dev-ns --image=busybox -- echo hello kubectl create job echo2 -n dev-ns --image=busybox -- echo hello ResourceQuota 限制了名字空间里最多只能有一个 Job，所以在创建第二个 Job 对象时会失败，提示超出了资源配额。\n使用命令 kubectl describe 查看，会发现 Job 资源已经到达了上限：\n2.18.5 默认资源配额 # 在名字空间加上了资源配额限制之后，会有一个合理但比较烦人的约束：要求所有在里面运行的 Pod 都必须用字段 resources 声明资源需求，否则就无法创建。\n比如，想用命令 kubectl run 创建一个 Pod：\nkubectl run ngx --image=nginx:alpine -n dev-ns 给出了一个 Forbidden 的错误提示，说不满足配额要求。\n如果 Pod 里没有 resources 字段，就可以无限制地使用 CPU 和内存，这显然与名字空间的资源配额相冲突。为了保证名字空间的资源总量可管可控，Kubernetes 就只能拒绝创建这样的 Pod。\n如果想让 Kubernetes 自动为 Pod 加上资源限制，给个默认值。这里就需要用到一个很小但很有用的辅助对象 —— LimitRange，简称是 limits，它可以为 API 对象添加默认的资源配额限制。\n可以用命令 kubectl explain limits 来查看它的 YAML 字段详细说明：\nspec.limits 是它的核心属性，描述了默认的资源限制。\ntype 是要限制的对象类型，可以是 Container、Pod、PersistentVolumeClaim。\ndefault 是默认的资源上限，对应容器里的 resources.limits，只适用于 Container。\ndefaultRequest 默认申请的资源，对应容器里的 resources.requests，同样也只适用于 Container。\nmax、min 是对象能使用的资源的最大最小值。\n以下是一个 LimitRange YAML 描述：\napiVersion: v1 kind: LimitRange metadata: name: dev-limits namespace: dev-ns spec: limits: - type: Container defaultRequest: cpu: 200m memory: 50Mi default: cpu: 500m memory: 100Mi - type: Pod max: cpu: 800m memory: 200Mi 设置了每个容器默认申请 0.2 的 CPU 和 50MB 内存，容器的资源上限是 0.5 的 CPU 和 100MB 内存，每个 Pod 的最大使用量是 0.8 的 CPU 和 200MB 内存。\n使用 kubectl apply 创建 LimitRange 之后，再用 kubectl describe 可以看到它的状态：\n有了这个默认的资源配额作为 “保底”，就可以不用编写 resources 字段直接创建 Pod：\n使用 kubectl describe pod -n dev-ns 查看状态可知，LimitRange 为它自动加上的资源配额：\n不是所有的 API 对象都可以划分进名字空间管理，比如 Node、PV 等这样的全局资源就不属于任何名字空间。\n"},{"id":27,"href":"/kubernetes/docs/part2-break-ice/2.19-system-monitor/","title":"2.19 系统监控","section":"第二部分 入门","content":" 2.19 系统监控 # 2.19.1 Metrics Server # Linux top 命令能够实时显示当前系统的 CPU 和内存利用率，是性能分析和调优的工具。Kubernetes 也提供了类似的命令，就是 kubectl top，不过默认情况下这个命令不会生效，必须要安装插件 Metrics Server 才可以。\nMetrics Server 是一个专门用来收集 Kubernetes 核心资源指标（metrics）的工具，它定时从所有节点的 kubelet 里采集信息，但是对集群的整体性能影响极小，每个节点只大约会占用 1m 的 CPU 和 2MB 的内存，性价比非常高。项目网址在 https://github.com/kubernetes-sigs/metrics-server。\nMetrics Server 调用 kubelet 的 API 拿到节点和 Pod 的指标，再把这些信息交给 apiserver，这样 kubectl、HPA 就可以利用 apiserver 来读取指标了。\nMetrics Server 的所有依赖都放在了一个 YAML 描述文件里，你可以使用 wget 或者 curl 下载：\nwget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml 下载下来的 YAML 描述文件不能直接使用，需要修改下。\n需要在 Metrics Server 的 Deployment 对象里，加上一个额外的运行参数 --kubelet-insecure-tls，也就是这样： apiVersion: apps/v1 kind: Deployment metadata: name: metrics-server namespace: kube-system spec: ... ... template: spec: containers: - args: - --kubelet-insecure-tls ... ... Metrics Server 默认使用 TLS 协议，要验证证书才能与 kubelet 实现安全通信，加上这个参数可以让部署工作简单很多（生产环境里就要慎用）。\nMetrics Server 的镜像仓库用的是 gcr.io，在国内下载很困难。可以通过科学上网的方式，下载后把镜像加载到集群里的节点上。如果已经可以科学上网可以忽略这项。 完整的 components.yaml 如下：\napiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: metrics-server name: metrics-server namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: k8s-app: metrics-server rbac.authorization.k8s.io/aggregate-to-admin: \u0026#34;true\u0026#34; rbac.authorization.k8s.io/aggregate-to-edit: \u0026#34;true\u0026#34; rbac.authorization.k8s.io/aggregate-to-view: \u0026#34;true\u0026#34; name: system:aggregated-metrics-reader rules: - apiGroups: - metrics.k8s.io resources: - pods - nodes verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: k8s-app: metrics-server name: system:metrics-server rules: - apiGroups: - \u0026#34;\u0026#34; resources: - nodes/metrics verbs: - get - apiGroups: - \u0026#34;\u0026#34; resources: - pods - nodes verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: k8s-app: metrics-server name: metrics-server-auth-reader namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: extension-apiserver-authentication-reader subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: k8s-app: metrics-server name: metrics-server:system:auth-delegator roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: k8s-app: metrics-server name: system:metrics-server roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:metrics-server subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: v1 kind: Service metadata: labels: k8s-app: metrics-server name: metrics-server namespace: kube-system spec: ports: - name: https port: 443 protocol: TCP targetPort: https selector: k8s-app: metrics-server --- apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: metrics-server name: metrics-server namespace: kube-system spec: selector: matchLabels: k8s-app: metrics-server strategy: rollingUpdate: maxUnavailable: 0 template: metadata: labels: k8s-app: metrics-server spec: containers: - args: - --kubelet-insecure-tls - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --metric-resolution=15s image: registry.k8s.io/metrics-server/metrics-server:v0.6.3 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 httpGet: path: /livez port: https scheme: HTTPS periodSeconds: 10 name: metrics-server ports: - containerPort: 4443 name: https protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /readyz port: https scheme: HTTPS initialDelaySeconds: 20 periodSeconds: 10 resources: requests: cpu: 100m memory: 200Mi securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 1000 volumeMounts: - mountPath: /tmp name: tmp-dir nodeSelector: kubernetes.io/os: linux priorityClassName: system-cluster-critical serviceAccountName: metrics-server volumes: - emptyDir: { } name: tmp-dir --- apiVersion: apiregistration.k8s.io/v1 kind: APIService metadata: labels: k8s-app: metrics-server name: v1beta1.metrics.k8s.io spec: group: metrics.k8s.io groupPriorityMinimum: 100 insecureSkipTLSVerify: true service: name: metrics-server namespace: kube-system version: v1beta1 versionPriority: 100 准备工作完成后，就可以使用 YAML 部署 Metrics Server 了：\nMetrics Server 属于名字空间 “kube-system”，可以用 kubectl get pod 加上 -n 参数查看是否正常运行：\nkubectl get pod -n kube-system 有了 Metrics Server 插件，就可以使用命令 kubectl top 来查看 Kubernetes 集群当前的资源状态了。它有两个子命令，node 查看节点的资源使用率，pod 查看 Pod 的资源使用率。\nkubectl top node kubectl top pod -n kube-system 2.19.2 HorizontalPodAutoscaler # Metrics Server 可以轻松地查看集群的资源使用状况，但是它另外一个更重要的功能是辅助实现应用的 “水平自动伸缩”。\nkubectl scale 命令可以任意增减 Deployment 部署的 Pod 数量，也就是水平方向的 “扩容” 和 “缩容”。但是手动调整应用实例数量比较麻烦，需要人工参与，也很难准确把握时机，难以及时应对生产环境中突发的大流量，最好能把 “扩容” “缩容” 变成自动化的操作，这在 Kubernetes 里就是 API “HorizontalPodAutoscaler” 的能力，简称是 “hpa”。\nHorizontalPodAutoscaler 的能力完全基于 Metrics Server，它从 Metrics Server 获取当前应用的运行指标，主要是 CPU 使用率，再依据预定的策略增加或者减少 Pod 的数量。\n使用 HorizontalPodAutoscaler，首先要定义 Deployment 和 Service，创建一个 Nginx 应用，作为自动伸缩的目标对象：\napiVersion: apps/v1 kind: Deployment metadata: name: ngx-hpa-dep spec: replicas: 1 selector: matchLabels: app: ngx-hpa-dep template: metadata: labels: app: ngx-hpa-dep spec: containers: - image: nginx:alpine name: nginx ports: - containerPort: 80 resources: requests: cpu: 50m memory: 10Mi limits: cpu: 100m memory: 20Mi --- apiVersion: v1 kind: Service metadata: name: ngx-hpa-svc spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: ngx-hpa-dep 在这个 YAML 里只部署了一个 Nginx 实例，名字是 ngx-hpa-dep。在它的 spec 里一定要用 resources 字段写清楚资源配额，否则 HorizontalPodAutoscaler 会无法获取 Pod 的指标，就无法实现自动化扩缩容。\n可以使用 kubectl autoscale 命令创建一个 HorizontalPodAutoscaler 的样板 YAML 文件，它有三个参数：\nmin，Pod 数量的最小值，也就是缩容的下限。\nmax，Pod 数量的最大值，也就是扩容的上限。\ncpu-percent，CPU 使用率指标，当大于这个值时扩容，小于这个值时缩容。\n为刚才的 Nginx 应用创建 HorizontalPodAutoscaler，指定 Pod 数量最少 2 个，最多 10 个，CPU 使用率指标设置的小一点，5%，方便观察扩容现象：\nexport out=\u0026#34;--dry-run=client -o yaml\u0026#34; # 定义Shell变量 kubectl autoscale deploy ngx-hpa-dep --min=2 --max=10 --cpu-percent=5 $out 得到的 YAML 描述文件就是如下：\napiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: ngx-hpa-dep spec: maxReplicas: 10 minReplicas: 2 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: ngx-hpa-dep targetCPUUtilizationPercentage: 5 apply 这个 YAML 描述，创建 HorizontalPodAutoscaler 后，它会发现 Deployment 里的实例只有 1 个，不符合 min 定义的下限的要求，就先扩容到 2 个：\n可以给 Nginx 加上压力流量，运行一个测试 Pod，使用的镜像是 “httpd:alpine”，它里面有 HTTP 性能测试工具 ab（Apache Bench）：\nkubectl run test -it --image=httpd:alpine -- sh 然后向 Nginx 发送一百万个请求，持续 1 分钟，再用 kubectl get hpa 来观察 HorizontalPodAutoscaler 的运行状况：\nab -c 10 -t 60 -n 1000000 \u0026#39;http://ngx-hpa-svc/\u0026#39; 因为 Metrics Server 大约每 15 秒采集一次数据，所以 HorizontalPodAutoscaler 的自动化扩容和缩容也是按照这个时间点来逐步处理的。\n当它发现目标的 CPU 使用率超过了预定的 5% 后，就会以 2 的倍数开始扩容，一直到数量上限，然后持续监控一段时间，如果 CPU 使用率回落，就会再缩容到最小值。\n2.19.3 Prometheus # Metrics Server 能够获取的指标太少，只有 CPU 和内存，而想要监控到更多更全面的应用运行状况，需要用到权威项目 Prometheus，它是云原生监控领域的 “事实标准”。\nPrometheus 系统的核心是 Server，里面有一个时序数据库 TSDB，用来存储监控数据，另一个组件 Retrieval 使用拉取（Pull）的方式从各个目标收集数据，再通过 HTTP Server 把这些数据交给外界使用。\n在 Prometheus Server 之外还有三个重要的组件：\nPush Gateway，用来适配一些特殊的监控目标，把默认的 Pull 模式转变为 Push 模式。\nAlert Manager，告警中心，预先设定规则，发现问题时就通过邮件等方式告警。\nGrafana 是图形化界面，可以定制大量直观的监控仪表盘。\n由于 prometheus 包含的组件太多，部署起来麻烦，这里可以使用 “kube-prometheus” 项目 https://github.com/prometheus-operator/kube-prometheus/，操作起来相对容易。\n先下载 kube-prometheus 的源码包，这里使用的版本是 0.11：\nwget https://github.com/prometheus-operator/kube-prometheus/archive/refs/tags/v0.11.0.tar.gz 通过 tar -xf 解压缩后，Prometheus 部署相关的 YAML 文件都在 manifests 目录里。\n在安装 Prometheus 之前，需要做一些准备工作。\n修改 prometheus-service.yaml、grafana-service.yaml。这两个文件定义了 Prometheus 和 Grafana 服务对象，可以给它们添加 type: NodePort，这样就可以直接通过节点的 IP 地址访问（当然也可以配置成 Ingress）。 prometheus-service.yaml：\napiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/component: prometheus app.kubernetes.io/instance: k8s app.kubernetes.io/name: prometheus app.kubernetes.io/part-of: kube-prometheus app.kubernetes.io/version: 2.36.1 name: prometheus-k8s namespace: monitoring spec: type: NodePort ports: - name: web port: 9090 targetPort: web - name: reloader-web port: 8080 targetPort: reloader-web selector: app.kubernetes.io/component: prometheus app.kubernetes.io/instance: k8s app.kubernetes.io/name: prometheus app.kubernetes.io/part-of: kube-prometheus sessionAffinity: ClientIP grafana-service.yaml：\napiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/component: grafana app.kubernetes.io/name: grafana app.kubernetes.io/part-of: kube-prometheus app.kubernetes.io/version: 8.5.5 name: grafana namespace: monitoring spec: type: NodePort ports: - name: http port: 3000 targetPort: http selector: app.kubernetes.io/component: grafana app.kubernetes.io/name: grafana app.kubernetes.io/part-of: kube-prometheus 解决 kubeStateMetrics-deployment.yaml、prometheusAdapter-deployment.yaml 的镜像下载问题，因为它们里面有两个存放在 gcr.io 的镜像。通过可以科学上网方式解决，或者把镜像下载到本地，通过 docker save，docker load 方式把镜像上传到节点机器上。 将 prometheusAdapter-deployment.yaml 中的镜像从 k8s.gcr.io/prometheus-adapter/prometheus-adapter:v0.9.1 改成 willdockerhub/prometheus-adapter:v0.9.1\nimage: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.5.0 image: willdockerhub/prometheus-adapter:v0.9.1 在准备工作完成后，可以执行两个 kubectl create 命令来部署 Prometheus，先是 manifests/setup 目录，创建名字空间等基本对象，然后是 manifests 目录：\nkubectl create -f manifests/setup kubectl create -f manifests Prometheus 的对象都在名字空间 “monitoring” 里，创建之后可以用 kubectl get 来查看状态：\nkubectl get pod -n monitoring 确定 Pod 都运行正常，再看看它对外的服务端口：\nkubectl get svc -n monitoring 由于修改了 Grafana 和 Prometheus 的 Service 对象，所以这两个服务就在节点上开了端口，由上截图可知，Grafana 是 32274，Prometheus 有两个端口，其中 9090 对应的 32561 是 Web 端口。\n在浏览器里输入节点的 IP 地址（任何一个节点的 IP 都可以），再加上端口号 32561，就能看到 Prometheus 自带的 Web 界面：\nWeb 界面上有一个查询框，可以使用 PromQL 来查询指标，生成可视化图表，以上截图选择了 “node_memory_Active_bytes” 这个指标，表示当前正在使用的内存容量。Prometheus 的 Web 界面比较简单，通常只用来调试、测试，不适合实际监控。\nGrafana 访问节点的端口 32274，会要求先登录，默认的用户名和密码都是 admin：\nGrafana 内部预置了很多强大易用的仪表盘，可以在左侧菜单栏的 “Dashboards - Browse” 里任意挑选一个：\n比如选择了 “Kubernetes / Compute Resources / Namespace (Pods)” 这个仪表盘，就会出来一个非常漂亮图表，比 Metrics Server 的 kubectl top 命令要好看得多，各种数据一目了然：\n问题解决 # 我在部署时，执行 kubectl get pod -n monitoring 发现 alertmanager-main Pod 和 prometheus-k8s Pod 没有起来：\n执行 describe 命令 kubectl describe pod prometheus-k8s-1 -n monitoring 发现有如下错误：\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 33m default-scheduler 0/2 nodes are available: 1 Insufficient memory, 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn\u0026#39;t tolerate. Warning FailedScheduling 27m (x4 over 32m) default-scheduler 0/2 nodes are available: 1 Insufficient memory, 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn\u0026#39;t tolerate. 这个错误大概率是由于集群中机器不够导致的，可以通过去除掉 master 上的 tolerate 污点，让 master 节点也参与调度：\n去除 master 节点的污点后，Kubernetes 会自动调度，再次执行 kubectl get pod -n monitoring 命令会发现 Pod 已经都运行正常了：\n2.19.4 小结 # Metrics Server 是一个 Kubernetes 插件，能够收集系统的核心资源指标，相关的命令是 kubectl top。\nPrometheus 是云原生监控领域的 “事实标准”，用 PromQL 语言来查询数据，配合 Grafana 可以展示直观的图形界面，方便监控。\nHorizontalPodAutoscaler 实现了应用的自动水平伸缩功能，它从 Metrics Server 获取应用的运行指标，再实时调整 Pod 数量，可以很好地应对突发流量。\n参考 # https://prometheus.io/ "},{"id":28,"href":"/kubernetes/docs/part2-break-ice/2.20-network-communications/","title":"2.20 网络通信","section":"第二部分 入门","content":" 2.20 网络通信 # 2.20.1 网络模型 # Docker 有三种常见的网络模式： null、host 和 bridge。下图，描述了 Docker 里最常用的 bridge 网络模式：\nDocker 会创建一个名字叫 “docker0” 的网桥，默认是私有网段 “172.17.0.0/16”。每个容器都会创建一个虚拟网卡对（veth pair），两个虚拟网卡分别 “插” 在容器和网桥上，这样容器之间就可以互联互通了。Docker 的网络方案简单有效，但只局限在单机环境里工作，跨主机通信非常困难（需要做端口映射和网络地址转换）。\nKubernetes 的网络模型 “IP-per-pod”，能够很好地适应集群系统的网络需求，它有下面的这 4 点基本假设：\n集群里的每个 Pod 都会有唯一的一个 IP 地址。\nPod 里的所有容器共享这个 IP 地址。\n集群里的所有 Pod 都属于同一个网段。\nPod 直接可以基于 IP 地址直接访问另一个 Pod，不需要做麻烦的网络地址转换（NAT）。\n这种网络让 Pod 摆脱了主机的硬限制，是一个 “平坦” 的网络模型，通信也非常简单。因为 Pod 都具有独立的 IP 地址，相当于一台虚拟机，而且直连互通，也就可以很容易地实施域名解析、负载均衡、服务发现等工作，对应用的管理和迁移都非常友好。\n2.20.2 什么是 CNI # CNI（Container Networking Interface）为网络插件定义了一系列通用接口，开发者只要遵循这个规范就可以接入 Kubernetes，为 Pod 创建虚拟网卡、分配 IP 地址、设置路由规则，最后就能够实现 “IP-per-pod” 网络模型。依据实现技术的不同，CNI 插件可以大致上分成 “Overlay” “Route” 和 “Underlay” 三种。\nOverlay 是指它构建了一个工作在真实底层网络之上的 “逻辑网络”，把原始的 Pod 网络数据封包，再通过下层网络发送出去，到了目的地再拆包。因为这个特点，它对底层网络的要求低，适应性强，缺点就是有额外的传输成本，性能较低。\nRoute 也是在底层网络之上工作，但它没有封包和拆包，而是使用系统内置的路由功能来实现 Pod 跨主机通信。它的好处是性能高，不过对底层网络的依赖性比较强，如果底层不支持就没办法工作了。\nUnderlay 就是直接用底层网络来实现 CNI，也就是说 Pod 和宿主机都在一个网络里，Pod 和宿主机是平等的。它对底层的硬件和网络的依赖性是最强的，因而不够灵活，但性能最高。\n常见插件 # Flannel https://github.com/flannel-io/flannel/ 最早是一种 Overlay 模式的网络插件，使用 UDP 和 VXLAN 技术，后来又用 Host-Gateway 技术支持了 Route 模式。Flannel 简单易用，是 Kubernetes 里最流行的 CNI 插件，但它在性能方面表现不太好，一般不建议在生产环境中使用。\nCalico https://github.com/projectcalico/calico 是一种 Route 模式的网络插件，使用 BGP 协议（Border Gateway Protocol）来维护路由信息，性能比 Flannel 好，而且支持多种网络策略，具备数据加密、安全隔离、流量整形等功能。\nCilium https://github.com/cilium/cilium 同时支持 Overlay 模式和 Route 模式，特点是深度使用了 Linux eBPF 技术，在内核层次操作网络数据，性能很高，可以灵活实现各种功能，是非常有前途的 CNI 插件。\n2.20.3 CNI 插件工作方式 # 以 Flannel 为例，看看 CNI 在 Kubernetes 里的工作方式。\n先用 Deployment 创建 3 个 Nginx Pod，作为研究对象：\nkubectl create deploy ngx-dep --image=nginx:alpine --replicas=3 使用命令 kubectl get pod 可以看到，有两个 Pod 运行在 master 节点上，另一个 Pod 运行在 worker 节点上。\nFlannel 默认使用的是基于 VXLAN 的 Overlay 模式，整个集群的网络结构如下：\n在 Pod 里执行命令 ip addr 就可以看到它里面的虚拟网卡 “eth0”：\n第一个数字 “3” 是序号，意思是第 3 号设备，“@if13” 就是它另一端连接的虚拟网卡，序号是 13。\n这个 Pod 的宿主机是 master，可以登录到 master 节点，看看这个节点上的网络情况，同样还是用命令 ip addr：\n这里就可以看到 master 节点上的第 13 号设备了，它的名字是 vethae5665c5@if3，“veth” 表示它是一个虚拟网卡，而后面的 “@if3” 就是 Pod 里对应的 3 号设备，也就是 “eth0” 网卡了。\n“cni0” 网桥的信息可以在 master 节点上使用命令 brctl show：\n如果没有 brctl 命令可以通过 apt install bridge-utils 安装\n可以看到 “cni0” 网桥上有一个 “vethae5665c5” 网卡，所以这个网卡就被 “插” 在了 “cni0” 网桥上，因为虚拟网卡的 “结对” 特性，Pod 也就连上了 “cni0” 网桥。\n关于跨主机网络，关键是节点的路由表，可以用命令 route 查看：\n10.10.0.0/24 网段的数据，都要走 cni0 设备，也就是 “cni0” 网桥。\n10.10.1.0/24 网段的数据，都要走 flannel.1 设备，也就是 Flannel。\n192.168.14.0/24 网段的数据，都要走 eth0 设备，也就是宿主机的网卡。\n假设要从 master 节点的 10.10.0.3 访问 worker 节点的 10.10.1.77，因为 master 节点的 “cni0” 网桥管理的只是 10.10.0.0/24 这个网段，所以按照路由表，凡是 10.10.1.0/24 都要让 flannel.1 来处理，这样就进入了 Flannel 插件的工作流程。\nFlannel 将要决定如何把数据发到另一个节点，在各种表里去查询。如下图，用到的命令有 ip neighbor、bridge fdb 等：\nFlannel 得到的结果就是要把数据发到 192.168.14.143 worker 节点，它会在原始网络包前面加上这些额外的信息，封装成 VXLAN 报文，用 eth0 网卡发出去，worker 节点收到后再拆包，执行类似的反向处理，就可以把数据交给真正的目标 Pod 了。\n2.20.4 Calico 网络插件 # 可以在 Calico 的网站 https://www.tigera.io/project-calico/ 上找到它的安装方式，这里选择 “本地自助安装（Self-managed on-premises）”，直接下载 YAML 文件：\nwget https://projectcalico.docs.tigera.io/archive/v3.23/manifests/calico.yaml 由于 Calico 使用的镜像比较大，为了加快安装速度，可以考虑在每个节点上预先使用 docker pull 拉取镜像：\ndocker pull calico/cni:v3.23.5 docker pull calico/node:v3.23.5 docker pull calico/kube-controllers:v3.23.5 Calico 的安装只需要用 kubectl apply calico.yaml 就可以。如果有安装 Flannel 插件，最好先把 Flannel 删除：\nkubectl apply -f calico.yaml 查看一下 Calico 的运行状态，它也是在 “kube-system” 名字空间：\nTODO ... "},{"id":29,"href":"/kubernetes/docs/","title":"Docs","section":"简介","content":""}]