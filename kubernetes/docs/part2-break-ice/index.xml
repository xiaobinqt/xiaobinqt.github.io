<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>第二部分 入门 on Kubernetes 学习笔记</title>
    <link>https://example.com/kubernetes/docs/part2-break-ice/</link>
    <description>Recent content in 第二部分 入门 on Kubernetes 学习笔记</description>
    <generator>Hugo</generator>
    <language>zh</language>
    <atom:link href="https://example.com/kubernetes/docs/part2-break-ice/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>2.1 简介</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.1-k8s-overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.1-k8s-overview/</guid>
      <description>2.1 k8s 简介 # 2.1.1 容器编排 # 容器技术其实只是解决了运维部署工作中一个很小的问题，在现实中的生产环境复杂程度特别高，除了最基本的安装，还会有各式各样的需求，比如服务发现、负载均衡、状态监控、健康检查、扩容缩容、应用迁移、高可用等等。&#xA;这些问题已经不再是隔离一两个进程的普通问题，而是要隔离数不清的进程，还有它们之间互相通信、互相协作的超级问题，困难程度可以说是指数级别的上升。这些容器之上的管理、调度工作，就是 “容器编排”（Container Orchestration）。&#xA;2.1.2 什么是 k8s # 简单来说，Kubernetes 就是一个生产级别的容器编排平台和集群管理系统，不仅能够创建、调度容器，还能够监控、管理服务器，它凝聚了 Google 等大公司和开源社区的集体智慧，从而让中小型公司也可以具备轻松运维海量计算节点 —— 也就是 “云计算” 的能力。&#xA;k8s 脱胎与 Google 内部代号为 Borg 的集群应用管理系统。在 2015 年，Google 又联合 Linux 基金会成立了 CNCF（Cloud Native Computing Foundation，云原生基金会），并把 Kubernetes 捐献出来作为种子项目。&#xA;有了 Google 和 Linux 两大家族的保驾护航，再加上宽容开放的社区，作为 CNCF 的 “头把交椅”，Kubernetes 旗下很快就汇集了众多行业精英，仅用几年的时间就打败了同期的竞争对手 Apache Mesos 和 Docker Swarm，成为了容器编排和集群管理这个领域的唯一霸主。</description>
    </item>
    <item>
      <title>2.2 minikube</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.2-minikube/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.2-minikube/</guid>
      <description>2.2 minikube # minikube 是一个 “迷你” 版本的 Kubernetes，自从 2016 年发布以来一直在积极地开发维护，紧跟 Kubernetes 的版本更新，同时也兼容较旧的版本（最多可以到之前的 6 个小版本）。&#xA;minikube 最大特点就是 “小而美”，可执行文件仅有不到 100MB，运行镜像也不过 1GB。minikube 集成了 Kubernetes 的绝大多数功能特性，不仅有核心的容器编排功能，还有丰富的插件，例如 Dashboard、GPU、Ingress、Istio、Kong、Registry 等。&#xA;2.2.1 安装 Docker # 我的系统是 debian 11，本文所有的操作都是在 debian 11 的环境下进行。&#xA;Docker 的安装可以参考官网 Install Docker Engine on Debian，其他系统的安装方式都可以在官网找到。debian 11 的安装步骤大致如下。&#xA;apt-get remove docker docker-engine docker.io containerd runc # 卸载旧版本 apt-get update apt-get install \ ca-certificates \ curl \ gnupg \ lsb-release mkdir -p /etc/apt/keyrings curl -fsSL https://download.</description>
    </item>
    <item>
      <title>2.3 工作机制</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.3-working-mechanism/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.3-working-mechanism/</guid>
      <description>2.3 工作机制 # 2.3.1 基本架构 # Kubernetes 采用了 “控制面 / 数据面”（Control Plane / Data Plane）架构，集群里的计算机被称为 “节点”（Node），可以是物理机也可以是虚拟机，少量的节点用作控制面来执行集群的管理维护工作，其他的大部分节点都被划归数据面，用来跑业务应用。&#xA;控制面的节点在 Kubernetes 里叫做 Master Node，一般简称为 Master，它是整个集群里最重要的部分，可以说是 Kubernetes 的大脑和心脏。数据面的节点叫做 Worker Node，一般就简称为 Worker 或者 Node，相当于 Kubernetes 的手和脚，在 Master 的指挥下干活。Node 的数量非常多，构成了一个资源池，Kubernetes 就在这个池里分配资源，调度应用。因为资源被 “池化”了，所以管理也就变得比较简单，可以在集群中任意添加或者删除节点。&#xA;Master 和 Node 的划分不是绝对的。当集群的规模较小，工作负载较少的时候，Master 也可以承担 Node 的工作，就像 minikube 环境，它就只有一个节点，这个节点既是 Master 又是 Node。&#xA;在下面这张架构图中，可以看到有一个 kubectl，它是 Kubernetes 的客户端工具，用来操作 Kubernetes，但它位于集群之外，理论上不属于集群。&#xA;2.3.2 节点内部结构 # Kubernetes 的节点内部具有非常复杂的结构，由很多的模块构成的，这些模块又可以分成组件（Component）和插件（Addon）两类。&#xA;组件实现了 Kubernetes 的核心功能特性，没有这些组件 Kubernetes 就无法启动，而插件则是 Kubernetes 的一些附加功能，属于 “锦上添花”，不安装也不会影响 Kubernetes 的正常运行。&#xA;组件 # Master 有 4 个组件，分别是 apiserver、etcd、scheduler、controller-manager。</description>
    </item>
    <item>
      <title>2.4 Pod</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.4-pod/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.4-pod/</guid>
      <description>2.4 Pod # 2.4.1 什么是 Pod # 当容器进入到现实的生产环境中时，容器的隔离性就带来了一些麻烦。因为很少有应用是完全独立运行的，经常需要几个进程互相协作才能完成任务。比如可能有多个应用结合得非常紧密以至于无法把它们拆开，但是将它们都放在同一个容器中又不是一种好的做法，因为容器的理念是对应用的独立封装，它里面就应该是一个进程、一个应用，如果里面有多个应用，不仅违背了容器的初衷，也会让容器更难以管理。&#xA;为了解决多应用联合运行的问题，同时还要不破坏容器的隔离，就需要在容器外面再建立一个 “收纳舱”，让多个容器既保持相对独立，又能够小范围共享网络、存储等资源，而且永远是 “绑在一起” 的状态。这就是 Pod 的初衷，实际上，“spec.containers” 字段其实是一个数组，里面允许定义多个容器。&#xA;Pod 是对容器的 “打包”，里面的容器是一个整体，总是能够一起调度、一起运行，绝不会出现分离的情况。Pod 属于 Kubernetes，可以在不触碰下层容器的情况下任意定制修改。Kubernetes 让 Pod 去编排处理容器，然后把 Pod 作为应用调度部署的最小单位，Pod 也因此成为了 Kubernetes 世界里的 “原子”，基于 Pod 就可以构建出更多更复杂的业务形态了。&#xA;2.4.2 YAML 描述 Pod # 可以理解为所有的 API 对象都天然具有 apiVersion、kind、metadata、spec 这四个基本组成部分，当然也包括 Pod。&#xA;在使用 Docker 创建容器的时候，可以不给容器起名字，但在 Kubernetes 里，Pod 必须要有一个名字，这也是 Kubernetes 里所有资源对象的一个约定。通常会为 Pod 名字统一加上 pod 后缀，这样可以和其他类型的资源区分开。&#xA;name 只是一个基本的标识，信息有限，所以 labels 字段就很有用，它可以添加任意数量的 Key-Value，给 Pod “贴” 上归类的标签，结合 name 就更方便识别和管理。比如：&#xA;apiVersion: v1 kind: Pod metadata: name: busy-pod labels: owner: xiaobinqt env: demo region: north tier: back “metadata” 一般写上 name 和 labels 就足够了，但是 “spec” 字段由于需要管理、维护 Pod 这个基本调度单元，里面有非常多的关键信息，厂常见的有 containers、hostname、restartPolicy 等字段。</description>
    </item>
    <item>
      <title>2.5 Job</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.5-job/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.5-job/</guid>
      <description>2.5 Job # 2.5.1 业务分类 # Kubernetes 里的有两大业务类型。一类是像 Nginx、MySQL 这样长时间运行的 “在线业务”，一旦运行起来基本上不会停，也就是永远在线。另一类是像日志分析这样短时间运行的 “离线业务”，“离线业务” 的特点是必定会退出，不会无期限地运行下去。&#xA;“离线业务” 可以分为两种。一种是 “临时任务”，跑完就完事了，下次有需求再重新安排；另一种是 “定时任务”，可以按时按点周期运行，不需要过多干预。在 Kubernetes 里，“临时任务” 是 API 对象 Job，“定时任务” 是 API 对象 CronJob，使用这两个对象就能够在 Kubernetes 里调度管理任意的离线业务。&#xA;2.5.2 Job # 比如用 busybox 创建一个 “echo-job”，命令就是这样的：&#xA;export out=&amp;#34;--dry-run=client -o yaml&amp;#34; # 定义Shell变量 kubectl create job echo-job --image=busybox $out 会生成一个基本的 YAML 文件，保存之后做点修改，就有了一个 Job 对象：&#xA;apiVersion: batch/v1 kind: Job metadata: name: echo-job spec: template: spec: restartPolicy: OnFailure containers: - image: busybox name: echo-job imagePullPolicy: IfNotPresent command: [ &amp;#34;/bin/echo&amp;#34; ] args: [ &amp;#34;hello&amp;#34;, &amp;#34;world&amp;#34; ] Job 的描述与 Pod 很像，但又有些不一样，主要的区别在 “spec” 字段里多了一个 template 字段，然后又是一个 “spec”，显得很奇怪。这主要是在 Job 对象里应用了组合模式，template 字段定义了一个 “应用模板”，里面嵌入了一个 Pod，这样 Job 就可以从这个模板来创建出 Pod。而这个 Pod 因为受 Job 的管理控制，不直接和 apiserver 打交道，也就没必要重复 apiVersion 等 “头字段”，只需要定义好关键的 spec，描述清楚容器相关的信息就可以了，可以说是一个 “无头” 的 Pod 对象。</description>
    </item>
    <item>
      <title>2.6 配置管理</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.6-config-manage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.6-config-manage/</guid>
      <description>2.6 配置管理 # 服务中的配置信息，从数据安全的角度来看可以分成两类：一类是明文配置，可以任意查询修改，比如服务端口、运行参数、文件路径等等。另一类则是机密配置，由于涉及敏感信息需要保密，不能随便查看，比如密码、密钥、证书等等。这两类配置信息本质上都是字符串，只是由于安全性的原因，在存放和使用方面有些差异。&#xA;Kubernetes 中的 ConfigMap API 用来保存明文配置，Secret API 用来保存秘密配置。&#xA;2.6.1 ConfigMap # export out=&amp;#34;--dry-run=client -o yaml&amp;#34; # 定义Shell变量 kubectl create cm info --from-literal=k=v $out ConfigMap 里的数据都是 Key-Value 结构，所以 --from-literal 参数使用 k=v 的形式生成数据。ConfigMap 的 YAML 描述大概如下：&#xA;apiVersion: v1 kind: ConfigMap metadata: name: info data: count: &amp;#39;10&amp;#39; debug: &amp;#39;on&amp;#39; path: &amp;#39;/etc/systemd&amp;#39; greeting: | say hello to kubernetes. 由上图可知，现在 ConfigMap 的 Key-Value 信息就已经存入了 etcd 数据库，后续就可以被其他 API 对象使用。&#xA;2.6.2 Secret # Secret 和 ConfigMap 的结构和用法很类似，不过 Secret 对象又细分出很多类，比如：</description>
    </item>
    <item>
      <title>2.7 常用命令</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.7-general-cmd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.7-general-cmd/</guid>
      <description>2.7 常用命令 # port-forward 端口映射&#xA;因为 Pod 都是运行在 Kubernetes 内部的私有网段里的，外界无法直接访问，想要对外暴露服务，需要使用一个专门的 kubectl port-forward 命令，它专门负责把本机的端口映射到在目标对象的端口号，有点类似 Docker 的参数 -p，经常用于 Kubernetes 的临时调试和测试。&#xA;比如将本地的 8080 映射到 a-pod 的 80 端口，kubectl 会把这个端口的所有数据都转发给集群内部的 Pod：&#xA;kubectl port-forward a-pod 8080:80 &amp;amp; 命令的末尾使用了一个 &amp;amp; 符号，让端口转发工作在后台进行，这样就不会阻碍我们后续的操作。如果想关闭端口转发，需要敲命令 fg ，它会把后台的任务带回到前台，然后就可以简单地用 “Ctrl + C” 来停止转发了。</description>
    </item>
    <item>
      <title>2.8 kubeadm 搭建</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.8-kubeadm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.8-kubeadm/</guid>
      <description>2.8 kubeadm 搭建 # kubeadm 和 minikube 类似，也是用容器和镜像来封装 Kubernetes 的各种组件，但它的目标不是单机部署，而是要能够轻松地在集群环境里部署 Kubernetes，并且让这个集群接近甚至达到生产级别质量。&#xA;kubeadm 具有了和 minikube 一样的易用性，只要很少的几条命令，如 init、join、upgrade、reset 就能够完成 Kubernetes 集群的管理维护工作，让它不仅适用于集群管理员，也适用于开发、测试人员。&#xA;2.8.1 准备工作 # 所谓的多节点集群，要求服务器应该有两台或者更多，其实最小可用的 Kubernetes 集群就只有两台主机，一台是 Master 节点，另一台是 Worker 节点。Master 节点需要运行 apiserver、etcd、scheduler、controller-manager 等组件，管理整个集群，Worker 节点只运行业务应用。&#xA;因为 Kubernetes 对系统有一些特殊要求，所以要先在 Master 和 Worker 节点上做一些准备，包括改主机名、改 Docker 配置、改网络设置、改交换分区这四步。&#xA;第一，由于 Kubernetes 使用主机名来区分集群里的节点，所以每个节点的 hostname 必须不能重名。需要修改 /etc/hostname 这个文件，把它改成容易辨识的名字，比如 Master 节点就叫 master，Worker 节点就叫 worker。&#xA;第二，虽然 Kubernetes 目前支持多种容器运行时，但 Docker 还是最方便最易用的一种，所以使用 Docker 作为 Kubernetes 的底层支持，这里可以参考 Docker 官网 安装 Docker Engine。安装完成后需要再对 Docker 的配置做一点修改，在 /etc/docker/daemon.</description>
    </item>
    <item>
      <title>2.9 Deployment</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.9-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.9-deployment/</guid>
      <description>2.9 Deployment # 在线业务远不是单纯启动一个 Pod 这么简单，还有多实例、高可用、版本更新等许多复杂的操作。比如多实例需求，为了提高系统的服务能力，应对突发的流量和压力，需要创建多个应用的副本，还要即时监控它们的状态。如果只使用 Pod，但有人不小心用 kubectl delete 误删了 Pod，又或者 Pod 运行的节点发生了断电故障，那么 Pod 就会在集群里彻底消失，Pod 容器里运行的服务也会消息，这样就会导致业务出现异常。&#xA;处理这种问题的思路就是 “单一职责” 和 “对象组合”。既然 Pod 管理不了自己，那么就再创建一个新的对象，由它来管理 Pod，采用 “对象套对象” 的形式。这个用来管理 Pod，实现在线业务应用的新 API 对象，就是 Deployment。&#xA;2.9.1 创建 # Deployment 的简称是 deploy，它的 apiVersion 是 apps/v1，kind 是 Deployment。&#xA;Deployment 的 YAML 描述大致如下：&#xA;apiVersion: apps/v1 kind: Deployment metadata: labels: app: ngx-dep name: ngx-dep spec: replicas: 2 selector: matchLabels: app: ngx-dep template: metadata: labels: app: ngx-dep spec: containers: - image: nginx:alpine name: nginx replicas 字段 # replicas 字段的含义比较简单明了，就是 “副本数量” 的意思，也就是说，指定要在 Kubernetes 集群里运行多少个 Pod 实例。有了这个字段，就相当于为 Kubernetes 明确了应用部署的 “期望状态”，Deployment 对象就可以扮演运维监控人员的角色，自动地在集群里调整 Pod 的数量。</description>
    </item>
    <item>
      <title>2.10 DaemonSet</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.10-daemonset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.10-daemonset/</guid>
      <description>2.10 DaemonSet # DaemonSet 会在 Kubernetes 集群的每个节点上都运行一个 Pod，就好像是 Linux 系统里的 “守护进程”（Daemon）一样。&#xA;DaemonSet 和 Deployment 有很大区别，Deployment 能够创建任意多个的 Pod 实例，并且维护这些 Pod 的正常运行，保证应用始终处于可用状态。但是，Deployment 并不关心这些 Pod 会在集群的哪些节点上运行，在它看来，Pod 的运行环境与功能是无关的，只要 Pod 的数量足够，应用程序应该会正常工作。但是对一些业务比较特殊服务，它们不是完全独立于系统运行的，而是与主机存在 “绑定” 关系，必须要依附于节点才能产生价值，比如：&#xA;网络应用（如 kube-proxy），必须每个节点都运行一个 Pod，否则节点就无法加入 Kubernetes 网络。&#xA;监控应用（如 Prometheus），必须每个节点都有一个 Pod 用来监控节点的状态，实时上报信息。&#xA;日志应用（如 Fluentd），必须在每个节点上运行一个 Pod，才能够搜集容器运行时产生的日志数据。&#xA;安全应用，每个节点都要有一个 Pod 来执行安全审计、入侵检查、漏洞扫描等工作。&#xA;以上这些业务如果用 Deployment 来部署就不太合适了，因为 Deployment 所管理的 Pod 数量是固定的，而且可能会在集群里 “漂移”，但，实际的需求却是要在集群里的每个节点上都运行 Pod，也就是说 Pod 的数量与节点数量保持同步。&#xA;DaemonSet，它在形式上和 Deployment 类似，都是管理控制 Pod，但管理调度策略却不同。DaemonSet 的目标是在集群的每个节点上运行且仅运行一个 Pod。&#xA;2.10.1 描述 DaemonSet # kubectl 不提供自动创建 DaemonSet YAML 样板的功能，不过可以在 Kubernetes 的官网 https://kubernetes.</description>
    </item>
    <item>
      <title>2.11 Service</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.11-service/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.11-service/</guid>
      <description>2.11 Service # 2.11.1 什么是 Service # Service 是集群内部的负载均衡机制，用来解决服务发现的关键问题。在 Kubernetes 集群里 Pod 的生命周期是比较 “短暂” 的，虽然 Deployment 和 DaemonSet 可以维持 Pod 总体数量的稳定，但在运行过程中，难免会有 Pod 销毁又重建，这就会导致 Pod 集合处于动态的变化之中。这种 “动态稳定” 对于现在流行的微服务架构来说是非常致命的，如果后台 Pod 的 IP 地址老是变来变去，客户端该怎么访问呢？&#xA;对于这种 “不稳定” 的后端服务问题，业内的解决方案是 “负载均衡”，典型的应用有 LVS、Nginx 等，它们在前端与后端之间加入了一个 “中间层”，屏蔽后端的变化，为前端提供一个稳定的服务。Service 的工作原理和 LVS、Nginx 差不多，Kubernetes 会给它分配一个静态 IP 地址，然后它再去自动管理、维护后面动态变化的 Pod 集合，当客户端访问 Service，它就根据某种策略，把流量转发给后面的某个 Pod。&#xA;LVS 即 Linux Virtual Server，是由章文嵩发起的一个开源项目，后来被集成进 Linux 内核。&#xA;Service 使用了 iptables 技术，每个节点上的 kube-proxy 组件自动维护 iptables 规则，客户不再关心 Pod 的具体地址，只要访问 Service 的固定 IP 地址，Service 就会根据 iptables 规则转发请求给它管理的多个 Pod，是典型的负载均衡架构。</description>
    </item>
    <item>
      <title>2.12 Ingress</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.12-ingress/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.12-ingress/</guid>
      <description>2.12 Ingress # Service 的负载均衡功能有限，只能够依据 IP 地址和端口号做一些简单的判断和组合，而更多的高级路由条件，比如主机名、URI、请求头、证书等 Service 无法实现。Service 还有一个缺点，它比较适合代理集群内部的服务。如果想要把服务暴露到集群外部，就只能使用 NodePort 或者 LoadBalancer 这两种方式，而它们缺乏足够的灵活性，难以管控。&#xA;Ingress 对象可以作为流量的总入口，统管集群的进出口数据，“扇入” “扇出” 流量（也就是常说的 “南北向”），让外部用户能够安全、顺畅、便捷地访问内部服务。&#xA;2.12.1 Ingress Controller &amp;amp; Class # Service 本身是没有服务能力的，它只是一些 iptables 规则，真正配置、应用这些规则的实际上是节点里的 kube-proxy 组件。如果没有 kube-proxy，Service 定义得再完善也没有用。&#xA;Ingress 只是一些 HTTP 路由规则的集合，相当于一份静态的描述文件，真正要把这些规则在集群里实施运行，需要的是 Ingress Controller，它的作用就相当于 Service 的 kube-proxy，能够读取、应用 Ingress 规则，处理、调度流量。&#xA;由于 Ingress Controller 与上层业务联系密切，所以 Kubernetes 把 Ingress Controller 的实现交给了社区，只要遵守 Ingress 规则，任何人都可以开发 Ingress Controller。在众多 Ingress Controller 中，Nginx 公司开发实现 Ingress Controller 是最多使用的。&#xA;最初 Kubernetes 的构想是，一个集群里有一个 Ingress Controller，再给它配上许多不同的 Ingress 规则，应该就可以解决请求的路由和分发问题了。但随着 Ingress 在实践中的大量应用，有很多问题逐渐显现出来，比如：</description>
    </item>
    <item>
      <title>2.13 PersistentVolume</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.13-persistent-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.13-persistent-volume/</guid>
      <description>2.13 PersistentVolume # Pod 里的容器是由镜像产生的，而镜像文件本身是只读的，进程要读写磁盘只能用一个临时的存储空间，一旦 Pod 销毁，临时存储也就会立即回收释放，数据也就丢失了。&#xA;Kubernetes 的 Volume 对数据存储已经给出了一个很好的抽象，它只是定义了有这么一个 “存储卷”，而这个 “存储卷” 是什么类型、有多大容量、怎么存储，可以自由发挥。Pod 不需要关心那些专业、复杂的细节，只要设置好 volumeMounts，就可以把 Volume 加载进容器里使用。所以，由 Volume 的概念，延伸出了 PersistentVolume 对象，它专门用来表示持久存储设备，但隐藏了存储的底层实现，使用者只需要知道它能安全可靠地保管数据就可以了（由于 PersistentVolume 这个词很长，一般把它简称为 PV）。&#xA;作为存储的抽象，PV 实际上就是一些存储设备、文件系统，比如 Ceph、GlusterFS、NFS，甚至是本地磁盘，管理它们已经超出了 Kubernetes 的能力范围，所以，一般会由系统管理员单独维护，然后再在 Kubernetes 里创建对应的 PV。PV 属于集群的系统资源，是和 Node 平级的一种对象，Pod 对它没有管理权，只有使用权。&#xA;2.13.1 PersistentVolumeClaim/StorageClass # 由于不同存储设备的差异实在是太大了：有的速度快，有的速度慢；有的可以共享读写，有的只能独占读写；有的容量小，只有几百 MB，有的容量大到 TB、PB 级别等，这么多种存储设备，只用一个 PV 对象来管理不符合 “单一职责” 的原则，让 Pod 直接去选择 PV 也不灵活。所以 Kubernetes 就又增加了两个新对象，PersistentVolumeClaim 和 StorageClass，这种 “中间层” 的思想，把存储卷的分配管理过程再次细化。&#xA;PersistentVolumeClaim，简称 PVC，用来向 Kubernetes 申请存储资源。PVC 是给 Pod 使用的对象，它相当于是 Pod 的代理，代表 Pod 向系统申请 PV。一旦资源申请成功，Kubernetes 就会把 PV 和 PVC 关联在一起，这个动作叫做 “绑定”（bind）。</description>
    </item>
    <item>
      <title>2.14 网络共享存储</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.14-persistentvolume-nfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.14-persistentvolume-nfs/</guid>
      <description>2.14 网络共享存储 # 由于 Kubernetes 里的 Pod 经常会在集群里 “漂移”，要想让存储卷真正能被 Pod 任意挂载，就不能限定在本地磁盘，而是要改成网络存储，这样 Pod 无论在哪里运行，只要知道 IP 地址或者域名，就可以通过网络通信访问存储设备。&#xA;在网络存储中有比较简单的 NFS 系统（Network File System），可以通过 NFS 理解在 Kubernetes 里使用网络存储，以及静态存储卷和动态存储卷的概念。&#xA;2.14.1 安装 NFS 服务器 # NFS 采用的是 Client/Server 架构，需要选定一台主机作为 Server，安装 NFS 服务端；其他要使用存储的主机作为 Client，安装 NFS 客户端工具。&#xA;可以在 Kubernetes 集群里增添一台名字叫 Storage 的服务器，在上面安装 NFS，实现网络存储、共享网盘的功能。这台 Storage 只是一个逻辑概念，在实际安装部署的时候完全可以把它合并到集群里的某台主机里。&#xA;在 Ubuntu/Debian 系统里安装 NFS 服务端很容易，使用 apt 即可：&#xA;sudo apt -y install nfs-kernel-server 安装好之后，需要给 NFS 指定一个存储位置，也就是网络共享目录。一般来说，应该建立一个专门的 /data 目录，这里使用了临时目录 /tmp/nfs：&#xA;mkdir -p /tmp/nfs 接下来需要配置 NFS 访问共享目录，修改 /etc/exports，指定目录名、允许访问的网段，还有权限等参数。把下面这行加上就行，注意目录名和 IP 地址要改成和自己的环境一致：</description>
    </item>
    <item>
      <title>2.15 StatefulSet</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.15-statefulset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.15-statefulset/</guid>
      <description>2.15 StatefulSet # 2.15.1 状态和应用 # 理论上任何应用都是有状态的，只是有的应用的状态信息不是很重要，即使不恢复状态也能够正常运行，这就是 “无状态应用”。“无状态应用” 典型的例子就是 Nginx 这样的 Web 服务器，它只是处理 HTTP 请求，本身不生产数据（日志除外），不需要特意保存状态，无论以什么状态重启都能很好地对外提供服务。&#xA;还有一些应用，运行状态信息很重要，如果因为重启而丢失了状态是绝对无法接受的，这样的应用是 “有状态应用”。比如 Redis、MySQL 这样的数据库，它们的 “状态” 就是在内存或者磁盘上产生的数据，是应用的核心价值所在，如果不能够把这些数据及时保存再恢复，那绝对会是灾难性的后果。&#xA;对于 Deployment 来说，多个实例之间是无关的，启动的顺序不固定，Pod 的名字、IP 地址、域名也都是完全随机的，这正是 “无状态应用” 的特点。对于 “有状态应用”，多个实例之间可能存在依赖关系，比如 master/slave、active/passive，需要依次启动才能保证应用正常运行，外界的客户端也可能要使用固定的网络标识来访问实例，而且这些信息还必须要保证在 Pod 重启后不变。&#xA;Kubernetes 定义了一个新的 API 对象 StatefulSet，专门用来管理有状态的应用。&#xA;2.15.2 描述 StatefulSet # StatefulSet 也可以看做是 Deployment 的一个特例，它不能直接用 kubectl create 创建样板文件，它的对象描述和 Deployment 差不多，可以把 Deployment 适当修改一下，就变成了 StatefulSet 对象。以下是一个使用 Redis 的 StatefulSet 描述文件：&#xA;apiVersion: apps/v1 kind: StatefulSet metadata: name: redis-sts spec: serviceName: redis-svc replicas: 2 selector: matchLabels: app: redis-sts template: metadata: labels: app: redis-sts spec: containers: - image: redis:5-alpine name: redis ports: - containerPort: 6379 YAML 文件里除了 kind 必须是 “StatefulSet”，在 spec 里还多出了一个 “serviceName” 字段外，其余的部分和 Deployment 是一模一样的，比如 replicas、selector、template。</description>
    </item>
    <item>
      <title>2.16 滚动更新</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.16-rolling-update/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.16-rolling-update/</guid>
      <description>2.16 滚动更新 # 在实际生产环境中，只是把应用发布到集群里是远远不够的，要让应用稳定可靠地运行，还需要有持续的运维工作。比如 Deployment 的 “应用伸缩” 功能就是一种常见的运维操作，在 Kubernetes 里，使用命令 kubectl scale，可以轻松调整 Deployment 下属的 Pod 数量。除了 “应用伸缩”，其他的运维操作比如应用更新、版本回退等工作也是日常运维中经常会遇到的问题。&#xA;2.16.1 应用版本 # 版本更新实际做起来是一个相当棘手的事。因为系统已经上线运行，必须要保证不间断地对外提供服务。尤其在特殊时候可能需要开发、测试、运维、监控、网络等各个部门的一大堆人来协同工作，费时又费力。&#xA;在 Kubernetes 里，版本更新使用的不是 API 对象，而是两个命令：kubectl apply 和 kubectl rollout，需要搭配部署应用所需要的 Deployment、DaemonSet 等 YAML 文件。&#xA;在 Kubernetes 里应用都是以 Pod 的形式运行的，而 Pod 通常又会被 Deployment 等对象来管理，所以应用的 “版本更新” 实际上更新的是整个 Pod。Pod 是由 YAML 描述文件来确定的，是 Deployment 等对象里的字段 template。所以，在 Kubernetes 里应用的版本变化就是 template 里 Pod 的变化，哪怕 template 里只变动了一个字段，那也会形成一个新的版本，也算是版本变化。但在 template 里的内容太多了，拿这么长的字符串来当做 “版本号” 不太现实，所以 Kubernetes 就使用了 “摘要” 功能，用摘要算法计算 template 的 Hash 值作为 “版本号”。</description>
    </item>
    <item>
      <title>2.17 应用保障</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.17-app-assurance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.17-app-assurance/</guid>
      <description>2.17 应用保障 # 2.17.1 容器资源配额 # 创建容器有三大隔离技术：namespace、cgroup、chroot。其中的 namespace 实现了独立的进程空间，chroot 实现了独立的文件系统，cgroup 的作用是管控 CPU、内存，保证容器不会无节制地占用基础资源，进而影响到系统里的其他应用。&#xA;因为 CPU、内存与存储卷有明显的不同，它是直接 “内置” 在系统里的，不像硬盘那样需要 “外挂”，所以申请和管理的过程会简单很多。Kubernetes 在管控容器使用 CPU 和内存的做法是，只要在 Pod 容器的描述部分添加一个新字段 resources 就可以了，它就相当于申请资源的 Claim。&#xA;以下是一个 YAML 描述示例：&#xA;apiVersion: v1 kind: Pod metadata: name: ngx-pod-resources spec: containers: - image: nginx:alpine name: ngx resources: requests: cpu: 10m memory: 100Mi limits: cpu: 20m memory: 200Mi requests 意思是容器要申请的资源，也就是说要求 Kubernetes 在创建 Pod 的时候必须分配这里列出的资源，否则容器就无法运行。&#xA;limits 意思是容器使用资源的上限，不能超过设定值，否则就有可能被强制停止运行。&#xA;内存的写法和磁盘容量一样，使用 Ki、Mi、Gi 来表示 KB、MB、GB，比如 512Ki、100Mi、0.5Gi 等。&#xA;因为 CPU 因为在计算机中数量有限，非常宝贵，所以 Kubernetes 允许容器精细分割 CPU，既可以 1 个、2 个地完整使用 CPU，也可以用小数 0.</description>
    </item>
    <item>
      <title>2.18 集群管理</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.18-cluster-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.18-cluster-management/</guid>
      <description>2.18 集群管理 # 2.18.1 名字空间 # Kubernetes 的名字空间并不是一个实体对象，只是一个逻辑上的概念。它可以把集群切分成一个个彼此独立的区域，然后把对象放到这些区域里，就实现了类似容器技术里 namespace 的隔离效果，应用只能在自己的名字空间里分配资源和运行，不会干扰到其他名字空间里的应用。&#xA;在 Master/Node 架构里引入名字空间，是因为集群很大、计算资源充足，会有非常多的用户在 Kubernetes 里创建各式各样的应用，可能会有百万数量级别的 Pod，这就使得资源争抢和命名冲突的概率大大增加了，情形和单机 Linux 系统里是非常相似的。&#xA;比如，现在有一个 Kubernetes 集群，前端组、后端组、测试组都在使用它。这个时候就很容易命名冲突，比如后端组先创建了一个 Pod 叫 Web，这个名字就被 “占用” 了，之后前端组和测试组就只能绞尽脑汁再新起一个不冲突的名字。资源争抢也容易出现，比如，测试组不小心部署了有 Bug 的应用，在节点上把资源都给 “吃” 完了，就会导致其他组的同事根本无法工作。&#xA;当多团队、多项目共用 Kubernetes 的时候，就需要把集群给适当地 “局部化”，为每一类用户创建出只属于它自己的 “工作空间”。&#xA;2.18.2 使用名字空间 # 名字空间也是一种 API 对象，使用命令 kubectl api-resources 可以看到它的简称是 “ns”，命令 kubectl create 不需要额外的参数，可以很容易地创建一个名字空间，比如：&#xA;kubectl create ns test-ns kubectl get ns Kubernetes 初始化集群的时会预设 4 个名字空间：default、kube-system、kube-public、kube-node-lease。default 是用户对象默认的名字空间，kube-system 是系统组件所在的名字空间。&#xA;想要把一个对象放入特定的名字空间，需要在它的 metadata 里添加一个 namespace 字段，比如要在 “test-ns” 名字空间里创建一个简单的 Nginx Pod，就要这样写：</description>
    </item>
    <item>
      <title>2.19 系统监控</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.19-system-monitor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.19-system-monitor/</guid>
      <description>2.19 系统监控 # 2.19.1 Metrics Server # Linux top 命令能够实时显示当前系统的 CPU 和内存利用率，是性能分析和调优的工具。Kubernetes 也提供了类似的命令，就是 kubectl top，不过默认情况下这个命令不会生效，必须要安装插件 Metrics Server 才可以。&#xA;Metrics Server 是一个专门用来收集 Kubernetes 核心资源指标（metrics）的工具，它定时从所有节点的 kubelet 里采集信息，但是对集群的整体性能影响极小，每个节点只大约会占用 1m 的 CPU 和 2MB 的内存，性价比非常高。项目网址在 https://github.com/kubernetes-sigs/metrics-server。&#xA;Metrics Server 调用 kubelet 的 API 拿到节点和 Pod 的指标，再把这些信息交给 apiserver，这样 kubectl、HPA 就可以利用 apiserver 来读取指标了。&#xA;Metrics Server 的所有依赖都放在了一个 YAML 描述文件里，你可以使用 wget 或者 curl 下载：&#xA;wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml 下载下来的 YAML 描述文件不能直接使用，需要修改下。&#xA;需要在 Metrics Server 的 Deployment 对象里，加上一个额外的运行参数 --kubelet-insecure-tls，也就是这样： apiVersion: apps/v1 kind: Deployment metadata: name: metrics-server namespace: kube-system spec: .</description>
    </item>
    <item>
      <title>2.20 网络通信</title>
      <link>https://example.com/kubernetes/docs/part2-break-ice/2.20-network-communications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/kubernetes/docs/part2-break-ice/2.20-network-communications/</guid>
      <description>2.20 网络通信 # 2.20.1 网络模型 # Docker 有三种常见的网络模式： null、host 和 bridge。下图，描述了 Docker 里最常用的 bridge 网络模式：&#xA;Docker 会创建一个名字叫 “docker0” 的网桥，默认是私有网段 “172.17.0.0/16”。每个容器都会创建一个虚拟网卡对（veth pair），两个虚拟网卡分别 “插” 在容器和网桥上，这样容器之间就可以互联互通了。Docker 的网络方案简单有效，但只局限在单机环境里工作，跨主机通信非常困难（需要做端口映射和网络地址转换）。&#xA;Kubernetes 的网络模型 “IP-per-pod”，能够很好地适应集群系统的网络需求，它有下面的这 4 点基本假设：&#xA;集群里的每个 Pod 都会有唯一的一个 IP 地址。&#xA;Pod 里的所有容器共享这个 IP 地址。&#xA;集群里的所有 Pod 都属于同一个网段。&#xA;Pod 直接可以基于 IP 地址直接访问另一个 Pod，不需要做麻烦的网络地址转换（NAT）。&#xA;这种网络让 Pod 摆脱了主机的硬限制，是一个 “平坦” 的网络模型，通信也非常简单。因为 Pod 都具有独立的 IP 地址，相当于一台虚拟机，而且直连互通，也就可以很容易地实施域名解析、负载均衡、服务发现等工作，对应用的管理和迁移都非常友好。&#xA;2.20.2 什么是 CNI # CNI（Container Networking Interface）为网络插件定义了一系列通用接口，开发者只要遵循这个规范就可以接入 Kubernetes，为 Pod 创建虚拟网卡、分配 IP 地址、设置路由规则，最后就能够实现 “IP-per-pod” 网络模型。依据实现技术的不同，CNI 插件可以大致上分成 “Overlay” “Route” 和 “Underlay” 三种。</description>
    </item>
  </channel>
</rss>
