[{"categories":["读书与生活"],"content":"几年前，一位好朋友去世了，九零后，跟我年纪一样。我跟他从小就认识，我们一起上的小学，一起上的初中，高中之后便联系的少了，后来我去外地读书，联系的就更少了。 那还是二零一九，那时我刚从西安来北京。一天夜里，都很晚了，我妈打电话跟我说，他去世了，好像是心梗，让我在外面多注意身体。天啊，当我听到这个消息的时候，我简直不敢相信，我反复确认了几次，无疑的确是他。 那晚我很难过，因为我不久之前还见过他。二零一九的春节，那天应该是初二的早晨，我骑着电瓶车去外公家拜年，外公家跟他老家离的不远，就几步路，那天早晨我在路边看到了他，我没有停下来，心想就几步路，我回来的时候再去找他，但是等我再往回走的时候他就不在家了。如今听到噩耗，再想起这件事，我真的特别后悔当时应该停下来见他一面。后来，我把这件事说给我女朋友听，她也特别感慨的说，想做什么事一定要赶紧去做。是啊，一定要赶紧去做，毕竟世事无常。 我跟他太久没有联系了，没有他的电话，也没有他的微信，后来在QQ 里找到他的联系方式。我尝试着发了一条消息过去，QQ 的那边，他媳妇回了一条消息，说他人已经不在了。后来有个初中同学联系到了我，是他班上的，建了一个微信群，想尽点绵薄之力，我们一人凑了点钱，由一个在老家的同学给他家里送了去，但他妈妈只是领了我们的心意。他实在是太年轻了，而且新婚不久，孩子才一岁。 这张照片是上初中时我们一起去皖南事变烈士陵园拍的，也是我跟他唯一的一张合影。 2020 年时家里发大水，家里的东西都泡水了，这张照片后来也不知所踪。 左三是他\" 左三是他 有一次我回家，我爸还跟我说在一次婚宴上见过他。他过世后，一次在我大舅家吃饭的时候，他家的一个亲戚也在，在聊起他的时候，直夸他在外面干活能吃苦，人不错。 我跟他从小相识，一起在村小学读书，一起在田埂上疯跑，他教我掏鸟窝，网知了，在我眼里，他好像什么都会，他教了我很多技能，带给我很多快乐。上初中的时候，我跟他一起骑车上学，放学也一起回家。他每天早上都是骑着车来我外婆家等我，等我吃完早饭一起走，一路上我们有好几个同学都一起。下午放学他有时也在我外婆家跟我一起做完作业才回家，这些事如今历历在目，但是他却永远不在了。 也许是年纪大了，不知不觉对有些事越来越伤感。几次提笔想写点什么，但是每次都写不出来，心里总记挂这件事，可能是那次我没有停下来见他吧。 2021年10月15日完 ","date":"2022-03-16","objectID":"/old-pal/:0:0","tags":["随笔"],"title":"纪念一位老友","uri":"/old-pal/"},{"categories":["算法与数学"],"content":"LeetCode 热题 HOT 100,Leetcode,两数之和","date":"2022-03-25","objectID":"/hot100/","tags":["leetcode"],"title":"LeetCode 热题 HOT 100","uri":"/hot100/"},{"categories":["算法与数学"],"content":"1. 两数之和 题目地址：https://leetcode-cn.com/problems/two-sum/ ","date":"2022-03-25","objectID":"/hot100/:1:0","tags":["leetcode"],"title":"LeetCode 热题 HOT 100","uri":"/hot100/"},{"categories":["算法与数学"],"content":"解题思路 嵌套遍历数组，外层遍历的值和内层遍历的值相加，如果相加等于目标值，则返回结果，否则继续遍历。内层遍历开始的位置是外层遍历的位置加 1，结束的位置是数组长度。 ","date":"2022-03-25","objectID":"/hot100/:1:1","tags":["leetcode"],"title":"LeetCode 热题 HOT 100","uri":"/hot100/"},{"categories":["算法与数学"],"content":"go 实现 package main import \"fmt\" func twoSum(nums []int, target int) []int { for index, value := range nums { for i := index + 1; i \u003c len(nums); i++ { if (value + nums[i]) == target { return []int{index, i} } } } return nil } func main() { fmt.Println(twoSum([]int{3, 2, 4}, 6)) } ","date":"2022-03-25","objectID":"/hot100/:1:2","tags":["leetcode"],"title":"LeetCode 热题 HOT 100","uri":"/hot100/"},{"categories":["算法与数学"],"content":"20. 有效的括号 题目地址：https://leetcode-cn.com/problems/valid-parentheses/ ","date":"2022-03-25","objectID":"/hot100/:2:0","tags":["leetcode"],"title":"LeetCode 热题 HOT 100","uri":"/hot100/"},{"categories":["算法与数学"],"content":"解题思路 判断括号的有效性可以使用「栈」这一数据结构来解决。 我们遍历给定的字符串 s。当我们遇到一个左括号时，我们会期望在后续的遍历中，有一个相同类型的右括号将其闭合。由于后遇到的左括号要先闭合，因此我们可以将这个左括号放入栈顶。 当我们遇到一个右括号时，我们需要将一个相同类型的左括号闭合。此时，我们可以取出栈顶的左括号并判断它们是否是相同类型的括号。如果不是相同的类型，或者栈中并没有左括号，那么字符串 s 无效，返回 False。为了快速判断括号的类型，我们可以使用哈希表存储每一种括号。哈希表的键为右括号，值为相同类型的左括号。 在遍历结束后，如果栈中没有左括号，说明我们将字符串 s 中的所有左括号闭合，返回 True，否则返回 False。 注意到有效字符串的长度一定为偶数，因此如果字符串的长度为奇数，我们可以直接返回 False，省去后续的遍历判断过程。 ","date":"2022-03-25","objectID":"/hot100/:2:1","tags":["leetcode"],"title":"LeetCode 热题 HOT 100","uri":"/hot100/"},{"categories":["算法与数学"],"content":"go 实现 package main import \"fmt\" func isValid(s string) bool { n := len(s) if n%2 != 0 { // 奇数直接退出 return false } pairs := map[byte]byte{ ')': '(', ']': '[', '}': '{', } stack := []byte{} for i := 0; i \u003c n; i++ { if pairs[s[i]] \u003e 0 { // 如果是右括号,判断栈顶是否是对应的左括号,果然有对应的左括号,则弹出栈顶元素,否则直接退出 if len(stack) == 0 || stack[len(stack)-1] != pairs[s[i]] { return false } stack = stack[:len(stack)-1] } else { stack = append(stack, s[i]) } } return len(stack) == 0 } func main() { fmt.Println(isValid(\"()[]{}\")) } 版权信息 作者：LeetCode-Solution 链接：https://leetcode-cn.com/problem-list/2cktkvj 来源：力扣（LeetCode） ","date":"2022-03-25","objectID":"/hot100/:2:2","tags":["leetcode"],"title":"LeetCode 热题 HOT 100","uri":"/hot100/"},{"categories":["开发者手册"],"content":"xiaobinqt,软件alpha、beta、rc、stable各个版本有什么区别","date":"2022-05-06","objectID":"/alpha-beta-rc-stable-diff/","tags":["chore"],"title":"alpha、beta、rc、stable 的区别","uri":"/alpha-beta-rc-stable-diff/"},{"categories":["开发者手册"],"content":"版本管理版本管理 \" 版本管理 ","date":"2022-05-06","objectID":"/alpha-beta-rc-stable-diff/:0:0","tags":["chore"],"title":"alpha、beta、rc、stable 的区别","uri":"/alpha-beta-rc-stable-diff/"},{"categories":["开发者手册"],"content":"alpha 内部测试版。α是希腊字母的第一个，表示最早的版本，一般用户不要下载这个版本，这个版本包含很多 bug，功能也不全，主要是给开发人员和测试人员测试和找 bug 用的。 ","date":"2022-05-06","objectID":"/alpha-beta-rc-stable-diff/:1:0","tags":["chore"],"title":"alpha、beta、rc、stable 的区别","uri":"/alpha-beta-rc-stable-diff/"},{"categories":["开发者手册"],"content":"beta 公开测试版。β是希腊字母的第二个，顾名思义，这个版本比 alpha 版发布得晚一些，主要是给“部落”用户和忠实用户测试用的，该版本任然存在很多 bug，但是相对 alpha 版要稳定一些。这个阶段版本的软件还会不断增加新功能。如果你是发烧友，可以下载这个版本。 ","date":"2022-05-06","objectID":"/alpha-beta-rc-stable-diff/:2:0","tags":["chore"],"title":"alpha、beta、rc、stable 的区别","uri":"/alpha-beta-rc-stable-diff/"},{"categories":["开发者手册"],"content":"rc Release Candidate候选版本，该版本又较 beta 版更进一步了，该版本功能不再增加，和最终发布版功能一样。这个版本有点像最终发行版之前的一个类似预览版，这个的发布就标明离最终发行版不远了。作为普通用户，如果你很急着用这个软件的话，也可以下载这个版本。 ","date":"2022-05-06","objectID":"/alpha-beta-rc-stable-diff/:3:0","tags":["chore"],"title":"alpha、beta、rc、stable 的区别","uri":"/alpha-beta-rc-stable-diff/"},{"categories":["开发者手册"],"content":"stable 稳定版。在开源软件中，都有 stable 版，这个就是开源软件的最终发行版，用户可以放心大胆的用了。 ","date":"2022-05-06","objectID":"/alpha-beta-rc-stable-diff/:4:0","tags":["chore"],"title":"alpha、beta、rc、stable 的区别","uri":"/alpha-beta-rc-stable-diff/"},{"categories":["开发者手册"],"content":"GA GAGeneral Availability：正式发布的版本；在国外都是用 GA 来说明 release 版本的。 ","date":"2022-05-06","objectID":"/alpha-beta-rc-stable-diff/:5:0","tags":["chore"],"title":"alpha、beta、rc、stable 的区别","uri":"/alpha-beta-rc-stable-diff/"},{"categories":["开发者手册"],"content":"RTM RTMRelease to Manufacturing：给生产商的 release 版本；RTM 版本并不一定意味着创作者解决了软件所有问题，仍有可能向公众发布前更新版本。 ","date":"2022-05-06","objectID":"/alpha-beta-rc-stable-diff/:6:0","tags":["chore"],"title":"alpha、beta、rc、stable 的区别","uri":"/alpha-beta-rc-stable-diff/"},{"categories":["开发者手册"],"content":"xiaobinqt,docker root用户执行","date":"2022-05-06","objectID":"/docker-summary-of-common-usage/","tags":["docker","备忘"],"title":"Docker 常用命令备忘","uri":"/docker-summary-of-common-usage/"},{"categories":["开发者手册"],"content":"root 用户执行 有时进入容器后，用户就是变成非 root 用户，这种时候又没有密码，在执行一些操作的时候就会非常不方便，这是可以用 -u root 来指定用户。 非root用户非root用户 \" 非root用户 执行简单命令可以这样👇 图01图01 \" 图01 如果需要进入容器，可以这样👇 docker exec -u root -it 容器名 bash #或者 docker exec -u root -it 容器名 sh 图02图02 \" 图02 ","date":"2022-05-06","objectID":"/docker-summary-of-common-usage/:1:0","tags":["docker","备忘"],"title":"Docker 常用命令备忘","uri":"/docker-summary-of-common-usage/"},{"categories":["mysql"],"content":"xiaobinqt,mysql workbench 查看 Triggers 触发器","date":"2022-04-20","objectID":"/mysql-workbench-show-triggers/","tags":["mysql"],"title":"mysql workbench 查看触发器","uri":"/mysql-workbench-show-triggers/"},{"categories":["mysql"],"content":"mysql workbench 是官方推荐的数据库工具，用了很长时间却一直不知道触发器在哪儿😢。 触发器是对单个表的操作，而不是整个数据库的操作，所以 Alter Table 就可以看到触发器： 图1图1 \" 图1 图2图2 \" 图2 点这个扳手图标也可以看到触发器，跟 Alter Table 效果一样： 图3图3 \" 图3 ","date":"2022-04-20","objectID":"/mysql-workbench-show-triggers/:0:0","tags":["mysql"],"title":"mysql workbench 查看触发器","uri":"/mysql-workbench-show-triggers/"},{"categories":["mysql"],"content":"参考 MySQL Workbench : How to Configure Triggers in MySQL ","date":"2022-04-20","objectID":"/mysql-workbench-show-triggers/:1:0","tags":["mysql"],"title":"mysql workbench 查看触发器","uri":"/mysql-workbench-show-triggers/"},{"categories":["开发者手册"],"content":"xiaobinqt,Redis 缓存击穿,缓存穿透,缓存雪崩,Redis","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/","tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/"},{"categories":["开发者手册"],"content":"缓存击穿 高并发流量，访问的这个数据是热点数据，请求的数据在 DB 中存在，但是 Redis 存的那一份已经过期，后端需要从 DB 从加载数据并写到 Redis。 总结起来就是：单一热点数据、高并发、数据失效。 缓存击穿缓存击穿 \" 缓存击穿 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:1:0","tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/"},{"categories":["开发者手册"],"content":"解决方案 过期时间➕随机值 对于热点数据，我们不设置过期时间，这样就可以把请求都放在缓存中处理，充分把 Redis 高吞吐量性能利用起来。或者过期时间再加一个随机值。 设计缓存的过期时间时，使用公式：过期时间 = baes 时间 + 随机时间。 即相同业务数据写缓存时，在基础过期时间之上，再加一个随机的过期时间，让数据在未来一段时间内慢慢过期，避免瞬时全部过期，对 DB 造成过大压力。 预热 预先把热门数据提前存入 Redis 中，并设热门数据的过期时间超大值。 使用锁 当发现缓存失效的时候，不是立即从数据库加载数据。 而是先获取分布式锁，获取锁成功才执行数据库查询和写数据到缓存的操作，获取锁失败，则说明当前有线程在执行数据库查询操作，当前线程睡眠一段时间再重试。这样只让一个请求去数据库读取数据。 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:1:1","tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/"},{"categories":["开发者手册"],"content":"缓存穿透 数据库本就没有这个数据，请求直奔数据库，缓存系统形同虚设。 大量请求的 key 根本不存在于缓存中也不存在数据库，导致请求直接到了数据库上，根本没有经过缓存这一层，对数据库造成压力而影响正常服务。 缓存穿透缓存穿透 \" 缓存穿透 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:2:0","tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/"},{"categories":["开发者手册"],"content":"解决方案 最基本的首先就是做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。 缓存无效的 key 如果缓存和数据库都查不到某个 key 的数据就写一个到 Redis 中去并设置过期时间。这种方式可以解决请求的 key 变化不频繁的情况，如果黑客恶意攻击，每次构建不同的请求 key，会导致 Redis 中缓存大量无效的 key 。这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，应该尽量将无效的 key 的过期时间设置短一点比如 1 分钟。 布隆过滤器 布隆过滤器是一个非常神奇的数据结构，通过它我们可以非常方便地判断一个给定数据是否存在于海量数据中。我们需要的仅仅就是判断 key 是否合法。 具体是这样做的：把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话再走其他的判断流程。 布隆过滤器布隆过滤器 \" 布隆过滤器 需要注意的是布隆过滤器可能会存在误判的情况。 布隆过滤器说某个元素存在，小概率会误判。布隆过滤器说某个元素不在，这个元素一定不在。 这是因为，当一个元素加入布隆过滤器中的时候，会进行如下操作： 使用布隆过滤器中的哈希函数对元素值进行计算，得到哈希值（有几个哈希函数得到几个哈希值）。 根据得到的哈希值，在位数组中把对应下标的值置为 1。 当需要判断一个元素是否存在于布隆过滤器的时候，会进行如下操作： 对给定元素再次进行相同的哈希计算； 得到值之后判断位数组中的每个元素是否都为 1，如果值都为 1，那么说明这个值在布隆过滤器中，如果存在一个值不为 1，说明该元素不在布隆过滤器中。 然而，一定会出现这样一种情况：不同的字符串可能哈希出来的位置相同。我们可以适当增加位数组大小或者调整我们的哈希函数来降低概率。 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:2:1","tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/"},{"categories":["开发者手册"],"content":"缓存雪崩 缓存在同一时间大面积的失效，后面的请求都直接落到了数据库上，造成数据库短时间内承受大量请求。或是有一些被大量访问数据（热点缓存）在某一时刻大面积失效，导致对应的请求直接落到了数据库上。 而出现该原因主要有两种： 大量热点数据同时过期，导致大量请求需要查询数据库并写到缓存。 Redis 故障宕机，缓存系统异常。 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:3:0","tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/"},{"categories":["开发者手册"],"content":"缓存大量数据同时过期 数据保存在缓存系统并设置了过期时间，但是由于在同时一刻，大量数据同时过期。系统就把请求全部打到数据库获取数据，并发量大的话就会导致数据库压力激增。 缓存雪崩是发生在大量数据同时失效的场景，而缓存击穿是在某个热点数据失效的场景，这是他们最大的区别。 缓存大量数据同时过期缓存大量数据同时过期 \" 缓存大量数据同时过期 过期时间添加随机值 要避免给大量的数据设置一样的过期时间，过期时间 = baes 时间+ 随机时间（较小的随机数，比如随机增加 1~5 分钟）。 这样一来，就不会导致同一时刻热点数据全部失效，同时过期时间差别也不会太大，既保证了相近时间失效，又能满足业务需求。 接口限流 当访问的不是核心数据的时候，在查询的方法上加上接口限流保护。比如设置 10000 req/s。如果访问的是核心数据接口，缓存不存在允许从数据库中查询并设置到缓存中。这样的话，只有部分请求会发送到数据库，减少了压力。 限流，就是指，我们在业务系统的请求入口前端控制每秒进入系统的请求数，避免过多的请求被发送到数据库。 限流限流 \" 限流 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:3:1","tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/"},{"categories":["开发者手册"],"content":"Redis 故障 一旦 Redis 故障或宕机，会导致大量请求打到数据库，从而发生缓存雪崩。对于 Redis 故障，主要有以下两种解决方案。 服务熔断和限流 在业务系统中，针对高并发的使用服务熔断来有损提供服务从而保证系统的可用性。 服务熔断就是当从缓存获取数据发现异常，则直接返回错误数据给前端，防止所有流量打到数据库导致宕机。 服务熔断和限流属于在发生了缓存雪崩，如何降低雪崩对数据库造成的影响的方案。 高可用缓存集群 缓存系统一定要构建一套 Redis 高可用集群，比如 Redis 哨兵集群 TODO 或者 Redis Cluster 集群 TODO，如果 Redis 的主节点故障宕机了，从节点还可以切换成为主节点，继续提供缓存服务，避免了由于缓存实例宕机而导致的缓存雪崩问题。 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:3:2","tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/"},{"categories":["开发者手册"],"content":"总结 缓存穿透指的是数据库本就没有这个数据，请求直奔数据库，缓存系统形同虚设。 缓存击穿（失效）指的是数据库有数据，缓存本应该也有数据，但是缓存过期了，Redis 这层流量防护屏障被击穿了，请求直奔数据库。 缓存雪崩指的是大量的热点数据无法在 Redis 缓存中处理（大面积热点数据缓存失效、Redis 宕机），流量全部打到数据库，导致数据库极大压力。 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:4:0","tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/"},{"categories":["开发者手册"],"content":"参考 Redis 缓存击穿（失效）、缓存穿透、缓存雪崩怎么解决？ ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:5:0","tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/"},{"categories":["golang"],"content":"golang GC 原理,Go 垃圾回收,Go 垃圾标记清除算法,golang 三色标记法,golang 屏障机制,go 垃圾回收算法,go 插入/删除屏障,golang 混合写屏障,垃圾回收 STW,STW","date":"2022-04-06","objectID":"/go-gc/","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"垃圾回收（Garbage Collection，GC）是编程语言中提供的自动的内存管理机制，自动释放不需要的内存对象，让出存储器资源。GC 过程中无需程序员手动执行。 GC 机制在现代很多编程语言都支持，GC 能力的性能与优劣也是不同语言之间对比度指标之一。 ","date":"2022-04-06","objectID":"/go-gc/:0:0","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"堆和栈 栈：由操作系统自动分配释放，存放函数的参数值，局部变量的值等。 堆：一般由程序员分配和释放，若程序员不释放，程序结束时可能由 OS 回收。 栈使用的是一级缓存，他们通常都是被调用时处于存储空间中，调用完毕立即释放。 堆则是存放在二级缓存中，生命周期由虚拟机的垃圾回收算法来决定，但并不是一旦成为孤儿对象就能被回收。 申请到栈内存好处：函数返回直接释放，不会引起垃圾回收，对性能没有影响。 ","date":"2022-04-06","objectID":"/go-gc/:1:0","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"标记清除算法 Go v1.3 之前使用普通的标记-清除（mark and sweep）算法，主要有两个主要的步骤： 标记(Mark phase)，找出不可达的对象，然后做上标记。 清除(Sweep phase)，回收标记好的对象。 ","date":"2022-04-06","objectID":"/go-gc/:2:0","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"第一步 暂停程序业务逻辑， 分类出可达和不可达的对象，然后做上标记。 程序与对象的可达关系程序与对象的可达关系 \" 程序与对象的可达关系 👆图中表示是程序与对象的可达关系，目前程序的可达对象有对象 1-2-3，对象 4-7 等五个对象。 ","date":"2022-04-06","objectID":"/go-gc/:2:1","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"第二步 开始标记，程序找出它所有可达的对象，并做上标记👇。 找出可达对象找出可达对象 \" 找出可达对象 对象 1-2-3 、对象 4-7 等五个对象被做上标记。 ","date":"2022-04-06","objectID":"/go-gc/:2:2","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"第三步 标记完了之后，然后开始清除未标记的对象。 清除对象清除对象 \" 清除对象 对象 5，6 不可达，被 GC 清除。 操作简单，但是，mark and sweep 算法在执行的时候，需要程序暂停！即 STW（stop the world），STW 的过程中，CPU 不执行用户代码，全部用于垃圾回收，这个过程的影响很大，所以 STW 也是一些回收机制最大的难题和希望优化的点。 在执行第三步的这段时间，程序会暂定停止任何工作，卡在那等待回收执行完毕。 ","date":"2022-04-06","objectID":"/go-gc/:2:3","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"第四步 停止暂停，让程序继续执行。然后循环重复这个过程，直到 process 程序生命周期结束。 ","date":"2022-04-06","objectID":"/go-gc/:2:4","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"缺点与优化 标记清除算法明了，过程鲜明干脆，但是也有非常严重的问题，就是 STW，让程序暂停，程序出现卡顿。 Go V1.3版本之前就是用这种方式来实施的。执行 GC 的基本流程就是首先启动 STW 暂停，然后执行标记，再执行数据回收，最后停止 STW ，如下图👇 STWSTW \" STW 从👆来看，全部的 GC 时间都是包裹在 STW 范围之内的，这样貌似程序暂停的时间过长，影响程序的运行性能。所以Go v1.3 做了简单的优化，将 STW 的步骤提前，减少 STW 暂停的时间范围 👇 STW优化STW优化 \" STW优化 主要是将 STW 的步骤提前了一步，因为在 Sweep 清除的时候，可以不需要 STW 停止，因为这些对象已经是不可达对象了，不会出现回收写冲突等问题，清除操作和用户逻辑可以并发。 但是无论怎么优化，Go v1.3 都面临这个一个重要问题，就是 mark-and-sweep 算法会暂停整个程序 。 ","date":"2022-04-06","objectID":"/go-gc/:2:5","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"有STW的三色标记法 Go 中的垃圾回收主要应用三色标记法，GC 过程和其他用户 goroutine 可并发运行，但需要一定时间的 STW。 所谓三色标记法实际上就是通过三个阶段的标记来确定需要清除的对象有哪些。 ","date":"2022-04-06","objectID":"/go-gc/:3:0","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"第一步 每次新创建的对象，默认的颜色都是标记为“白色”👇 白色对象白色对象 \" 白色对象 上图所示，我们的程序可抵达的内存对象关系如左图所示，右边的标记表，是用来记录目前每个对象的标记颜色分类。 这里所谓“程序”，是一些对象的根节点集合。如果我们将“程序”展开，会得到类似如下的表现形式： 程序的根节点集合展开程序的根节点集合展开 \" 程序的根节点集合展开 ","date":"2022-04-06","objectID":"/go-gc/:3:1","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"第二步 每次 GC 回收开始， 会从根节点开始遍历所有对象，把遍历到的对象从白色集合放入“灰色”集合： 遍历根对象遍历根对象 \" 遍历根对象 本次遍历是一次遍历，非递归形式，是从程序初次可抵达的对象遍历一层，如上图所示，当前可抵达的对象是对象1和对象4，那么自然本轮遍历结束，对象1和对象4就会被标记为灰色，灰色标记表就会多出这两个对象。 ","date":"2022-04-06","objectID":"/go-gc/:3:2","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"第三步 遍历灰色集合，将灰色对象引用的对象从白色集合放入灰色集合，之后将此灰色对象放入黑色集合： 遍历_2遍历_2 \" 遍历_2 这一次遍历是只扫描灰色对象，将灰色对象的第一层遍历可抵达的对象由白色变为灰色，如：对象2、对象7。 而之前的灰色对象1 和对象4 则会被标记为黑色，同时由灰色标记表移动到黑色标记表中。 ","date":"2022-04-06","objectID":"/go-gc/:3:3","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"第四步 重复第三步， 直到灰色中无任何对象，如图👇所示： 遍历_3遍历_3 \" 遍历_3 遍历_4遍历_4 \" 遍历_4 当我们全部的可达对象都遍历完后，灰色标记表将不再存在灰色对象。 目前全部内存的数据只有两种颜色，黑色和白色。那么，黑色对象就是我们程序逻辑可达（需要的）对象，这些数据是目前支撑程序正常业务运行的，是合法的有用数据，不可删除。白色的对象是全部不可达对象，目前程序逻辑并不依赖他们，那么白色对象就是内存中目前的垃圾数据，需要被清除。 ","date":"2022-04-06","objectID":"/go-gc/:3:4","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"第五步 回收所有的白色标记表的对象，也就是回收垃圾，如图所示👇： GC 回收GC 回收 \" GC 回收 将全部的白色对象进行删除回收，剩下的就是全部依赖的黑色对象。 这里面可能会有很多并发流程均会被扫描，执行并发流程的内存可能相互依赖，为了在 GC 过程中保证数据的安全，在开始三色标记之前就会加上 STW，在扫描确定黑白对象之后再放开 STW。但是很明显这样的 GC 扫描的性能实在是太低了。 所以现在的三色标记法还是会 STW。 ","date":"2022-04-06","objectID":"/go-gc/:3:5","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"没有STW的三色标记法 如果没有 STW，那么也就不会再存在性能上的问题，那么假设如果三色标记法不加入STW会发生什么事情❓ 还是基于上述的三色标记法来分析，他是一定要依赖 STW 的，因为如果不暂停程序，程序的逻辑可能会改变对象的引用关系，这种动作如果在标记阶段做了修改，会影响标记结果的正确性。 来看看一个场景，如果三色标记法，标记过程不使用 STW 将会发生什么事情❓ 我们把初始状态设置为已经经历了第一轮扫描，目前黑色的有对象1和对象4，灰色的有对象2和对象7，其他的为白色对象，且对象2是通过指针 p 指向对象3的，如下图所示。 no ST2 01no ST2 01 \" no ST2 01 现在如果三色标记过程不启动 STW，那么在 GC 扫描过程中，任意的对象均可能发生读写操作，如下图所示，在还没有扫描到对象2的时候，已经标记为黑色的对象4，此时创建指针 q，并且指向白色的对象3。 no ST2 02no ST2 02 \" no ST2 02 与此同时灰色的对象2将指针 p 移除，那么白色的对象3实则是被挂在了已经扫描完成的黑色的对象4下，如下图所示。 no ST2 03no ST2 03 \" no ST2 03 然后我们正常执行三色标记的算法逻辑，将所有灰色的对象标记为黑色，那么对象2和对象7就被标记成了黑色，如下图所示。 no ST2 04no ST2 04 \" no ST2 04 那么就执行了三色标记的最后一步，将所有白色对象当做垃圾进行回收，如图所示。 no ST2 05no ST2 05 \" no ST2 05 但是最后我们才发现，本来是对象4合法引用的对象3，却被GC给“误杀”回收掉了。 可以看出，有两种情况，在三色标记法中，是不希望被发生的。 👉 一个白色对象被黑色对象引用 （白色被挂在黑色下） 👉 灰色对象与它之间可达关系的白色对象遭到破坏 （灰色同时丢了该白色） 如果当以上两个条件同时满足时，就会出现对象丢失现象！ 并且，上面所示的场景中，如果示例中的白色对象3还有很多下游对象的话， 也会一并都清理掉。 为了防止这种现象的发生，最简单的方式就是 STW，直接禁止掉其他用户程序对对象引用关系的干扰，但是 STW的过程有明显的资源浪费，对所有的用户程序都有很大影响 。 那么是否可以在保证对象不丢失的情况下合理的尽可能的提高 GC 效率，减少 STW 时间呢❓ 答案是可以的，我们只要使用一种机制，尝试去破坏上面的两个必要条件就可以了。 ","date":"2022-04-06","objectID":"/go-gc/:4:0","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"屏障机制 如果让 GC 回收器，满足下面两种情况之一时，即可保证对象不丢失。这两种方式就是强三色不变式和弱三色不变式。 ","date":"2022-04-06","objectID":"/go-gc/:5:0","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"强三色不变式 强三色不变式实际上是强制性的不允许黑色对象引用白色对象，这样就不会出现有白色对象被误删的情况。 强三色不变式强三色不变式 \" 强三色不变式 ","date":"2022-04-06","objectID":"/go-gc/:5:1","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"弱三色不变式 所有被黑色对象引用的白色对象都处于灰色保护状态。 弱三色不变式弱三色不变式 \" 弱三色不变式 弱三色不变式强调，黑色对象可以引用白色对象，但是这个白色对象必须存在其他灰色对象对它的引用，或者可达它的链路上游存在灰色对象。 这样实则是黑色对象引用白色对象，白色对象处于一个危险被删除的状态，但是由于上游灰色对象的引用，可以保护该白色对象，使其安全。 为了遵循上述的两个方式，GC 算法演进到两种屏障方式，分别是插入屏障和删除屏障。 ","date":"2022-04-06","objectID":"/go-gc/:5:2","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"插入屏障 具体操作： 在 A 对象引用 B 对象的时候，B 对象被标记为灰色。将 B 挂在 A 下游，B 必须被标记为灰色。 满足：强三色不变式。不存在黑色对象引用白色对象的情况了， 因为白色会强制变成灰色。 伪代码如下： 添加下游对象(当前下游对象slot, 新下游对象ptr) { //1 标记灰色(新下游对象ptr) //2 当前下游对象slot = 新下游对象ptr } 场景： A.添加下游对象(nil, B) //A 之前没有下游， 新添加一个下游对象B， B被标记为灰色 A.添加下游对象(C, B) //A 将下游对象C 更换为B， B被标记为灰色 这段伪代码逻辑就是写屏障，我们知道，黑色对象的内存槽有两种位置，栈和堆。 栈空间的特点是容量小，但是要求响应速度快，因为函数调用弹出频繁使用，所以插入屏障机制，在栈空间的对象操作中不使用，而仅仅使用在堆空间对象的操作中。 接下来，我们用几张图，来模拟一下整个详细的过程，希望能更可观的看清整体流程。 插入屏障01插入屏障01 \" 插入屏障01 插入屏障02插入屏障02 \" 插入屏障02 插入屏障03插入屏障03 \" 插入屏障03 插入屏障04插入屏障04 \" 插入屏障04 插入屏障05插入屏障05 \" 插入屏障05 插入屏障06插入屏障06 \" 插入屏障06 但是如果栈不添加，当全部三色标记扫描之后，栈上有可能依然存在白色对象被引用的情况（如上图的对象9）。 所以要对栈重新进行三色标记扫描，但这次为了对象不丢失，要对本次标记扫描启动 STW 暂停，直到栈空间的三色标记结束。 插入屏障07插入屏障07 \" 插入屏障07 插入屏障08插入屏障08 \" 插入屏障08 插入屏障09插入屏障09 \" 插入屏障09 最后将栈和堆空间扫描剩余的全部白色节点清除，这次 STW 大约的时间在 10~100ms 间。 插入屏障10插入屏障10 \" 插入屏障10 ","date":"2022-04-06","objectID":"/go-gc/:5:3","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"删除屏障 具体操作：被删除的对象，如果自身为灰色或者白色，那么被标记为灰色。 满足：弱三色不变式，保护灰色对象到白色对象的路径不会断。 伪代码： 添加下游对象(当前下游对象slot， 新下游对象ptr) { //1 if (当前下游对象slot是灰色 || 当前下游对象slot是白色) { 标记灰色(当前下游对象slot) //slot为被删除对象， 标记为灰色 } //2 当前下游对象slot = 新下游对象ptr } 场景： A.添加下游对象(B, nil) //A对象，删除B对象的引用。 B被A删除，被标记为灰(如果B之前为白) A.添加下游对象(B, C) //A对象，更换下游B变成C。 B被A删除，被标记为灰(如果B之前为白) 接下来，我们用几张图，来模拟一个详细的过程，希望能够更可观的看清楚整体流程。 删除屏障01删除屏障01 \" 删除屏障01 删除屏障02删除屏障02 \" 删除屏障02 删除屏障03删除屏障03 \" 删除屏障03 删除屏障04删除屏障04 \" 删除屏障04 删除屏障05删除屏障05 \" 删除屏障05 删除屏障06删除屏障06 \" 删除屏障06 删除屏障07删除屏障07 \" 删除屏障07 这种方式的回收精度低，一个对象即使被删除了最后一个指向它的指针也依旧可以活过这一轮，在下一轮 GC 中被清理掉。 ","date":"2022-04-06","objectID":"/go-gc/:5:4","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"混合写屏障 插入写屏障和删除写屏障的短板： 插入写屏障：结束时需要 STW 来重新扫描栈，标记栈上引用的白色对象的存活。 删除写屏障：回收精度低，GC 开始时 STW 扫描堆栈来记录初始快照（监控对象的内存修改，判断对象是否删除），这个过程会保护开始时刻的所有存活对象。 Go v1.8 版本引入了混合写屏障机制（hybrid write barrier），避免了对栈 re-scan 的过程，极大的减少了 STW 的时间，结合了两者的优点。 ","date":"2022-04-06","objectID":"/go-gc/:6:0","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"规则 具体操作： GC 开始将栈上的对象全部扫描并标记为黑色（之后不再进行第二次重复扫描，无需 STW ）。 GC 期间，任何在栈上创建的新对象，均为黑色。 被删除的对象标记为灰色。 被添加的对象标记为灰色。 满足：变形的弱三色不变式。 伪代码： 添加下游对象(当前下游对象slot, 新下游对象ptr) { //1 标记灰色(当前下游对象slot) //只要当前下游对象被移走，就标记灰色 //2 标记灰色(新下游对象ptr) //3 当前下游对象slot = 新下游对象ptr } 屏障技术是不在栈上应用的，因为要保证栈的运行效率。 ","date":"2022-04-06","objectID":"/go-gc/:6:1","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"具体场景 我们用几张图，来模拟一个详细的过程，希望能够更可观的看清楚整体流程。 混合写屏障是 GC 的一种屏障机制，所以只是当程序执行 GC 的时候，才会触发这种机制。 GC开始：优先扫描栈区，将可达对象全部标记为黑 混合写屏障01混合写屏障01 \" 混合写屏障01 混合写屏障02混合写屏障02 \" 混合写屏障02 场景一 对象被一个堆对象删除引用，成为栈对象的下游。 伪代码 //前提：堆对象4-\u003e对象7 = 对象7； //对象7 被 对象4引用 栈对象1-\u003e对象7 = 堆对象7； //将堆对象7 挂在 栈对象1 下游 堆对象4-\u003e对象7 = null； //对象4 删除引用 对象7 场景1-01场景1-01 \" 场景1-01 场景1-02场景1-02 \" 场景1-02 场景二 对象被一个栈对象删除引用，成为另一个栈对象的下游。 伪代码： new 栈对象9； 对象8-\u003e对象3 = 对象3； //将栈对象3 挂在 栈对象9 下游 对象2-\u003e对象3 = null； //对象2 删除引用 对象3 场景2-01场景2-01 \" 场景2-01 场景2-02场景2-02 \" 场景2-02 场景2-03场景2-03 \" 场景2-03 场景三 对象被一个堆对象删除引用，成为另一个堆对象的下游。 伪代码： 堆对象10-\u003e对象7 = 堆对象7； //将堆对象7 挂在 堆对象10 下游 堆对象4-\u003e对象7 = null； //对象4 删除引用 对象7 场景3-01场景3-01 \" 场景3-01 场景3-02场景3-02 \" 场景3-02 场景3-03场景3-03 \" 场景3-03 场景四 对象从一个栈对象删除引用，成为另一个堆对象的下游。 伪代码： 堆对象10-\u003e对象7 = 堆对象7； //将堆对象7 挂在 堆对象10 下游 堆对象4-\u003e对象7 = null； //对象4 删除引用 对象7 场景4-01场景4-01 \" 场景4-01 场景4-02场景4-02 \" 场景4-02 场景4-03场景4-03 \" 场景4-03 Go 中的混合写屏障满足弱三色不变式，结合了删除写屏障和插入写屏障的优点，只需要在开始时并发扫描各个 goroutine 的栈，使其变黑并一直保持，这个过程不需要 STW，而标记结束后，因为栈在扫描后始终是黑色的，也无需再进行 re-scan 操作了，减少了 STW 的时间。 ","date":"2022-04-06","objectID":"/go-gc/:6:2","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"总结 GoV1.3：普通标记清除法，整体过程需要启动 STW，效率极低。 GoV1.5：三色标记法，堆空间启动写屏障，栈空间不启动，全部扫描之后，需要重新扫描一次栈(需要 STW )，效率普通。 GoV1.8：三色标记法，混合写屏障机制， 栈空间不启动，堆空间启动。整个过程几乎不需要STW，效率较高。 ","date":"2022-04-06","objectID":"/go-gc/:7:0","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"参考 golang 的GC原理 Golang三色标记+混合写屏障GC模式全分析 ","date":"2022-04-06","objectID":"/go-gc/:8:0","tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["开发者手册"],"content":"github actions replace env vars in file, 将配置文件中的变量替换为环境变量,github actions,github actions 替换配置文件","date":"2022-04-02","objectID":"/github-actions-replace-env-vars-in-file/","tags":["github-actions"],"title":"Github Actions replace env vars in file","uri":"/github-actions-replace-env-vars-in-file/"},{"categories":["开发者手册"],"content":"Github Actions 是个好东西😀，最近在使用的时候有个需求是，我项目不想把设置成私有的，但是有些配置又比较私密，比如 github 的 Personal access token，这种配置就不能暴露出来。 呃，这种需求前辈们估计也遇到过，github actions marketplace 是个好地方，我去里面搜了搜，果然有很多轮子，但是不知道能不能满足需求。 marketplacemarketplace \" marketplace Replace env vars in file 是我选中的一个轮子。 Replace env vars in fileReplace env vars in file \" Replace env vars in file Replace env vars in file 的文档就一句话， Replaces __TOKENS__ with environment variables in file. 我刚开始还不太理解。 好吧，其实是所有的环境变量都必须以__开头，然后以__结尾，这样才能被替换。 我在项目中是这样使用的： 配置文件配置文件 \" 配置文件 action 中替换action 中替换 \" action 中替换 具体可以参看 config.toml 配置文件，workflows 工作流 ","date":"2022-04-02","objectID":"/github-actions-replace-env-vars-in-file/:0:0","tags":["github-actions"],"title":"Github Actions replace env vars in file","uri":"/github-actions-replace-env-vars-in-file/"},{"categories":["开发者手册"],"content":"xiaobinqt,Kubernetes 核心概念，什么是 k8s,k8s 自动化容器平台","date":"2022-04-02","objectID":"/k8s-glance/","tags":["k8s"],"title":"k8s 核心概念","uri":"/k8s-glance/"},{"categories":["开发者手册"],"content":"什么是 Kubernetes Kubernetes（k8s）是自动化容器操作的开源平台，这些操作包括部署，调度和节点集群间扩展。 如果你曾经用过 Docker 容器技术部署容器，那么可以将 Docker 看成 Kubernetes 内部使用的低级别组件。Kubernetes 不仅仅支持 Docker，还支持 Rocket，这是另一种容器技术。 使用 Kubernetes 可以： 自动化容器的部署和复制 随时扩展或收缩容器规模 将容器组织成组，并且提供容器间的负载均衡 很容易地升级应用程序容器的新版本 提供容器弹性，如果容器失效就替换它，等等… 实际上，使用 Kubernetes 只需一个部署文件，使用一条命令就可以部署多层容器（前端，后台等）的完整集群： kubectl create -f single-config-file.yaml kubectl 是和 Kubernetes API 交互的命令行程序。 ","date":"2022-04-02","objectID":"/k8s-glance/:1:0","tags":["k8s"],"title":"k8s 核心概念","uri":"/k8s-glance/"},{"categories":["开发者手册"],"content":"集群 集群是一组节点，这些节点可以是物理服务器或者虚拟机，之上安装了 Kubernetes 平台。下图展示这样的集群。注意该图为了强调核心概念有所简化。这里可以看到一个典型的 Kubernetes 架构图。 集群架构集群架构 \" 集群架构 上图可以看到如下组件，使用特别的图标表示Service和Label： Pod Container（容器） Label （标签） Replication Controller（复制控制器） Service （服务） Node（节点） Kubernetes Master（Kubernetes主节点） ","date":"2022-04-02","objectID":"/k8s-glance/:2:0","tags":["k8s"],"title":"k8s 核心概念","uri":"/k8s-glance/"},{"categories":["开发者手册"],"content":"Pod Pod（上图绿色方框）安排在节点上，包含一组容器和卷。同一个Pod里的容器共享同一个网络命名空间，可以使用localhost互相通信。Pod是短暂的，不是持续性实体。你可能会有这些问题： 如果Pod是短暂的，那么我怎么才能持久化容器数据使其能够跨重启而存在呢？ 是的，Kubernetes支持卷的概念，因此可以使用持久化的卷类型。 是否手动创建Pod，如果想要创建同一个容器的多份拷贝，需要一个个分别创建出来么？可以手动创建单个Pod，但是也可以使用Replication Controller使用Pod模板创建出多份拷贝，下文会详细介绍。 如果Pod是短暂的，那么重启时IP地址可能会改变，那么怎么才能从前端容器正确可靠地指向后台容器呢？这时可以使用Service，下文会详细介绍。 ","date":"2022-04-02","objectID":"/k8s-glance/:3:0","tags":["k8s"],"title":"k8s 核心概念","uri":"/k8s-glance/"},{"categories":["开发者手册"],"content":"Lable 正如图所示，一些Pod有Label（）。一个 Label 是 attach 到 Pod 的一对键/值对，用来传递用户定义的属性。比如，你可能创建了一个“tier”和“app”标签，通过 Label（tier=frontend , app=myapp ）来标记前端 Pod 容器，使用 Label（tier=backend, app=myapp）标记后台 Pod 。然后可以使用 Selectors 选择带有特定 Label 的 Pod，并且将 Service 或者 Replication Controller 应用到上面。 ","date":"2022-04-02","objectID":"/k8s-glance/:4:0","tags":["k8s"],"title":"k8s 核心概念","uri":"/k8s-glance/"},{"categories":["开发者手册"],"content":"Replication Controller 是否手动创建 Pod ，如果想要创建同一个容器的多份拷贝，需要一个个分别创建出来么，能否将 Pods 划到逻辑组里？ Replication Controller 确保任意时间都有指定数量的 Pod “副本”在运行。如果为某个 Pod 创建了 Replication Controller 并且指定3个副本，它会创建 3 个 Pod，并且持续监控它们。如果某个 Pod 不响应，那么 Replication Controller 会替换它，保持总数为3.如下面的动画所示： Replication ControllerReplication Controller \" Replication Controller 如果之前不响应的Pod恢复了，现在就有4个Pod了，那么Replication Controller会将其中一个终止保持总数为3。如果在运行中将副本总数改为5，Replication Controller会立刻启动2个新Pod，保证总数为5。还可以按照这样的方式缩小Pod，这个特性在执行滚动升级时很有用。 当创建Replication Controller时，需要指定两个东西： Pod模板：用来创建Pod副本的模板 Label：Replication Controller需要监控的Pod的标签。 现在已经创建了Pod的一些副本，那么在这些副本上如何均衡负载呢？我们需要的是Service。 ","date":"2022-04-02","objectID":"/k8s-glance/:5:0","tags":["k8s"],"title":"k8s 核心概念","uri":"/k8s-glance/"},{"categories":["开发者手册"],"content":"Service 如果Pods是短暂的，那么重启时IP地址可能会改变，怎么才能从前端容器正确可靠地指向后台容器呢？ Service 是定义一系列 Pod 以及访问这些 Pod 的策略的一层抽象。Service 通过 Label 找到 Pod 组。因为 Service 是抽象的，所以在图表里通常看不到它们的存在，这也就让这一概念更难以理解。 现在，假定有 2 个后台 Pod，并且定义后台 Service 的名称为 ‘backend-service’ ，lable 选择器为（tier=backend, app=myapp）。backend-service 的 Service 会完成如下两件重要的事情： 会为 Service 创建一个本地集群的DNS入口，因此前端Pod只需要DNS查找主机名为 ‘backend-service’，就能够解析出前端应用程序可用的IP地址。 现在前端已经得到了后台服务的 IP 地址，但是它应该访问 2 个后台Pod的哪一个呢？Service 在这 2 个后台 Pod 之间提供透明的负载均衡，会将请求分发给其中的任意一个（如下面的动画所示）。通过每个 Node 上运行的代理（kube-proxy）完成。 下述动画展示了 Service 的功能。注意该图作了很多简化。如果不进入网络配置，那么达到透明的负载均衡目标所涉及的底层网络和路由相对先进。 serviceservice \" service 有一个特别类型的Kubernetes Service，称为 ‘LoadBalancer’ ，作为外部负载均衡器使用，在一定数量的 Pod 之间均衡流量。比如，对于负载均衡Web流量很有用。 ","date":"2022-04-02","objectID":"/k8s-glance/:6:0","tags":["k8s"],"title":"k8s 核心概念","uri":"/k8s-glance/"},{"categories":["开发者手册"],"content":"Node 节点（上图橘色方框）是物理或者虚拟机器，作为Kubernetes worker，通常称为 Minion。每个节点都运行如下 Kubernetes 关键组件： Kubelet：是主节点代理。 Kube-proxy：Service 使用其将链接路由到 Pod，如上文所述。 Docker 或 Rocket：Kubernetes 使用的容器技术来创建容器。 ","date":"2022-04-02","objectID":"/k8s-glance/:7:0","tags":["k8s"],"title":"k8s 核心概念","uri":"/k8s-glance/"},{"categories":["开发者手册"],"content":"Kubernetes Master 集群拥有一个Kubernetes Master（紫色方框）。Kubernetes Master提供集群的独特视角，并且拥有一系列组件，比如Kubernetes API Server。API Server提供可以用来和集群交互的REST端点。master节点包括用来创建和复制Pod的Replication Controller。 ","date":"2022-04-02","objectID":"/k8s-glance/:8:0","tags":["k8s"],"title":"k8s 核心概念","uri":"/k8s-glance/"},{"categories":["开发者手册"],"content":"参考 十分钟带你理解Kubernetes核心概念 ","date":"2022-04-02","objectID":"/k8s-glance/:9:0","tags":["k8s"],"title":"k8s 核心概念","uri":"/k8s-glance/"},{"categories":["开发者手册"],"content":"Gitalk 初始化 issue,Gitalk,自动化,init issue,python 脚本自动初始 gitalk issue,hugo 主题,hugo theme","date":"2022-04-01","objectID":"/gitalk-init-issue/","tags":["gitalk","python","hugo"],"title":"Gitalk 初始化 issue","uri":"/gitalk-init-issue/"},{"categories":["开发者手册"],"content":"在用 Gitalk 作为个人博客评论系统时，发现有个恶心的点是，每篇文章必须手动初始化一个 issue 或是登录 github 后，把文章一个一个点开界面去初始化 issue，不然就会出现以下的提示 no issusno issus \" no issus 个人觉得这件事情非常麻烦，Gitalk 使用 labels 来映射 issuse，可以看下我用的主题 Gitalk 在初始化评论时发出的网络请求 创建 issue 的请求创建 issue 的请求 \" 创建 issue 的请求 labels 第一个参数是 Gitalk，第二个参数是文章的发布时间，呃，感觉改成文章的 path 会更好，但是 github label 的最大长度是 50 个字符，所以把 path md5 会更好。我看了下源码修改成了 URL path 的 md5 格式 themes/LoveIt/layouts/partials/comment.html comment idcomment id \" comment id 初始工作做完，就可以写脚本了。 ","date":"2022-04-01","objectID":"/gitalk-init-issue/:0:0","tags":["gitalk","python","hugo"],"title":"Gitalk 初始化 issue","uri":"/gitalk-init-issue/"},{"categories":["开发者手册"],"content":"分析 我们要做的事其实就是给每篇新文章初始化一个 issue，可以用 github Actions 来做这件事。 初始化 issue 大致逻辑初始化 issue 大致逻辑 \" 初始化 issue 大致逻辑 这里有几个稍微麻烦的地方，以下是我的实现方案，仅仅是提供一个思路。 ","date":"2022-04-01","objectID":"/gitalk-init-issue/:1:0","tags":["gitalk","python","hugo"],"title":"Gitalk 初始化 issue","uri":"/gitalk-init-issue/"},{"categories":["开发者手册"],"content":"获取所有文章信息 怎么获取所有的文章❓，我用的 LoveIt 主题在 build 时在 public 目录里会有一个 index.json 文件，里面包含了所有的文章的信息。 public index.jsonpublic index.json \" public index.json 其他的主题可以使用 sitemap.xml 来获取所有的文章信息，hugo 在 build 时会生成 sitemap.xml 文件。 sitemap.xmlsitemap.xml \" sitemap.xml ","date":"2022-04-01","objectID":"/gitalk-init-issue/:1:1","tags":["gitalk","python","hugo"],"title":"Gitalk 初始化 issue","uri":"/gitalk-init-issue/"},{"categories":["开发者手册"],"content":"issue 如何初始化 issue内容issue内容 \" issue内容 如上截图👆是我创建的 issue 内容。body 是文章的 URL，title 是文章标题，labels 有 Gitalk 和文章的 URL path 的 md5 两个。那么问题就简单了，我们只需要给每篇文章初始化一个这样的 issue 就可以了。 固定文章的 URL 为唯一标识，组成两个 map ，map 键就是文章的 URL。一个 map 是 github 已存在的 issue 暂定为 issue_map，一个 map 是我们所有文章的 map 暂定为 posts_map ，URL 在 posts_map 中存在但是 issue_map 不存在的就是新增 。URL 在 posts_map 和 issue_map 中都存在但是 posts_map 中的标题跟 issue_map 中的标题不相同可能就是文章标题被修改了。 对于新的 URL 我的做法是承认它是新文章，或是旧文章的 URL 被修改了那只能去 github 手动修改 issue body 为新的 URL, label 为新的 uri 的 md5 值。 ","date":"2022-04-01","objectID":"/gitalk-init-issue/:1:2","tags":["gitalk","python","hugo"],"title":"Gitalk 初始化 issue","uri":"/gitalk-init-issue/"},{"categories":["开发者手册"],"content":"python 脚本实现 import hashlib import json import sys import time import requests site_url = \"https://xiaobinqt.github.io\" if len(sys.argv) != 4: print(\"Usage:\") print(sys.argv[0], \"token username repo_name\") sys.exit(1) # issue 的 body 就是文章的 URL token = sys.argv[1] username = sys.argv[2] repo_name = sys.argv[3] issue_map = dict() ## [issue_body] = {\"issue_number\": issue_number, \"issue_title\": issue_title} posts_map = dict() # [post_url] = {\"post_uri\":uri,\"post_date\":date,\"post_title\":title} def get_all_gitalk_issues(token, username, repo_name): for i in range(1, 150): # 15000 个 issue 基本够用了,不够可以再加 _, ret = get_issues_page(i) time.sleep(5) if ret == -1: break ## 删除的文章不管.... ## 文章 title 修改了的文章该怎么处理？ 标题可能修改,但是 uri 不变,issue 的 body 是文章地址,只要文章地址不变，就可以直接 update issue title ## uri 如果也变了，相当于是文件的重命名了，这时只能去手动 update issue title 了?..... def update_issue(issue_number, title): if title == \"\": return url = 'https://api.github.com/repos/%s/%s/issues/%d' % (username, repo_name, issue_number) print(\"update_issue url: %s\" % url) data = { 'title': title, } print(\"create_issue req json: %s\" % json.dumps(data)) r = requests.patch(url, data=json.dumps(data), headers={ \"Authorization\": \"token %s\" % token, }, verify=False) if r.status_code == 200: print(\"update_issue success\") else: print(\"update_issue fail, status_code: %d,title: %s,issue_number: %d\" % (r.status_code, title, issue_number)) # 获取所有 label 为 gitalk 的 issue def get_issues_page(page=1): url = 'https://api.github.com/repos/%s/%s/issues?labels=Gitalk\u0026per_page=100\u0026page=%d' % (username, repo_name, page) print(\"get_issues url: %s\" % url) r = requests.get(url, headers={ \"Authorization\": \"token %s\" % token, \"Accept\": \"application/vnd.github.v3+json\" }) if r.status_code != 200: print(\"get_issues_page fail, status_code: %d\" % r.status_code) sys.exit(2) if r.json() == []: return (issue_map, -1) for issue in r.json(): if issue['body'] not in issue_map and issue[\"body\"] != \"\": issue_map[issue['body']] = { \"issue_number\": issue['number'], \"issue_title\": issue['title'] } return (issue_map, 0) # 通过 public/index.json 获取所有的文章 def get_post_titles(): with open(file='public/index.json', mode='r', encoding='utf-8') as f: file_data = f.read() if file_data == \"\" or file_data == [] or file_data == {}: return posts_map file_data = json.loads(file_data) for data in file_data: key = \"%s%s\" % (site_url, data['uri']) if key not in posts_map: posts_map[key] = { \"post_uri\": data['uri'], \"post_date\": data['date'], \"post_title\": data['title'] } return posts_map def create_issue(title=\"\", uri=\"\", date=\"\"): if title == \"\": return url = 'https://api.github.com/repos/%s/%s/issues' % (username, repo_name) print(\"create_issue title: %suri: %sdate: %s\" % (title, uri, date)) m = hashlib.md5() m.update(uri.encode('utf-8')) urlmd5 = m.hexdigest() data = { 'title': title, 'body': '%s%s' % (site_url, uri), 'labels': [ 'Gitalk', urlmd5 ] } print(\"create_issue req json: %s\" % json.dumps(data)) r = requests.post(url, data=json.dumps(data), headers={ \"Authorization\": \"token %s\" % token, }) if r.status_code == 201: print(\"create_issue success\") else: print(\"create_issue fail, status_code: %d,title: %s,req url: %s\\n\" % (r.status_code, title, url)) # 创建 gitalk 创建 issue,如果 issue 已经存在，则不创建 def init_gitalk(): for post_url, item in posts_map.items(): ## 标题被修改了 if post_url in issue_map and item['post_title'] != issue_map[post_url]['issue_title']: update_issue(issue_map[post_url][\"issue_number\"], item['post_title']) elif post_url not in issue_map: # 新增的文章 print(\"title: [%s] , body [%s] issue 不存在,创建...\" % (item[\"post_title\"], post_url)) create_issue(item[\"post_title\"], item[\"post_uri\"], item[\"post_date\"]) # 延迟 5 秒，防止 github api 请求过于频繁： https://docs.github.com/en/rest/guides/best-practices-for-integrators#dealing-with-secondary-rate-limits time.sleep(5) def get_uri_md5(uri): m = hashlib.md5() m.update(uri.encode('utf-8')) return m.hexdigest() if __name__ == \"__main__\": # print(get_uri_md5(\"/gmp-model/\")) ## 执行.... get_all_gitalk_issues(token, username, ","date":"2022-04-01","objectID":"/gitalk-init-issue/:2:0","tags":["gitalk","python","hugo"],"title":"Gitalk 初始化 issue","uri":"/gitalk-init-issue/"},{"categories":["开发者手册"],"content":"参考 自动初始化 Gitalk 和 Gitment 评论 利用 Github Action 自动初始化 Gitalk 评论之Python篇 ","date":"2022-04-01","objectID":"/gitalk-init-issue/:3:0","tags":["gitalk","python","hugo"],"title":"Gitalk 初始化 issue","uri":"/gitalk-init-issue/"},{"categories":["开发者手册"],"content":"Node-red,Low-code,自定义nodered节点,nodered,节点开发,how to create node-red node","date":"2022-04-01","objectID":"/node-red-glance/","tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/"},{"categories":["开发者手册"],"content":"概述 Node-RED 是构建物联网 (IOT,Internet of Things) 应用程序的一个强大工具，其重点是简化代码块的“连接\"以执行任务。它使用可视 化编程方法，允许开发人员将预定义的代码块（称为“节点”，Node) 连接起来执行任务。连接的节点，通常是输入节点、处理节点和输出节点的组合，当它们连接在一起时，构成一个“流”(Flows)。 ","date":"2022-04-01","objectID":"/node-red-glance/:1:0","tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/"},{"categories":["开发者手册"],"content":"安装node-red 安装 node-red 的方式大致有 2 种，使用 docker 和 npm ，docker 安装可以参考。这里使用 npm 安装。个人觉得在本地调试 npm 比 docker 更方便一点，源码都在本地，docker 的话还需要把目录映射出来。 npm 安装直接一行命令就可以搞定，具体可以参考 npm i node-red 安装成功后，会在用户目录下生成一个 .node-red 目录，我用的是 Windows 系统，所以这里的目录是 C:\\Users\\weibin\\.node-red，这个目录下有配置文件 settings.js，里面有一些 node-red 配置项，比如默认端口等。 node-red 目录node-red 目录 \" node-red 目录 ","date":"2022-04-01","objectID":"/node-red-glance/:2:0","tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/"},{"categories":["开发者手册"],"content":"启动 安装完成后，直接执行 node-red 就可以启动服务。 cmd 启动 node-redcmd 启动 node-red \" cmd 启动 node-red node-red 的默认端口是 1880，直接用浏览器访问 http://127.0.0.1:1880 就可以看到 node-red 的页面。 node-red 界面node-red 界面 \" node-red 界面 ","date":"2022-04-01","objectID":"/node-red-glance/:3:0","tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/"},{"categories":["开发者手册"],"content":"创建自定义节点 每一个 node-red 节点都是一个 npm 包，开发 npm 节点跟开发 npm 组件包是一样。 一个 node-red 节点主要包括两个文件，一个是 html 文件，一个是 js 文件。html 是界面配置，js 处理逻辑，加上 npm 的 package.json 文件，正常三个文件就可以实现一个 node-red 节点。 ","date":"2022-04-01","objectID":"/node-red-glance/:4:0","tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/"},{"categories":["开发者手册"],"content":"加法器节点开发 我们创建一个自定义节点实现一个加法器，输入两个数字，输出两个数字的和。 ","date":"2022-04-01","objectID":"/node-red-glance/:4:1","tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/"},{"categories":["开发者手册"],"content":"新建项目 我们新建一个节点项目 node-sum，这个项目随便放在那个目录下都行，这里我的目录是 D:\\tmp\\node-sum。 新建项目新建项目 \" 新建项目 ","date":"2022-04-01","objectID":"/node-red-glance/:4:2","tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/"},{"categories":["开发者手册"],"content":"npm 初始化 切到项目目录下，执行 npm init 将项目进行 npm 初始化，然后根据提示填写即可。 npm initnpm init \" npm init 用 IDE 打开 node-sum 项目就可以看到已经给我们初始化好了 package.json 文件。 package.jsonpackage.json \" package.json ","date":"2022-04-01","objectID":"/node-red-glance/:4:3","tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/"},{"categories":["开发者手册"],"content":"功能实现 sum.html \u003cscript type=\"text/javascript\"\u003e RED.nodes.registerType('sum', { // 这个值 必须和 js 中 RED.nodes.registerType 的值一致 category: '自定义节点', // 分类 color: '#a6bbcf', // 节点颜色 defaults: { name: {value: \"\"}, // name 默认是空 add1: {value: 0}, // add1 默认值 0 add2: {value: 0}, // add2 默认值 0 }, inputs: 0, // 节点有多少输入 0 或者多个 outputs: 1, // 节点有多少输出 0 或者多个 icon: \"file.png\", // 节点使用的图标 paletteLabel: \"加法器\", // 节点显示的名称 label: function () { // 节点的工作区的标签 return this.name || \"加法器\"; }, // 钩子函数,双节节点调出 template 时触发 oneditprepare: function () { console.log(\"oneditprepare 被调用\"); }, // 钩子函数,点击 template 中的完成按钮时触发 oneditsave: function () { console.log(\"oneditsave 被调用\"); } }); \u003c/script\u003e \u003c!--data-template-name 必须和 js 中 RED.nodes.registerType 的值一致 --\u003e \u003c!--template 是模板，可以理解成表单，节点需要的信息可以从这里输入--\u003e \u003cscript type=\"text/html\" data-template-name=\"sum\"\u003e \u003cdiv class=\"form-row\"\u003e \u003clabel for=\"node-input-name\"\u003e\u003ci class=\"fa fa-tag\"\u003e\u003c/i\u003e Name\u003c/label\u003e \u003cinput type=\"text\" id=\"node-input-name\" placeholder=\"Name\"\u003e \u003c/div\u003e \u003cdiv class=\"form-row\"\u003e \u003clabel for=\"node-input-add1\"\u003e\u003ci class=\"fa fa-tag\"\u003e\u003c/i\u003e加数1\u003c/label\u003e \u003cinput type=\"text\" id=\"node-input-add1\" placeholder=\"加数1\"\u003e \u003c/div\u003e \u003cdiv class=\"form-row\"\u003e \u003clabel for=\"node-input-add2\"\u003e\u003ci class=\"fa fa-tag\"\u003e\u003c/i\u003e加数2\u003c/label\u003e \u003cinput type=\"text\" id=\"node-input-add2\" placeholder=\"加数2\"\u003e \u003c/div\u003e \u003c/script\u003e \u003c!--data-help-name 必须和 js 中 RED.nodes.registerType 的值一致 --\u003e \u003c!--help 是节点的帮助文档--\u003e \u003cscript type=\"text/html\" data-help-name=\"sum\"\u003e \u003cp\u003e一个简单的加法器\u003c/p\u003e \u003c/script\u003e sum.js module.exports = function (RED) { function Sum(config) { RED.nodes.createNode(this, config); var node = this; // 获取输入的参数 let add1 = parseInt(config.add1) let add2 = parseInt(config.add2) node.send({ // 向下一个节点输出信息 payload: `${add1}+ ${add2}结果为 ` + (add1 + add2) }); node.on('input', function (msg) { // 接收上游节点接收消息 }); } // 注册一个节点 sum,注册的节点不能重复也就是说同一个 node-red 项目不能有 2 个 registerType sum 节点 RED.nodes.registerType(\"sum\", Sum); } 需要在 package.json 文件里添加 node-red 信息，完整的 package.json 如下： { \"name\": \"node-sum\", \"version\": \"1.0.0\", \"description\": \"node-red 加法器\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" \u0026\u0026 exit 1\" }, \"keywords\": [ \"node-red\", \"add\" ], \"author\": \"xiaobinqt@163.com\", \"license\": \"ISC\", \"node-red\": { \"nodes\": { \"sum\": \"sum.js\" } } } 在 package.json 中添加的 node-red 信息是固定写法，可以理解成向 node-red 中注册了 nodes 的名称为 sum，注册的 js 文件为 sum.js。 \"node-red\": { \"nodes\": { \"sum\": \"sum.js\" } } ","date":"2022-04-01","objectID":"/node-red-glance/:4:4","tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/"},{"categories":["开发者手册"],"content":"本地安装 可以通过 npm i 安装刚才的 sum 节点到 node-red 中。切到.node-red 目录下，执行 npm i d:\\tmp\\node-sum 安转本地节点并重启安转本地节点并重启 \" 安转本地节点并重启 然后重启 node-red 就可以看到刚才安装的节点了。 节点安装成功节点安装成功 \" 节点安装成功 ","date":"2022-04-01","objectID":"/node-red-glance/:4:5","tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/"},{"categories":["开发者手册"],"content":"测试功能 把节点拖到工作区，双击节点（会触发oneditprepare函数）打开编辑区 双节节点填写编辑区双节节点填写编辑区 \" 双节节点填写编辑区 填写完编辑区内容后点击完成（会触发oneditsave函数），点击部署就会在调试窗口输出 node.send 信息。 部署部署 \" 部署 ","date":"2022-04-01","objectID":"/node-red-glance/:4:6","tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/"},{"categories":["开发者手册"],"content":"参考 Creating your first node Design: i18n ","date":"2022-04-01","objectID":"/node-red-glance/:5:0","tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/"},{"categories":["开发者手册"],"content":"https,google强制跳到https,ERR_SSL_PROTOCOL_ERROR,How to Stop Chrome from Automatically Redirecting to https","date":"2022-03-29","objectID":"/stop-chrome-auto-redirect-2-https/","tags":["chrome","http/https"],"title":"禁止Google浏览器强制跳转https","uri":"/stop-chrome-auto-redirect-2-https/"},{"categories":["开发者手册"],"content":"这几天在使用 google 浏览器打开公司的一个网站时，发现总是自动跳转到 https，以至于出现下面这个页面： ERR_SSL_PROTOCOL_ERRORERR_SSL_PROTOCOL_ERROR \" ERR_SSL_PROTOCOL_ERROR 有时候浏览器太智能了也不是一件好事🤣。 ","date":"2022-03-29","objectID":"/stop-chrome-auto-redirect-2-https/:0:0","tags":["chrome","http/https"],"title":"禁止Google浏览器强制跳转https","uri":"/stop-chrome-auto-redirect-2-https/"},{"categories":["开发者手册"],"content":"解决方法 复制链接 chrome://net-internals/#hsts用 Google 浏览器打开，这个页面，在最下面的 Delete domain security policies 填上需要禁止跳转的网站，然后点击Delete。 Delete domain security policiesDelete domain security policies \" Delete domain security policies 这里有个需要注意的地方是，如果我们的网址是 http://g.xiaobinqt.cn:8000，那么Domain 的值填的是 xiaobinqt.cn。 ","date":"2022-03-29","objectID":"/stop-chrome-auto-redirect-2-https/:1:0","tags":["chrome","http/https"],"title":"禁止Google浏览器强制跳转https","uri":"/stop-chrome-auto-redirect-2-https/"},{"categories":["开发者手册"],"content":"参考 How to Stop Chrome from Automatically Redirecting to https ","date":"2022-03-29","objectID":"/stop-chrome-auto-redirect-2-https/:2:0","tags":["chrome","http/https"],"title":"禁止Google浏览器强制跳转https","uri":"/stop-chrome-auto-redirect-2-https/"},{"categories":["开发者手册"],"content":"hugo,algolia,algoliasearch,exceptions.AlgoliaUnreachableHostException: Unreachable hosts, algolia索引","date":"2022-03-28","objectID":"/hugo-algolia/","tags":["hugo","algolia"],"title":"hugo algolia Unreachable hosts","uri":"/hugo-algolia/"},{"categories":["开发者手册"],"content":"最近在使用 hugo algolia 时，在 github actions 同步索引到 algolia 时总是出现这样的错误： action error listaction error list \" action error list Unreachable hostsUnreachable hosts \" Unreachable hosts 我用的 action 插件是Algolia Index Uploader，找了半天发现是参数 algolia_index_id 写的有问题😥： algolia_index_id 填的值algolia_index_id 填的值 \" algolia_index_id 填的值 上传成功后可以去 algolia 官网查看效果： Settings -\u003e Applications -\u003e 进入到应用 -\u003e Search -\u003e Browse 上传索引效果上传索引效果 \" 上传索引效果 ","date":"2022-03-28","objectID":"/hugo-algolia/:0:0","tags":["hugo","algolia"],"title":"hugo algolia Unreachable hosts","uri":"/hugo-algolia/"},{"categories":["开发者手册"],"content":"参考 Algolia Hosts unreachable ","date":"2022-03-28","objectID":"/hugo-algolia/:1:0","tags":["hugo","algolia"],"title":"hugo algolia Unreachable hosts","uri":"/hugo-algolia/"},{"categories":["理解计算机"],"content":"TCP,UDP,网络模型,实体层,链接层,网络层,传输层,应用层,网络数据包,以太网协议,MAC地址,广播,Physical Layer,Application Layer,Transport Layer,Network Layer,Internet Protocol Suite,Ethernet,subnet mask,IPv4,IPv6,互联网协议的通信过程","date":"2022-03-27","objectID":"/net-protocol-glance/","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"概述 ","date":"2022-03-27","objectID":"/net-protocol-glance/:1:0","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"五层模型 互联网的实现，分成好几层。每一层都有自己的功能，就像建筑物一样，每一层都靠下一层支持。 用户接触到的，只是最上面的一层，根本没有感觉到下面的层。理解互联网，需要从最下层开始，自下而上理解每一层的功能。 如何分层有不同的模型，有的模型分七层，有的分四层。把互联网分成五层，比较容易解释。 五层模型五层模型 \" 五层模型 如上图所示，最底下的一层叫做\"实体层\"（Physical Layer），最上面的一层叫做\"应用层\"（Application Layer），中间的三层（自下而上）分别是\"链接层\"（Link Layer）、“网络层”（Network Layer）和\"传输层\"（Transport Layer）。越下面的层，越靠近硬件；越上面的层，越靠近用户。 名字只是一个代号，它们叫什么名字，其实并不重要。只需要知道，互联网分成若干层就可以了。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:1:1","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"层与协议 每一层都是为了完成一种功能。为了实现这些功能，就需要大家都遵守共同的规则。 大家都遵守的规则，就叫做\"协议\"（protocol）。 互联网的每一层，都定义了很多协议。这些协议的总称，就叫做\"互联网协议\"（Internet Protocol Suite），它们是互联网的核心。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:1:2","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"实体层 电脑要组网，第一件事是先把电脑连起来，可以用光缆、电缆、双绞线、无线电波等方式。 这就叫做\"实体层\"，它就是把电脑连接起来的物理手段。它主要规定了网络的一些电气特性，作用是负责传送 0 和 1 的电信号。 实体层实体层 \" 实体层 ","date":"2022-03-27","objectID":"/net-protocol-glance/:2:0","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"链接层 单纯的 0 和 1 没有任何意义，必须规定解读方式：多少个电信号算一组？每个信号位有何意义？ 这就是\"链接层\"的功能，它在\"实体层\"的上方，确定了 0 和 1 的分组方式。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:3:0","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"以太网协议 早期的时候，每家公司都有自己的电信号分组方式。逐渐地，一种叫做\"以太网\"（Ethernet）的协议，占据了主导地位。 以太网规定，一组电信号构成一个数据包，叫做\"帧\"（Frame）。每一帧分成两个部分：标头（Head）和数据（Data）。 head-datahead-data \" head-data “标头\"包含数据包的一些说明项，比如发送者、接受者、数据类型等等；“数据\"则是数据包的具体内容。 “标头\"的长度，固定为 18 字节。“数据\"的长度，最短为 46 字节，最长为 1500 字节。因此，整个\"帧\"最短为 64 字节，最长为 1518 字节。如果数据很长，就必须分割成多个帧进行发送。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:3:1","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"MAC 地址 以太网数据包的\"标头”，包含了发送者和接受者的信息。那么，发送者和接受者是如何标识呢？ 以太网规定，连入网络的所有设备，都必须具有\"网卡\"接口。数据包必须是从一块网卡，传送到另一块网卡。网卡的地址，就是数据包的发送地址和接收地址，这叫做 MAC 地址。 每块网卡出厂的时候，都有一个全世界独一无二的 MAC 地址，长度是 48 个二进制位，通常用 12 个十六进制数表示。 前 6 个十六进制数是厂商编号，后 6 个是该厂商的网卡流水号。有了 MAC 地址，就可以定位网卡和数据包的路径了。 MAC addressMAC address \" MAC address 上图的 MAC 地址的二进制位为 00000000-10110000-11010000-10000110-10111011-11110111。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:3:2","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"广播 以太网数据包必须知道接收方的 MAC 地址，然后才能发送，那么问题来了， 一块网卡怎么会知道另一块网卡的MAC地址？ 就算有了 MAC 地址，系统怎样才能把数据包准确送到接收方？ 回答是以太网采用了一种很\"原始\"的方式，它不是把数据包准确送到接收方，而是向本网络内所有计算机发送，让每台计算机自己判断，是否为接收方。 广播广播 \" 广播 上图中，1 号计算机向 2 号计算机发送一个数据包，同一个子网络的 3 号、4 号、5 号计算机都会收到这个包。它们读取这个包的\"标头”，找到接收方的 MAC 地址，然后与自身的 MAC 地址相比较，如果两者相同，就接受这个包，做进一步处理，否则就丢弃这个包。这种发送方式就叫做\"广播”（broadcasting）。 有了数据包的定义、网卡的 MAC 地址、广播的发送方式，“链接层\"就可以在多台计算机之间传送数据了。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:3:3","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"网络层 ","date":"2022-03-27","objectID":"/net-protocol-glance/:4:0","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"网络层的由来 以太网协议，依靠 MAC 地址发送数据。理论上，单单依靠 MAC 地址，上海的网卡就可以找到洛杉矶的网卡了，技术上是可以实现的。 但是，这样做有一个重大的缺点。以太网采用广播方式发送数据包，所有成员人手一\"包”，不仅效率低，而且局限在发送者所在的子网络。也就是说，如果两台计算机不在同一个子网络，广播是传不过去的 。这种设计是合理的，否则互联网上每一台计算机都会收到所有包，那会引起灾难。 互联网是无数子网络共同组成的一个巨型网络，很像想象上海和洛杉矶的电脑会在同一个子网络，这几乎是不可能的。 子网络子网络 \" 子网络 因此，必须找到一种方法，能够区分哪些 MAC 地址属于同一个子网络，哪些不是。如果是同一个子网络，就采用广播方式发送，否则就采用\"路由\"方式发送。（“路由\"的意思，就是指如何向不同的子网络分发数据包。），MAC 地址本身无法做到这一点，它只与厂商有关，与所处网络无关。 这就导致了\"网络层\"的诞生。它的作用是引进一套新的地址，使得我们能够区分不同的计算机是否属于同一个子网络。这套地址就叫做\"网络地址”，简称\"网址”。 于是，“网络层\"出现以后，每台计算机有了两种地址，一种是 MAC 地址，另一种是网络地址。两种地址之间没有任何联系，MAC 地址是绑定在网卡上的，网络地址则是管理员分配的，它们只是随机组合在一起。 网络地址帮助我们确定计算机所在的子网络，MAC 地址则将数据包送到该子网络中的目标网卡。因此，从逻辑上可以推断，必定是先处理网络地址，然后再处理 MAC 地址。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:4:1","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"IP协议和子网掩码 规定网络地址的协议，叫做 IP 协议。它所定义的地址，就被称为 IP 地址。 目前，广泛采用的是 IP 协议第四版，简称 IPv4。这个版本规定，网络地址由 32 个二进制位组成。 IP协议IP协议 \" IP协议 习惯上，我们用分成四段的十进制数表示 IP 地址，从 0.0.0.0 一直到 255.255.255.255。 互联网上的每一台计算机，都会分配到一个 IP 地址。 IP 地址分成两个部分，前一部分代表网络，后一部分代表主机。 比如，IP 地址 172.16.254.1，这是一个 32 位的地址，假定它的网络部分是前 24 位（172.16.254），那么主机部分就是后 8 位（最后的那个 1 ）。处于同一个子网络的电脑，它们 IP 地址的网络部分必定是相同的，也就是说 172.16.254.2 应该与 172.16.254.1 处在同一个子网络。 单单从 IP 地址，我们无法判断网络部分。还是以 172.16.254.1 为例，它的网络部分，到底是前 24 位，还是前 16 位，甚至前 28 位，从 IP 地址上是看不出来的。 那么，怎样才能从IP地址，判断两台计算机是否属于同一个子网络呢？这就要用到另一个参数\"子网掩码”（subnet mask）。 所谓 “子网掩码”，就是表示子网络特征的一个参数。它在形式上等同于 IP 地址，也是一个 32 位二进制数字，它的网络部分全部为 1，主机部分全部为 0 。比如，IP 地址 172.16.254.1 ，如果已知网络部分是前 24 位，主机部分是后 8 位，那么子网络掩码就是 11111111.11111111.11111111.00000000，写成十进制就是 255.255.255.0。 知道\"子网掩码\"，我们就能判断，任意两个 IP 地址是否处在同一个子网络。方法是将两个 IP 地址与子网掩码分别进行 AND 运算（两个数位都为 1 ，运算结果为 1，否则为 0），然后比较结果是否相同，如果是的话，就表明它们在同一个子网络中，否则就不是。 比如，已知IP地址 172.16.254.1 和 172.16.254.233 的子网掩码都是 255.255.255.0，请问它们是否在同一个子网络？两者与子网掩码分别进行 AND 运算，结果都是 172.16.254.0，因此它们在同一个子网络。 10101100.00010000.11111110.00000001 # 172.16.254.1 11111111.11111111.11111111.00000000 # 255.255.255.0 10101100.00010000.11111110.00000000 # AND 结果二进制位 172.16.254.0 # AND 结果转成十进制 所以，IP 协议的作用主要有两个，一个是为每一台计算机分配 IP 地址，另一个是确定哪些地址在同一个子网络。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:4:2","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"IP数据包 根据 IP 协议发送的数据，就叫做 IP 数据包。不难想象，其中必定包括 IP 地址信息。 但是前面说过，以太网数据包只包含 MAC 地址，并没有 IP 地址的栏位。那么是否需要修改数据定义，再添加一个栏位呢？ 回答是不需要，我们可以把 IP 数据包直接放进以太网数据包的\"数据\"部分，因此完全不用修改以太网的规格。这就是互联网分层结构的好处：上层的变动完全不涉及下层的结构。 具体来说，IP 数据包也分为\"标头\"和\"数据\"两个部分。 IP数据包1IP数据包1 \" IP数据包1 “标头\"部分主要包括版本、长度、IP 地址等信息，“数据\"部分则是 IP 数据包的具体内容。它放进以太网数据包后，以太网数据包就变成了下面这样。 IP数据包2IP数据包2 \" IP数据包2 IP 数据包的“标头”部分的长度为 20 到 60 字节，整个数据包的总长度最大为 65,535 字节。因此，理论上，一个 IP 数据包的\"数据\"部分，最长为 65,515 字节。前面说过，以太网数据包的\"数据\"部分，最长只有 1500 字节。因此，如果 IP 数据包超过了 1500 字节（上图红色部分），它就需要分割成几个以太网数据包，分开发送了。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:4:3","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"ARP协议 由于 IP 数据包是放在以太网数据包里发送的，所以我们必须同时知道两个地址，一个是对方的 MAC 地址，另一个是对方的 IP 地址。通常情况下，对方的 IP 地址是已知的，但是我们不知道它的 MAC 地址。 所以，我们需要一种机制，能够从 IP 地址得到 MAC 地址。 这里又可以分成两种情况。 第一种情况，如果两台主机不在同一个子网络，那么事实上没有办法得到对方的 MAC 地址，只能把数据包传送到两个子网络连接处的\"网关”（gateway），让网关去处理。 第二种情况，如果两台主机在同一个子网络，那么我们可以用 ARP 协议，得到对方的 MAC 地址。ARP 协议也是发出一个数据包（包含在以太网数据包中），其中包含它所要查询主机的 IP 地址，在对方的 MAC 地址这一栏，填的是FF:FF:FF:FF:FF:FF，表示这是一个\"广播” 地址。它所在子网络的每一台主机，都会收到这个数据包，从中取出 IP 地址，与自身的 IP 地址进行比较。如果两者相同，都做出回复，向对方报告自己的 MAC 地址，否则就丢弃这个包。 有了 ARP 协议之后，我们就可以得到同一个子网络内的主机 MAC 地址，可以把数据包发送到任意一台主机之上。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:4:4","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"传输层 ","date":"2022-03-27","objectID":"/net-protocol-glance/:5:0","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"传输层的由来 有了 MAC 地址和 IP 地址，我们已经可以在互联网上任意两台主机上建立通信。 接下来的问题是，同一台主机上有许多程序都需要用到网络，比如，你一边浏览网页，一边与朋友在线聊天。当一个数据包从互联网上发来的时候，你怎么知道，它是表示网页的内容，还是表示在线聊天的内容？ 也就是说，我们还需要一个参数，表示这个数据包到底供哪个程序（进程）使用。这个参数就叫做\"端口\"（port），它其实是每一个使用网卡的程序的编号。每个数据包都发到主机的特定端口，所以不同的程序就能取到自己所需要的数据。 “端口\"是 0 到 65535 之间的一个整数，正好 16 个二进制位。0 到 1023 的端口被系统占用，用户只能选用大于 1023 的端口。不管是浏览网页还是在线聊天，应用程序会随机选用一个端口，然后与服务器的相应端口联系。 “传输层\"的功能，就是建立\"端口到端口\"的通信。相比之下，“网络层\"的功能是建立\"主机到主机\"的通信。只要确定主机和端口，我们就能实现程序之间的交流。因此，Unix系统就把主机+端口，叫做\"套接字” （socket）。有了它，就可以进行网络应用程序开发了。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:5:1","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"UDP 协议 我们必须在数据包中加入端口信息，这就需要新的协议。最简单的实现叫做 UDP 协议，它的格式几乎就是在数据前面，加上端口号。 UDP 数据包，也是由\"标头\"和\"数据\"两部分组成。 UDP数据格式_1UDP数据格式_1 \" UDP数据格式_1 “标头\"部分主要定义了发出端口和接收端口，“数据\"部分就是具体的内容。然后，把整个 UDP 数据包放入 IP 数据包的\"数据\"部分，而前面说过，IP 数据包又是放在以太网数据包之中的，所以整个以太网数据包现在变成了下面这样： UDP数据格式_2UDP数据格式_2 \" UDP数据格式_2 UDP 数据包非常简单，“标头\"部分一共只有 8 个字节，总长度不超过 65,535 字节，正好放进一个IP数据包。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:5:2","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"TCP 协议 UDP 协议的优点是比较简单，容易实现，但是缺点是可靠性较差，一旦数据包发出，无法知道对方是否收到。 为了解决这个问题，提高网络可靠性，TCP 协议就诞生了。这个协议非常复杂，但可以近似认为，它就是有确认机制的 UDP 协议，每发出一个数据包都要求确认。如果有一个数据包遗失，就收不到确认，发出方就知道有必要重发这个数据包了。 因此，TCP 协议能够确保数据不会遗失。它的缺点是过程复杂、实现困难、消耗较多的资源。 TCP 数据包和 UDP 数据包一样，都是内嵌在 IP 数据包的“数据”部分。TCP 数据包没有长度限制，理论上可以无限长，但是为了保证网络的效率，通常 TCP 数据包的长度不会超过 IP 数据包的长度，以确保单个 TCP 数据包不必再分割。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:5:3","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"应用层 应用程序收到\"传输层\"的数据，接下来就要进行解读。由于互联网是开放架构，数据来源五花八门，必须事先规定好格式，否则根本无法解读。 “应用层”的作用，就是规定应用程序的数据格式。 举例来说，TCP 协议可以为各种各样的程序传递数据，比如 Email、WWW、FTP 等等。那么，必须有不同协议规定电子邮件、网页、FTP 数据的格式，这些应用程序协议就构成了\"应用层”。 这是最高的一层，直接面对用户。它的数据就放在 TCP 数据包的\"数据\"部分。因此，现在的以太网的数据包就变成下面这样。 应用层数据包应用层数据包 \" 应用层数据包 ","date":"2022-03-27","objectID":"/net-protocol-glance/:6:0","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"小结 网络通信就是交换数据包。电脑 A 向电脑 B 发送一个数据包，后者收到了，回复一个数据包，从而实现两台电脑之间的通信。数据包的结构，基本上是下面这样： 数据包数据包 \" 数据包 发送这个包，需要知道两个地址： 对方的 MAC 地址 对方的 IP 地址 有了这两个地址，数据包才能准确送到接收者手中。但是，MAC 地址有局限性，如果两台电脑不在同一个子网络，就无法知道对方的 MAC 地址，必须通过网关（gateway）转发。 网关网关 \" 网关 上图中，1 号电脑要向 4 号电脑发送一个数据包。它先判断 4 号电脑是否在同一个子网络，结果发现不是，于是就把这个数据包发到网关 A。网关 A 通过路由协议，发现 4 号电脑位于子网络 B，又把数据包发给网关 B，网关 B 再转发到 4 号电脑。 1 号电脑把数据包发到网关 A，必须知道网关 A 的 MAC 地址。所以，数据包的目标地址，实际上分成两种情况： 场景 数据包地址 同一个子网络 对方的MAC地址，对方的IP地址 非同一个子网络 网关的MAC地址，对方的IP地址 发送数据包之前，电脑必须判断对方是否在同一个子网络，然后选择相应的 MAC 地址。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:7:0","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"用户的上网设置 ","date":"2022-03-27","objectID":"/net-protocol-glance/:8:0","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"静态IP地址 new computercomputer \" new computer 通常你必须做一些设置。有时，管理员会告诉你下面四个参数，你把它们填入操作系统，计算机就能连上网了： 本机的IP地址 子网掩码 网关的IP地址 DNS的IP地址 下图是Windows系统的设置窗口。 系统设置系统设置 \" 系统设置 这四个参数缺一不可。由于它们是给定的，计算机每次开机，都会分到同样的IP地址，所以这种情况被称作\"静态IP地址上网”。 但是，这样的设置很专业，普通用户望而生畏，而且如果一台电脑的IP地址保持不变，其他电脑就不能使用这个地址，不够灵活。出于这两个原因，大多数用户使用\"动态IP地址上网”。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:8:1","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"动态IP地址 所谓\"动态IP地址”，指计算机开机后，会自动分配到一个IP地址，不用人为设定。它使用的协议叫做DHCP协议。 这个协议规定，每一个子网络中，有一台计算机负责管理本网络的所有IP地址，它叫做\"DHCP服务器”。新的计算机加入网络，必须向\"DHCP服务器\"发送一个\"DHCP请求\"数据包，申请IP地址和相关的网络参数。 前面说过，如果两台计算机在同一个子网络，必须知道对方的MAC地址和IP地址，才能发送数据包。但是，新加入的计算机不知道这两个地址，怎么发送数据包呢？ DHCP协议做了一些巧妙的规定。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:8:2","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"DHCP协议 首先，它是一种应用层协议，建立在UDP协议之上，所以整个数据包是这样的： HDCP协议数据包HDCP协议数据包 \" HDCP协议数据包 最前面的\"以太网标头\"，设置发出方（本机）的MAC地址和接收方（DHCP服务器）的MAC地址。前者就是本机网卡的MAC地址，后者这时不知道，就填入一个广播地址：FF-FF-FF-FF-FF-FF。 后面的\"IP标头\"，设置发出方的IP地址和接收方的IP地址。这时，对于这两者，本机都不知道。于是，发出方的IP地址就设为0.0.0.0，接收方的IP地址设为255.255.255.255。 最后的\"UDP标头\"，设置发出方的端口和接收方的端口。这一部分是DHCP协议规定好的，发出方是68端口，接收方是67端口。 这个数据包构造完成后，就可以发出了。以太网是广播发送，同一个子网络的每台计算机都收到了这个包。因为接收方的MAC地址是FF-FF-FF-FF-FF-FF ，看不出是发给谁的，所以每台收到这个包的计算机，还必须分析这个包的IP地址，才能确定是不是发给自己的。当看到发出方IP地址是0.0.0.0，接收方是255.255.255.255，于是DHCP服务器知道\" 这个包是发给我的\"，而其他计算机就可以丢弃这个包。 接下来，DHCP服务器读出这个包的数据内容，分配好IP地址，发送回去一个\"DHCP响应\" 数据包。这个响应包的结构也是类似的，以太网标头的MAC地址是双方的网卡地址，IP标头的IP地址是DHCP服务器的IP地址（发出方）和255.255.255.255 （接收方），UDP标头的端口是67（发出方）和68（接收方），分配给请求端的IP地址和本网络的具体参数则包含在Data部分。 新加入的计算机收到这个响应包，于是就知道了自己的IP地址、子网掩码、网关地址、DNS服务器等等参数。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:8:3","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"小结 不管是\"静态IP地址\"还是\"动态IP地址\"，电脑上网的首要步骤，是确定四个参数。这四个值很重要，值得重复一遍： 本机的IP地址 子网掩码 网关的IP地址 DNS的IP地址 有了这几个数值，电脑就可以上网\"冲浪\"了。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:8:4","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"一个实例 ","date":"2022-03-27","objectID":"/net-protocol-glance/:9:0","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"本机参数 我们假定，用户设置好了自己的网络参数： 本机的IP地址：192.168.1.100 子网掩码：255.255.255.0 网关的IP地址：192.168.1.1 DNS的IP地址：8.8.8.8 然后他打开浏览器，想要访问Google，在地址栏输入了网址：www.google.com。 访问google访问google \" 访问google 这意味着，浏览器要向Google发送一个网页请求的数据包。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:9:1","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"DNS协议 我们知道，发送数据包，必须要知道对方的IP地址。但是，现在，我们只知道网址www.google.com，不知道它的IP地址。 DNS协议可以帮助我们，将这个网址转换成IP地址。已知DNS服务器为8.8.8.8，于是我们向这个地址发送一个DNS数据包（53端口）。 DNS数据包DNS数据包 \" DNS数据包 然后，DNS服务器做出响应，告诉我们Google的IP地址是172.194.72.105。于是，我们知道了对方的IP地址。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:9:2","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"子网掩码 接下来，我们要判断，这个IP地址是不是在同一个子网络，这就要用到子网掩码。 已知子网掩码是255.255.255.0，本机用它对自己的IP地址192.168.1.100，做一个二进制的AND运算（两个数位都为1，结果为1，否则为0），计算结果为192.168.1.0 ；然后对Google的IP地址172.194.72.105也做一个AND运算，计算结果为172.194.72.0。这两个结果不相等，所以结论是，Google与本机不在同一个子网络。 因此，我们要向Google发送数据包，必须通过网关192.168.1.1转发，也就是说，接收方的MAC地址将是网关的MAC地址。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:9:3","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"应用层协议 浏览网页用的是HTTP协议，它的整个数据包构造是这样的： HTTP协议数据包HTTP协议数据包 \" HTTP协议数据包 HTTP部分的内容，类似于下面这样： GET / HTTP/1.1 Host: www.google.com Connection: keep-alive User-Agent: Mozilla/5.0 (Windows NT 6.1) ...... Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Encoding: gzip,deflate,sdch Accept-Language: zh-CN,zh;q=0.8 Accept-Charset: GBK,utf-8;q=0.7,*;q=0.3 Cookie: ... ... 我们假定这个部分的长度为 4960 字节，它会被嵌在TCP数据包之中。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:9:4","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"TCP协议 TCP数据包需要设置端口，接收方（Google）的HTTP端口默认是80，发送方（本机）的端口是一个随机生成的1024-65535之间的整数，假定为51775。 TCP数据包的标头长度为20字节，加上嵌入HTTP的数据包，总长度变为4980字节。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:9:5","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"IP协议 然后，TCP数据包再嵌入IP数据包。IP数据包需要设置双方的IP地址，这是已知的，发送方是192.168.1.100（本机），接收方是172.194.72.105（Google）。 IP数据包的标头长度为20字节，加上嵌入的TCP数据包，总长度变为5000字节。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:9:6","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"以太网协议 最后，IP数据包嵌入以太网数据包。以太网数据包需要设置双方的MAC地址，发送方为本机的网卡MAC地址，接收方为网关192.168.1.1的MAC地址（通过ARP协议得到）。 以太网数据包的数据部分，最大长度为1500字节，而现在的IP数据包长度为5000字节。因此，IP数据包必须分割成四个包。因为每个包都有自己的IP标头（20字节），所以四个包的IP数据包的长度分别为1500、1500、1500、560。 以太网协议以太网协议 \" 以太网协议 ","date":"2022-03-27","objectID":"/net-protocol-glance/:9:7","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"服务器端响应 经过多个网关的转发，Google的服务器172.194.72.105，收到了这四个以太网数据包。 根据IP标头的序号，Google将四个包拼起来，取出完整的TCP数据包，然后读出里面的\"HTTP请求\"，接着做出\"HTTP响应\"，再用TCP协议发回来。 本机收到HTTP响应以后，就可以将网页显示出来，完成一次网络通信。 服务器相应服务器相应 \" 服务器相应 上面的例子，虽然经过了简化，但它大致上反映了互联网协议的整个通信过程。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:9:8","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":"参考 互联网协议入门 ","date":"2022-03-27","objectID":"/net-protocol-glance/:10:0","tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":null,"content":"我们已经在一起 var countDownDate=new Date('2019-09-17T21:21:00').getTime(); window.setInterval(function(){ var distance=new Date().getTime()-countDownDate;var days=Math.floor(distance/(1000*60*60*24)); var hours=Math.floor((distance%(1000*60*60*24))/(1000*60*60)); var minutes=Math.floor((distance%(1000*60*60))/(1000*60)); var seconds=Math.floor((distance%(1000*60))/1000); document.getElementById(\"since\").innerHTML=days+' 天 '+hours+' 时 '+minutes+' 分 '+seconds+' 秒';},1000); ","date":"2022-03-25","objectID":"/love/:0:0","tags":null,"title":"Since 2019/09/17","uri":"/love/"},{"categories":["开发者手册"],"content":"git使用，git，git基本操作，git clone,git push,git remote,.gitignore,git pull,git status,git add,git commit,git log,git diff,git rebase,git merge,git stash,git rebase,git rebase --continue,git rebase --skip,git rebase --abort,git","date":"2022-03-23","objectID":"/git-glance/","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"基本概念 基本概念基本概念 \" 基本概念 git workgit work \" git work ","date":"2022-03-23","objectID":"/git-glance/:1:0","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":".gitignore文件 # 此为注释 – 将被 Git 忽略 *.a # 忽略所有 .a 结尾的文件 !lib.a # 但 lib.a 除外 /TODO # 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO build/ # 忽略 build/ 目录下的所有文件 doc/*.txt # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt ","date":"2022-03-23","objectID":"/git-glance/:2:0","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"git 常用命令 ","date":"2022-03-23","objectID":"/git-glance/:3:0","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"git reset 当已经把代码从暂存区提交到版本库了，git rest命令可以恢复到暂存区的状态。 git rest --hard HEAD $commit_id，如果只会退上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100。 如果知道 commit id 的话，可以直接用 commit id，commit id 没必要写全，前6位基本就可以用，git会自动去找。 commit id 可以通过git log命令查看，格式化log可以使用git log --pretty=oneline git log --pretty=onelinegit log \u0026ndash;pretty=oneline \" git log --pretty=oneline --hard 参数的作用 当文件被修改过，并且 add 到了暂存区，git reset 命令会把文件状态恢复到最初的状态，也就是从暂存区撤销掉，此时跟git reset HEAD 命令一样。 如果文件从暂存区 commit 了，说明已经生成了最新的版本号了，此时回退，则需要回退到之前的一个版本，如果知道前一个版本的版本号，git reset 版本号这样就可以了，但是一般我们不会去记版本号， 可以执行git log 命令去查看，也可以使用 git reset HEAD^ 命令用于回退到上一个版本，会重新回到工作区，也就是 add 之前的状态。 如果使用了 --hard 参数会连工作区的状态内容也修改了。 以下是同样的 commit 之后，不加 --hard 参数和使用 --hard 参数的区别： 不使用 --hard 参数不使用 \u0026ndash;hard 参数 \" 不使用 --hard 参数 使用 --hard 参数使用 \u0026ndash;hard 参数 \" 使用 --hard 参数 ","date":"2022-03-23","objectID":"/git-glance/:3:1","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"git diff cmd 说明 git diff [file] 显示暂存区和工作区的差异 git diff --cached [file]/git diff --staged [file] 显示暂存区和上一次提交(commit)的差异 ","date":"2022-03-23","objectID":"/git-glance/:3:2","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"git reflog git reflog 用来记录你的每一次命令，可以查看你最近执行过的命令，可以用来回退到某一个时刻。所以为了好查记录，commit -m 的提交说明文案尽量写清楚。 git refloggit reflog \" git reflog cmd 说明 git reflog –date=local –all | grep dev 查看 dev 分支是基于哪个分支创建的 Tips markdown 表格中使用 | 可以使用\u0026#124; ","date":"2022-03-23","objectID":"/git-glance/:3:3","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"git log git log 不传入任何参数的默认情况下，git log 会按时间先后顺序列出所有的提交，最近的更新排在最上面，当记录太多时会出现分页，可以按空格键翻页，按 q 键退出。 git loggit log \" git log git log --pretty=oneline 将每个提交放在一行显示，在浏览大量的提交时非常有用。 git log --pretty=onelinegit log pretty oneline \" git log --pretty=oneline git log --graph --pretty=oneline --abbrev-commit 仅显示 SHA-1 校验和所有 40 个字符中的前几个字符。--oneline 是 --pretty=oneline --abbrev-commit 合用的简写。 所以 git log --graph --pretty=oneline 可以也可以写为 git log --graph --pretty=oneline --abbrev-commit。 --graph 在日志旁以 ASCII 图形显示分支与合并历史。 git graphgit graph \" git graph ","date":"2022-03-23","objectID":"/git-glance/:3:4","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"git checkout cmd 说明 git checkout dev 切换到dev分支 git checkout -- file 可以丢弃工作区的修改， git checkout -- readme.txt 意思是，把readme.txt文件在工作区的修改全部撤销。 这里有两种情况： 1. readme.txt自修改后还没有被放到暂存区，撤销修改就回到和版本库一模一样的状态。 2. readme.txt已经添加到暂存区后，又作了修改，撤销修改就回到添加到暂存区后的状态 git checkout -b yourbranchname origin/oldbranchname 在本地创建和远程分支对应的分支 ","date":"2022-03-23","objectID":"/git-glance/:3:5","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"git rm git rm 有 2 个常用命令： git rm \u003cfile\u003e：同时从工作区和索引中删除文件。即本地的文件也被删除了，并把此次删除操作提交到了暂存区。 git rm filegit rm file \" git rm file git rm --cached ：从索引中删除文件。但是本地文件还存在，只是不希望这个文件被版本控制。 git rm --cachedgit rm \u0026ndash;cached \" git rm --cached 如果是文件夹需要加上 -r 参数，比如： git rm -r --cached 文件/文件夹名字 Tips 先手动删除文件，然后使用git rm \u003cfile\u003e和git add\u003cfile\u003e效果是一样的。 ","date":"2022-03-23","objectID":"/git-glance/:3:6","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"git remote cmd 说明 git remote add 名字 地址 关联一个远程库时必须给远程库指定一个名字，如：git remote add origin git@server-name:path/repo-name.git git remote -v 查看远程库信息 git remote rm \u003cname\u003e 解除了本地和远程的绑定关系，如：git remote rm origin ","date":"2022-03-23","objectID":"/git-glance/:3:7","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"git push 把本地库的内容推送到远程，用git push命令，比如： git push -u origin master 实际上是把当前分支推送到远程的 master 分支上。 加上了-u参数，git不但会把本地的分支内容推送的远程的master分支，还会把本地的分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令，直接使用 git push。 ","date":"2022-03-23","objectID":"/git-glance/:3:8","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"git branch cmd 说明 git brahcn -b 分支名 新建并切换到新分支，如新建并切换到dev分支：git branch -b dev git branch 列出所有分支，当前分支前面会标一个*号 git branch -d 分支名 删除某个分支 git branch -a 查看远程分支，远程分支会用红色表示出来（如果开了颜色支持的话） git branch -D \u003cname\u003e 强行删除一个没有被合并过的分支 git branch --set-upstream branch-name origin/branch-name 建立本地分支和远程分支的关联 ","date":"2022-03-23","objectID":"/git-glance/:3:9","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"git merge git merge命令用于合并指定分支到当前分支。如：git merge dev 合并 dev 分支到当前分支。 ","date":"2022-03-23","objectID":"/git-glance/:3:10","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"git switch git 2.23+ 版本支持了 switch 命令用来切换分支，实际上，切换分支这个动作，用switch更好理解。 之前切换分支使用git checkout \u003cbranch\u003e，而撤销修改则是git checkout -- \u003cfile\u003e，同一个命令，有两种作用，确实有点令人迷惑。 操作 version 2.23- version 2.23+ 切换分支 git branch dev git switch dev 新建并切换分支 git branch -b dev git switch -c dev ","date":"2022-03-23","objectID":"/git-glance/:3:11","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"git cherry-pick 在合并代码的时候，有两种情况： 需要另一个分支的所有代码变动，那么就采用合并git merge。 只需要部分代码变动（某几个提交），这时可以采用 cherry pick。 git cherry-pick \u003ccommid_1\u003e \u003ccommit_2\u003e ","date":"2022-03-23","objectID":"/git-glance/:3:12","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"git stash 新增的文件，直接执行 git stash 是不会被存储的，需要先执行 git add 把文件加到版本控制里。 文件在版本控制里，并不等于就被stash起来了，git add 和 git stash 没有必然的关系，但是执行 git stash 能正确存储的前提是文件必须在 git 版本控制中才行。 可以多次stash，恢复的时候，先用git stash list查看，然后恢复指定的stash。 cmd 说明 git stash save \"save message\" 执行存储时，添加备注，方便查找，只有git stash 也要可以的，但查找时不方便识别 git stash list 查看stash了哪些存储 git stash show 显示做了哪些改动，默认show第一个存储,如果要显示其他存贮，后面加 stash@{$num}，比如第二个 git stash show stash@{1} git stash show -p 显示第一个存储的改动，如果想显示其他存存储，命令：git stash show stash@{$num} -p ，比如第二个：git stash show stash@{1} -p git stash apply 应用某个存储，但不会把存储从存储列表中删除，默认使用第一个存储，即stash@{0}，如果要使用其他个，git stash apply stash@{$num} ， 比如第二个：git stash apply stash@{1} git stash pop 命令恢复之前缓存的工作目录，将缓存堆栈中的对应stash删除，并将对应修改应用到当前的工作目录下，默认为第一个stash，即stash@{0}，如果要应用并删除其他stash，命令：git stash pop stash@{$num} ，比如应用并删除第二个：git stash pop stash@{1} git stash drop stash@{$num} 丢弃stash@{$num}存储，从列表中删除这个存储 git stash clear 删除所有缓存的stash ","date":"2022-03-23","objectID":"/git-glance/:3:13","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"分支管理 HEAD严格来说不是指向提交，而是指向某个分支，如master分支，master才是指向提交的，所以，HEAD指向的就是当前分支。 HEADHEAD \" HEAD 在合并分支时如果出现冲突，Git用\u003c\u003c\u003c\u003c\u003c\u003c\u003c，=======，\u003e\u003e\u003e\u003e\u003e\u003e\u003e标记出不同分支的内容。 合并分支有冲突merge conflict \" 合并分支有冲突 ","date":"2022-03-23","objectID":"/git-glance/:4:0","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"Fast forward 通常，合并分支时，如果可能，git 会用 Fast forward 模式，但这种模式下，删除分支后，会丢掉分支信息。 可以使用--no-ff强制禁用 Fast forward 模式，git就会在merge时生成一个新的 commit，这样，从分支历史上就可以看出分支信息。 git merge --no-ffgit merge \u0026ndash;no-ff \" git merge --no-ff 因为本次合并要创建一个新的 commit，所以加上 -m 参数，把 commit 描述写进去。 分支历史分支历史 \" 分支历史 ","date":"2022-03-23","objectID":"/git-glance/:4:1","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"标签Tag tag 是基于某个分支下的某次 commit。如只执行 git tag v1.0，那么标签是打在该分支最新提交的 commit 上的。 创建的标签都只存储在本地，不会自动推送到远程，打错的标签可以在本地安全删除。如果标签已经推送到远程，得先删除本地标签，再删除远程标签。 cmd 说明 git tag -a v0.1 -m \"version 0.1 released\" 1094adb 基于某次 commit 打 tag，-a指定标签名，-m指定说明文字 git show \u003ctagname\u003e 显示 tag 的说明文字 git tag 可以查看所有标签 git tag -d v0.1 删除标签 git push origin \u003ctagname\u003e 推送某个标签到远程，如：git push origin v1.0 git push origin --tags 一次性推送全部尚未推送到远程的本地标签 git push origin :refs/tags/\u003ctagname\u003e 删除一个远程标签 ","date":"2022-03-23","objectID":"/git-glance/:5:0","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"FAQ ","date":"2022-03-23","objectID":"/git-glance/:6:0","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"pull 和 fetch 区别 git fetch 是将远程主机的最新内容拉到本地，用户检查后决定是否合并到分支中。 git pull 是将远程主机的最新内容拉下来直接合并，可以理解成，git pull = git fetch + git merge。在 merge 合并时如果产生冲突需要手动解决。 尽量不要用 git pull，用 git fetch 和 git merge 代替 git pull。 将 fetch 和 merge 放到一个命令里的另外一个弊端是，本地工作目录在未经确认的情况下就会被远程分支更新。 ","date":"2022-03-23","objectID":"/git-glance/:6:1","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"将提交回到暂存区 git reset --soft $commitID 将 commit 后的代码回退到暂存区将 commit 后的代码回退到暂存区 \" 将 commit 后的代码回退到暂存区 ","date":"2022-03-23","objectID":"/git-glance/:6:2","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"参考 git-fast-version-control Git教程 Git Cheat Sheet git cherry-pick 教程 git stash 用法总结和注意点 详解git pull和git fetch的区别 ","date":"2022-03-23","objectID":"/git-glance/:7:0","tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":"OAuth2.0,第三方登录,令牌,TOKEN,授权码,权限","date":"2022-03-22","objectID":"/oauth2.0/","tags":["web","oauth"],"title":"OAuth2.0的理解与应用","uri":"/oauth2.0/"},{"categories":["开发者手册"],"content":"什么是 OAuth2.0 OAuth 的核心就是向第三方应用颁发令牌，比如网站A想用Github的信息，那么对于Github来说，网站A就是第三方应用。 第三方应用申请令牌之前，都必须先到系统备案，比如申请Github的令牌，得先到github备案登记， 说明自己的身份，然后会拿到两个身份识别码：客户端 ID（client ID）和客户端密钥（client secret）。这是为了防止令牌被滥用，没有备案过的第三方应用，是不会拿到令牌的。 关于 OAuth2.0 是什么可以参考一下文章： OAuth 2.0 的一个简单解释 [简易图解]『 OAuth2.0』 『进阶』 授权模式总结 ","date":"2022-03-22","objectID":"/oauth2.0/:1:0","tags":["web","oauth"],"title":"OAuth2.0的理解与应用","uri":"/oauth2.0/"},{"categories":["开发者手册"],"content":"第三方登录Github 所谓第三方登录，实质就是 OAuth 授权。用户想要登录 A 网站，A 网站让用户提供第三方网站的数据，证明自己的身份。获取第三方网站的身份数据，就需要 OAuth 授权。 比如，A 网站允许 GitHub 登录，背后就是下面的流程： A 网站让用户跳转到 GitHub。 GitHub 要求用户登录，然后询问\"A 网站要求获得 xx 权限，你是否同意？\" 用户同意，GitHub 就会重定向回 A 网站，同时发回一个授权码。 A 网站使用授权码，向 GitHub 请求令牌。 GitHub 返回令牌. A 网站使用令牌，向 GitHub 请求用户数据。 ","date":"2022-03-22","objectID":"/oauth2.0/:2:0","tags":["web","oauth"],"title":"OAuth2.0的理解与应用","uri":"/oauth2.0/"},{"categories":["开发者手册"],"content":"注册 OAuth 应用 现在在 Github 上注册一个 OAuth 应用。 github注册oauth应用 \" 字段 描述 Application name 应用名称 Homepage URL 首页URL，如https://www.xiaobinqt.cn Authorization callback URL 用户在 Github 登录成功后重定向回的 URL 注册成功后会生成 Client ID 和 Client Secret，这两个是用来请求令牌的。 生成的Client信息 \" ","date":"2022-03-22","objectID":"/oauth2.0/:2:1","tags":["web","oauth"],"title":"OAuth2.0的理解与应用","uri":"/oauth2.0/"},{"categories":["开发者手册"],"content":"通过 OAuth 获取用户信息 前端界面 oauth.html \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003eTitle\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ca href=\"https://github.com/login/oauth/authorize?client_id={{.ClientId}}\u0026redirect_uri={{.RedirectUrl}}\"\u003e Github 第三方授权登录\u003c/a\u003e \u003c/body\u003e \u003c/html\u003e go 代码通过OAuth获取用户信息 package main import ( \"encoding/json\" \"flag\" \"fmt\" \"html/template\" \"io/ioutil\" \"log\" \"net/http\" \"os\" ) var ( clientSecret = flag.String(\"cs\", \"\", \"github oauth client secret\") clientID = flag.String(\"ci\", \"\", \"github oauth client id\") ) type Conf struct { ClientId string ClientSecret string RedirectUrl string } type Token struct { AccessToken string `json:\"access_token\"` } // 认证并获取用户信息 func OAuth(w http.ResponseWriter, r *http.Request) { var ( err error ) // 获取 code code := r.URL.Query().Get(\"code\") // 通过 code, 获取 token var tokenAuthUrl = GetTokenAuthURL(code) var token *Token if token, err = GetToken(tokenAuthUrl); err != nil { fmt.Println(err) return } // 通过token，获取用户信息 var userInfo map[string]interface{} if userInfo, err = GetUserInfo(token); err != nil { fmt.Println(\"获取用户信息失败，错误信息为:\", err) return } // 将用户信息返回前端 var userInfoBytes []byte if userInfoBytes, err = json.Marshal(userInfo); err != nil { fmt.Println(\"在将用户信息(map)转为用户信息([]byte)时发生错误，错误信息为:\", err) return } w.Header().Set(\"Content-Type\", \"application/json\") if _, err = w.Write(userInfoBytes); err != nil { fmt.Println(\"在将用户信息([]byte)返回前端时发生错误，错误信息为:\", err) return } } // 通过code获取token认证url func GetTokenAuthURL(code string) string { return fmt.Sprintf( \"https://github.com/login/oauth/access_token?client_id=%s\u0026client_secret=%s\u0026code=%s\", *clientID, *clientSecret, code, ) } // 获取 token func GetToken(url string) (*Token, error) { // 形成请求 var req *http.Request var err error if req, err = http.NewRequest(http.MethodGet, url, nil); err != nil { return nil, err } req.Header.Set(\"accept\", \"application/json\") // 发送请求并获得响应 var ( httpClient = http.Client{} res *http.Response respBody = make([]byte, 0) token Token ) if res, err = httpClient.Do(req); err != nil { return nil, err } respBody, err = ioutil.ReadAll(res.Body) if err != nil { return nil, err } log.Printf(\"token: %s\", string(respBody)) // 将响应体解析为 token，并返回 err = json.Unmarshal(respBody, \u0026token) if err != nil { return nil, err } return \u0026token, nil } // 获取用户信息 func GetUserInfo(token *Token) (map[string]interface{}, error) { // 形成请求 var userInfoUrl = \"https://api.github.com/user\" // github用户信息获取接口 var req *http.Request var err error if req, err = http.NewRequest(http.MethodGet, userInfoUrl, nil); err != nil { return nil, err } req.Header.Set(\"accept\", \"application/json\") req.Header.Set(\"Authorization\", fmt.Sprintf(\"token %s\", token.AccessToken)) // 发送请求并获取响应 var client = http.Client{} var res *http.Response if res, err = client.Do(req); err != nil { return nil, err } // 将响应的数据写入 userInfo 中，并返回 var userInfo = make(map[string]interface{}) if err = json.NewDecoder(res.Body).Decode(\u0026userInfo); err != nil { return nil, err } return userInfo, nil } func Html(w http.ResponseWriter, r *http.Request) { // 解析指定文件生成模板对象 var ( temp *template.Template err error ) dir, _ := os.Getwd() if temp, err = template.ParseFiles(dir + \"/oauth.html\"); err != nil { fmt.Println(\"读取文件失败，错误信息为:\", err) return } // 利用给定数据渲染模板(html页面)，并将结果写入w，返回给前端 if err = temp.Execute(w, Conf{ ClientId: *clientID, ClientSecret: *clientSecret, RedirectUrl: \"http://127.0.0.1:9000/oauth/callback\", }); err != nil { fmt.Println(\"读取渲染html页面失败，错误信息为:\", err) return } } func UserInfo(w http.ResponseWriter, r *http.Request) { token := r.URL.Query().Get(\"token\") log.Printf(\"UserInfo token: %s\", token) var ( err error userInfo map[string]interface{} ) if userInfo, err = GetUserInfo(\u0026Token{AccessToken: token}); err != nil { fmt.Println(\"获取用户信息失败，错误信息为:\", err) return } // 将用户信息返回前端 var userInfoBytes []byte if userInfoBytes, err = json.Marshal(userInfo); err != nil { fmt.Println(\"在将用户信息(map)转为用户信息([]byte)时发生错误，错误信息为:\", err) return } w.Header().Set(\"Content-Type\", \"application/json\") if _, err = w.Write(u","date":"2022-03-22","objectID":"/oauth2.0/:2:2","tags":["web","oauth"],"title":"OAuth2.0的理解与应用","uri":"/oauth2.0/"},{"categories":["开发者手册"],"content":"效果 前端界面前端界面 \" 前端界面 授权页面授权页面 \" 授权页面 github返回的用户信息github返回的用户信息 \" github返回的用户信息 ","date":"2022-03-22","objectID":"/oauth2.0/:3:0","tags":["web","oauth"],"title":"OAuth2.0的理解与应用","uri":"/oauth2.0/"},{"categories":["开发者手册"],"content":"源码 源码地址 ","date":"2022-03-22","objectID":"/oauth2.0/:4:0","tags":["web","oauth"],"title":"OAuth2.0的理解与应用","uri":"/oauth2.0/"},{"categories":["开发者手册"],"content":"参考 Building OAuth Apps OAuth 2.0 的一个简单解释 Go语言实现第三方登录Github (通过OAuth2.0) basics of authentication [简易图解]『 OAuth2.0』 『进阶』 授权模式总结 ","date":"2022-03-22","objectID":"/oauth2.0/:5:0","tags":["web","oauth"],"title":"OAuth2.0的理解与应用","uri":"/oauth2.0/"},{"categories":["开发者手册"],"content":"HUGO,hugo主题标题支持emoji,emoji表情","date":"2022-03-21","objectID":"/hugo-title-support-emoji/","tags":["hugo","emoji"],"title":"hugo主题标题支持emoji:smile:","uri":"/hugo-title-support-emoji/"},{"categories":["开发者手册"],"content":"解决方法 hugo 在渲染时默认是不支持标题中的emoji的（有的主题也许是支持的），可以通过修改主题源码来支持。 我用的主题是LoveIt，找到 simple.html 文件，路径为 themes/LoveIt/layouts/posts/single.html 修改标题的渲染方式为 {{ .Title | emojify }}，如下： 修改主题的渲染方式 \" 这样就可以支持 emoji 了。 title support emoji \" 此时列表中还不支持 emoji，同样的修改方式。 list not support emoji \" 修改 themes/LoveIt/layouts/_default/summary.html 文件文件中的 title 的渲染方式为 {{ .Title | emojify }}。 ","date":"2022-03-21","objectID":"/hugo-title-support-emoji/:1:0","tags":["hugo","emoji"],"title":"hugo主题标题支持emoji:smile:","uri":"/hugo-title-support-emoji/"},{"categories":["开发者手册"],"content":"参考 Hugo should render emojis in page titles if enableEmoji = true ","date":"2022-03-21","objectID":"/hugo-title-support-emoji/:2:0","tags":["hugo","emoji"],"title":"hugo主题标题支持emoji:smile:","uri":"/hugo-title-support-emoji/"},{"categories":["理解计算机"],"content":" tcp,tcp连接管理,三次握手,四次挥手,为什么建立连接需要三次握手,为什么不能用两次握手进行连接,SYN,FIN,ACK,PSH,SYN_SENT,SYN_RECV,ESTABLISHED,FIN_WAIT_1,FIN_WAIT_2,CLOSE_WAIT,LAST_ACK,TIME_WAIT,CLOSE","date":"2022-03-21","objectID":"/tcp-handshark/","tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/"},{"categories":["理解计算机"],"content":"名词解释 名词 解释 SYN 同步序号，用于建立连接过程，在连接请求中，SYN=1 和 ACK=0 表示该数据段没有使用捎带的确认域，而连接应答捎带一个确认，即 SYN=1 和 ACK=1 FIN finish 标志，用于释放连接，为 1 时表示发送方已经没有数据发送了，即关闭本方数据流 ACK 确认序号标志，为 1 时表示确认号有效，为 0 表示报文中不含确认信息，忽略确认号字段 PSH push 标志，为 1 表示是带有 push 标志的数据，指示接收方在接收到该报文段以后，应尽快将这个报文段交给应用程序，而不是在缓冲区排队 RST 重置连接标志，用于重置由于主机崩溃或其他原因而出现错误的连接。或者用于拒绝非法的报文段和拒绝连接请求 序列号 seq 占 4 个字节，用来标记数据段的顺序，TCP 把连接中发送的所有数据字节都编上一个序号，第一个字节的编号由本地随机产生；给字节编上序号后，就给每一个报文段指派一个序号；序列号 seq 就是这个报文段中的第一个字节的数据编号 确认号 ack 占 4 个字节，期待收到对方下一个报文段的第一个数据字节的序号；序列号表示报文段携带数据的第一个字节的编号；而确认号指的是期望接收到下一个字节的编号；因此当前报文段最后一个字节的编号 +1 （ACK会占一个序号）即为确认号 Info ACK、SYN 和 FIN 这些大写的单词表示标志位，其值要么是 1，要么是 0；ack、seq 小写的单词表示序号。 ACK 是可能与 SYN，FIN 等同时使用的。比如 SYN和ACK可能同时为 1，它表示的就是建立连接之后的响应搜索 如果只是单个的一个SYN，它表示的只是建立连接。 SYN与FIN是不会同时为 1 的，因为前者表示的是建立连接，而后者表示的是断开连接。 RST一般是在FIN之后才会出现为 1 的情况，表示的是连接重置。 一般，当出现FIN包或RST包时，便认为客户端与服务器端断开了连接；而当出现SYN和SYN＋ACK包时，我们认为客户端与服务器建立了一个连接。 PSH为 1 的情况，一般只出现在DATA内容不为 0 的包中，也就是说PSH为1表示的是有真正的 TCP 数据包内容被传递。 ","date":"2022-03-21","objectID":"/tcp-handshark/:1:0","tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/"},{"categories":["理解计算机"],"content":"三次握手 三次握手三次握手 \" 三次握手 ","date":"2022-03-21","objectID":"/tcp-handshark/:2:0","tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/"},{"categories":["理解计算机"],"content":"第一次握手 客户端 主动打开（active open），向服务端发送 SYN 报文段SYN=1, SN=client_isn, OPT=client_mss，请求建立连接。 client_isn 是客户端初始序号，动态生成，用于实现可靠传输，client_sn-client_isn 等于客户端已发送字节数。 SYN 报文段虽然不能携带数据，但是会消耗一个序号（相当于发送了1个字节的有效数据），下次客户端再向服务端发送的报文段中 SN=client_isn+1。 除了 SYN 报文段和 ACK-SYN 报文段，其他所有后续报文段的序号 SN 值都等于上次接收的 ACK 报文段中的确认号 AN 值。 client_mss 是客户端最大报文段长度，在 TCP 首部的选项和填充部分，会在客户端与服务端的 MSS 中选择一个较小值使用。 客户端变为 SYN_SENT 状态，然后等待服务端 ACK 报文段。 ","date":"2022-03-21","objectID":"/tcp-handshark/:2:1","tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/"},{"categories":["理解计算机"],"content":"第二次握手 服务端 接收来自客户端的 SYN 报文段，得知客户端发送能力正常。 被动打开passive open，向客户端发送 SYN-ACK 报文段ACK=1, AN=client_isn+1, SYN=1, SN=server_isn, OPT=server_mss ，应答来自客户端的建立连接请求并向客户端发起建立连接请求。 SN=server_isn 是服务端初始序号，ACK-SYN 报文段虽然不能携带数据，但是会消耗一个序号（相当于发送了1个字节的有效数据），下次服务端再向客户端发送的报文中 SN=server_isn+1 。 OPT=server_mss 是服务端最大报文段长度。 AN=client_isn+1 是确认号，表明服务端接下来要开始接收来自客户端的第 client_isn+1 个字节的有效数据。 服务端变为 SYN_RCVD 状态，并等待客户端 ACK 报文段。 ","date":"2022-03-21","objectID":"/tcp-handshark/:2:2","tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/"},{"categories":["理解计算机"],"content":"第三次握手 客户端 接收来自服务端的 SYN-ACK 报文段，得知服务端发送能力和接收能力都正常。 向客户端发送 ACK 报文段ACK=1, AN=server_isn+1, SN=client_isn+1, MESSAGE=message，应答来自服务端的建立连接请求。 SN=client_isn+1 是序号，表明当前报文段发送的有效数据首字节是从请求建立连接到现在为止客户端向服务端发送的第 (client_isn+1)-clien_isn+1=2 个字节的有效数据。 有效数据：一般有效数据指的是应用层的报文数据，不过 SYN 报文段、 ACK-SYN 报文段和 FIN 报文段虽然没有携带报文数据，但认为发送了1个字节的有效数据。 AN=server_isn+1 是确认号，表明客户端接下来要开始接收来自服务端的第 server_isn+1 个字节的有效数据。 MESSAGE=message 此时可以在报文段中携带客户端到服务端的报文数据；该 ACK 报文段消耗的序号个数等于 message_length（注意 message_length 可以等于0，即不携带有效数据，此时 ACK报文段不消耗序号），下次客户端再向服务端发送的报文段中 SN=client_isn+1+message_length 。 客户端变为 ESTABLISHED 状态，client——\u003eserver 数据流建立。 服务端 接收来自客户端的 ACK 报文段，得知客户端接收能力正常。 变为 ESTABLISHED 状态，server——\u003eclient 数据流也建立。 ","date":"2022-03-21","objectID":"/tcp-handshark/:2:3","tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/"},{"categories":["理解计算机"],"content":"四次挥手 TCP四次挥手TCP四次挥手 \" TCP四次挥手 断开连接前，客户端和服务端都处于 ESTABLISHED 状态，两者谁都可以先发起断开连接请求。以下假设客户端先发起断开连接请求。 ","date":"2022-03-21","objectID":"/tcp-handshark/:3:0","tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/"},{"categories":["理解计算机"],"content":"第一次挥手 客户端 向服务端发送 FIN 报文段FIN=1, SN=client_sn，请求断开连接。 SN=client_sn是序号，表明当前报文段发送的有效数据首字节是从请求建立连接到现在为止客户端向服务端发送的第 client_sn-clien_isn+1 个字节的有效数据。 FIN 报文段虽然不能携带数据，但是会消耗一个序号（相当于发送了1个字节的有效数据），下次客户端再向服务端发送的报文中 SN=client_isn+1 。 客户端变为 FIN_WAIT1 状态，等待服务端 ACK 报文段。 ","date":"2022-03-21","objectID":"/tcp-handshark/:3:1","tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/"},{"categories":["理解计算机"],"content":"第二次挥手 服务端 接收来自客户端的 FIN 报文段。 向客户端发送 ACK 报文段ACK=1, AN=client_sn+1, SN=server_sn_wave2，应答客户端的断开连接请求。 SN=server_sn_wave2 是序号，表明当前报文段发送的有效数据首字节是从请求建立连接到现在为止服务端向客户端发送的第 server_sn_wave2-client_isn+1 个字节的有效数据。 AN=client_sn+1 是确认号，表明服务端接下来要开始接收来自客户端的第 client_sn+1 个字节的有效数据。 此时服务端变为 CLOSE_WAIT 状态。 客户端 接收来自服务端的 ACK 包。 变为 FIN_WAIT2 状态，等待服务端关闭连接请求FIN报文段。 ","date":"2022-03-21","objectID":"/tcp-handshark/:3:2","tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/"},{"categories":["理解计算机"],"content":"第三次挥手 服务端 （服务端想断开连接时）向客户端发送 FIN 报文段FIN=1, SN=server_sn，请求断开连接。 SN=server_sn 是序号，表明当前报文段发送的有效数据首字节是从请求建立连接到现在为止服务端向客户端发送的第 server_sn-clien_isn+1 个字节的有效数据。 FIN 报文段虽然不能携带数据，但是会消耗一个序号（相当于发送了1个字节的有效数据），下次服务端再向客户端发送的报文中 SN=client_isn+2 （若断开连接成功，则服务端不会再向客户端发送下一个报文段）。 第二次挥手和第三次挥手之间，服务端又向客户端发送了 server_sn - server_sn_wave2 个字节的有效数据。 服务端变为 LAST_ACK 状态，等待客户端的 ACK 报文段。 ","date":"2022-03-21","objectID":"/tcp-handshark/:3:3","tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/"},{"categories":["理解计算机"],"content":"第四次挥手 客户端 接收来自服务端的 FIN 报文段。 向服务端发送 ACK 报文段ACK=1, AN=server_sn+1, SN=client_sn+1，应答服务端断开连接请求。 client_sn+1 是序号，表明当前报文段发送的有效数据首字节是从请求建立连接到现在为止客户端向客户端发送的第 client_isn+1)-clien_isn+1 个字节的有效数据。AN=server_sn+1 是确认号，表明服务端接下来要开始接收来自客户端的第 client_sn+1 个字节的有效数据。 客户端变为 TIME_WAIT 状态，等待2MSL时间后进入 CLOSED 状态，至此 client——\u003eserver 数据流被关闭。 服务端 接收来自客户端的 ACK 报文段。 变为 CLOSED 状态，至此 server——\u003eclient 数据流被关闭。 Tips 当收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据。 ","date":"2022-03-21","objectID":"/tcp-handshark/:3:4","tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/"},{"categories":["理解计算机"],"content":"常见问题 ❓ 为什么建立连接需要“三次”握手 客户端和服务端之间建立的TCP是全双工通信，双方都要确保对方发送能力和接收能力正常。 一次握手后，服务端得知客户端发送能力正常。 二次握手后，客户端得知服务端接收能力和发送能力正常。 三次握手后，服务端得知客户端接收能力正常。 ❓ 为什么第四次挥手时要等待2MSL的时间再进入CLOSED状态 MSL（Maximum Segment Lifetime，报文段最大生存时间）是一个未被接受的报文段在网络中被丢弃前存活的最大时间。 保证建立新连接时网络中不存在上次连接时发送的数据包，进入 CLOSED 状态意味着可以建立新连接，等待 \u003eMSL 的时间再进入 CLOSED 状态可以保证建立新连接后，网络中不会存在上次连接时发送出去的数据包。若网络中同时存在发送端在两次连接中发出的数据包，对接收端接收数据可能会有影响。 保证第四次挥手发送的 ACK 能到达接收端，第四次挥手发送的 ACK 可能会出现丢包，另一端接收不到 ACK 会重新发送 FIN。等待 2MSL 的时间可以应对该情况，重发 ACK ，保证另一端能正常关闭连接。 ❓ 已经建立了连接，客户端突然出现故障怎么办 TCP 设有一个保活计时器，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为 2 小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔 75 秒钟发送一次。若一连发送 10 个探测报文仍然没反应， 服务器就认为客户端出了故障，接着就关闭连接。 ❓ 为什么不能用两次握手进行连接 三次握手完成两个重要的功能，既要双方做好发送数据的准备工作（双方都知道彼此已准备好），也要允许双方就初始序列号进行协商，这个序列号在握手过程中被发送和确认。 现在把三次握手改成仅需要两次握手，死锁是可能发生的。 比如，计算机 S 和 C 之间的通信，假定 C 给 S 发送一个连接请求分组，S 收到了这个分组，并发送了确认应答分组。按照两次握手的协定，S 认为连接已经成功地建立了，可以开始发送数据分组。 可是，C 在 S 的应答分组在传输中被丢失的情况下，将不知道 S 是否已准备好，不知道 S 建立什么样的序列号，C 甚至怀疑 S 是否收到自己的连接请求分组。在这种情况下，C 认为连接还未建立成功，将忽略 S 发来的任何数据分 组，只等待连接确认应答分组，而 S 在发出的分组超时后，重复发送同样的分组，这样就形成了死锁。 ","date":"2022-03-21","objectID":"/tcp-handshark/:4:0","tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/"},{"categories":["理解计算机"],"content":"参考 计算机网络——TCP连接管理（三次握手和四次挥手） “三次握手，四次挥手”你真的懂吗？ TCP的三次握手与四次挥手理解及面试题（很全面） TCP报文格式详解 面试官，不要再问我三次握手和四次挥手 ","date":"2022-03-21","objectID":"/tcp-handshark/:5:0","tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/"},{"categories":[""],"content":"前小端 ","date":"2022-03-19","objectID":"/links/:1:0","tags":[""],"title":"友情链接","uri":"/links/"},{"categories":[""],"content":"MakerLi ","date":"2022-03-19","objectID":"/links/:2:0","tags":[""],"title":"友情链接","uri":"/links/"},{"categories":[""],"content":"西瓜 ","date":"2022-03-19","objectID":"/links/:3:0","tags":[""],"title":"友情链接","uri":"/links/"},{"categories":[""],"content":"liupray ","date":"2022-03-19","objectID":"/links/:4:0","tags":[""],"title":"友情链接","uri":"/links/"},{"categories":[""],"content":"xiaowuneng ","date":"2022-03-19","objectID":"/links/:5:0","tags":[""],"title":"友情链接","uri":"/links/"},{"categories":["web"],"content":"JS运行机制,javascript异步运行机制,js异步同步,js Promise,js catch reject,js执行顺讯,async/await,js微任务和宏任务","date":"2022-03-18","objectID":"/js-cb-asyn/","tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/"},{"categories":["web"],"content":"执行模式 JS的执行模式是单线程的，当有多个任务时必须排队执行，优点是执行环境简单，缺点是性能低下，当有多个任务时，需要等待上一个任务执行完成才能执行下一个任务， 如果某个任务出现了死循环，那么就会导致程序崩溃。 所以JS出现了同步和异步的概念。 ","date":"2022-03-18","objectID":"/js-cb-asyn/:1:0","tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/"},{"categories":["web"],"content":"同步 后一个任务等待前一个任务结束，然后再执行，程序的执行顺序与任务的排列顺序是一致的。 ","date":"2022-03-18","objectID":"/js-cb-asyn/:1:1","tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/"},{"categories":["web"],"content":"异步 每一个任务有一个或多个回调函数（callback），前一个任务结束后，不是执行后一个任务，而是执行回调函数，后一个任务则是不等前一个任务结束就执行，所以程序的执行顺序与任务的排列顺序可能是不一致的。 ","date":"2022-03-18","objectID":"/js-cb-asyn/:1:2","tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/"},{"categories":["web"],"content":"Event Loop // TODO ","date":"2022-03-18","objectID":"/js-cb-asyn/:2:0","tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/"},{"categories":["web"],"content":"Promise Promise 对象代表一个异步操作，then() 第一个参数是成功resolve的回调函数，第二个参数是失败reject的回调函数，当不写第二个 then() 参数时，可以用 catch() 捕获 reject 异常。 ","date":"2022-03-18","objectID":"/js-cb-asyn/:3:0","tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/"},{"categories":["web"],"content":"使用 var p1 = new Promise(function (resolve, reject) { // resolve('成功'); reject(\"失败\") }); p1.then(function (res) { console.log(\"第一个fn: \", res) }, function (res) { console.log(\"第二个 fn: \", res) }); resolve和reject除了正常的值外，还可能是另一个promise实例。 const p1 = new Promise(function (resolve, reject) { resolve(1) }); const p2 = new Promise(function (resolve, reject) { // ... resolve(p1); }) p2.then(function (res) { console.log(res) }, function (res) { }) 用 catch 捕获 reject 异常 var p1 = new Promise(function (resolve, reject) { // todo... reject(111111) }); p1.then(function (res) { console.log(\"第一个fn: \", res) }).catch(function (err) { console.log(\"err :\", err) }).finally(function () { console.log(\"finally exec...\") }) ","date":"2022-03-18","objectID":"/js-cb-asyn/:3:1","tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/"},{"categories":["web"],"content":"执行顺序 ","date":"2022-03-18","objectID":"/js-cb-asyn/:3:2","tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/"},{"categories":["web"],"content":"async/await的用法和理解 async 函数是非常新的语法功能，在 ES7 中可用。 async 函数返回一个 Promise 对象，可以使用 then 方法添加回调函数。await 作为修饰符，只能放在 async 内部使用。 当函数执行的时候，一旦遇到 await 就会先返回，等到触发的异步操作完成，再接着执行函数体内后面的语句。 await 等待右侧表达式的结果。 如果等到的不是一个 promise 对象，那 await 表达式的运算结果就是它等到的东西。 如果它等到的是一个 promise 对象，它会阻塞后面的代码，等着 promise 对象 resolve，然后得到 resolve 的值，作为 await 表达式的运算结果。 async function test() { let promise = new Promise(resolve =\u003e { setTimeout(() =\u003e resolve(\"test\"), 2000); }); await promise.then((ret) =\u003e { console.log(ret) }) let test1Ret = await test1() console.log(test1Ret) console.log(\"test end...\") } function test1() { return \"test1_return\" } test(); console.log('end') 运行结果 \" ","date":"2022-03-18","objectID":"/js-cb-asyn/:4:0","tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/"},{"categories":["web"],"content":"宏任务和微任务 // TODO ","date":"2022-03-18","objectID":"/js-cb-asyn/:5:0","tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/"},{"categories":["web"],"content":"参考 Javascript异步编程的4种方法 JavaScript 运行机制详解：再谈Event Loop async 函数的含义和用法 JS执行——Promise 你真的了解回调? 回调地狱 js中微任务和宏任务的区别 ","date":"2022-03-18","objectID":"/js-cb-asyn/:6:0","tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/"},{"categories":["理解计算机"],"content":"HTTP,HTTP协议,超文本传输协议,互联网,TCP/IP,Transmission Control Protocol,传输控制协议,ISO","date":"2022-03-17","objectID":"/http-glance/","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"该笔记是在学习《透视 HTTP 协议》时整理，还参考了网上的其他资料。鄙人只是网络世界的搬运整理工😂。 ","date":"2022-03-17","objectID":"/http-glance/:0:0","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"总览 http总览 \" ","date":"2022-03-17","objectID":"/http-glance/:1:0","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"http 协议 http（超文本传输协议）是一个用在计算机世界里的协议。它使用计算机能够理解的语言确立了一种计算机之间交流通信的规范，以及相关的各种控制和错误处理方式。 http 是一个在计算机世界里专门在两点之间传输文字、图片、音频、视频等超文本数据的约定和规范。 http 不是编程语言，但是可以用编程语言去实现 HTTP，告诉计算机如何用 HTTP 来与外界通信。 在互联网世界里，HTTP 通常跑在 TCP/IP 协议栈之上，依靠 IP 协议实现寻址和路由、TCP 协议实现可靠数据传输、DNS 协议实现域名查找、SSL/TLS 协议实现安全通信。此外，还有一些协议依赖于 HTTP，例如 WebSocket、HTTPDNS 等。这些协议相互交织，构成了一个协议网，而 HTTP 则处于中心地位。 HTTP 传输的不是 TCP/UDP 这些底层协议里被切分的杂乱无章的二进制包（datagram），而是完整的、有意义的数据，可以被浏览器、服务器这样的上层应用程序处理。 ","date":"2022-03-17","objectID":"/http-glance/:2:0","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"互联网和万维网的区别 我们通常所说的“上网”实际上访问的只是互联网的一个子集“万维网”（World Wide Web），它基于 HTTP 协议，传输 HTML 等超文本资源，能力被限制在 HTTP 协议之内。 互联网上还有许多万维网之外的资源，例如常用的电子邮件、BT 和 Magnet 点对点下载、FTP 文件下载、SSH 安全登录、各种即时通信服务等等，它们需要用各自的专有协议来访问。 不过由于 HTTP 协议非常灵活、易于扩展，而且“超文本”的表述能力很强，所以很多其他原本不属于 HTTP 的资源也可以“包装”成 HTTP 来访问，这就是我们为什么能够总看到各种“网页应用”——例如“微信网页版”“邮箱网页版”——的原因。 ","date":"2022-03-17","objectID":"/http-glance/:2:1","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"TCP/IP TCP/IP 协议实际上是一系列网络通信协议的统称， 其中最核心的两个协议是TCP（Transmission Control Protocol/传输控制协议）和IP（Internet Protocol），其他的还有 UDP、ICMP、ARP 等等，共同构成了一个复杂但有层次的协议栈。 HTTP 是超文本传输协议，TCP 是传输控制协议，都是传输，区别是，HTTP 传输的是完整的、有意义的数据，可以被浏览器、 服务器这样的上层应用程序处理，HTTP 不关心寻址、路由、数据完整性等传输细节，而要求这些工作都由下层（基本都由 TCP）来处理。 TCP 传输的是可靠的、字节流和二进制包。 TCP 是 HTTP 得以实现的基础，HTTP 协议运行在 TCP/IP 上，HTTP 可以更准确地称为 “HTTP over TCP/IP”。 ","date":"2022-03-17","objectID":"/http-glance/:2:2","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"URI/URL URI（Uniform Resource Identifier），中文名称是 统一资源标识符，使用它就能够唯一地标记互联网上资源。 URI 另一个更常用的表现形式是 URL（Uniform Resource Locator）， 统一资源定位符，也就是我们俗称的“网址”，它实际上是 URI 的一个子集，这两者几乎是相同的，差异不大，除非写论文，否则不用特意区分。 ","date":"2022-03-17","objectID":"/http-glance/:2:3","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"SSL/TSL SSL 的全称是“Secure Socket Layer”，网景公司发明，当发展到 3.0 时被标准化，改名为 TLS，即“Transport Layer Security”。 所以 TLS 跟 SSL 是一个东西，相当于张君宝的 2.0 版本是张三丰。 SSL 是一个负责加密通信的安全协议，建立在 TCP/IP 之上，在 HTTP 协议之下。 ","date":"2022-03-17","objectID":"/http-glance/:2:4","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"Proxy 代理 匿名代理：完全“隐匿”了被代理的机器，外界看到的只是代理服务器； 透明代理：顾名思义，它在传输过程中是“透明开放”的，外界既知道代理，也知道客户端； 正向代理：靠近客户端，代表客户端向服务器发送请求； 正向代理正向代理 \" 正向代理 反向代理：靠近服务器端，代表服务器响应客户端的请求； 反向代理反向代理 \" 反向代理 Tip 如何理解反向代理服务器 ","date":"2022-03-17","objectID":"/http-glance/:2:5","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"http 版本 万维网关键技术 URI：即统一资源标识符，作为互联网上资源的唯一身份； HTML：即超文本标记语言，描述超文本文档； HTTP：即超文本传输协议，用来传输超文本。 基于这三项关键技术就可以把超文本系统完美地运行在互联网上，让各地的人们能够自由地共享信息，这个系统称为“万维网”（World Wide Web），也就是我们现在所熟知的 Web。 ","date":"2022-03-17","objectID":"/http-glance/:3:0","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"http/0.9 结构简单，设置之初设想系统里的文档都是只读的，所以只允许用 GET 动作从服务器上获取 HTML 纯文本格式的文档，并且在响应请求之后立即关闭连接，功能非常有限。 ","date":"2022-03-17","objectID":"/http-glance/:3:1","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"http/1.0 HTTP/1.0 并不是一个标准，只是记录已有实践和模式的一份参考文档，不具有实际的约束力，相当于一个备忘录。 在多方面增强了 0.9 版，形式上已经和我们现在的 HTTP 差别不大了，例如： 增加了 HEAD、POST 等新方法； 增加了响应状态码，标记可能的错误原因； 引入了协议版本号概念； 引入了 HTTP Header（头部）的概念，让 HTTP 处理请求和响应更加灵活； 传输的数据不再仅限于文本。 ","date":"2022-03-17","objectID":"/http-glance/:3:2","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"http/1.1 是一个正式的标准，而不是一份可有可无的参考文档，只要用到 HTTP 协议，就必须严格遵守这个标准。 主要变更： 增加了 PUT、DELETE 等新的方法； 增加了缓存管理和控制； 明确了连接管理，允许持久连接； 允许响应数据分块（chunked），利于传输大文件； 强制要求 Host 头，让互联网主机托管成为可能。 ","date":"2022-03-17","objectID":"/http-glance/:3:3","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"http/2 由 google 主导，基于 google 的 SPDY 协议为基础开始制定新版本的 HTTP 协议，最终在 2015 年发布了 HTTP/2。 主要特点： 二进制协议，不再是纯文本； 可发起多个请求，废弃了 1.1 里的管道； 使用专用算法压缩头部，减少数据传输量； 允许服务器主动向客户端推送数据； 增强了安全性，“事实上”要求加密通信。 ","date":"2022-03-17","objectID":"/http-glance/:3:4","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"http/3 由 google 主导，基于 google 的 QUIC 协议为基础开始制定新版本的 HTTP 协议。 ","date":"2022-03-17","objectID":"/http-glance/:3:5","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"网络分层模型 ","date":"2022-03-17","objectID":"/http-glance/:4:0","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"TCP/IP TCP/IP分层模型tcp/ip分层模型 \" TCP/IP分层模型 这里的层次顺序是“从下往上”数的，所以第一层就是最下面的一层。 链接层 第一层叫“链接层”（link layer），负责在以太网、WiFi 这样的底层网络上发送原始数据包，工作在网卡这个层次，使用 MAC 地址来标记网络上的设备，所以有时候也叫 MAC 层。 网络互联层 第二层叫“网际层”或者“网络互连层”（internet layer），IP 协议就处在这一层。因为 IP 协议定义了“IP 地址”的概念，所以就可以在“链接层”的基础上，用 IP 地址取代 MAC 地址，把许许多多的局域网、广域网连接成一个虚拟的巨大网络，在这个网络里找设备时只要把 IP 地址再“翻译”成 MAC 地址就可以了。 传输层 第三层叫“传输层”（transport layer），这个层次协议的职责是保证数据在 IP 地址标记的两点之间“可靠”地传输，是 TCP 协议工作的层次，另外还有它的一个“小伙伴”UDP。 TCP 是一个有状态的协议，需要先与对方建立连接然后才能发送数据，而且保证数据不丢失不重复。而 UDP 则比较简单，它无状态，不用事先建立连接就可以任意发送数据，但不保证数据一定会发到对方。两个协议的另一个重要区别在于数据的形式。TCP 的数据是连续的“字节流”，有先后顺序，而 UDP 则是分散的小数据包，是顺序发，乱序收。 应用层 协议栈的第四层叫“应用层”（application layer），由于下面的三层把基础打得非常好，所以在这一层就“百花齐放”了，有各种面向具体应用的协议。例如 Telnet、SSH、FTP、SMTP，HTTP 等等。 Tip MAC 层（链接层）的传输单位是帧（frame），IP 层（网络互联层）的传输单位是包（packet），TCP 层传输层的传输单位是段（segment）， HTTP （应用层）的传输单位则是消息或报文（message）。这些名词并没有什么本质的区分，可以统称为数据包。 ","date":"2022-03-17","objectID":"/http-glance/:4:1","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"OSI 网络分层模型 OSI 分层模型在发布的时候就明确地表明是一个“参考”，不是强制标准。这是因为 TCP/IP 等协议已经在许多网络上实际运行，不可能推翻重来。 OSI网络模型OSI模型 \" OSI网络模型 第一层：物理层，网络的物理形式，例如电缆、光纤、网卡、集线器等等； 第二层：数据链路层，它基本相当于 TCP/IP 的链接层； 第三层：网络层，相当于 TCP/IP 里的网际层； 第四层：传输层，相当于 TCP/IP 里的传输层； 第五层：会话层，维护网络中的连接状态，即保持会话和同步； 第六层：表示层，把数据转换为合适、可理解的语法和语义； 第七层：应用层，面向具体的应用传输数据。 对比一下就可以发现，TCP/IP 是一个纯软件的栈，没有网络应有的最根基的电缆、网卡等物理设备的位置。而 OSI 则补足了这个缺失， 在理论层面上描述网络更加完整。 OSI 还为每一层标记了明确了编号，最底层是一层，最上层是七层，而 TCP/IP 的层次从来只有名字而没有编号。 ","date":"2022-03-17","objectID":"/http-glance/:4:2","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"两个分层模型的对应关系 两个分层模型的对应关系对应关系 \" 两个分层模型的对应关系 所谓的“四层负载均衡”就是指工作在传输层上，基于 TCP/IP 协议的特性，例如 IP 地址、端口号等实现对后端服务器的负载均衡。 所谓的“七层负载均衡”就是指工作在应用层上，看到的是 HTTP 协议，解析 HTTP 报文里的 URI、主机名、资源类型等数据，再用适当的策略转发给后端服务器。 有一个辨别四层和七层比较好的（但不是绝对的）小窍门，“两个凡是”：凡是由操作系统负责处理的就是四层或四层以下，否则，凡是需要由应用程序（也就是你自己写代码）负责处理的就是七层。 ","date":"2022-03-17","objectID":"/http-glance/:4:3","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"http协议核心 由于 HTTP 是在 TCP/IP 协议之上的，而 TCP/IP 协议负责底层的具体传输工作，所以 http 在传输方面不用太操心，TCP/IP 会去解决，所以 HTTP 关心的就只有他所传输的报文内容，又因为 HTTP 是“纯文本”的，包括头信息都是 ASCII 码的文本，不用借助程序解析可以直接阅读。 http报文 \" ","date":"2022-03-17","objectID":"/http-glance/:5:0","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"常用头字段 注意事项 字段名不区分大小写，例如“Host”也可以写成“host”，但首字母大写的可读性更好； 字段名里不允许出现空格，可以使用连字符“-”，但不能使用下划线“_”。例如，“test-name”是合法的字段名，而“test name”“test_name”是不正确的字段名； 字段名后面必须紧接着“:”，不能有空格，而“:”后的字段值前可以有多个空格； 分类 通用字段：在请求头和响应头里都可以出现； 请求字段：仅能出现在请求头里，进一步说明请求信息或者额外的附加条件； 响应字段：仅能出现在响应头里，补充说明响应报文的信息； 实体字段：它实际上属于通用字段，但专门描述 body 的额外信息。 字段 类型 说明 Host 请求字段 唯一一个 HTTP/1.1 规范里要求必须出现的字段，如果请求头里没有 Host，那这就是一个错误的报文。Host 字段告诉服务器这个请求应该由哪个主机来处理 User-Agent 请求字段 描述发起 HTTP 请求的客户端，服务器可以依据它来返回最合适此浏览器显示的页面 Date 通用字段 表示 HTTP 报文创建的时间，客户端可以使用这个时间再搭配其他字段决定缓存策略 Server 响应字段 告诉客户端当前正在提供 Web 服务的软件名称和版本号 Content-Length 实体字段 报文里 body 的长度，也就是请求头或响应头空行后面数据的长度。服务器看到这个字段，就知道了后续有多少数据，可以直接接收。如果没有这个字段，那么 body 就是不定长的，需要使用 chunked 方式分段传输 ","date":"2022-03-17","objectID":"/http-glance/:5:1","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"请求方式 请求方式 \" 方式 说明 GET 获取资源，可以理解为读取或者下载数据 HEAD 获取资源的元信息，不会返回请求的实体数据，只会传回响应头 POST 向资源提交数据，相当于写入或上传数据 PUT 类似 POST DELETE 删除资源 CONNECT 建立特殊的连接隧道 OPTIONS 列出可对资源实行的方法 TRACE 追踪请求 - 响应的传输路径 ","date":"2022-03-17","objectID":"/http-glance/:5:2","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"状态码 状态码 含义 1×x 提示信息，表示目前是协议处理的中间状态，还需要后续的操作 2×× 成功，报文已经收到并被正确处理 3×× 重定向，资源位置发生变动，需要客户端重新发送请求 4×× 客户端错误，请求报文有误，服务器无法处理 5×× 服务器错误，服务器在处理请求时内部发生了错误 一些常用状态码说明 status code 说明 301 永久重定向，含义是此次请求的资源已经不存在了，需要改用改用新的 URI 再次访问 302 临时重定向，意思是请求的资源还在，但需要暂时用另一个 URI 来访问。比如，你的网站升级到了 HTTPS，原来的 HTTP 不打算用了，这就是“永久”的，所以要配置 301 跳转，把所有的 HTTP 流量都切换到 HTTPS。 再比如，今天夜里网站后台要系统维护，服务暂时不可用，这就属于“临时”的，可以配置成 302 跳转，把流量临时切换到一个静态通知页面，浏览器看到这个 302 就知道这只是暂时的情况，不会做缓存优化，第二天还会访问原来的地址。 304 Not Modified，它用于 If-Modified-Since 等条件请求，表示资源未修改，用于缓存控制。它不具有通常的跳转含义，但可以理解成“重定向已到缓存的文件”（即“缓存重定向”） 405 不允许使用某些方法操作资源，例如不允许 POST 只能 GET 406 Not Acceptable 资源无法满足客户端请求的条件，例如请求中文但只有英文 408 Request Timeout：请求超时，服务器等待了过长的时间 409 Conflict：多个请求发生了冲突，可以理解为多线程并发时的竞态 413 Request Entity Too Large：请求报文里的 body 太大 414 Request-URI Too Long：请求行里的 URI 太大 429 Too Many Requests 客户端发送了太多的请求，通常是由于服务器的限连策略 431 Request Header Fields Too Large 请求头某个字段或总体太大 500 Internal Server Error 与 400 类似，也是一个通用的错误码，服务器究竟发生了什么错误我们是不知道的。不过对于服务器来说这应该算是好事，通常不应该把服务器内部的详细信息，例如出错的函数调用栈告诉外界。虽然不利于调试，但能够防止黑客的窥探或者分析 501 Not Implemented 表示客户端请求的功能还不支持，这个错误码比 500 要温和一些，和“即将开业，敬请期待”的意思差不多，不过具体什么时候“开业”就不好说了 502 Bad Gateway”通常是服务器作为网关或者代理时返回的错误码，表示服务器自身工作正常，访问后端服务器时发生了错误，但具体的错误原因也是不知道的 503 Service Unavailable 表示服务器当前很忙，暂时无法响应服务，我们上网时有时候遇到的“网络服务正忙，请稍后重试”的提示信息就是状态码 503 ","date":"2022-03-17","objectID":"/http-glance/:5:3","tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["golang"],"content":"golang GMP 模型,go 数学模型,GMP,进程,线程,协程,goroutine,go 调度器,golang调度器,go协程调度模型","date":"2022-03-16","objectID":"/gmp-model/","tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/"},{"categories":["golang"],"content":"进程、线程、协程的区别 ","date":"2022-03-16","objectID":"/gmp-model/:1:0","tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/"},{"categories":["golang"],"content":"进程 进程是操作系统对一个正在运行的程序的一种抽象，进程是资源分配的最小单位。 进程在操作系统中的抽象表现进程在操作系统中的抽象表现 \" 进程在操作系统中的抽象表现 进程存在的意义是为了合理压榨 CPU 的性能和分配运行的时间片，不能 “闲着“。 在计算机中，其计算核心是 CPU，负责所有计算相关的工作和资源。单个 CPU 一次只能运行一个任务。如果一个进程跑着，就把唯一一个 CPU 给完全占住，那是非常不合理的。 如果总是在运行一个进程上的任务，就会出现一个现象。就是任务不一定总是在执行 ”计算型“ 的任务，会有很大可能是在执行网络调用，阻塞了，CPU 岂不就浪费了？ 进程上下文切换进程上下文切换 \" 进程上下文切换 所以出现了多进程，多个 CPU，多个进程。多进程就是指计算机系统可以同时执行多个进程，从一个进程到另外一个进程的转换是由操作系统内核管理的，一般是同时运行多个软件。 ","date":"2022-03-16","objectID":"/gmp-model/:1:1","tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/"},{"categories":["golang"],"content":"线程 有了多进程，在操作系统上可以同时运行多个进程。那么为什么有了进程，还要线程呢？这是因为， 进程间的信息难以共享数据，父子进程并未共享内存，需要通过进程间通信（IPC），在进程间进行信息交换，性能开销较大。 创建进程（一般是调用 fork 方法）的性能开销较大。 大家又把目光转向了进程内，能不能在进程里做点什么呢？ 进程由多个线程组成进程由多个线程组成 \" 进程由多个线程组成 一个进程可以由多个称为线程的执行单元组成。每个线程都运行在进程的上下文中，共享着同样的代码和全局数据。 多个进程，就可以有更多的线程。多线程比多进程之间更容易共享数据，在上下文切换中线程一般比进程更高效。这是因为， 线程之间能够非常方便、快速地共享数据。 只需将数据复制到进程中的共享区域就可以了，但需要注意避免多个线程修改同一份内存。 创建线程比创建进程要快 10 倍甚至更多。 线程都是同一个进程下自家的孩子，像是内存页、页表等就不需要了。 ","date":"2022-03-16","objectID":"/gmp-model/:1:2","tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/"},{"categories":["golang"],"content":"协程 协程（Coroutine）是用户态的线程。通常创建协程时，会从进程的堆中分配一段内存作为协程的栈。 线程的栈有 8 MB，而协程栈的大小通常只有 KB，而 Go 语言的协程更夸张，只有 2-4KB，非常的轻巧。 协程有以下优势👋： 👉节省 CPU：避免系统内核级的线程频繁切换，造成的 CPU 资源浪费。好钢用在刀刃上。而协程是用户态的线程，用户可以自行控制协程的创建于销毁，极大程度避免了系统级线程上下文切换造成的资源浪费。 👉节约内存：在 64 位的Linux中，一个线程需要分配 8MB 栈内存和 64MB 堆内存，系统内存的制约导致我们无法开启更多线程实现高并发。而在协程编程模式下，可以轻松有十几万协程，这是线程无法比拟的。 👉稳定性：前面提到线程之间通过内存来共享数据，这也导致了一个问题，任何一个线程出错时，进程中的所有线程都会跟着一起崩溃。 👉开发效率：使用协程在开发程序之中，可以很方便的将一些耗时的IO操作异步化，例如写文件、耗时 IO 请求等。 ","date":"2022-03-16","objectID":"/gmp-model/:1:3","tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/"},{"categories":["golang"],"content":"goroutine 是什么 Goroutine 是一个由 Go 运行时管理的轻量级线程，我们称为 “协程”。 go f(x, y, z) 操作系统本身是无法明确感知到 Goroutine 的存在的，Goroutine 的操作和切换归属于 “用户态” 中。 Goroutine 由特定的调度模式来控制，以 “多路复用” 的形式运行在操作系统为 Go 程序分配的几个系统线程上。 同时创建 Goroutine 的开销很小，初始只需要 2-4k 的栈空间。Goroutine 本身会根据实际使用情况进行自伸缩，非常轻量。 Tips Go程序中没有语言级的关键字让你去创建一个内核线程，你只能创建 goroutine，内核线程只能由 runtime 根据实际情况去创建。 Go运行时系统并没有内核调度器的中断能力，内核调度器会发起抢占式调度将长期运行的线程中断并让出CPU资源，让其他线程获得执行机会。 ","date":"2022-03-16","objectID":"/gmp-model/:2:0","tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/"},{"categories":["golang"],"content":"什么是调度 用户态的 Goroutine，操作系统看不到它。必然需要有某个东西去管理他，才能更好的运作起来。 这就是 Go 语言中的调度，也就是 GMP 模型。 Go scheduler /ˈskedʒuːlər/ 的主要功能是针对在处理器上运行的 OS 线程分发可运行的 Goroutine，而我们一提到调度器，就离不开三个经常被提到的缩写，分别是： G：Goroutine，实际上我们每次调用 go func 就是生成了一个 G。 P：Processor，处理器，一般 P 的数量就是处理器的核数，可以通过 GOMAXPROCS 进行修改。 M：Machine，系统线程。 这三者交互实际来源于 Go 的 M: N 调度模型。也就是 M 必须与 P 进行绑定，然后不断地在 M 上循环寻找可运行的 G 来执行相应的任务。 ","date":"2022-03-16","objectID":"/gmp-model/:3:0","tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/"},{"categories":["golang"],"content":"调度流程 调度流程调度流程 \" 调度流程 当我们执行 go func() 时，实际上就是创建一个全新的 Goroutine，我们称它为 G。 新创建的 G 会被放入 P 的本地队列（Local Queue）或全局队列（Global Queue）中，准备下一步的动作。需要注意的一点，这里的 P 指的是创建 G 的 P。 唤醒或创建 M 以便执行 G。 不断地进行事件循环 寻找在可用状态下的 G 进行执行任务 清除后，重新进入事件循环 在描述中有提到全局和本地这两类队列，其实在功能上来讲都是用于存放正在等待运行的 G，但是不同点在于，本地队列有数量限制，不允许超过 256 个。 并且在新建 G 时，会优先选择 P 的本地队列，如果本地队列满了，则将 P 的本地队列的一半的 G 移动到全局队列。 可以理解为调度资源的共享和再平衡。 ","date":"2022-03-16","objectID":"/gmp-model/:4:0","tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/"},{"categories":["golang"],"content":"窃取行为 可以看到图上有 steal 行为，这是用来做什么的呢，我们都知道当你创建新的 G 或者 G 变成可运行状态时，它会被推送加入到当前 P 的本地队列中。 其实当 P 执行 G 完毕后，它也会 “干活”，它会将其从本地队列中弹出 G，同时会检查当前本地队列是否为空，如果为空会随机的从其他 P 的本地队列中尝试窃取一半可运行的 G 到自己的名下。 窃取行为窃取行为 \" 窃取行为 上图中👆，P2 在本地队列中找不到可以运行的 G，它会执行 work-stealing 调度算法，随机选择其它的处理器 P1，并从 P1 的本地队列中窃取了三个 G 到它自己的本地队列中去。 至此，P1、P2 都拥有了可运行的 G，P1 多余的 G 也不会被浪费，调度资源将会更加平均的在多个处理器中流转。 ","date":"2022-03-16","objectID":"/gmp-model/:5:0","tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/"},{"categories":["golang"],"content":"限制条件 ","date":"2022-03-16","objectID":"/gmp-model/:6:0","tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/"},{"categories":["golang"],"content":"M 的限制 在协程的执行中，真正干活的是 GPM 中的 M（系统线程） ，因为 G 是用户态上的东西，最终执行都是得映射，对应到 M 这一个系统线程上去运行。 那么 M 有没有限制呢？ 答案是：有的。在 Go 语言中，M 的默认数量限制是 10000，如果超出则会报错： GO: runtime: program exceeds 10000-thread limit 但是通常只有在 Goroutine 出现阻塞操作的情况下，才会遇到这种情况。这可能也预示着你的程序有问题。 若确切是需要那么多，还可以通过 debug.SetMaxThreads 方法进行设置。 ","date":"2022-03-16","objectID":"/gmp-model/:6:1","tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/"},{"categories":["golang"],"content":"G 的限制 Goroutine 的创建数量是否有限制？ 答案是：没有。但理论上会受内存的影响，假设一个 Goroutine 创建需要 4k 的连续的内存块： 4k * 80,000 = 320,000k ≈ 0.3G内存 4k * 1,000,000 = 4,000,000k ≈ 4G内存 以此就可以相对计算出来一台单机在通俗情况下，所能够创建 Goroutine 的大概数量级别。 ","date":"2022-03-16","objectID":"/gmp-model/:6:2","tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/"},{"categories":["golang"],"content":"P 的限制 P 的数量是否有限制，受什么影响？ 答案是：有限制。P 的数量受环境变量 GOMAXPROCS 的直接影响。 环境变量 GOMAXPROCS 又是什么？在 Go 语言中，通过设置 GOMAXPROCS，用户可以调整调度中 P（Processor）的数量。 另一个重点在于，与 P 相关联的的 M（系统线程），是需要绑定 P 才能进行具体的任务执行的，因此 P 的多少会影响到 Go 程序的运行表现。 P 的数量基本是受本机的核数影响，没必要太过度纠结他。 那 P 的数量是否会影响 Goroutine 的数量创建呢？ 答案是：不影响。且 Goroutine 多了少了，P 也该干嘛干嘛，不会带来灾难性问题。 ","date":"2022-03-16","objectID":"/gmp-model/:6:3","tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/"},{"categories":["golang"],"content":"小结 M：有限制，默认数量限制是 10000，可调整。 G：没限制，但受内存影响。 P：受本机的核数影响，可大可小，不影响 G 的数量创建。 所以Goroutine 数量怎么预算，才叫合理？ 在真实的应用场景中，如果你 Goroutine： 在频繁请求 HTTP，MySQL，打开文件等，那假设短时间内有几十万个协程在跑，那肯定就不大合理了（可能会导致 too many files open）。 常见的 Goroutine 泄露所导致的 CPU、Memory 上涨等，还是得看你的 Goroutine 里具体在跑什么东西。 跑的如果是 “资源怪兽”，只运行几个 Goroutine 都可以跑死。 ","date":"2022-03-16","objectID":"/gmp-model/:6:4","tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/"},{"categories":["golang"],"content":"为什么要有 P // TODO ","date":"2022-03-16","objectID":"/gmp-model/:7:0","tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/"},{"categories":["golang"],"content":"参考 Go 为什么这么“快” 让你很快就能理解-go的协程调度原理 Goroutine 数量控制在多少合适，会影响 GC 和调度？ Golang goroutine与调度器 进程与线程的一个简单解释 [典藏版] Golang 调度器 GMP 原理与调度全分析 ","date":"2022-03-16","objectID":"/gmp-model/:8:0","tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/"},{"categories":["golang"],"content":"go build 常用命令,golang,编译,go 编译","date":"2022-03-16","objectID":"/go-build-args/","tags":["golang"],"title":"Go 常用命令","uri":"/go-build-args/"},{"categories":["golang"],"content":"常用编译参数 参数 说明 -o 指定输出可执行文件名 -v 编译时显示包名，可以理解成输出详细编译信息 -race 开启竞态检测 *.go 编译当前目录下的所有go文件，也可以写成 f2.go f2.go … -a 强制重新构建 -w 去掉DWARF调试信息，得到的程序就不能用gdb调试了 -s 去掉符号表,panic时候的stack trace就没有任何文件名/行号信息了，这个等价于普通C/C++程序被strip的效果 -X 设置包中的变量值 -gcflags \"-N -l\" 编译目标程序的时候会嵌入运行时(runtime)的二进制，禁止优化和内联可以让运行时(runtime)中的函数变得更容易调试。gcflags 其实是给go编译器传入参数，也就是传给go tool compile的参数，因此可以用go tool compile --help查看所有可用的参数 -ldflags 给go链接器传入参数，实际是给go tool link的参数，可以用go tool link --help查看可用的参数。 -ldflags '-extldflags \"-static\"' 静态编译 ","date":"2022-03-16","objectID":"/go-build-args/:1:0","tags":["golang"],"title":"Go 常用命令","uri":"/go-build-args/"},{"categories":["golang"],"content":"交叉编译 参数 说明 GOOS GOARCH linux 386 / amd64 / arm darwin 386 / amd64 feedbsd 386 / amd64 windows 386 / amd64 对于编译给ARM使用的Go程序，需要根据实际情况配置$GOARM，这是用来控制CPU的浮点协处理器的参数。 $GOARM默认是6，对于不支持VFP使用软件运算的老版本ARM平台要设置成5，支持VFPv1的设置成6，支持VFPv3的设置成7。 示例 GOARM=7 GOARCH=arm GOOS=linux go build -v -o fca ","date":"2022-03-16","objectID":"/go-build-args/:2:0","tags":["golang"],"title":"Go 常用命令","uri":"/go-build-args/"},{"categories":["golang"],"content":"go mod // TODO ","date":"2022-03-16","objectID":"/go-build-args/:3:0","tags":["golang"],"title":"Go 常用命令","uri":"/go-build-args/"},{"categories":["golang"],"content":"参考 golang编译时的参数传递（gcflags, ldflags） Golang交叉编译（跨平台编译）简述 交叉编译Go程序 ARM flags GOARM go mod使用 ","date":"2022-03-16","objectID":"/go-build-args/:4:0","tags":["golang"],"title":"Go 常用命令","uri":"/go-build-args/"},{"categories":["web"],"content":"node,nodejs, heap out of memory","date":"2022-03-16","objectID":"/node-oom/","tags":["web","node"],"title":"JavaScript heap out of memory","uri":"/node-oom/"},{"categories":["web"],"content":"刚在打包项目时执行 yarn run build 时出现了 oom 的情况，具体报错信息如下： JavaScript heap out of memory \" 我的环境是 win10 专业版 WSL。 解决办法，设置 export NODE_OPTIONS=--max_old_space_size=4096，设置完之后重新执行 yarn run build 即可。 ","date":"2022-03-16","objectID":"/node-oom/:0:0","tags":["web","node"],"title":"JavaScript heap out of memory","uri":"/node-oom/"},{"categories":["web"],"content":"参考 Node.js heap out of memory ","date":"2022-03-16","objectID":"/node-oom/:1:0","tags":["web","node"],"title":"JavaScript heap out of memory","uri":"/node-oom/"},{"categories":["开发者手册"],"content":"Google,浏览器插件,插件下载","date":"2022-03-16","objectID":"/googe-plugin-download/","tags":["chrome"],"title":"将google浏览器插件下载到本地","uri":"/googe-plugin-download/"},{"categories":["开发者手册"],"content":"国内的网络太复杂了，在不能访问 google 的情况下，甚至都不能打开网上应用商店，所以我们需要一个方便的方式来下载google浏览器插件并分享 给需要的小伙伴。 我们打开任意一个浏览器插件，如： 浏览器插件 \" URL 地址栏中有一串字符串，这是唯一的，通过这个字符串可以获取到插件的下载地址，如： 插件UUID \" 下载地址为： https://clients2.google.com/service/update2/crx?response=redirect\u0026os=win\u0026arch=x64\u0026os_arch=x86_64\u0026nacl_arch=x86-64\u0026prod=chromecrx\u0026prodchannel=\u0026prodversion=77.0.3865.90\u0026lang=zh-CN\u0026acceptformat=crx2,crx3\u0026x=id%3D{XXXX}%26installsource%3Dondemand%26uc 将以上的 {XXXX} 替换为插件的 ID，就可以下载到本地了。 以下这个地址是Mote：语音笔记和反馈插件的下载地址，成功下载的插件是 .crx 结尾的文件。直接拖到浏览器中就会自动安装。 https://clients2.google.com/service/update2/crx?response=redirect\u0026os=win\u0026arch=x64\u0026os_arch=x86_64\u0026nacl_arch=x86-64\u0026prod=chromecrx\u0026prodchannel=\u0026prodversion=77.0.3865.90\u0026lang=zh-CN\u0026acceptformat=crx2,crx3\u0026x=id%3Dajphlblkfpppdpkgokiejbjfohfohhmk%26installsource%3Dondemand%26uc ","date":"2022-03-16","objectID":"/googe-plugin-download/:0:0","tags":["chrome"],"title":"将google浏览器插件下载到本地","uri":"/googe-plugin-download/"},{"categories":null,"content":"xiaobinqt's personal website,xiaobinqt,程序员,程序猿,xiaobinqt@163.com","date":"2022-03-06","objectID":"/about/","tags":null,"title":"About Me","uri":"/about/"},{"categories":null,"content":"缘起 很喜欢电影《五亿探长雷洛传》，电影一开头，由潮州迁居香港的青年雷洛，为了生计，投考香港警察，面试官问他，为什么要当警察，他回答，为了吃饭。 为了吃饭为了吃饭 \" 为了吃饭 大学毕业后，为了吃饭，便利用自己了解的一点编程知识开始码农之路。大学学的是通信工程，非计算机专业，编程之路困难重重，但好歹自己有自知之明，笨人多努力，相信通过自己的努力，可以让自己的编程技术更加完善。 好记性不如烂笔头，想着把自己工作期间遇到的问题，平时看到的好的文章，记录下来，方便查找和温习，也是记录自己的不足和成长，便有了这个网站。 建站的初衷不是为了炫耀所知，而是记录无知。 人知道得越多，就会发现无知的越多。有更广袤的世界可以探索，是莫大的快乐呀！ ","date":"2022-03-06","objectID":"/about/:1:0","tags":null,"title":"About Me","uri":"/about/"},{"categories":null,"content":"关于作者 👨‍💻 半路出家的程序猿，技术不精，但有一个成为技术大拿的梦想。 🤪 拖延症患者，持续性混吃等死，间接性踌躇满志。 💕 爱好读书，但总是边读边忘 😢。 ","date":"2022-03-06","objectID":"/about/:2:0","tags":null,"title":"About Me","uri":"/about/"},{"categories":null,"content":"职业生涯 2017/4 - 2019/3 在西安一家科技公司从事 PHP 开发。 2019/4 - 2021/3 在北京腾讯外包从事 PHP 和 Go 开发。 2021/4 - 至今 版权说明 本站图片和文字，除原创作品之外，部分来自互联网。 此类资源的原版权所有者可在任何时候、以任何理由要求本站停止使用，其中包括被本站编辑（比如加注说明）过的资源， 联系方式见下文。 ","date":"2022-03-06","objectID":"/about/:3:0","tags":null,"title":"About Me","uri":"/about/"},{"categories":null,"content":"联系方式 邮箱：xiaobinqt@163.com Github：@xiaobinqt 微信： 个人微信\" 个人微信 ","date":"2022-03-06","objectID":"/about/:4:0","tags":null,"title":"About Me","uri":"/about/"},{"categories":["golang"],"content":"golang grpc,golang,rpc,grpc,grpc 入门","date":"2022-03-05","objectID":"/grpc-demo/","tags":["golang"],"title":"grpc 入门应用","uri":"/grpc-demo/"},{"categories":["golang"],"content":"RPC 是一种跨语言的协议，它可以让我们在不同的语言之间进行通信。 远程过程调用（英语：Remote Procedure Call，缩写为 RPC）是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一个 地址空间（通常为一个开放网络的一台计算机）的子程序，而程序员就像调用本地程序一样，无需额外地为这个交互作用编程（无需关注细节）。 RPC是一种服务器-客户端（Client/Server）模式，经典实现是一个通过发送请求-接受回应进行信息交互的系统。 ","date":"2022-03-05","objectID":"/grpc-demo/:0:0","tags":["golang"],"title":"grpc 入门应用","uri":"/grpc-demo/"},{"categories":["golang"],"content":"安装 go install github.com/golang/protobuf/protoc-gen-go@v1.4.0 go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.1 不推荐使用 google.golang.org/protobuf/cmd/protoc-gen-go@v1.26 这个版本太高了，可能会遇到以下这个问题， --go_out: protoc-gen-go: plugins are not supported; use 'protoc --go-grpc_out=...' to generate gRPC See https://grpc.io/docs/languages/go/quickstart/#regenerate-grpc-code for more information. 生成代码遇到的问题 \" 参考解决方案记一次奇妙的go-protobuf包升级之旅 protoc 工具安装 下载地址，下载解压将 bin 目录添加到环境变量中。 protoc \" ","date":"2022-03-05","objectID":"/grpc-demo/:1:0","tags":["golang"],"title":"grpc 入门应用","uri":"/grpc-demo/"},{"categories":["golang"],"content":"定义 proto 文件 syntax = \"proto3\"; // 使用protobuf版本3 option go_package = \"./protobuf\"; // 这个影响生成的目录及go的package命名 // 定义一个计算服务, 输入为CalcRequest, 输出为CalcResponse service CalculatorService { rpc calc(CalcRequest) returns (CalcResponse) {};}// 计算两个数某种运算(如加法)的参数 message CalcRequest { double a = 1; double b = 2; string op = 3;}// 计算结果 message CalcResponse { double r = 1;} ","date":"2022-03-05","objectID":"/grpc-demo/:2:0","tags":["golang"],"title":"grpc 入门应用","uri":"/grpc-demo/"},{"categories":["golang"],"content":"生成 .pb.go 文件 protoc --go_out=plugins=grpc:. calculator.proto 整体目录结构 \" ","date":"2022-03-05","objectID":"/grpc-demo/:3:0","tags":["golang"],"title":"grpc 入门应用","uri":"/grpc-demo/"},{"categories":["golang"],"content":"rpc server package main import ( \"context\" \"fmt\" \"net\" \"go.src/grpc/calculator/protobuf\" \"google.golang.org/grpc\" ) // 实现: CalculatorServiceServer接口, 在calculator.pb.go中定义 type server struct{} func (s server) Calc(ctx context.Context, req *protobuf.CalcRequest) (resp *protobuf.CalcResponse, err error) { a := req.GetA() b := req.GetB() op := req.GetOp() resp = \u0026protobuf.CalcResponse{} switch op { case \"+\": resp.R = a + b case \"-\": resp.R = a - b case \"*\": resp.R = a * b case \"/\": if b == 0 { err = fmt.Errorf(\"divided by zero\") return } resp.R = a / b } return } // 启动rpc server func main() { listener, err := net.Listen(\"tcp\", \"localhost:3233\") if err != nil { panic(err) } s := grpc.NewServer() protobuf.RegisterCalculatorServiceServer(s, \u0026server{}) fmt.Println(\"server start\") err = s.Serve(listener) if err != nil { panic(err) } } ","date":"2022-03-05","objectID":"/grpc-demo/:4:0","tags":["golang"],"title":"grpc 入门应用","uri":"/grpc-demo/"},{"categories":["golang"],"content":"rpc client package main import ( \"context\" \"fmt\" \"log\" \"go.src/grpc/calculator/protobuf\" \"google.golang.org/grpc\" \"google.golang.org/grpc/credentials/insecure\" ) func main() { // 连上grpc server //conn, err := grpc.Dial(\"localhost:3233\", grpc.WithInsecure()) conn, err := grpc.Dial(\"localhost:3233\", grpc.WithTransportCredentials(insecure.NewCredentials())) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer conn.Close() c := protobuf.NewCalculatorServiceClient(conn) // 调用远程方法 resp, err := c.Calc(context.Background(), \u0026protobuf.CalcRequest{ A: 1, B: 2, Op: \"+\", }) if err != nil { fmt.Println(\"calc err: \", err.Error()) return } fmt.Println(\"calc success,respR: \", resp.GetR()) // 3 } ","date":"2022-03-05","objectID":"/grpc-demo/:5:0","tags":["golang"],"title":"grpc 入门应用","uri":"/grpc-demo/"},{"categories":["golang"],"content":"运行测试 serverserver \" server clientclient \" client ","date":"2022-03-05","objectID":"/grpc-demo/:6:0","tags":["golang"],"title":"grpc 入门应用","uri":"/grpc-demo/"},{"categories":["golang"],"content":"示例下载 示例源码地址 ","date":"2022-03-05","objectID":"/grpc-demo/:7:0","tags":["golang"],"title":"grpc 入门应用","uri":"/grpc-demo/"},{"categories":["web"],"content":"Ajax 在请求时携带 cookie 信息,cookie 信息会被添加到请求头中,Cookie","date":"2022-03-01","objectID":"/ajax-req-add-cookie/","tags":["web"],"title":"ajax 在请求时携带 cookie 信息","uri":"/ajax-req-add-cookie/"},{"categories":["web"],"content":"最近有个需求在使用 $.ajax 时需要把 cookie 信息也带着，google 下发现可以这么写： $.ajax({ url: \"/nodered/nodes\", headers: { Accept: \"text/html\", }, xhrFields: { withCredentials: true // 携带 cookie 信息 }, success: function (data) { console.log(data) $(\"#red-ui-palette-container\").html(data) }, error: function (jqXHR) { console.log(jqXHR) } }); ","date":"2022-03-01","objectID":"/ajax-req-add-cookie/:0:0","tags":["web"],"title":"ajax 在请求时携带 cookie 信息","uri":"/ajax-req-add-cookie/"},{"categories":["golang"],"content":"running gcc failed: exit status 1","date":"2022-02-10","objectID":"/build-running-gcc-failed/","tags":["golang","build"],"title":"running gcc failed: exit status 1","uri":"/build-running-gcc-failed/"},{"categories":["golang"],"content":"今天在编译 go 项目时出现了如下错误： /usr/local/go/pkg/tool/linux_amd64/link: running gcc failed: exit status 1 /usr/bin/ld: cannot find -lpthread /usr/bin/ld: cannot find -lc collect2: error: ld returned 1 exit status 解决办法： yum install glibc-static.x86_64 -y ","date":"2022-02-10","objectID":"/build-running-gcc-failed/:0:0","tags":["golang","build"],"title":"running gcc failed: exit status 1","uri":"/build-running-gcc-failed/"},{"categories":["算法与数学"],"content":"一致性Hash,hash算法,数据倾斜","date":"2022-01-15","objectID":"/consistent-hash/","tags":["算法"],"title":"一致性 hash","uri":"/consistent-hash/"},{"categories":["算法与数学"],"content":"存在的意义 一致性哈希算法解决了普通余数 Hash 算法伸缩性差的问题，可以保证在上线、下线服务器的情况下尽量有多的请求命中原来路由到的服务器。 ","date":"2022-01-15","objectID":"/consistent-hash/:1:0","tags":["算法"],"title":"一致性 hash","uri":"/consistent-hash/"},{"categories":["算法与数学"],"content":"优化 一致性哈希算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜问题。可以通过通过增加虚拟节点来解决数据倾斜问题。 如果存在大量的虚拟节点，节点的查找性能就成为必须考虑的因数。可以使用红黑树 来加快查找速度， ","date":"2022-01-15","objectID":"/consistent-hash/:2:0","tags":["算法"],"title":"一致性 hash","uri":"/consistent-hash/"},{"categories":["算法与数学"],"content":"参考 一致性Hash(Consistent Hashing)原理剖析及Java实现 图解一致性哈希算法 golang实现一致性hash环及优化方法 一致性哈希 ","date":"2022-01-15","objectID":"/consistent-hash/:3:0","tags":["算法"],"title":"一致性 hash","uri":"/consistent-hash/"},{"categories":["web"],"content":"xiaobinqt,node-addon-api,js 调用 c 语言,jc call c function,How to call C function from nodeJS,node 调用C/C++ 方法","date":"2021-11-18","objectID":"/js-call-c/","tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/"},{"categories":["web"],"content":"最近在 node 项目开发中，有个需求是 nodeJS 需要支持调用 C 语言的函数，node-addon-api 可以支持这个需求。 ","date":"2021-11-18","objectID":"/js-call-c/:0:0","tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/"},{"categories":["web"],"content":"开发环境 我用的开发环境 docker 起的 code-server 环境，code-server 版本为 code-server:version-v3.11.1 。可以把 code-server 理解成一个在线 vscode 环境，就像 github 的在线 web 编辑器一样。 docker pull linuxserver/code-server:version-v3.11.1 code-servercode-server \" code-server github web 编辑器github web 编辑器 \" github web 编辑器 ","date":"2021-11-18","objectID":"/js-call-c/:1:0","tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/"},{"categories":["web"],"content":"加法器示例 开发环境搭建成功后，可以实现一个小功能，以熟悉 node-addon-api 的使用。 现在实现一个加法器，JS 调用 C 语言的 add 方法，传入 2 个参数，C 语言累加后返回结果。 ","date":"2021-11-18","objectID":"/js-call-c/:2:0","tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/"},{"categories":["web"],"content":"项目初始化 创建项目并进行 npm init 初始化： 创建项目并初始化创建项目并初始化 \" 创建项目并初始化 安装 node-addon-api： npm i node-addon-api 安装 npm 依赖包安装 npm 依赖包 \" 安装 npm 依赖包 ","date":"2021-11-18","objectID":"/js-call-c/:2:1","tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/"},{"categories":["web"],"content":"c 代码 新建一个 cal.cc 文件，内容如下： #include \u003cnapi.h\u003e // 定义一个 Add() 方法 Napi::Value Add(const Napi::CallbackInfo\u0026 info) { Napi::Env env = info.Env(); // 获取 js 上下文信息 if (info.Length() \u003c 2) { Napi::TypeError::New(env, \"Wrong number of arguments\") .ThrowAsJavaScriptException(); return env.Null(); } if (!info[0].IsNumber() || !info[1].IsNumber()) { Napi::TypeError::New(env, \"Wrong arguments\").ThrowAsJavaScriptException(); return env.Null(); } int arg0 = info[0].As\u003cNapi::Number\u003e().Int32Value(); int arg1 = info[1].As\u003cNapi::Number\u003e().Int32Value(); int arg2 = arg0 + arg1; Napi::Number num = Napi::Number::New(env, arg2); return num; } // 导出函数，可使用 exports.Set() 导出多个函数 Napi::Object Init(Napi::Env env, Napi::Object exports) { exports.Set(Napi::String::New(env, \"add\"), Napi::Function::New(env, Add)); return exports; } NODE_API_MODULE(addon, Init) ","date":"2021-11-18","objectID":"/js-call-c/:2:2","tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/"},{"categories":["web"],"content":"binding.gyp 编译带第三方扩展库的 c/c++ 程序，通常需要在编译时指定额外的头文件包含路径和链接第三方库，这些都是在 binding.gyp 文件中指定的，这些指定在 nodeJS 自动编译的时候，会解析并应用在命令行的编译工具中。 新建一个 binding.gyp 文件，内容如下： { \"targets\": [ { \"target_name\": \"test\", \"sources\": [ \"cal.cc\" ], \"include_dirs\": [ \"\u003c!@(node -p \\\"require('node-addon-api').include\\\")\" ], \"libraries\": [ ], \"dependencies\": [ \"\u003c!(node -p \\\"require('node-addon-api').gyp\\\")\" ], \"cflags!\": [ \"-fno-exceptions\" ], \"cflags_cc!\": [ \"-fno-exceptions\" ], \"defines\": [ \"NAPI_CPP_EXCEPTIONS\" ], \"xcode_settings\": { \"GCC_ENABLE_CPP_EXCEPTIONS\": \"YES\" } } ] } target_name 指定了编译之后模块的名称。 sources 指明 c/c++ 的源文件，如果有多个文件，需要用逗号隔开，放到同一个数组中。 include_dirs 是编译时使用的头文件引入路径，这里使用 node -p 执行 node-addon-api 模块中的预置变量。 dependencies 是必须的，一般不要改变。 cflags!，cflags_cc!，defines 三行指定如果c++程序碰到意外错误的时候，由 NAPI 接口来处理，而不是通常的由 c/c++ 程序自己处理。这防止因为 c/c++ 部分程序碰到意外直接就退出了程序，而是由 nodeJS 程序来捕获处理，如果是在Linux中编译使用，有这三行就够了。 ","date":"2021-11-18","objectID":"/js-call-c/:2:3","tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/"},{"categories":["web"],"content":"编译调用 编译编译 \" 编译 每次修改代码后都需要执行 npm i 重新编译 npm i 编译后，进入 nodeJS 中可以直接 require 调用。 调用调用 \" 调用 这里 require 的 test.node，.node 后缀是固定的，test 就是 binding.gyp 文件里 target_name 的值。 1+3=4 从调用结果来看，符合预期。 ","date":"2021-11-18","objectID":"/js-call-c/:2:4","tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/"},{"categories":["web"],"content":"bindings 包 现在我们 require 编译后的 node 需要这样写： require('./build/Release/nodecamera.node'); 可以用 bindings 包简化 require 。 npm i bindings --save 通估👆命令安装 bindings 包。 bindings 包使用bindings 包使用 \" bindings 包使用 所以以上示例简化后的 require 为： const addon = require('bindings')('test.node'); ","date":"2021-11-18","objectID":"/js-call-c/:2:5","tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/"},{"categories":["web"],"content":"常见数据类型转换 JS 与 C 的数据类型有较大差别，比如 C 中没有字符串的概念，只有字节数组等。node-addon-api 可以很好的支持 JS 与 C 数据类型的转换。 ","date":"2021-11-18","objectID":"/js-call-c/:3:0","tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/"},{"categories":["web"],"content":"字符串 std::string temp = info[0].As\u003cNapi::String\u003e().ToString(); 字符串转换示例字符串转换示例 \" 字符串转换示例 ","date":"2021-11-18","objectID":"/js-call-c/:3:1","tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/"},{"categories":["web"],"content":"ArrayBuffer Napi::ArrayBuffer ABuffer(const Napi::CallbackInfo\u0026 info) { Napi::Env env = info.Env(); int8_t num[4] = {14,25,45,88}; Napi::ArrayBuffer x = Napi::ArrayBuffer::New(env,num,4); return x; } ArrayBuffer 示例ArrayBuffer 示例 \" ArrayBuffer 示例 ","date":"2021-11-18","objectID":"/js-call-c/:3:2","tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/"},{"categories":["web"],"content":"数组 JS 将数组作为 C 函数参数。 Napi::Value ArrayArg(const Napi::CallbackInfo\u0026 info) { Napi::Env env = info.Env(); Napi::Array b = info[0].As\u003cNapi::Array\u003e(); for (int i = 0; i \u003c b.Length(); i++) { Napi::Value v = b[i]; if (v.IsString()){ std::string value = (std::string)v.As\u003cNapi::String\u003e(); return Napi::String::New(env,value); } } } 编译可能有 warning编译可能有 warning \" 编译可能有 warning 编译时可能有 warning，但是不影响。 数组参数数组参数 \" 数组参数 ","date":"2021-11-18","objectID":"/js-call-c/:3:3","tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/"},{"categories":["web"],"content":"FAQ ","date":"2021-11-18","objectID":"/js-call-c/:4:0","tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/"},{"categories":["web"],"content":"持久化函数 这个功能可以理解成在 C 的内存空间中有一个 JS 的函数对象且在生命周期内不会被 C 垃圾回收，可以直接在 C 中调用这个 JS 函数。 以下示例，C 提供了 debug 函数，但是参数是一个函数，这个函数会持久在 C 的内存中，在 C 的 Str 函数中用 Call 来调用这个函数并传入对应的参数。 js-call-c-demo.js const addon = require('bindings')('test.node'); // 调用 c 中的 debug 函数，将函数注入到 c 中 addon.debug(msg =\u003e { console.log(\"debug console, c 中传入的 msg 需要打印的参数值为：\", msg) }) // 调用 c 的 str 函数，在 str 函数中会调用 debug 函数中的 console.log() console.log(\"str 函数的返回值为: \", addon.str(\"xiaobinqt\")) cal.cc #include \u003cnapi.h\u003e Napi::FunctionReference Debug; napi_env DebugEnv; Napi::Value DebugFun(const Napi::CallbackInfo\u0026 info) { Napi::Env env = info.Env(); Debug = Napi::Persistent(info[0].As\u003cNapi::Function\u003e()); DebugEnv = env; return Napi::String::New(env,\"OK\"); } Napi::Value Str(const Napi::CallbackInfo\u0026 info) { Napi::Env env = info.Env(); std::string temp = info[0].As\u003cNapi::String\u003e().ToString(); Napi::String s = Napi::String::New(env, temp); // 调用 Debug 函数 Debug.Call({Napi::String::New(DebugEnv,\"我是一个测试 debug\")}); return s; } // 定义一个 Add() 方法 Napi::Value Add(const Napi::CallbackInfo\u0026 info) { Napi::Env env = info.Env(); // 获取 js 上下文信息 if (info.Length() \u003c 2) { Napi::TypeError::New(env, \"Wrong number of arguments\") .ThrowAsJavaScriptException(); return env.Null(); } if (!info[0].IsNumber() || !info[1].IsNumber()) { Napi::TypeError::New(env, \"Wrong arguments\").ThrowAsJavaScriptException(); return env.Null(); } int arg0 = info[0].As\u003cNapi::Number\u003e().Int32Value(); int arg1 = info[1].As\u003cNapi::Number\u003e().Int32Value(); int arg2 = arg0 + arg1; Napi::Number num = Napi::Number::New(env, arg2); return num; } Napi::ArrayBuffer ABuffer(const Napi::CallbackInfo\u0026 info) { Napi::Env env = info.Env(); int8_t num[4] = {14,25,45,88}; Napi::ArrayBuffer x = Napi::ArrayBuffer::New(env,num,4); return x; } Napi::Value ArrayArg(const Napi::CallbackInfo\u0026 info) { Napi::Env env = info.Env(); Napi::Array b = info[0].As\u003cNapi::Array\u003e(); for (int i = 0; i \u003c b.Length(); i++) { Napi::Value v = b[i]; if (v.IsString()){ std::string value = (std::string)v.As\u003cNapi::String\u003e(); return Napi::String::New(env,value); } } } // 导出函数，可使用 exports.Set() 导出多个函数 Napi::Object Init(Napi::Env env, Napi::Object exports) { exports.Set(Napi::String::New(env, \"add\"), Napi::Function::New(env, Add)); exports.Set(Napi::String::New(env, \"str\"), Napi::Function::New(env, Str)); exports.Set(Napi::String::New(env, \"ab\"), Napi::Function::New(env, ABuffer)); exports.Set(Napi::String::New(env, \"arr\"), Napi::Function::New(env, ArrayArg)); exports.Set(Napi::String::New(env, \"debug\"), Napi::Function::New(env, DebugFun)); return exports; } NODE_API_MODULE(addon, Init) 测试结果测试结果 \" 测试结果 ","date":"2021-11-18","objectID":"/js-call-c/:4:1","tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/"},{"categories":["web"],"content":"参考 简单上手nodejs调用c++(c++和js的混合编程) node-addon-api-doc https://github.com/nodejs/node-addon-api https://nodejs.github.io/node-addon-examples/special-topics/object-function-refs#persistent-reference ","date":"2021-11-18","objectID":"/js-call-c/:5:0","tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/"},{"categories":["开发者手册"],"content":"在线文档 build-web-application-with-golang Mastering_Go golang 修养之路 Go语言101 PHP扩展开发及内核应用 JavaScript 标准参考教程 ES6 入门教程 JavaScript 教程 Docker-从入门到实战 网道 PHP编程之道 gairuo Kubernetes 文档 Uber Go 语言编码规范 地鼠文档 面向信仰编程 煎鱼 xargin go Standard library alblue 刘丹冰aceld 床长人工智能教程 ","date":"2021-10-17","objectID":"/memo/:1:0","tags":["备忘"],"title":"电子书/工具收藏","uri":"/memo/"},{"categories":["开发者手册"],"content":"工具收藏 navicat premium15破解教程 ","date":"2021-10-17","objectID":"/memo/:2:0","tags":["备忘"],"title":"电子书/工具收藏","uri":"/memo/"},{"categories":["开发者手册"],"content":"文章收藏 go1.16 embed 用法 ","date":"2021-10-17","objectID":"/memo/:3:0","tags":["备忘"],"title":"电子书/工具收藏","uri":"/memo/"},{"categories":["golang"],"content":"go构造函数,go初始化函数,go函数式选项模式,golang函数式编程","date":"2021-08-23","objectID":"/functional-options-pattern/","tags":["golang"],"title":"go 函数式选项模式","uri":"/functional-options-pattern/"},{"categories":["golang"],"content":"Go 语言没有构造函数，一般通过定义 New 函数来充当构造函数。但是，如果结构有较多字段，要初始化这些字段，就有很多种方式，有一种方式被认为是最优雅的，就是函数式选项模式（Functional Options Pattern）。 函数式选项模式用于构造函数和其他公共 API 中的可选参数，你预计这些参数需要扩展，尤其是在这些函数上已经有三个或更多参数的情况下。 ","date":"2021-08-23","objectID":"/functional-options-pattern/:0:0","tags":["golang"],"title":"go 函数式选项模式","uri":"/functional-options-pattern/"},{"categories":["golang"],"content":"常规方式 我们有如下结构体： type Server struct { host string // 必填 port int // 必填 timeout time.Duration // 可选 maxConn int // 可选 } host 和 port 字段是必须的，timeout 和 maxConn 字段是可选的。 之前我的做法是这样处理的，定义一个 New 函数，初始化必填字段，对每个可选字段都定义了一个 SetXXX 函数，如下： package main import \"time\" type Server struct { host string // 必填 port int // 必填 timeout time.Duration // 可选 maxConn int // 可选 } func New(host string, port int) *Server { return \u0026Server{ host: host, port: port, timeout: 0, maxConn: 0, } } func (s *Server) SetTimeout(timeout time.Duration) { s.timeout = timeout } func (s *Server) SetMaxConn(maxConn int) { s.maxConn = maxConn } func main() { } 个人觉得这种方式其实已经很优雅了，一般情况下也是够用的。 ","date":"2021-08-23","objectID":"/functional-options-pattern/:1:0","tags":["golang"],"title":"go 函数式选项模式","uri":"/functional-options-pattern/"},{"categories":["golang"],"content":"Functional Option Pattern package main import ( \"log\" \"time\" ) type Server struct { host string // 必填 port int // 必填 timeout time.Duration // 可选 maxConn int // 可选 } type Option func(*Server) func New(options ...Option) *Server { svr := \u0026Server{} for _, f := range options { f(svr) } return svr } func WithHost(host string) Option { return func(s *Server) { s.host = host } } func WithPort(port int) Option { return func(s *Server) { s.port = port } } func WithTimeout(timeout time.Duration) Option { return func(s *Server) { s.timeout = timeout } } func WithMaxConn(maxConn int) Option { return func(s *Server) { s.maxConn = maxConn } } func (s *Server) Run() error { // ... return nil } func main() { svr := New( WithHost(\"localhost\"), WithPort(8080), WithTimeout(time.Minute), WithMaxConn(120), ) if err := svr.Run(); err != nil { log.Fatal(err) } } 在这个模式中，我们定义一个 Option 函数类型，它接收一个参数：*Server。然后，Server 的构造函数接收一个 Option 类型的不定参数。 func New(options ...Option) *Server { svr := \u0026Server{} for _, f := range options { f(svr) } return svr } 选项的定义需要定义一系列相关返回 Option 的函数，如： func WithPort(port int) Option { return func(s *Server) { s.port = port } } 如果增加选项，只需要增加对应的 WithXXX 函数即可。 ","date":"2021-08-23","objectID":"/functional-options-pattern/:2:0","tags":["golang"],"title":"go 函数式选项模式","uri":"/functional-options-pattern/"},{"categories":["golang"],"content":"参考 Golang Functional Options Pattern ","date":"2021-08-23","objectID":"/functional-options-pattern/:3:0","tags":["golang"],"title":"go 函数式选项模式","uri":"/functional-options-pattern/"},{"categories":["golang"],"content":"go make 和 new 的区别,golang make,golang new,defference with golang make and new","date":"2021-06-21","objectID":"/new-make-difference/","tags":["golang"],"title":"golang make 和 new 的区别","uri":"/new-make-difference/"},{"categories":["golang"],"content":" make 的作用是初始化内置的数据结构，也就是 slice、map和 channel。 new 的作用是根据传入的类型分配一片内存空间并返回指向这片内存空间的指针。 ","date":"2021-06-21","objectID":"/new-make-difference/:0:0","tags":["golang"],"title":"golang make 和 new 的区别","uri":"/new-make-difference/"},{"categories":["golang"],"content":"make 内置函数 make 仅支持 slice、map、channel 三种数据类型的内存创建，其返回值是所创建类型的本身，而不是新的指针引用。 func make(t Type, size ...IntegerType) Type func main() { v1 := make([]int, 1, 5) v2 := make(map[int]bool, 5) v3 := make(chan int, 1) fmt.Println(v1, v2, v3) } 在☝️代码中，我们分别对三种类型调用了 make 函数进行了初始化。会发现有的入参是有多个长度指定，有的没有。 这里的区别主要是长度（len）和容量（cap）的指定，有的类型是没有容量这一说法。 输出结果： [0] map[] 0xc000044070 调用 make函数去初始化切片（slice）的类型时，会带有零值。 ","date":"2021-06-21","objectID":"/new-make-difference/:1:0","tags":["golang"],"title":"golang make 和 new 的区别","uri":"/new-make-difference/"},{"categories":["golang"],"content":"new 内置函数 new 可以对任意类型进行内存创建和初始化。其返回值是所创建类型的指针引用。 func new(Type) *Type new(T) 和 \u0026T{} 效果是一样的。 ","date":"2021-06-21","objectID":"/new-make-difference/:2:0","tags":["golang"],"title":"golang make 和 new 的区别","uri":"/new-make-difference/"},{"categories":["golang"],"content":"区别 make 函数在初始化时，会初始化 slice、chan、map 类型的内部数据结构，new 函数并不会。 例如，在 map 类型中，合理的长度（len）和容量（cap）可以提高效率和减少开销。 make 函数： 能够分配并初始化类型所需的内存空间和结构，返回引用类型的本身。 具有使用范围的局限性，仅支持 channel、map、slice 三种类型。 具有独特的优势，make 函数会对三种类型的内部数据结构（长度、容量等）赋值。 new 函数： 能够分配类型所需的内存空间，返回指针引用（指向内存的指针），同时把分配的内存置为零，也就是类型的零值。 可被替代，其实不常用，我们通常都是采用短语句声明以及结构体的字面量达到我们的目的，比如： i := 0 u := user{} ","date":"2021-06-21","objectID":"/new-make-difference/:3:0","tags":["golang"],"title":"golang make 和 new 的区别","uri":"/new-make-difference/"},{"categories":["golang"],"content":"参考 make、new操作 Go make 和 new的区别 ","date":"2021-06-21","objectID":"/new-make-difference/:4:0","tags":["golang"],"title":"golang make 和 new 的区别","uri":"/new-make-difference/"},{"categories":["开发者手册"],"content":"xiaobinqt,github PR,PR,pull request,如何使用 PR, Can’t automatically merge ","date":"2021-04-29","objectID":"/pull-request/","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"总览 之前在 CSDN 上写过一篇关于 RP 的笔记 github fork PR 的简单使用 ，那篇文章写的比较随意且不是用命令行操作的，大部分操作都是基于 IDE，所以想着重新整理下那篇文章，同时也复习下 git 常用命令。 pull requestpull request \" pull request ","date":"2021-04-29","objectID":"/pull-request/:1:0","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"模拟场景 公司有个项目为 xiao1996cc/git-dev ，有两个开发者，分别为 xiaobinqt 和 lovenarcissus 对这个项目进行开发，开发的新功能都会提交 PR 请求合并。 ","date":"2021-04-29","objectID":"/pull-request/:2:0","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"fork 两个开发者分别把项目 fork 项目到自己的账号下，以 xiaobinqt 为例： forkingforking \" forking fork 成功后fork 成功后 \" fork 成功后 xiaobinqt 现在已经 fork 了 xiao1996cc/git-dev 项目到自己的账号下 xiaobinqt/git-dev 。 ","date":"2021-04-29","objectID":"/pull-request/:3:0","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"xiaobinqt 开发新功能 每次开发新功能之前一定要先 fetch 下远程仓库。 ","date":"2021-04-29","objectID":"/pull-request/:4:0","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"clone xiaobinqt fork 完成后，clone 项目到本地。 cloneclone \" clone ","date":"2021-04-29","objectID":"/pull-request/:4:1","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"添加远程仓库 添加远程仓库 xiao1996cc/git-dev git remote add upstream git@github.com:xiao1996cc/git-dev.git 添加远程仓库添加远程仓库 \" 添加远程仓库 fetch 远程仓库 git fetch upstream fetch 远程仓库fetch 远程仓库 \" fetch 远程仓库 ","date":"2021-04-29","objectID":"/pull-request/:4:2","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"新建分支并开发 基于 upstream/main 创建新分支 dev_xiaobinqt git checkout -b dev_xiaobinqt upstream/main 创建 dev_xiaobinqt 分支创建 dev_xiaobinqt 分支 \" 创建 dev_xiaobinqt 分支 新建新文件 xiaobinqt.txt 并提交 新建文件并提交新建文件并提交 \" 新建文件并提交 push 成功后再 github 项目下可以看到成功提示 push 成功提示push 成功提示 \" push 成功提示 ","date":"2021-04-29","objectID":"/pull-request/:4:3","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"新建 PR new pull requestnew pull request \" new pull request 选择要合并的分支并创建 pull request，这里我们将 xiabinqt/git-dev 下面的 dev_xiaobinqt 分支里的代码合并到 xiao1996cc/git-dev 下的 main 分支。 创建 pull request创建 pull request \" 创建 pull request 创建 pull request 成功创建 pull request 成功 \" 创建 pull request 成功 ","date":"2021-04-29","objectID":"/pull-request/:4:4","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"合并 PR xiao1996cc/git-dev 项目下的 PR 只有管理这个项目的用户才有权限处理。管理员看到的界面是如下这样的： 管理者看到的 PR 界面管理者看到的 PR 界面 \" 管理者看到的 PR 界面 处理完的 PR 状态会变成 Merged Merge PRMerge PR \" Merge PR 至此 xiaobinqt 开发的一个功能已经成功提交到远程 upstream 仓库，也就是 xiao1996cc/git-dev 仓库，虽然只新建了一个文件，但是一个完整的 pull request 流程。 ","date":"2021-04-29","objectID":"/pull-request/:4:5","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"lovenarcissus 开发新功能 对于 lovenarcissus 开发者来说，也是先 fork 项目再 clone 项目到本地。 可以看到 lovenarcissus fork 后的项目是有 xiaobinqt.txt 这个文件的，同时也验证了 xiaobinqt 开发者成功向远程仓库提交了代码。这里默认已经 clone 到了本地仓库。 lovenarcissus forklovenarcissus fork \" lovenarcissus fork ","date":"2021-04-29","objectID":"/pull-request/:5:0","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"添加远程仓库 添加远程仓库添加远程仓库 \" 添加远程仓库 fetch 远程仓库代码 git fetch upsteamgit fetch upsteam \" git fetch upsteam ","date":"2021-04-29","objectID":"/pull-request/:5:1","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"新建分支并开发 基于 upstream/main 创建新分支 dev_lovenarcissus 开发分支 git checkout -b dev_lovenarcissus upstream/main 新建开发分支新建开发分支 \" 新建开发分支 新建 lovenarcissus.txt 文件并修改 xiaobinqt.txt 文件 开发代码开发代码 \" 开发代码 提交代码 提交代码提交代码 \" 提交代码 ","date":"2021-04-29","objectID":"/pull-request/:5:2","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"新建 PR 新建 pull request新建 pull request \" 新建 pull request ","date":"2021-04-29","objectID":"/pull-request/:5:3","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"合并 PR Merge pull requestMerge pull request \" Merge pull request ","date":"2021-04-29","objectID":"/pull-request/:5:4","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"模拟并解决冲突 ","date":"2021-04-29","objectID":"/pull-request/:6:0","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"出现冲突 现在开发者 xiaobinqt 和开发者 lovenarcissus 都提了代码到远程仓库 xiao1996cc/git-dev，仓库里文件如下： 远程代码仓库远程代码仓库 \" 远程代码仓库 现在开发者 xiaobinqt 需要开发一个新的功能，新增一个 xiaobinqt2.txt 文件，并且修改 xiaobinqt.txt 文件，但是这时 xiaobinqt 并没有 fetch 合并远程仓库的代码，xiaobinqt 本地仓库文件如下： 文件列表文件列表 \" 文件列表 直接新建 xiaobinqt2.txt 文件，并且修改 xiaobinqt.txt 文件 新建并修改文件新建并修改文件 \" 新建并修改文件 push 代码到远程仓库，并创建 pull request push 代码push 代码 \" push 代码 可以看到在创建 pull request 时出现了冲突，提示 Can’t automatically merge ， PR 出现冲突PR 出现冲突 \" PR 出现冲突 ","date":"2021-04-29","objectID":"/pull-request/:6:1","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"解决冲突 出现冲突的原因是本地代码跟 origin 仓库的代码是一样的，但是 origin 仓库跟 upstream 仓库代码不一致。我们需要先 fetch 下 upstream 仓库的代码跟本地代码合并后再 push 到 origin 仓库，再从 origin 仓库提 pull request 到 upsteam 仓库。 由👇图可知，在 merge upstream/main 分支时出现了冲突 Automatic merge failed; fix conflicts and then commit the result. merge conflictsmerge conflicts \" merge conflicts vim 打开冲突文件 xiaobinqt.txt 解决冲突 冲突文件内容冲突文件内容 \" 冲突文件内容 解决冲突后的文件 👇 解决冲突后的文件解决冲突后的文件 \" 解决冲突后的文件 以下👇是解决这次冲突的具体步骤： 解决冲突具体步骤解决冲突具体步骤 \" 解决冲突具体步骤 ","date":"2021-04-29","objectID":"/pull-request/:6:2","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"重提 PR 这时对于 xiaobinqt 开发者来说，冲突已经解决，可以重新提 pull request 👇，可以看到现在的状态是 Able to merge.。 重提 PR重提 PR \" 重提 PR 创建新的 pull request 成功 创建新的 PR 成功创建新的 PR 成功 \" 创建新的 PR 成功 ","date":"2021-04-29","objectID":"/pull-request/:6:3","tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":"mongodb , mongodb 概念,mongodb 查询语句,golang 操作 mongodb,globalsign/mgo 使用","date":"2021-04-14","objectID":"/mongodb-glance/","tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/"},{"categories":["开发者手册"],"content":"win10 安装 在 windows 下安装可以参考这篇文章mongodb-window-install。 ","date":"2021-04-14","objectID":"/mongodb-glance/:1:0","tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/"},{"categories":["开发者手册"],"content":"小坑 我使用的是 windows 10 企业版，在安装时出现了个问题，如下： 问题截图问题截图 \" 问题截图 我是在网上找了大半天没有找到解决的办法，都是写文章作者可用，但是我一直不生效，我觉得的必须要用管理员权限安装导致的。后来我直接忽略了，用管理员权限运行。 ","date":"2021-04-14","objectID":"/mongodb-glance/:1:1","tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/"},{"categories":["开发者手册"],"content":"运行服务端 用管理员 power shell 运行 具体命令可以参看文档 运行 mongodb 服务 01运行 mongodb 服务 01 \" 运行 mongodb 服务 01 .\\mongod.exe --dbpath D:\\mySoft\\mongoDB\\data\\db 运行 mongodb 服务 02运行 mongodb 服务 02 \" 运行 mongodb 服务 02 ","date":"2021-04-14","objectID":"/mongodb-glance/:1:2","tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/"},{"categories":["开发者手册"],"content":"运行客户端 .\\mongo.exe 客户端连接客户端连接 \" 客户端连接 ","date":"2021-04-14","objectID":"/mongodb-glance/:1:3","tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/"},{"categories":["开发者手册"],"content":"Navicat Premium navicat premium 是一个数据库管理工具，可以支持 mysql，mongodb，oracle 等几乎所有的数据库。 navicat premiumnavicat premium \" navicat premium navicat premuim 使用navicat premuim 使用 \" navicat premuim 使用 windows 安转教程可以参考navicat premium15破解教程 ","date":"2021-04-14","objectID":"/mongodb-glance/:2:0","tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/"},{"categories":["开发者手册"],"content":"概念解析 SQL术语/概念 MongoDB术语/概念 说明 database database 数据库 table collection 数据库表/集合 row document 数据记录行/文档 column field 数据字段/域 index index 索引 table joins - 表连接，MongoDB 不支持 primary key primary key 主键，MongoDB 自动将 _id 字段设置为主键 RDBMS（关系型数据库） MongoDB 数据库 数据库 表格 集合 行 文档 列 字段 表联合 嵌入文档 主键 主键 (MongoDB 提供了 key 为 _id ) 对比图对比图 \" 对比图 ","date":"2021-04-14","objectID":"/mongodb-glance/:3:0","tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/"},{"categories":["开发者手册"],"content":"常用命令 CMD 说明 db.help() 查看命令帮助 show dbs 查看所有数据库 use db_name 如果数据库不存在，则创建数据库，否则切换到指定数据库 show tables 查看该库下的所有表 db.getName() 查看当前所在的库 db 命令可以显示当前数据库对象或集合 db.version() 当前db版本 db.stats() 当前db状态 db.getMongo() 查看当前db的链接机器地址 db.dropDatabase() 删除当前使用的数据库 db.copyDatabase(\"mydb\", \"temp\", \"127.0.0.1\") 从指定的机器上复制指定数据库数据到某个数据库，本示例为：将本机的 mydb 的数据复制到 temp 数据库中 db.cloneDatabase(\"127.0.0.1\") 将指定机器上的数据库的数据克隆到当前数据库 db.repairDatabase() 修复当前数据库 show collections/show tables 查看已有集合 db.yourColl.help() 查看帮助 db.yourColl.count() 查询当前集合的数据条数 db.yourColl.dataSize() 查看数据空间大小 db.yourColl.getDB() 得到当前聚集集合所在的 db db.yourColl.stats() 得到当前集合的状态 db.yourColl.totalSize() 得到集合总大小 db.yourColl.storageSize() 聚集集合储存空间大小 db.yourColl.getShardVersion() Shard版本信息 db.userInfo.renameCollection(\"users\") 集合重命名。示例为：将 userInfo 重命名为 users db.yourColl.drop() 删除当前集合 yourColl 表示集合名 ","date":"2021-04-14","objectID":"/mongodb-glance/:4:0","tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/"},{"categories":["开发者手册"],"content":"集合查询 📌 查询所有记录 db.mgotest.find() # 格式化输出 db.mgotest.find().pretty() 对比 SQL：select * from mgotest; 📌 查询去掉后的当前集合中的某列的重复数据 db.mgotest.distinct(\"interests\") 对比 SQL：select distinct interests from mgotest; distinctdistinct \" distinct 📌 查询 age = 22 的记录 db.userInfo.find({\"age\": 22}); 对比 SQL：select * from userInfo where age = 22; 📌 查询 age \u003e 22 的记录 db.userInfo.find({age: {$gt: 22}}) 对比 SQL：select * from userInfo where age \u003e22; 📌 查询 age \u003c 22 的记录 db.userInfo.find({age: {$lt: 22}}); 对比 SQL：select * from userInfo where age \u003c 22; 📌 查询 age \u003e= 25 的记录 db.userInfo.find({age: {$gte: 25}}); 对比 SQL：select * from userInfo where age \u003e= 25; 📌 查询 age \u003c= 25 的记录 db.userInfo.find({age: {$lte: 25}}); 📌 查询 age \u003e= 23 并且 age \u003c= 26 db.userInfo.find({age: {$gte: 23, $lte: 26}}); 📌 查询 name 中包含 mongo 的数据 db.userInfo.find({name: /mongo/}); 对比 SQL：select * from userInfo where name like ‘%mongo%'; 📌 查询 name 中以 mongo 开头的 db.userInfo.find({name: /^mongo/}); 对比 SQL：select * from userInfo where name like ‘mongo%'; 📌 查询指定列 name、age 数据 db.userInfo.find({}, {name: 1, age: 1}); 对比 SQL：select name, age from userInfo; 📌 查询指定列 name、age 数据, age \u003e 25 db.userInfo.find({age: {$gt: 25}}, {name: 1, age: 1}); 对比 SQL：select name, age from userInfo where age \u003e25; 📌 按照年龄排序 # 升序 db.userInfo.find().sort({age: 1}); # 降序 db.userInfo.find().sort({age: -1}); 📌 查询 name = zhangsan, age = 22 的数据 db.userInfo.find({name: 'zhangsan', age: 22}); 对比 SQL：select * from userInfo where name = 'zhangsan' and age = '22'; 📌 查询前 5 条数据 db.userInfo.find().limit(5); 对比 SQL：select * from userInfo limit 5; 📌 查询 10 条以后的数据 db.userInfo.find().skip(10); 对比 SQL： select*fromuserInfowhereidnotin(selectidfromuserInfolimit10); 📌 查询在 5-10 之间的数据 db.userInfo.find().limit(10).skip(5); 可用于分页，limit 是 pageSize，skip 是 (第几页 * pageSize) 📌 or 查询 db.userInfo.find({$or: [{age: 22}, {age: 25}]}); 对比 SQL：select * from userInfo where age = 22 or age = 25; 📌 查询某个结果集的记录条数 db.userInfo.find({age: {$gte: 25}}).count(); 对比 SQL：select count(*) from userInfo where age \u003e= 20; 📌 按照某列进行排序 db.userInfo.find({sex: {$exists: true}}).count(); 对比 SQL：select count(sex) from userInfo; ","date":"2021-04-14","objectID":"/mongodb-glance/:5:0","tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/"},{"categories":["开发者手册"],"content":"更新文档 db.collection.update( \u003cquery\u003e, \u003cupdate\u003e, { upsert: \u003cboolean\u003e, multi: \u003cboolean\u003e, writeConcern: \u003cdocument\u003e } ) # 示例 db.col.update({'title':'MongoDB 教程'},{$set:{'title':'MongoDB'}}) 参数说明： query : update 的查询条件，类似 sql update 查询内 where 后面的。 update : update 的对象和一些更新的操作符（如$,$inc…）等，也可以理解为 sql update 查询内 set 后面的 upsert : 可选，这个参数的意思是，如果不存在 update 的记录，是否插入 objNew, true 为插入，默认是 false ，不插入。 multi : 可选，mongodb 默认是 false,只更新找到的第一条记录，如果这个参数为 true, 就把按条件查出来多条记录全部更新。 writeConcern :可选，抛出异常的级别。 ","date":"2021-04-14","objectID":"/mongodb-glance/:6:0","tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/"},{"categories":["开发者手册"],"content":"globalsign/mgo 使用 globalsign/mgo 是 Go 的 MongoDB 驱动，也是我现在维护的项目用的驱动，在这里简单介绍一下。 package main import ( \"fmt\" \"time\" \"github.com/globalsign/mgo\" \"github.com/globalsign/mgo/bson\" ) type User struct { Id bson.ObjectId `bson:\"_id\" json:\"id\"` Username string `bson:\"name\" json:\"username\"` Pass string `bson:\"pass\" json:\"pass\"` Regtime int64 `bson:\"regtime\" json:\"regtime\"` Interests []string `bson:\"interests\" json:\"interests\"` } const URL string = \"127.0.0.1:27017\" var ( c *mgo.Collection session *mgo.Session ) func (user User) ToString() string { return fmt.Sprintf(\"%#v\", user) } func init() { session, _ = mgo.Dial(URL) // 切换到数据库 db := session.DB(\"blog\") // 切换到collection c = db.C(\"mgotest\") } // 新增数据 func add() { // defer session.Close() stu1 := new(User) stu1.Id = bson.NewObjectId() stu1.Username = \"stu_name\" + time.Now().String() stu1.Pass = \"stu1_pass\" stu1.Regtime = time.Now().Unix() stu1.Interests = []string{\"象棋\", \"游泳\", \"跑步\"} err := c.Insert(stu1) if err == nil { fmt.Println(\"insert success\") } else { fmt.Printf(\"insert error:%s \\n\", err.Error()) } } // 查询 func find() { // defer session.Close() var ( users []User err error ) // c.Find(nil).All(\u0026users) err = c.Find(bson.M{\"name\": \"stu_name\"}).All(\u0026users) if err != nil { fmt.Printf(\"find err:%s \\n\", err.Error()) return } for index, value := range users { fmt.Printf(\"index:%d,val:%s \\n\", index, value.ToString()) } // 根据ObjectId进行查询 // idStr := \"577fb2d1cde67307e819133d\" // objectId := bson.ObjectIdHex(idStr) // user := new(User) // err = c.Find(bson.M{\"_id\": objectId}).One(user) // if err != nil { // fmt.Printf(\"db find err:%s \\n\", err.Error()) // return // } // fmt.Println(\"查找成功..\", user) } // 根据id进行修改 func update() { interests := []string{\"象棋\", \"游泳\", \"跑步\"} err := c.Update(bson.M{\"_id\": bson.ObjectIdHex(\"6076c3954e947b3944d4a38b\")}, bson.M{\"$set\": bson.M{ \"name\": \"修改后的name\", \"pass\": \"修改后的pass\", \"regtime\": time.Now().Unix(), \"interests\": interests, }}) if err != nil { fmt.Println(\"修改失败\") } else { fmt.Println(\"修改成功\") } } // 删除 func del() { err := c.Remove(bson.M{\"_id\": bson.ObjectIdHex(\"6076c3954e947b3944d4a38b\")}) if err != nil { fmt.Println(\"删除失败\" + err.Error()) } else { fmt.Println(\"删除成功\") } } func main() { add() find() update() del() } ","date":"2021-04-14","objectID":"/mongodb-glance/:7:0","tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/"},{"categories":["开发者手册"],"content":"FAQ ","date":"2021-04-14","objectID":"/mongodb-glance/:8:0","tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/"},{"categories":["开发者手册"],"content":"空库不显示 show dbsshow dbs \" show dbs 上图👆用 use test1 新建了一个数据库 test1，但是用 show dbs 却没有显示 test1，这是因为 test1 是空的，插入一条数据就可以显示。 显示新建的库显示新建的库 \" 显示新建的库 ","date":"2021-04-14","objectID":"/mongodb-glance/:8:1","tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/"},{"categories":["开发者手册"],"content":"定长表 最近有个需求是，需要某个集合实现只保留固定数量的记录，自动淘汰老旧数据。 通过创建集合的命令 db.createCollection(name, options) 可知，有个可选的 options 参数： 参数说明： name: 要创建的集合名称 options: 可选参数, 指定有关内存大小及索引的选项 options 可以是如下参数： 字段 类型 描述 capped 布尔 （可选）如果为 true，则创建固定集合。固定集合是指有着固定大小的集合，当达到最大值时，它会自动覆盖最早的文档。当该值为 true 时，必须指定 size 参数。 autoIndexId 布尔 3.2 之后不再支持该参数。（可选）如为 true，自动在 _id 字段创建索引。默认为 false。 size 数值 （可选）为固定集合指定一个最大值，即 字节数 。如果 capped 为 true，也需要指定该字段。 max 数值 （可选）指定固定集合中包含文档的最大数量。 如果是新建一个集合，这种方式肯定是可以的，但是如果要同步老数据呢？ 比如我有一个集合 daily_report 集合，里面有一些老数据： daily_report 集合daily_report 集合 \" daily_report 集合 我的想法是，将 daily_report 重命名为 daily_report_bak，新建集合 daily_report，就 daily_report_bak 数据同步到 daily_report。 流程流程 \" 流程 重命名集合： db.daily_report.renameCollection(\"daily_report_bak\") 新建集合： db.createCollection(\"daily_report\", {capped:true,size:6142800,max:4}) 该集合最大值字节数为：6142800字节 = 6142800B ≈ 6000KB ≈ 5M 。 该集合中包含文档的最大数量为 4 条。 同步旧数据 db.daily_report_bak.find().forEach(function(doc){db.daily_report.insert(doc)}) ","date":"2021-04-14","objectID":"/mongodb-glance/:8:2","tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/"},{"categories":["开发者手册"],"content":"参考 MongoDB 教程 MongoDB高级查询 MongoDB常用操作命令大全 查询, 更新, 投射, 和集合算符 Field Update Operators ","date":"2021-04-14","objectID":"/mongodb-glance/:9:0","tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/"},{"categories":["golang"],"content":"golang upd 简单使用,UDP,HTTP","date":"2021-04-01","objectID":"/upd-demo/","tags":["golang"],"title":"golang udp 简单使用","uri":"/upd-demo/"},{"categories":["golang"],"content":"server package main import ( \"fmt\" \"net\" \"time\" ) func main() { // 创建监听 socket, err := net.ListenUDP(\"udp4\", \u0026net.UDPAddr{ IP: []byte{127, 0, 0, 1}, Port: 8080, }) if err != nil { fmt.Println(\"监听失败!\", err) return } defer socket.Close() for { // 读取数据 data := make([]byte, 4096) read, remoteAddr, err := socket.ReadFromUDP(data) if err != nil { fmt.Println(\"读取数据失败!\", err) continue } fmt.Println(read, remoteAddr) fmt.Printf(\"接收到客户端数据，%s\\n\\n\", data) // 发送数据 senddata := []byte(\"server send data，hello client!\" + time.Now().Format(\"2006-01-02 15:04:05\")) _, err = socket.WriteToUDP(senddata, remoteAddr) if err != nil { fmt.Println(\"发送数据失败!\", err.Error()) return } } } ","date":"2021-04-01","objectID":"/upd-demo/:1:0","tags":["golang"],"title":"golang udp 简单使用","uri":"/upd-demo/"},{"categories":["golang"],"content":"client package main import ( \"fmt\" \"net\" \"time\" ) func main() { // 创建连接 socket, err := net.DialUDP(\"udp4\", nil, \u0026net.UDPAddr{ IP: []byte{127, 0, 0, 1}, Port: 8080, }) if err != nil { fmt.Println(\"连接失败!\", err) return } defer socket.Close() // 发送数据 senddata := []byte(\"client send message，hello server!\" + time.Now().Format(\"2006-01-02 15:04:05\")) _, err = socket.Write(senddata) if err != nil { fmt.Println(\"发送数据失败!\", err) return } // 接收数据 data := make([]byte, 4096) read, remoteAddr, err := socket.ReadFromUDP(data) if err != nil { fmt.Println(\"读取数据失败!\", err) return } fmt.Println(read, remoteAddr) fmt.Printf(\"接收到服务器端数据，%s\\n\", data) } ","date":"2021-04-01","objectID":"/upd-demo/:2:0","tags":["golang"],"title":"golang udp 简单使用","uri":"/upd-demo/"},{"categories":["golang"],"content":"源码 源码地址 ","date":"2021-04-01","objectID":"/upd-demo/:3:0","tags":["golang"],"title":"golang udp 简单使用","uri":"/upd-demo/"},{"categories":["开发者手册"],"content":"xiaobinqt,vmware,centos 7.9,静态 ip,vmware 安装,vmware 安装 CentOS 7.x,vmware 设置静态 ip,centos7,ifconfig command not found,修改网卡为 eth0","date":"2021-03-08","objectID":"/vmware-install-centos7.x/","tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/"},{"categories":["开发者手册"],"content":"下载镜像 在 CentOS 的官网 https://wiki.centos.org/Download 可以下载 CentOS 各个版本的镜像文件。 CentOS DownloadCentOS Download \" CentOS Download 包括已经不在维护的各个版本： Archived VersionsArchived Versions \" Archived Versions 也可以去阿里的镜像仓库去下载 mirrors.aliyun.com/centos。 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:1:0","tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/"},{"categories":["开发者手册"],"content":"安装 CentOS 7.9 下载完 CentOS-7-x86_64-Minimal-2009 就可以安装了，这里用的是 Minimal 版本，安装完成后，系统中只有最基本的组件，方便学习。 新建虚拟机新建虚拟机 \" 新建虚拟机 典型配置典型配置 \" 典型配置 选择 ios 文件选择 ios 文件 \" 选择 ios 文件 命名虚拟机并选择系统文件保存路径： 命名虚拟机命名虚拟机 \" 命名虚拟机 如果物理机支持大与 4GB 以上的单文件，可以选择“将虚拟磁盘存储为单个文件” 指定磁盘容量指定磁盘容量 \" 指定磁盘容量 自定义硬件自定义硬件 \" 自定义硬件 由于服务器用不到声卡，USB 控制器，打印机这些设备，可以将这些设备移除： 移除硬件移除硬件 \" 移除硬件 移除后的硬件移除后的硬件 \" 移除后的硬件 完成完成 \" 完成 开启虚拟机开启虚拟机 \" 开启虚拟机 Install CentOS7Install CentOS7 \" Install CentOS7 安装中安装中 \" 安装中 选择语言选择语言 \" 选择语言 点击 INSTALLATION DESTINATION INSTALLATION DESTINATION 默认值不用改，直接点 Done INSTALLATION DESTINATION DoneINSTALLATION DESTINATION Done \" INSTALLATION DESTINATION Done Begin InstallationBegin Installation \" Begin Installation 设置 ROOT 密码和创建用户 设置 ROOT 密码和创建用户设置 ROOT 密码和创建用户 \" 设置 ROOT 密码和创建用户 创建用户创建用户 \" 创建用户 等待安装任务 等待安装任务等待安装任务 \" 等待安装任务 安装任务完成重启安装任务完成重启 \" 安装任务完成重启 重启完成登录 root Loginroot Login \" root Login ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:2:0","tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/"},{"categories":["开发者手册"],"content":"网络配置 一般新装的最小化的 CentOS 7.x 系统是没有网络配置的，而安装命令就是联网下载软件，所以网络配置是必须的。 pingping \" ping CentOS 7.x 的默认网卡是 essxx，我们可以在配置文件 /etc/syconfig/network-scripts 中看到： network-scriptsnetwork-scripts \" network-scripts 可以先把 ifcfg-ensxx 这个文件备份下，然后修改 onboot 参数从 no 修改为 yes ，保存，重启机器。 update onbootupdate onboot \" update onboot 重启完后就可以 ping 通网络了： ping 2ping 2 \" ping 2 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:3:0","tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/"},{"categories":["开发者手册"],"content":"ifconfig command not found 安装 net-tools 工具 yum install -y net-tools net-tools installnet-tools install \" net-tools install ifconfigifconfig \" ifconfig ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:4:0","tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/"},{"categories":["开发者手册"],"content":"修改网卡为 eth0 CentOS 7.x 的网卡不是 eth0 而是 ensxx，这是 CentOS 7.x 的一致性网络设备命名导致的，可以使用以下方式，将网卡名称回到 eth0 格式。 ensxxensxx \" ensxx 修改配置 vim /etc/default/grub 中添加 biosdevname=0 net.ifnames=0 biosdevname=0 net.ifnames=0biosdevname=0 net.ifnames=0 \" biosdevname=0 net.ifnames=0 执行命令 grub2-mkconfig -o /boot/grub2/grub.cfg 执行命令执行命令 \" 执行命令 reboot 重启机器 修改成功修改成功 \" 修改成功 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:5:0","tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/"},{"categories":["开发者手册"],"content":"设置静态 IP NAT 模式是 VMware 虚拟机默认使用的模式，其最大的优势就是虚拟机接入网络非常简单，只要物理机可以访问网络，虚拟机就可以访问网络。网络结构如下图： NAT模式NAT模式 \" NAT模式 所谓的静态 ip ，就是设置后固定不变的，因为在真实环境中，需要为所有的服务器配置静态 ip，从而确保通过一个 ip 地址只能找到一台服务器。 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:6:0","tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/"},{"categories":["开发者手册"],"content":"设置 在修改配置文件之前，为了防止配置出错，建议提前备份配置文件 ifcfg-eth0。我把原来的 ifcfg-ens32 重命名为了 ifcfg-eth0。 cp /etc/sysconfig/network-scripts/ifcfg-eth0 /etc/sysconfig/network-scripts/ifcfg-eth0.bak 我们需要把 ifcfg-eth0 配置文件中的 BOOTPROTO 的值设置为 static，将 IPADDR(IP 地址)的值设置为其所在的子网中正确的，无冲突的 ip 地址即可。 在 NAT 模式中，我们需要找到 4 个地址才能确定我们的无冲突的 ip 到底是哪些。 子网 ip VMnet8 虚拟网卡 ip NAT 网关 ip DHCP 地址池 接下来找这几个参数： 图 1图 1 \" 图 1 图 2图 2 \" 图 2 点击【NAT设置】查看子网掩码和网关 IP。点击【DHCP 设置】查看起始 IP 地址和结束 IP 地址。 图 3图 3 \" 图 3 图 4图 4 \" 图 4 打开物理机的 cmd 输入 ipconfig 命令： 图 5图 5 \" 图 5 所以，除去这几个地址，192.168.48.3 ~ 192.168.48.127 范围内的 ip 都可以作为静态 ip 使用。 vi ifcfg-eth0 需要修改的地方为： 将 ONBOOT 改为 yes BOOTPROTO 由 dhcp 改为 static 增加 IPADDR(ip 地址) 增加 NETMASK(子网掩码) 增加 GATEWAY(网关) 增加 DNS1(首选域名服务器) 其中，网关不设置，虚拟机只能在局域网内访问，无法访问外部网络。DNS 不设置则无法解析域名。 DNS 可以设置成跟网关一样的地址。 图 6图 6 \" 图 6 设置完成后执行：systemctl restart network 命令使配置生效。 图 7图 7 \" 图 7 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:6:1","tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/"},{"categories":["开发者手册"],"content":"访问测试 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:6:2","tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/"},{"categories":["开发者手册"],"content":"物理机测试 在物理机中 ping 虚拟机 ip 地址： 图 8图 8 \" 图 8 物理机共向 ip 地址 192.168.48.8 发送了 4 次 ping 请求，4 次都是成功的，发送的数据包为 32 字节，TTL(生存时间值)为 64，其中 TTL 在发送时的默认值为 64，每经过一个路由则减 1 ，此次显示最终结果为 64 说明中间没有经过路由。 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:6:3","tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/"},{"categories":["开发者手册"],"content":"虚拟机测试 图 9图 9 \" 图 9 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:6:4","tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/"},{"categories":["开发者手册"],"content":"说明 因为我把默认网卡从 ensxx 改成了 eth0 所以在修改静态 ip 是把配置文件也改成了 ifcfg-eth0 图 10图 10 \" 图 10 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:6:5","tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/"},{"categories":["开发者手册"],"content":"FAQ ❓重启网卡问题 loaded (/etc/rc.d/init.d/network; bad; vendor preset: disabled) network errnetwork err \" network err ☝️ 以上这个问题可以参考centos7重启网卡提示错误的解决方法这篇文章。 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:7:0","tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/"},{"categories":["开发者手册"],"content":"参考 How To Configure Static IP Address in CentOS 7 / RHEL 7 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:8:0","tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/"},{"categories":["开发者手册"],"content":"Redis, Redis 数据结构,redis 缓存击穿,redis 布隆过滤器,redis 淘汰机制,redis 持久化机制,redis 分布式缓存","date":"2021-02-20","objectID":"/redis-glance-faq/","tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/"},{"categories":["开发者手册"],"content":"Redis 是一个使用 C 语言开发的数据库，与传统数据库不同的是 Redis 的数据是存在内存中的，我们把这种数据库叫做内存数据库。因为在内存中，所以读写速度非常快，因此 Redis 被广泛应用于缓存方向。 Redis 提供了多种数据类型来支持不同的业务场景，所以 Redis 除了做缓存之外，Redis 还经常用来做分布式锁，消息队列等。Redis 还支持事务、持久化、Lua 脚本、多集群方案。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:0:0","tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/"},{"categories":["开发者手册"],"content":"数据结构 Redis 五种数据结构Redis 五种数据结构 \" Redis 五种数据结构 在 Redis 中，所有的 key 都是字符串。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:1:0","tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/"},{"categories":["开发者手册"],"content":"字符串 字符串类型是 Redis 中最基本的数据类型，一个 key 对应一个 value。 字符串类型是二进制安全的，也就是说 Redis 中的字符串可以包含任何数据。如数字，字符串，jpg 图片或者序列化的对象。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:1:1","tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/"},{"categories":["开发者手册"],"content":"Hash hash 值本身又是一种键值对结构： hashhash \" hash 所有 hash 的命令都是 h 开头，如 hget、hset、hdel 等 ","date":"2021-02-20","objectID":"/redis-glance-faq/:1:2","tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/"},{"categories":["开发者手册"],"content":"List List 就是链表（redis 使用双端链表实现的 List），是有序的，value可以重复，可以通过下标取出对应的 value 值，左右两边都能进行插入和删除数据。 ListList \" List 常用命令 使用技巧 lpush + lpop = Stack(栈) lpush + rpop = Queue（队列） lpush + ltrim = Capped Collection（有限集合） lpush + brpop = Message Queue（消息队列） ","date":"2021-02-20","objectID":"/redis-glance-faq/:1:3","tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/"},{"categories":["开发者手册"],"content":"Set SetSet \" Set 集合类型也是用来保存多个字符串的元素，但和列表有几个不同的点： 👉 不允许有重复的元素 👉 集合中的元素是无序的，不能通过索引下标获取元素 👉 支持集合间的操作，可以取多个集合取交集、并集、差集 ","date":"2021-02-20","objectID":"/redis-glance-faq/:1:4","tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/"},{"categories":["开发者手册"],"content":"ZSet 有序集合和集合有着必然的联系，保留了集合不能有重复成员的特性。 区别是，有序集合中的元素是可以排序的，它给每个元素设置一个分数，作为排序的依据。 有序集合中的元素不可以重复，但是 score 分数可以重复，就和一个班里的同学学号不能重复，但考试成绩可以相同。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:1:5","tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/"},{"categories":["开发者手册"],"content":"FAQ ","date":"2021-02-20","objectID":"/redis-glance-faq/:2:0","tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/"},{"categories":["开发者手册"],"content":"分布式缓存 分布式缓存主要解决的是单机缓存的容量受服务器限制并且无法保存通用的信息，因为，本地缓存只在当前服务里有效。比如部署了两个相同的服务，他们两者之间的缓存数据是无法共同的。 分布式缓存的话，使用的较多的较多的解决方案是是 Memcached 和 Redis。不过，随着近些年 Redis 的发展，大家慢慢都转而使用更加强大的 Redis 而放弃 Memcached。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:2:1","tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/"},{"categories":["开发者手册"],"content":"Redis 和 Memcached 的区别 共同点 都是基于内存的数据库。 都有过期策略。 两者的性能都非常高。 区别 Redis 支持更丰富的数据类型，也就是说可以支持更复杂的应用场景。Redis 不仅仅支持简单的 k/v 类型的数据，同时还提供 list，set，zset，hash 等数据结构的存储。而 Memcached 只支持最简单的 k/v 数据类型。 Redis 的容灾更好，支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而 Memecache 把数据全部存在内存中。 Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据，但是 Redis 目前是原生支持 cluster 模式的。 Redis 在服务器内存使用完之后，可以将不用的数据放到磁盘上。而 Memcached 在服务器内存使用完之后，就会直接报异常。 Memcached 是多线程，非阻塞 IO 复用的网络模型，Redis 使用单线程的多路 IO 复用模型（Redis 6.0 引入了多线程 IO ）。 Memcached过期数据的删除策略只用了惰性删除，而 Redis 同时使用了惰性删除与定期删除。 Redis 支持发布订阅模型、Lua 脚本、事务等功能，而 Memcached 不支持。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:2:2","tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/"},{"categories":["开发者手册"],"content":"Redis 如何判断数据是否过期 Redis 通过一个叫做过期字典（可以看作是hash表）来保存数据过期的时间。过期字典的键指向 Redis 数据库中的某个key(键)，过期字典的值是一个long long 类型的整数，这个整数保存了 key 所指向的数据库键的过期时间（毫秒精度的UNIX时间戳）。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:2:3","tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/"},{"categories":["开发者手册"],"content":"过期删除策略 如果 Redis 中一批 key 只能存活 1 分钟，那么 1 分钟后，Redis 是怎么对这批 key 进行删除的呢？ 惰性删除：只会在取出 key 的时候才对数据进行过期检查。这样对 CPU 最友好，但是可能会造成太多过期 key 没有被删除。 定期删除：每隔一段时间抽取一批 key 执行删除过期 key 操作。Redis 底层会通过限制删除操作执行的时长和频率来减少删除操作对 CPU 时间的影响。 定期删除对内存更加友好，惰性删除对CPU更加友好。Redis 采用的是定期删除➕惰性删除。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:2:4","tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/"},{"categories":["开发者手册"],"content":"内存淘汰机制 我们通过给 key 设置过期时间还是有问题的，因为还是可能存在定期删除和惰性删除漏掉了很多过期 key 的情况。这样就导致大量过期 key 堆积在内存里，然后就 OOM 了，Redis 的数据淘汰机制可以解决这个问题。 策略 说明 volatile-lru （least recently used），从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl 从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random 从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru （least recently used），当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的） allkeys-random 从数据集（server.db[i].dict）中任意选择数据淘汰 no-eviction 禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错😱。 volatile-lfu （least frequently used），从已设置过期时间的数据集(server.db[i].expires)中挑选最不经常使用的数据淘汰 allkeys-lfu （least frequently used），当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的 key ","date":"2021-02-20","objectID":"/redis-glance-faq/:2:5","tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/"},{"categories":["开发者手册"],"content":"持久化机制 很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了备份数据，比如为了防止系统故障而将数据备份到一个另一台机器。 Redis 两种不同的持久化操作。一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file, AOF）。 RDB 快照持久化是 Redis 默认采用的持久化方式，在 Redis.conf 配置文件中默认有此下配置： save 900 1 # 在 900 秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 300 10 # 在 300 秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 60 10000 # 在 60 秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 Redis 可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。 Redis 创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本，还可以将快照留在原地以便重启服务器的时候使用。 AOF 与快照持久化 RDB 相比，AOF 持久化 的实时性更好，基本已成为主流的持久化方案。默认情况下 Redis 没有开启 AOF方式的持久化。 开启 AOF 持久化后每执行一条会更改 Redis 中的数据的命令，Redis 就会将该命令写入硬盘中的 AOF 文件。AOF 文件的保存位置和 RDB 文件的位置相同，都是通过 dir 参数设置的，默认的文件名是 appendonly.aof。 Redis 的配置文件中存在三种不同的 AOF 持久化方式，它们分别是： appendfsync always # 每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度 appendfsync everysec # 每秒钟同步一次，显示地将多个写命令同步到硬盘 appendfsync no # 让操作系统决定何时进行同步 为了兼顾数据和写入性能，可以考虑 appendfsync everysec 选项 ，让 Redis 每秒同步一次 AOF 文件，Redis 性能几乎没受到任何影响。 这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:2:6","tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/"},{"categories":["开发者手册"],"content":"参考 Redis 命令参考 Redis(一)、Redis五种数据结构 Redis ","date":"2021-02-20","objectID":"/redis-glance-faq/:3:0","tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/"},{"categories":["docker"],"content":"xiaobinqt,docker IPv4 forwarding is disabled. Networking will not work","date":"2020-11-08","objectID":"/ipv4-forwarding-is-disabled-networking-will-not-work/","tags":["docker"],"title":"IPv4 forwarding is disabled. Networking will not work","uri":"/ipv4-forwarding-is-disabled-networking-will-not-work/"},{"categories":["docker"],"content":"问题 今天在操作 docker 时遇到了一个问题IPv4 forwarding is disabled. Networking will not work👇 报错信息报错信息 \" 报错信息 我的系统是 CentOS7.9 系统信息系统信息 \" 系统信息 ","date":"2020-11-08","objectID":"/ipv4-forwarding-is-disabled-networking-will-not-work/:1:0","tags":["docker"],"title":"IPv4 forwarding is disabled. Networking will not work","uri":"/ipv4-forwarding-is-disabled-networking-will-not-work/"},{"categories":["docker"],"content":"解决方案 在宿主机执行 echo \"net.ipv4.ip_forward=1\" \u003e\u003e/usr/lib/sysctl.d/00-system.conf 然后重启网络和 docker systemctl restart network systemctl restart docker 问题解决问题解决 \" 问题解决 ","date":"2020-11-08","objectID":"/ipv4-forwarding-is-disabled-networking-will-not-work/:2:0","tags":["docker"],"title":"IPv4 forwarding is disabled. Networking will not work","uri":"/ipv4-forwarding-is-disabled-networking-will-not-work/"},{"categories":["理解计算机"],"content":"https,加密算法,什么是 https/ssl/tls,什么是安全,https 为什么安全,对称加密和非对称加密,CA 证书","date":"2020-10-27","objectID":"/what-is-https/","tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/"},{"categories":["理解计算机"],"content":"为什么有 HTTPS？因为 HTTP 不安全！ 现在的互联网已经不再是 “田园时代”，“黑暗森林” 已经到来。上网的记录会被轻易截获，网站是否真实也无法验证，黑客可以伪装成银行网站，盗取真实姓名、密码、银行卡等敏感信息，威胁人身安全和财产安全。 上网的时候必须步步为营、处处小心，否则就会被不知道埋伏在哪里的黑客所“猎杀”。 HTTPS 如何实现安全通信？如何构建出固若金汤的网络城堡？主要涉及的知识点如下： 什么是 HTTPS 什么样的才是安全的通信 对称加密与非对称加密、摘要算法、数字签名、完整性校验是什么 迁移 HTTPS 的必要性 ","date":"2020-10-27","objectID":"/what-is-https/:0:0","tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/"},{"categories":["理解计算机"],"content":"什么是安全 在通信过程中，具备以下特性则认为安全：机密性、完整性、不可否认、身份认证。 机密性：数据必须保密，只能有信任的人读取，其他人是不可见的秘密。就是不能让不相关的人看到不该看的东西。 完整性：也叫作一致性，也就是数据在传输过程中没有被非法篡改，内容不能多也不能少，一五一十的保持原状。 不可否认：不可抵赖，不能否认已经发生过的事情。 身份验证：确认对方的真实身份，“证明你是真的是你”，保证消息发送到可信的人，而不是非法之徒。 所以同时具备了机密性、完整性、身份认证、不可否认四个特性，通信双方的安全才有保证，才是真正的安全。 ","date":"2020-10-27","objectID":"/what-is-https/:1:0","tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/"},{"categories":["理解计算机"],"content":"什么是 HTTPS HTTPS 其实是一个“非常简单”的协议，规定了新的协议名“https”，默认端口号 443，至于其他的什么请求 - 应答模式、报文结构、请求方法、URI、头字段、连接管理等等都完全沿用 HTTP，没有任何新的东西。唯一的差别就是端口号不同、去掉明文传输。 那 HTTPS 凭啥就变得安全了呢？ 就是因为他在 TCP/IP 与 HTTP 之间加上了 SSL/TLS ，从原来的 HTTP over TCP/IP 变成了 HTTP over SSL/TLS，让 HTTP 运行在 安全的 SSL/TLS 协议上。 http 与 httpshttp 与 https \" http 与 https ","date":"2020-10-27","objectID":"/what-is-https/:2:0","tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/"},{"categories":["理解计算机"],"content":"SSL/TLS SSL 即安全套接层（Secure Sockets Layer），在 OSI 模型中处于第 5 层（会话层），由网景公司于 1994 年发明，有 v2 和 v3 两个版本，而 v1 因为有严重的缺陷从未公开过。 SSL 发展到 v3 时已经证明了它自身是一个非常好的安全通信协议，于是互联网工程组 IETF 在 1999 年把它改名为 TLS（传输层安全，Transport Layer Security），正式标准化，版本号从 1.0 重新算起，所以 TLS1.0 实际上就是 SSLv3.1。 TLS 由记录协议、握手协议、警告协议、变更密码规范协议、扩展协议等几个子协议组成，综合使用了对称加密、非对称加密、身份认证等许多密码学前沿技术。 浏览器与服务器在使用 TLS 建立连接的时候实际上就是选了一组加密算法实现安全通信，这些算法组合叫做 “密码套件（cipher suite）”。 套件命名很有规律，比如“ECDHE-RSA-AES256-GCM-SHA384”。按照 密钥交换算法 + 签名算法 + 对称加密算法 + 摘要算法”组成的. 所以这个套件的意思就是：使用 ECDHE 算法进行密钥交换，使用 RSA 签名和身份验证，握手后使用 AES 对称加密，密钥长度 256 位，分组模式 GCM，消息认证和随机数生成使用摘要算法 SHA384。 ","date":"2020-10-27","objectID":"/what-is-https/:3:0","tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/"},{"categories":["理解计算机"],"content":"对称加密和非对称加密 前面提到四个实现安全的必要条件，先说机密性，也就是消息只能给想给的人看到并且看得懂。 实现机密性的手段就是加密（encrypt），也就是将原本明文消息使用加密算法转换成别人看不懂的密文，只有掌握特有的密钥的人才能解密出原始内容。 钥匙也就是密钥（key），未加密的消息叫做明文 （plain text/clear text），加密后的内容叫做密文（cipher text），通过密钥解密出原文的过程叫做解密（decrypt） ，而加解密的整个过程就是加密算法。 由于 HTTPS、TLS 都运行在计算机上，所以“密钥”就是一长串的数字，但约定俗成的度量单位是“位”（bit），而不是“字节”（byte）。比如，说密钥长度是 128（位），就是 16 字节的二进制串，密钥长度 1024（位），就是 128 字节的二进制串。 加密算法通常有两大类：对称加密和非对称加密。 对称加密 加密和解密使用的密钥都是同一个，是 “对称的”。双方只要保证不会有泄露其他人知道这个密钥，通信就具有机密性。 对称加密对称加密 \" 对称加密 对称加密算法常见的有 RC4、DES、3DES、AES、ChaCha20 等，但前三种算法都被认为是不安全的，通常都禁止使用，目前常用的只有 AES 和 ChaCha20。 AES 的意思是“高级加密标准”（Advanced Encryption Standard），密钥长度可以是 128、192 或 256。它是 DES 算法的替代者，安全强度很高，性能也很好，而且有的硬件还会做特殊优化，所以非常流行，是** 应用最广泛的对称加密算法**。 加密分组模式 对称算法还有一个 “分组模式”的概念，目的是通过算法用固定长度的密钥加密任意长度的明文。 最新的分组模式被称为 AEAD（Authenticated Encryption with Associated Data），在加密的同时增加了认证的功能，常用的是 GCM、CCM 和 Poly1305。 非对称加密 有对称加密，为何还搞出一个非对称加密呢？ 对称加密确实解决了机密性，只有相关的人才能读取出信息。但是最大的问题是：如何安全的把密钥传递对方，专业术语 “密钥交换”。 所以为了解决秘钥交换，非对称加密诞生了。 非对称加密由两个密钥组成，分别是公钥（public key）和“私钥（private key）”，两个密钥是不一样的，这也就是不对称的由来，公钥可以任何人使用，私钥则自己保密。 这里需要注意的是：公钥和私钥都可以用来加密解密，公钥加密的密文只能用私钥解密，反之亦然。 服务端保存私钥，在互联网上分发公钥，当访问服务器网站的时候使用授予的公钥加密明文即可，服务端则使用对应的私钥来解密。 非对称加密非对称加密 \" 非对称加密 TLS 中常见的加密算法有 DH、RSA、ECC、DSA 等。其中的 RSA 最常用，它的安全性基于“整数分解”的数学难题，使用两个超大素数的乘积作为生成密钥的材料，想要从公钥推算出私钥是非常困难的。 ECC（Elliptic Curve Cryptography）是非对称加密里的“后起之秀”，它基于“椭圆曲线离散对数”的数学难题，使用特定的曲线方程和基点生成公钥和私钥，子算法 ECDHE 用于密钥交换，ECDSA 用于数字签名。 比起 RSA，ECC 在安全强度和性能上都有明显的优势。160 位的 ECC 相当于 1024 位的 RSA，而 224 位的 ECC 则相当于 2048 位的 RSA。因为密钥短，所以相应的计算量、消耗的内存和带宽也就少，加密解密的性能就上去了，对于现在的移动互联网非常有吸引力。 现在我们为了机密性从对称加密到非对称加密，而非对称加密还解决了密钥交换不安全的问题。那么是否可以直接使用非对称加密来实现机密性呢？ 答案是否定的！ 因为非对称加密运算速度比较慢。所以需要两者结合，混合模式实现机密性问题，同时又有很好的性能。 ","date":"2020-10-27","objectID":"/what-is-https/:3:1","tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/"},{"categories":["理解计算机"],"content":"混合加密流程 先创建一个随机数的对称加密密钥，会话密钥（session key）； 使用会话密钥加密需要传输的明文消息，因为对称加密性能较好，接着再使用非对称加密的公钥对会话密钥加密，因为会话密钥很短，通常只有 16 字节或 32 字节，所以加密也不会太慢 。 这里主要就是解决了非对称加密的性能问题，同时实现了会话密钥的机密交换。 另一方接收到密文后使用非对称加密的私钥解密出上一步加密的会话密钥，接着使用会话密钥解密出加密的消息明文。 混合加密混合加密 \" 混合加密 总结一下就是使用非对称加密算法来加密会话密钥，使用对称加密算法来加密消息明文，接收方则使用非对称加密算法的私钥解密出会话密钥，再利用会话密钥解密消息密文。 这样混合加密就解决了对称加密算法的密钥交换问题，而且安全和性能兼顾，完美地实现了机密性。 后面还有完整性、身份认证、不可否认等特性没有实现，所以现在的通信还不是绝对安全。 ","date":"2020-10-27","objectID":"/what-is-https/:3:2","tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/"},{"categories":["理解计算机"],"content":"摘要算法与完整性 摘要算法的主要目的就是实现完整性，通过常见的散列函数、哈希函数实现。 我们可以简单理解成这是一种特殊的压缩算法，将任意长度的明文数据处理成固定长度、又是独一无二的“摘要”字符串，就是该数据的指纹。 同时摘要算法是单向加密算法，没有密钥，加密后的数据也无法解密，也就是不能从“摘要”推导出明文。 比如我们听过或者用过的 MD5（Message-Digest 5）、SHA-1（Secure Hash Algorithm 1），它们就是最常用的两个摘要算法，能够生成 16 字节和 20 字节长度的数字摘要。 完整性实现 有了摘要算法生成的数字摘要，那么我们只需要在明文数据附上对应的摘要，就能保证数据的完整性。 但是由于摘要算法不具有机密性，不能明文传输，否则黑客可以修改消息后把摘要也一起改了，网站还是鉴别不出完整性。 所以完整性还是要建立在机密性上，我们结合之前提到的混合加密使用 ”会话密钥“ 加密明文消息 + 摘要，这样的话黑客也就无法得到明文，无法做修改了。这里有个专业术语叫“哈希消息认证码（HMAC）”。 哈希消息认证码（HMAC）哈希消息认证码（HMAC） \" 哈希消息认证码（HMAC） 比如诸葛亮使用上面提到的混合加密过程给关二爷发消息：“明天攻城” + “SHA-2 摘要”，关二爷收到后使用密钥将解密出来的会话密钥解密出明文消息，同时对明文消息使用解密出来的摘要算法进行摘要计算，接着比对两份“摘要”字符串是否一致，如果一致就说明消息完整可信，没有被敌军修改过。 消息被修改是很危险的，要以史为鉴，比如赵高与李斯伪造遗诏，直接把扶苏给送西天了，这太可怕了。 总结下就是通过摘要比对防止篡改，同时利用混合加密实现密文与摘要的安全传输。 ","date":"2020-10-27","objectID":"/what-is-https/:3:3","tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/"},{"categories":["理解计算机"],"content":"数字签名和 CA 到这里已经很安全了，但是还是有漏洞，就是通信的两头。黑客可以伪装成网站来窃取信息。而反过来，他也可以伪装成你，向网站发送支付、转账等消息，网站没有办法确认你的身份，钱可能就这么被偷走了。 现在如何实现身份认证呢？ 现实生活中，解决身份认证的手段是签名和印章，只要在纸上写下签名或者盖个章，就能够证明这份文件确实是由本人而不是其他人发出的。 非对称加密依然可以解决此问题，只不过跟之前反过来用，使用私钥再加上摘要算法，就能够实现“数字签名”，同时实现“身份认证”和“不可否认”。 就是把公钥私钥的用法反过来，之前是公钥加密、私钥解密，现在是私钥加密、公钥解密。但又因为非对称加密效率太低，所以私钥只加密原文的摘要，这样运算量就小的多，而且得到的数字签名也很小，方便保管和传输。 重点就是使用非对称加密的“私钥”加密原文的摘要，对方则使用非对称加密的公钥解密出摘要，再比对解密出的原文通过摘要算法计算摘要与解密出的摘要比对是否一致。 这样就能像签署文件一样证明消息确实是你发送的。 签名验签签名验签 \" 签名验签 只要你和网站互相交换公钥，就可以用“签名”和“验签”来确认消息的真实性，因为私钥保密，黑客不能伪造签名，就能够保证通信双方的身份。 CA 到这里似乎已经大功告成，可惜还不是。 综合使用对称加密、非对称加密和摘要算法，我们已经实现了安全的四大特性，是不是已经完美了呢？ 不是的，这里还有一个“公钥的信任”问题。因为谁都可以发布公钥，我们还缺少防止黑客伪造公钥的手段，也就是说，怎么来判断这个公钥就是你的公钥呢？ 我们常说的CA（Certificate Authority，证书认证机构），它就像网络世界里的公安局、教育部、公证中心，具有极高的可信度，由它来给各个公钥签名，用自身的信誉来保证公钥无法伪造，是可信的。 CA 对公钥的签名认证也是有格式的，不是简单地把公钥绑定在持有者身份上就完事了，还要包含序列号、用途、颁发者、有效时间等等，把这些打成一个包再签名，完整地证明公钥关联的各种信息，形成“数字证书”（Certificate）。 ","date":"2020-10-27","objectID":"/what-is-https/:3:4","tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/"},{"categories":["理解计算机"],"content":"OpenSSL 它是一个著名的开源密码学程序库和工具包，几乎支持所有公开的加密算法和协议，已经成为了事实上的标准，许多应用软件都会使用它作为底层库来实现 TLS 功能，包括常用的 Web 服务器 Apache、Nginx 等。 由于 OpenSSL 是开源的，所以它还有一些代码分支，比如 Google 的 BoringSSL、OpenBSD 的 LibreSSL，这些分支在 OpenSSL 的基础上删除了一些老旧代码，也增加了一些新特性，虽然背后有“大金主”，但离取代 OpenSSL 还差得很远。 总结下就是：OpenSSL 是著名的开源密码学工具包，是 SSL/TLS 的具体实现。 ","date":"2020-10-27","objectID":"/what-is-https/:3:5","tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/"},{"categories":["理解计算机"],"content":"迁移 HTTPS 必要性 如果你做移动应用开发的话，那么就一定知道，Apple、Android、某信等开发平台在 2017 年就相继发出通知，要求所有的应用必须使用 HTTPS 连接，禁止不安全的 HTTP。 在台式机上，主流的浏览器 Chrome、Firefox 等也早就开始“强推”HTTPS，把 HTTP 站点打上“不安全”的标签，给用户以“心理压力”。 Google 等搜索巨头还利用自身的“话语权”优势，降低 HTTP 站点的排名，而给 HTTPS 更大的权重，力图让网民只访问到 HTTPS 网站。 这些手段都逐渐“挤压”了纯明文 HTTP 的生存空间，“迁移到 HTTPS”已经不是“要不要做”的问题，而是“要怎么做”的问题了。HTTPS 的大潮无法阻挡，如果还是死守着 HTTP，那么无疑会被冲刷到互联网的角落里。 ","date":"2020-10-27","objectID":"/what-is-https/:4:0","tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/"},{"categories":["理解计算机"],"content":"顾虑 阻碍 HTTPS 实施的因素还有一些这样、那样的顾虑，三个比较流行的观点：“慢、贵、难”。 而“慢”则是惯性思维，拿以前的数据来评估 HTTPS 的性能，认为 HTTPS 会增加服务器的成本，增加客户端的时延，影响用户体验。 其实现在服务器和客户端的运算能力都已经有了很大的提升，性能方面完全没有担心的必要，而且还可以应用很多的优化解决方案 所谓“贵”，主要是指证书申请和维护的成本太高，网站难以承担。 这也属于惯性思维，在早几年的确是个问题，向 CA 申请证书的过程不仅麻烦，而且价格昂贵，每年要交几千甚至几万元。 但现在就不一样了，为了推广 HTTPS，很多云服务厂商都提供了一键申请、价格低廉的证书，而且还出现了专门颁发免费证书的 CA，其中最著名的就是“Let’s Encrypt”。 所谓的“难”，是指 HTTPS 涉及的知识点太多、太复杂，有一定的技术门槛，不能很快上手。 ","date":"2020-10-27","objectID":"/what-is-https/:5:0","tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/"},{"categories":["理解计算机"],"content":"总结 HTTPS 主要就是通过 SSL/TLS 实现安全，而安全又有对称加密与非对称加密，非对称加密性能较弱，所以我们使用非对称加密来加密对称加密的“会话密钥”，利用会话密钥加密明文解决了性能问题。 通过混合加密实现了机密性，利用摘要算法实现了完整性，通过数字签名使用非对称加密的“私钥”加密原文的摘要，对方则使用非对称加密的公钥解密出摘要，再比对解密出的原文通过摘要算法计算摘要与解密出的摘要比对是否一致实现了身份认证与不可否认。 ","date":"2020-10-27","objectID":"/what-is-https/:6:0","tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/"},{"categories":["理解计算机"],"content":"参考 透视HTTPS建造固若金汤的城堡 ","date":"2020-10-27","objectID":"/what-is-https/:7:0","tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/"},{"categories":["mysql"],"content":"xiaobinqt,mysql 存储程序,mysql 存储函数,存储过程,触发器,事件,mysql 游标,什么是游标,mysql 局部变量和自定义变量","date":"2020-10-08","objectID":"/mysql-stored-routine/","tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["mysql"],"content":"总览 存储程序存储程序 \" 存储程序 存储程序可以封装一些语句，然后给用户提供一种简单的方式来调用这个存储程序，从而间接地执行某些语句。根据调用方式的不同，可以把存储程序分为存储例程、触发器和事件，存储例程又分为存储函数和存储过程，如☝️ 上图。 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:1:0","tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["mysql"],"content":"存储函数 存储函数只有一个返回值，可以从 mysql 内置的函数理解，所有 mysql 内置的函数都是只有一个返回值，比如： mysql 内置函数mysql 内置函数 \" mysql 内置函数 存储函数很好理解，就是一个函数，跟普通函数一样有函数名，函数体，参数列表和返回值。创建存储函数语句如下： CREATEFUNCTION存储函数名称([参数列表])RETURNS返回值类型BEGIN函数体内容END CMD 说明 SHOW FUNCTION STATUS [LIKE 需要匹配的函数名] 查看所有存储函数 SHOW CREATE FUNCTION 函数名 查看某个存储函数 DROP FUNCTION 函数名 删除某个存储函数 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:2:0","tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["mysql"],"content":"示例 现在写一个存储函数，输入用户名 name，返回用户手机号 phone： CREATEFUNCTIONget_phone(qnameVARCHAR(45))RETURNSVARCHAR(11)BEGINRETURN(SELECTphonefromtwherename=qname);ENDEOF 存储函数的调用跟普通函数的调用也是一样的👇 效果效果 \" 效果 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:2:1","tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["mysql"],"content":"局部变量和自定义变量 在存储函数中可以使用局部变量和自定义变量，二者的区别是，局部变量用 DECLARE 申明，不用加 @符，局部变量随着函数调用结束，变量销毁且只能在存储函数中使用。自定义变量需要加 @ 符，且可以在函数外调用。 CREATE FUNCTION get_phone(qname VARCHAR (45)) RETURNS VARCHAR(11) BEGIN DECLARE ph varchar(11) default \"\"; # 局部变量 SET @ii = 100; # 自定义变量 SET ph = (select phone from t where name = qname); # 给局部变量赋值 RETURN ph ; END EOF 局部变量和自定义变量局部变量和自定义变量 \" 局部变量和自定义变量 ☝️ 可知，在存储函数get_phone中有一个局部变量 ph和自定义变量 @ii，函数调用结束后 @ii 被赋值为100 且可以在函数执行完后访问，但是 @ph 是空的。 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:3:0","tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["mysql"],"content":"存储过程 存储函数侧重于执行某些语句并返回一个值，而存储过程更侧重于单纯的去执行这些语句。存储过程的定义不需要声明返回值类型。 CREATEPROCEDURE存储过程名称([参数列表])BEGIN需要执行的语句END 调用存储过程使用 CALL 关键字。 CMD 说明 SHOW PROCEDURE STATUS [LIKE 需要匹配的存储过程名称] 查看所有存储过程 SHOW CREATE PROCEDURE 存储过程名称 查看某个存储过程 DROP PROCEDURE 存储过程名称 删除某个存储过程 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:4:0","tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["mysql"],"content":"示例 以下示例定义了一个 my_operate 的存储过程： CREATE PROCEDURE my_operate(pname varchar (45)) BEGIN SELECT * FROM t; INSERT INTO t(phone, name) VALUES(\"15214254125\", \"卢俊义\"); SELECT * FROM t; SELECT * from t where name = pname; END EOF 存储过程存储过程 \" 存储过程 ☝️ my_operate 定义并执行了 4 条 sql，完美诠释了存储过程更侧重于单纯的去执行这些语句。 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:4:1","tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["mysql"],"content":"存储过程的参数前缀 存储过程在定义参数的时候可以选择添加一些前缀👇，如果不写，默认的前缀是IN： 参数类型 [IN | OUT | INOUT] 参数名 数据类型 前缀 实际参数是否必须是变量 描述 IN 否 用于调用者向存储过程传递数据，如果IN参数在过程中被修改，调用者不可见。 OUT 是 用于把存储过程运行过程中产生的数据赋值给OUT参数，存储过程执行结束后，调用者可以访问到OUT参数。 INOUT 是 综合IN和OUT的特点，既可以用于调用者向存储过程传递数据，也可以用于存放存储过程中产生的数据以供调用者使用。 👇以下的示例，综合了 in，out，inout 参数： CREATE PROCEDURE my_arg( in pname varchar (45), out ophone char(11), inout io_name varchar(45) ) BEGIN SELECT * FROM t; INSERT INTO t(phone, name) VALUES(\"15225632145\", \"公孙胜\"); SELECT * FROM t; SELECT phone from t where name = pname into ophone; SET pname = \"公孙胜\"; SET io_name = \"公孙胜\"; END EOF 综合示例综合示例 \" 综合示例 由☝️可以看出，虽然在存储过程中修改了 pname 的值为 公孙胜，但是并没有生效，值依然是最初的宋江。IN参数只能被用于读取，对它赋值是不会被调用者看到的。 out 参数 ophone 最初是空的，通过存储过程赋值成功为公孙胜。 inout 参数 io_name 最初是空的，通过存储过程赋值成功为公孙胜，这里如果 io_name 不为空，也会被修改为公孙胜。 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:5:0","tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["mysql"],"content":"存储过程和存储函数的区别 存储函数在定义时需要显式用RETURNS语句标明返回的数据类型，而且在函数体中必须使用RETURN语句来显式指定返回的值，而存储过程不需要。 存储函数只支持IN参数，而存储过程支持IN参数、OUT参数、和INOUT参数。 存储函数只能返回一个值，而存储过程可以通过设置多个OUT参数或者INOUT参数来返回多个结果。 存储函数执行过程中产生的结果集并不会被显示到客户端，而存储过程执行过程中产生的结果集会被显示到客户端。 存储函数直接在表达式中调用，而存储过程只能通过CALL语句来显式调用。 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:6:0","tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["mysql"],"content":"游标 游标是为了方便访问结果集中的某一条记录，可以理解成循环。如果某个结果集中有 10 条记录，使用游标后，会一条一条的去访问这 10 条记录。 游标可以在存储函数和存储过程中使用。 cursorcursor \" cursor 使用游标分为 4 步： 创建游标：DECLARE 游标名称 CURSOR FOR 查询语句; 打开游标：OPEN 游标名称; 通过游标访问记录 关闭游标：CLOSE 游标名称; 不显式的使用CLOSE语句关闭游标的话，在该存储函数或存储过程执行完之后会自动关闭游标。 可以使用👇来获取结果集中的记录： FETCH 游标名 INTO 变量1, 变量2, ... 变量n ","date":"2020-10-08","objectID":"/mysql-stored-routine/:7:0","tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["mysql"],"content":"示例 🤑以下创建一个存储过程，在存储过程中使用游标。 创建游标t_cursor，游标执行语句为 SELECT phone, name FROM t。 DECLARE CONTINUE HANDLER FOR NOT FOUND 处理语句; 的作用是结果集遍历结束后会自动执行这句，这里也可以使用 WHILE 循环遍历，但是 while 有个弊端是需要提前知道结束条件，比如结果集的总数是多少。这样写的好处是直接遍历，遍历结束自动处理，将 done 变量设置为 1，也就是说只要 done = 1 就说明遍历结束了，利用 LEAVE 关键字跳出循环。 CREATE PROCEDURE my_cursor() BEGIN DECLARE v_phone char(11); DECLARE v_name varchar(45); DECLARE done INT DEFAULT 0; DECLARE t_cursor CURSOR FOR SELECT `phone`, `name` FROM t; DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = 1; OPEN t_cursor; flag: LOOP FETCH t_cursor INTO v_phone, v_name; IF done = 1 THEN LEAVE flag; END IF; SELECT v_phone, v_name, done; END LOOP flag; CLOSE t_cursor; END EOF 执行结果执行结果 \" 执行结果 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:7:1","tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["mysql"],"content":"触发器和事件 存储例程是需要手动调用的，而触发器和事件是 MySQL 服务器在特定情况下自动调用的。 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:8:0","tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["mysql"],"content":"触发器 创建触发器 CREATE TRIGGER 触发器名 {BEFORE|AFTER} {INSERT|DELETE|UPDATE} ON 表名 FOR EACH ROW BEGIN 触发器内容 END MySQL 目前只支持对INSERT、DELETE、UPDATE这三种类型的语句设置触发器。 FOR EACH ROW BEGIN ... END表示对具体语句影响的每一条记录都执行触发器内容。 对于INSERT语句来说，FOR EACH ROW影响的记录就是准备插入的那些新记录。 对于DELETE语句和UPDATE语句来说，FOR EACH ROW影响的记录就是符合条件的那些记录。 针对每一条受影响的记录，需要一种访问该记录中的内容的方式，MySQL提供了NEW和OLD两个单词来分别代表新记录和旧记录，它们在不同语句中的含义不同： 对于INSERT语句设置的触发器来说，NEW代表准备插入的记录，OLD无效。 对于DELETE语句设置的触发器来说，OLD代表删除前的记录，NEW无效。 对于UPDATE语句设置的触发器来说，NEW代表修改后的记录，OLD代表修改前的记录。 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:9:0","tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["mysql"],"content":"示例 🤦‍♂️以下示例，对表 t 创建一个 my_trigger触发器，表 t 有三个字段，name，phone，my_join，对于每条 insert 的语句，在执行 insert 之前判断如果 name = admin 那么将即将插入的 name 值改为 valid，如果 name 值为空，将即将插入的 name 值改为无名氏，除此之外将 name 和 phone 拼接后赋给 my_join 字段。 CREATE TRIGGER my_trigger BEFORE INSERT ON t FOR EACH ROW BEGIN IF NEW.name = 'admin' THEN SET NEW.name = 'valid'; ELSEIF NEW.name = '' THEN SET NEW.name = '无名氏'; ELSE SET NEW.my_join = CONCAT(NEW.name, \"--\", NEW.phone); END IF; END EOF 示例演示示例演示 \" 示例演示 CMD 说明 SHOW TRIGGERS; 查看所有触发器 SHOW CREATE TRIGGER 触发器名; 查看某个触发器 DROP TRIGGER 触发器名; 删除某个触发器 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:9:1","tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["mysql"],"content":"注意事项 触发器内容中不能有输出结果集的语句。 触发器内容中NEW代表记录的列的值可以被更改，OLD代表记录的列的值无法更改。 在BEFORE触发器中，我们可以使用SET NEW.列名 = 某个值的形式来更改待插入记录或者待更新记录的某个列的值，但是这种操作不能在AFTER触发器中使用，因为在执行AFTER 触发器的内容时记录已经被插入完成或者更新完成了。 如果我们的BEFORE触发器内容执行过程中遇到了错误，那这个触发器对应的具体语句将无法执行；如果具体的操作语句执行过程中遇到了错误，那与它对应的AFTER触发器的内容将无法执行。 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:9:2","tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["mysql"],"content":"事件 事件可以让 MySQL 服务器在某个时间点或者每隔一段时间自动地执行一些语句。 默认情况下，MySQL服务器并不会帮助我们执行事件，需要手动开启该功能： SET GLOBAL event_scheduler = ON; 开启事件功能开启事件功能 \" 开启事件功能 CREATE EVENT 事件名 ON SCHEDULE { AT 某个确定的时间点| EVERY 期望的时间间隔 [STARTS datetime][END datetime] } DO BEGIN 具体的语句 END ","date":"2020-10-08","objectID":"/mysql-stored-routine/:10:0","tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["mysql"],"content":"某个时间点执行 CREATE EVENT insert_t1_event ON SCHEDULE AT '2022-01-03 11:20:11' # 或者 AT DATE_ADD(NOW(), INTERVAL 2 DAY) DO BEGIN INSERT INTO t(phone, name) VALUES('15210214254', '宋江'); END ","date":"2020-10-08","objectID":"/mysql-stored-routine/:10:1","tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["mysql"],"content":"每隔一段时间执行 CREATE EVENT insert_t1 ON SCHEDULE EVERY 1 HOUR STARTS '2019-09-04 15:48:54' ENDS '2019-09-16 15:48:54' DO BEGIN INSERT INTO t(phone, name) VALUES('15210214254', '宋江'); END 在创建好事件之后，到了指定时间，MySQL 服务器会自动执行。 CMD 说明 SHOW EVENTS; 查看所有事件 SHOW CREATE EVENT 事件名; 查看某个事件 DROP EVENT 事件名; 删除某个事件 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:10:2","tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["golang"],"content":"Linux 环境下安装 Go,Linux Platform Install Go,如何在 Linux 环境下安装 Go,go,golang","date":"2020-08-12","objectID":"/linux-platform-install-go/","tags":["golang"],"title":"Linux 环境下安装 Go","uri":"/linux-platform-install-go/"},{"categories":["golang"],"content":"安装 在官网 https://go.dev/dl/，根据自己的环境下载对应的安装包： 官网安装包列表官网安装包列表 \" 官网安装包列表 可以直接用 wget 下载 下载安装包下载安装包 \" 下载安装包 执行 tar 解压到 /usr/loacl目录下（官方推荐），得到 go 文件夹等。 tar -C /usr/local -zxvf go1.17.7.linux-amd64.tar.gz go1.17.7.linux-amd64.tar.gz 换成你自己的 go 版本。 添加 /usr/loacl/go/bin 目录到 PATH 变量中。添加到 /etc/profile 或 $HOME/.profile 都可以。 vim /etc/profile # 在最后一行添加 export GOROOT=/usr/local/go export PATH=$PATH:$GOROOT/bin 添加环境变量添加环境变量 \" 添加环境变量 保存退出后source一下 source /etc/profile go envgo env \" go env ","date":"2020-08-12","objectID":"/linux-platform-install-go/:1:0","tags":["golang"],"title":"Linux 环境下安装 Go","uri":"/linux-platform-install-go/"},{"categories":["golang"],"content":"Go环境变量 $GOROOT 表示 Go 在你的电脑上的安装位置，值一般都是 $HOME/go，当然，也可以安装在别的地方。 $GOARCH 表示目标机器的处理器架构，它的值可以是 386、amd64 或 arm。 $GOOS 表示目标机器的操作系统，它的值可以是 darwin、freebsd、linux 或 windows。 $GOBIN 表示编译器和链接器的安装位置，默认是 $GOROOT/bin，如果使用的是 Go 1.0.3 及以后的版本，一般情况下你可以将它的值设置为空，Go 将会使用默认值。 $GOPATH 默认采用和 $GOROOT 一样的值，但从 Go 1.1 版本开始，你必须修改为其它路径。它可以包含多个包含 Go 语言源码文件、包文件和可执行文件的路径，而这些路径下又必须分别包含三个规定的目录：src、pkg 和 bin，这三个目录分别用于存放源码文件、包文件和可执行文件。 $GOARM 专门针对基于 arm 架构的处理器，它的值可以是 5~7，默认为 6。 $GOMAXPROCS 用于设置应用程序可使用的处理器个数与核数。 ","date":"2020-08-12","objectID":"/linux-platform-install-go/:2:0","tags":["golang"],"title":"Linux 环境下安装 Go","uri":"/linux-platform-install-go/"},{"categories":["golang"],"content":"go interface,golang 使用 interface,接口和实例的相互转换,接口的实现,接口是否实现了某个接口","date":"2020-06-18","objectID":"/go-interface/","tags":["golang"],"title":"Go interface","uri":"/go-interface/"},{"categories":["golang"],"content":"定义 interface 可以表示任意一种类型 interface 是接口的方法集合，只要实现了接口中的所有方法，那么就认为实现了这个接口 ","date":"2020-06-18","objectID":"/go-interface/:1:0","tags":["golang"],"title":"Go interface","uri":"/go-interface/"},{"categories":["golang"],"content":"用途 ","date":"2020-06-18","objectID":"/go-interface/:2:0","tags":["golang"],"title":"Go interface","uri":"/go-interface/"},{"categories":["golang"],"content":"实现多态 package main import ( \"fmt\" ) type animal interface { Say() string Color() string } type Cat struct{} func (c Cat) Say() string { return \"i am a cat\" } func (c Cat) Color() string { return \"i am black\" } type Dog struct{} func (d Dog) Say() string { return \"i am a dog\" } func (d Dog) Color() string { return \"i am white\" } type Car struct{} func introduceSelf(input animal) { fmt.Println(input.Say() + \" and \" + input.Color()) } func main() { c := Cat{} d := Dog{} introduceSelf(c) introduceSelf(d) // car 没有实现 animal 接口 //car := Car{} //introduceSelf(car) } ","date":"2020-06-18","objectID":"/go-interface/:2:1","tags":["golang"],"title":"Go interface","uri":"/go-interface/"},{"categories":["golang"],"content":"隐藏具体实现 以 go 中的 context 包为例，context.Context() 是一个接口： // A Context carries a deadline, a cancellation signal, and other values across // API boundaries. // // Context's methods may be called by multiple goroutines simultaneously. type Context interface { // Deadline returns the time when work done on behalf of this context // should be canceled. Deadline returns ok==false when no deadline is // set. Successive calls to Deadline return the same results. Deadline() (deadline time.Time, ok bool) // Done returns a channel that's closed when work done on behalf of this // context should be canceled. Done may return nil if this context can // never be canceled. Successive calls to Done return the same value. // The close of the Done channel may happen asynchronously, // after the cancel function returns. // // WithCancel arranges for Done to be closed when cancel is called; // WithDeadline arranges for Done to be closed when the deadline // expires; WithTimeout arranges for Done to be closed when the timeout // elapses. // // Done is provided for use in select statements: // // // Stream generates values with DoSomething and sends them to out // // until DoSomething returns an error or ctx.Done is closed. // func Stream(ctx context.Context, out chan\u003c- Value) error { // for { // v, err := DoSomething(ctx) // if err != nil { // return err // } // select { // case \u003c-ctx.Done(): // return ctx.Err() // case out \u003c- v: // } // } // } // // See https://blog.golang.org/pipelines for more examples of how to use // a Done channel for cancellation. Done() \u003c-chan struct{} // If Done is not yet closed, Err returns nil. // If Done is closed, Err returns a non-nil error explaining why: // Canceled if the context was canceled // or DeadlineExceeded if the context's deadline passed. // After Err returns a non-nil error, successive calls to Err return the same error. Err() error // Value returns the value associated with this context for key, or nil // if no value is associated with key. Successive calls to Value with // the same key returns the same result. // // Use context values only for request-scoped data that transits // processes and API boundaries, not for passing optional parameters to // functions. // // A key identifies a specific value in a Context. Functions that wish // to store values in Context typically allocate a key in a global // variable then use that key as the argument to context.WithValue and // Context.Value. A key can be any type that supports equality; // packages should define keys as an unexported type to avoid // collisions. // // Packages that define a Context key should provide type-safe accessors // for the values stored using that key: // // // Package user defines a User type that's stored in Contexts. // package user // // import \"context\" // // // User is the type of value stored in the Contexts. // type User struct {...} // // // key is an unexported type for keys defined in this package. // // This prevents collisions with keys defined in other packages. // type key int // // // userKey is the key for user.User values in Contexts. It is // // unexported; clients use user.NewContext and user.FromContext // // instead of using this key directly. // var userKey key // // // NewContext returns a new Context that carries value u. // func NewContext(ctx context.Context, u *User) context.Context { // return context.WithValue(ctx, userKey, u) // } // // // FromContext returns the User value stored in ctx, if any. // func FromContext(ctx context.Context) (*User, bool) { // u, ok := ctx.Value(userKey).(*User) // return u, ok // } Value(key interface{}) interface{} } WithCancel 和 WithValue 返回的第一个参数都是 context，但是各自返回的 Context 结构体又不是一样的： WithCancel 返回结构体为 cancelCtx WithValue 返回的结构体为 valueCtx 这样的话尽管返回的都是 context，但是具体实现却不一样，实现了功能的多样化。 ","date":"2020-06-18","objectID":"/go-interface/:2:2","tags":["golang"],"title":"Go interface","uri":"/go-interface/"},{"categories":["golang"],"content":"解耦依赖 下面的示例👇，如果缓存从 Redis 换成了 MemoryCache, 我们只需要修改 MemoryCache 的实现和初始化的地方，而不需要修改 Redis 的实现，这样就解耦了依赖，更加灵活。 package main type Cache interface { GetValue(key string) string } // 假设这是redis客户端 type Redis struct { } func (r Redis) GetValue(key string) string { panic(\"not implement\") } // 假设这是自定义的一个缓存器 type MemoryCache struct { } func (m MemoryCache) GetValue(key string) string { panic(\"not implement\") } // 通过接口实现：检查用户是否有权限的功能 func AuthExpire(token string, cache Cache) bool { res := cache.GetValue(token) if res == \"\" { return false } else { // 正常处理 return true } } func main() { token := \"test\" cache := Redis{} // cache := MemoryCache{},修改这一句即可 AuthExpire(token, cache) } ","date":"2020-06-18","objectID":"/go-interface/:2:3","tags":["golang"],"title":"Go interface","uri":"/go-interface/"},{"categories":["golang"],"content":"FAQ ","date":"2020-06-18","objectID":"/go-interface/:3:0","tags":["golang"],"title":"Go interface","uri":"/go-interface/"},{"categories":["golang"],"content":"var _ Interface = (*Type)(nil) var _ Person = (*Student)(nil) 以上☝️的语句：将空值 nil 转换为 *Student 类型，再转换为 Person 接口，如果转换失败，说明 Student 并没有实现 Person 接口的所有方法。 这是确保接口被实现常用的方式。即利用强制类型转换，确保 struct Student 实现了接口 Person。这样 IDE 和编译期间就可以检查，而不是等到使用的时候。 ","date":"2020-06-18","objectID":"/go-interface/:3:1","tags":["golang"],"title":"Go interface","uri":"/go-interface/"},{"categories":["golang"],"content":"实例、接口相互转换 实例可以强制类型转换为接口，接口也可以强制类型转换为实例。 package main import \"fmt\" type Student struct { name string age int } type Person interface { getName() string } func (stu *Student) getName() string { return stu.name } func (stu *Student) getAge() int { return stu.age } func main() { var s *Student = \u0026Student{ name: \"narcissus\", age: 20, } var p Person = \u0026Student{ name: \"Tom\", age: 18, } stu := p.(*Student) // 接口转为实例 fmt.Println(stu.getName(), \"---\", stu.getAge()) var pp Person = (*Student)(s) // 实例转为接口 // var _ Person = s // 实例转为接口 fmt.Println(pp.getName()) // 这里不能调用 pp.getAge() 因为 Person 接口中没有 getAge() 方法 } 通过运行后的打印结果👇可知，结果复合预期。 运行打印运行打印 \" 运行打印 ","date":"2020-06-18","objectID":"/go-interface/:3:2","tags":["golang"],"title":"Go interface","uri":"/go-interface/"},{"categories":["golang"],"content":"接口是否实现了某个接口 http/server.go 中有这样的判断语句： rw, err := l.Accept() if err != nil { select { case \u003c-srv.getDoneChan(): return ErrServerClosed default: } if ne, ok := err.(net.Error); ok \u0026\u0026 ne.Temporary() { if tempDelay == 0 { tempDelay = 5 * time.Millisecond } else { tempDelay *= 2 } if max := 1 * time.Second; tempDelay \u003e max { tempDelay = max } srv.logf(\"http: Accept error: %v; retrying in %v\", err, tempDelay) time.Sleep(tempDelay) continue } return err } ne, ok := err.(net.Error) 就是判断 err 变量是不是 err.Error 接口类型： // An Error represents a network error. type Error interface { error Timeout() bool // Is the error a timeout? Temporary() bool // Is the error temporary? } 以下示例自定了一个 MyError 接口验证以上推断： package main import \"fmt\" type MyError interface { Msg() string } type myErrorStruct struct { errMsg string } func (e *myErrorStruct) Msg() string { return e.errMsg } func myNew(msg string) MyError { return \u0026myErrorStruct{errMsg: msg} } func main() { err1 := fmt.Errorf(\"error\") e1, ok1 := err1.(MyError) fmt.Println(\"e1 = \", e1, \"ok1 = \", ok1) err2 := myNew(\"我是一个错误\") e2, ok2 := err2.(MyError) fmt.Printf(\"e2 = %#v ,,ok2: %t\", e2, ok2) } 从 👇 运行打印结果来看，符合预期。 ","date":"2020-06-18","objectID":"/go-interface/:3:3","tags":["golang"],"title":"Go interface","uri":"/go-interface/"},{"categories":["golang"],"content":"参考 Consider adding “var _ Interface = (*Type)(nil)” to the list of recommendations 类似var _ PeerPicker = (*HTTPPool)(nil)这种设计目的是什么 ","date":"2020-06-18","objectID":"/go-interface/:4:0","tags":["golang"],"title":"Go interface","uri":"/go-interface/"},{"categories":["golang"],"content":"go,continue,break,goto label的区别,Golang","date":"2020-05-16","objectID":"/break-continue-goto-label/","tags":["golang"],"title":"golang break，continue，goto label 的区别","uri":"/break-continue-goto-label/"},{"categories":["golang"],"content":"在其他语言，比如 php 中可以直接在 break 和 continue 后加 num ，比如 break 2或 continue 2。 break num 是结束外层第 num 层整个循环体，continue num 是结束外层第 num 层单次循环。 go 中不能直接在关键字后加 num ，但是可以用 label 关键代替 num。支持 goto label，也可以用 break label 和 continue label。 goto 可以跳到代码的任何地方，比如跳到 A 处，再从 A 继续往下执行。break label 如果跳到了循环外，不会再执行循环。continue label 如果跳出了循环，会再执行循环。 ","date":"2020-05-16","objectID":"/break-continue-goto-label/:0:0","tags":["golang"],"title":"golang break，continue，goto label 的区别","uri":"/break-continue-goto-label/"},{"categories":["golang"],"content":"continue label package main import ( \"fmt\" \"math\" ) func main() { // 找出 int 切片的最小值 var matrix = []int{10, 2, 4, 0} var min = math.MinInt64 next: for _, v := range matrix { for _, v1 := range matrix { if v \u003e v1 { continue next // 终止当前循环，跳到 label 继续下一次循环 } } min = v } fmt.Println(\"最小值为: \", min) } ","date":"2020-05-16","objectID":"/break-continue-goto-label/:1:0","tags":["golang"],"title":"golang break，continue，goto label 的区别","uri":"/break-continue-goto-label/"},{"categories":["golang"],"content":"break label 👇 以下例子，虽然 break 跳出循环到 label 处, label 在 for 循环上，但是不会再执行 for 循环，直接执行fmt.Println()。 package main import ( \"fmt\" \"math\" ) func main() { // 获取 index 2 的值，这里使用 2 层循环主要是为了说明问题 var matrix = []int{10, 2, 4, 0} var index2Val = math.MinInt64 next: for _, v := range matrix { fmt.Println(v) for index, v1 := range matrix { index2Val = v1 if index == 2 { break next } } } fmt.Println(\"index 3 值为: \", index2Val) } ","date":"2020-05-16","objectID":"/break-continue-goto-label/:2:0","tags":["golang"],"title":"golang break，continue，goto label 的区别","uri":"/break-continue-goto-label/"},{"categories":["golang"],"content":"goto label 非必要不使用，可以跳到任何地方。 package main import ( \"fmt\" \"math\" ) func main() { var matrix = []int{10, 2, 4, 0} var index2Val = math.MinInt64 for _, v := range matrix { fmt.Println(v) for index, v1 := range matrix { index2Val = v1 if index == 2 { goto next } } } fmt.Println(\"index 3 值为: \", index2Val) next: fmt.Println(\"goto this....\") } ","date":"2020-05-16","objectID":"/break-continue-goto-label/:3:0","tags":["golang"],"title":"golang break，continue，goto label 的区别","uri":"/break-continue-goto-label/"},{"categories":["算法与数学"],"content":"常见缓存淘汰策略，淘汰策略的实现，FIFO，LRU，LFU","date":"2020-03-05","objectID":"/common-cache-strategies/","tags":["算法"],"title":"常见缓存淘汰策略","uri":"/common-cache-strategies/"},{"categories":["算法与数学"],"content":"常见缓存淘汰策略 ","date":"2020-03-05","objectID":"/common-cache-strategies/:0:0","tags":["算法"],"title":"常见缓存淘汰策略","uri":"/common-cache-strategies/"},{"categories":["算法与数学"],"content":"FIFO First In First Out(FIFO)，先进先出，也就是淘汰缓存中最老(最早添加)的记录。FIFO 认为，最早添加的记录，其不再被使用的可能性比刚添加的可能性大。这种算法的实现也非常简单，创建一个队列，新增记录添加到队尾， 每次内存不够时，淘汰队首。但是很多场景下，部分记录虽然是最早添加但也最常被访问，而不得不因为呆的时间太长而被淘汰。这类数据会被频繁地添加进缓存，又被淘汰出去，导致缓存命中率降低。 ","date":"2020-03-05","objectID":"/common-cache-strategies/:1:0","tags":["算法"],"title":"常见缓存淘汰策略","uri":"/common-cache-strategies/"},{"categories":["算法与数学"],"content":"LFU Least Frequently Used(LFU)，最少使用，也就是淘汰缓存中访问频率最低的记录。LFU 认为，如果数据过去被访问多次， 那么将来被访问的频率也更高。LFU 的实现需要维护一个按照访问次数排序的队列，每次访问，访问次数加1，队列重新排序， 淘汰时选择访问次数最少的即可。LFU 算法的命中率是比较高的，但缺点也非常明显，维护每个记录的访问次数，对内存的消耗是很高的； 另外，如果数据的访问模式发生变化，LFU 需要较长的时间去适应，也就是说 LFU 算法受历史数据的影响比较大。例如某个数据历史上访问次数奇高，但在某个时间点之后几乎不再被访问，但因为历史访问次数过高，而迟迟不能被淘汰。 ","date":"2020-03-05","objectID":"/common-cache-strategies/:2:0","tags":["算法"],"title":"常见缓存淘汰策略","uri":"/common-cache-strategies/"},{"categories":["算法与数学"],"content":"LRU Least Recently Used(LRU)，最近最少使用，相对于仅考虑时间因素的 FIFO 和仅考虑访问频率的 LFU，LRU 算法可以认为是相对平衡的 一种淘汰算法。LRU 认为，如果数据最近被访问过，那么将来被访问的概率也会更高。LRU 算法的实现非常简单，维护一个队列，如果某条记录被访问了， 则移动到队尾，那么队首则是最近最少访问的数据，淘汰该条记录即可。 ","date":"2020-03-05","objectID":"/common-cache-strategies/:3:0","tags":["算法"],"title":"常见缓存淘汰策略","uri":"/common-cache-strategies/"},{"categories":["开发者手册"],"content":"缩写,SGTM,PR,LGTM,WIP,QPS,FD,OOM,TTL","date":"2020-02-18","objectID":"/funny-abbreviations/","tags":["chore"],"title":"那些迷之缩写","uri":"/funny-abbreviations/"},{"categories":["开发者手册"],"content":"互联网是个造词的行业，娴熟的司机们都会使用缩写来达到提高逼格的效果。 某些缩写在我们第一次看到时会有一脸懵逼的感觉，这里整理一下作者在工作和生活中遇到的一些缩写及其含义，以后我们也可以欢快地装逼了。 缩写 说明 PR Pull Request，给其他项目提交代码 LGTM Looks Good To Me. 朕知道了 代码已经过 review，可以合并 SGTM Sounds Good To Me. 和上面那句意思差不多，也是已经通过了 review 的意思 WIP Work In Progress. 传说中提 PR 的最佳实践是，如果你有个改动很大的 PR，可以在写了一部分的情况下先提交，但是在标题里写上 WIP，以告诉项目维护者这个功能还未完成，方便维护者提前 review 部分提交的代码。 PTAL Please Take A Look. 你来瞅瞅？用来提示别人来看一下 TBR To Be Reviewed，提示维护者进行 review TL;DR Too Long; Didn’t Read. 太长懒得看。也有很多文档在做简略描述之前会写这么一句 TBD To Be Done (or Defined/Discussed/Decided/Determined). 根据语境不同意义有所区别，但一般都是还没搞定的意思 QPS Query Per Second，服务器每秒可以执行的查询次数 fd 文件描述符File descriptor，是一个用于表述指向文件的引用的抽象化概念 OOM Out of memory，内存用完了 TTL the remaining time to live ，剩余存活时间 NFS Network File System，使用者访问网络上别处的文件就像在使用自己的计算机一样 FAQ frequently asked questions，常见问题 ","date":"2020-02-18","objectID":"/funny-abbreviations/:0:0","tags":["chore"],"title":"那些迷之缩写","uri":"/funny-abbreviations/"},{"categories":["mysql"],"content":"xiaobinqt, mysql 常见问题,主键和 unique 的区别,什么是 mysql 结束符,什么是 mysql 外键,什么是 ZEROFILL 显示宽度,limit offset 的区别,mysql 判断语句,mysql 循环语句,mysql DUPLICATE KEY UPDATE 用法,mysql 变量","date":"2019-10-03","objectID":"/mysql-simple-faq/","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"主键和 UNIQUE 的区别 主键和UNIQUE约束都能保证某个列或者列组合的唯一性，但是： 一张表中只能定义一个主键，却可以定义多个UNIQUE约束！ 主键列不允许存放NULL，而声明了UNIQUE属性的列可以存放NULL，而且NULL可以重复地出现在多条记录中。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:1:0","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"结束符 delimiter EOF # 将结束符改为 EOF 修改结束符修改结束符 \" 修改结束符 由☝️图可知，将默认的结束符从 ; 改为 EOF。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:2:0","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"什么是外键 如果A表中的某个列或者某些列依赖与B表中的某个列或者某些列，那么就称A表为子表，B表为父表。子表和父表可以使用外键来关联起来。 父表中被子表依赖的列或者列组合必须建立索引，如果该列或者列组合已经是主键或者有UNIQUE属性，那么也就被默认建立了索引。 定义外键的语法： CONSTRAINT[外键名称]FOREIGNKEY(列1,列2,...)REFERENCES父表名(父列1,父列2,...); ","date":"2019-10-03","objectID":"/mysql-simple-faq/:3:0","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"示例 CREATETABLEstudent_score(numberINT,-- 学号 subjectVARCHAR(30),scoreTINYINT,PRIMARYKEY(number,subject),CONSTRAINTFOREIGNKEY(number)REFERENCESstudent_info(number)); ☝️ 如上，在对student_score表插入数据的时候，MySQL都会检查插入的学号是否能在student_info表中找到，如果找不到则会报错，因为student_score表中的number 列依赖于student_info表的number列，也就是，如果没有这个学生，何来成绩？ ","date":"2019-10-03","objectID":"/mysql-simple-faq/:3:1","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"ZEROFILL 对于无符号整数类型的列，可以在查询数据的时候让数字左边补 0，如果想实现这个效果需要给该列加一个ZEROFILL属性： CREATETABLEzerofill_table(i1INT(10)UNSIGNEDZEROFILL,i2INTUNSIGNED); INT后边的(5)，这个 5 就是显示宽度，默认是10，也就是 INT 也 INT(10) 效果是一样的。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:4:0","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"注意 该列必须是整数类型 该列必须有 UNSIGNED ZEROFILL的属性 该列的实际值的位数必须小于显示宽度 在创建表的时候，如果声明了ZEROFILL属性的列没有声明UNSIGNED属性，MySQL会为该列自动生成UNSIGNED属性 显示宽度并不会影响实际类型的实际存储空间 对于没有声明ZEROFILL属性的列，显示宽度没有任何作用，只有在查询声明了ZEROFILL属性的列时，显示宽度才会起作用，否则可以忽略显示宽度这个东西的存在。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:4:1","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"limit、offset 区别 从 0 开始计数，第1条记录在 MYSQL 中是第 0 条。 limit 和 offset 都可以用来限制查询条数，一般用做分页。 当 limit 后面跟一个参数的时候，该参数表示要取的数据的数量 select*fromuserlimit3 表示直接取前三条数据。 当 limit 后面跟两个参数的时候，第一个数表示开始行，后一位表示要取的数量，例如 select*fromuserlimit1,3; 从 0 行开始计算，取第 1 - 3 条数据，也就是取 1,2,3 三条数据。 当 limit 和 offset 组合使用的时候，limit 后面只能有一个参数，表示要取的的数量，offset 表示开始行。 select*fromuserlimit3offset1; 从 0 行开始计算，取第 1 - 3 条数据，也就是取 1,2,3 三条数据。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:5:0","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"常用函数 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:6:0","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"文本处理函数 名称 调用示例 示例结果 描述 LEFT LEFT('abc123', 3) abc 给定字符串从左边取指定长度的子串 RIGHT RIGHT('abc123', 3) 123 给定字符串从右边取指定长度的子串 LENGTH LENGTH('abc') 3 给定字符串的长度 LOWER LOWER('ABC') abc 给定字符串的小写格式 UPPER UPPER('abc') ABC 给定字符串的大写格式 LTRIM LTRIM(' abc') abc 给定字符串左边空格去除后的格式 RTRIM RTRIM('abc ') abc 给定字符串右边空格去除后的格式 SUBSTRING SUBSTRING('abc123', 2, 3) bc1 给定字符串从指定位置截取指定长度的子串 CONCAT CONCAT('abc', '123', 'xyz') abc123xyz 将给定的各个字符串拼接成一个新字符串 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:6:1","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"时间处理函数 名称 调用示例 示例结果 描述 NOW NOW() 2019-08-16 17:10:43 返回当前日期和时间 CURDATE CURDATE() 2019-08-16 返回当前日期 CURTIME CURTIME() 17:10:43 返回当前时间 DATE DATE('2019-08-16 17:10:43') 2019-08-16 将给定日期和时间值的日期提取出来 DATE_ADD DATE_ADD('2019-08-16 17:10:43', INTERVAL 2 DAY) 2019-08-18 17:10:43 将给定的日期和时间值添加指定的时间间隔 DATE_SUB DATE_SUB('2019-08-16 17:10:43', INTERVAL 2 DAY) 2019-08-14 17:10:43 将给定的日期和时间值减去指定的时间间隔 DATEDIFF DATEDIFF('2019-08-16', '2019-08-17') -1 返回两个日期之间的天数（负数代表前一个参数代表的日期比较小） DATE_FORMAT DATE_FORMAT(NOW(),'%m-%d-%Y') 08-16-2019 用给定的格式显示日期和时间 常见时间单位 时间单位 描述 MICROSECOND 毫秒 SECOND 秒 MINUTE 分钟 HOUR 小时 DAY 天 WEEK 星期 MONTH 月 QUARTER 季度 YEAR 年 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:6:2","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"数值处理函数 名称 调用示例 示例结果 描述 ABS ABS(-1) 1 取绝对值 Pi PI() 3.141593 返回圆周率 COS COS(PI()) -1 返回一个角度的余弦 EXP EXP(1) 2.718281828459045 返回e的指定次方 MOD MOD(5,2) 1 返回除法的余数 RAND RAND() 0.7537623539136372 返回一个随机数 SIN SIN(PI()/2) 1 返回一个角度的正弦 SQRT SQRT(9) 3 返回一个数的平方根 TAN TAN(0) 0 返回一个角度的正切 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:6:3","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"COUNT 函数 COUNT函数使用来统计行数的，有下边两种使用方式： COUNT(*)：对表中行的数目进行计数，不管列的值是不是NULL。 COUNT(列名)：对特定的列进行计数，会忽略掉该列为NULL的行。 两者的区别是会不会忽略统计列的值为NULL的行。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:7:0","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"查询 where 竟然可以这么写😇 select*fromedgewhere(ip,mode)=('192.168.50.101',2); in 竟然可以这么写😂 select*fromedgewhere(ip,mode)in(select'192.168.50.101',2); ","date":"2019-10-03","objectID":"/mysql-simple-faq/:8:0","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"判断语句 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:9:0","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"if then IF 表达式 THEN 处理语句列表 [ELSEIF 表达式 THEN 处理语句列表] ... # 这里可以有多个ELSEIF语句 [ELSE 处理语句列表] END IF; ","date":"2019-10-03","objectID":"/mysql-simple-faq/:9:1","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"case when CASE WHEN 表达式 THEN 处理语句 else 表达式 end ## 或者 CASE when 表达式 then 处理语句 when 表达式 then 处理语句 ... 可以与多个 when 表达式 then 处理语句 END 示例： select *, CASE WHEN name='大彬' THEN '角色1' else '角色2' end as processed_name , case when status = 1 then '已处理' when status = 0 then '未处理' when status = 2 then '待处理' end as processed_status from user; ","date":"2019-10-03","objectID":"/mysql-simple-faq/:9:2","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"循环语句 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:10:0","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"WHILE WHILE 表达式 DO 处理语句列表 END WHILE; ","date":"2019-10-03","objectID":"/mysql-simple-faq/:10:1","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"REPEAT REPEAT 处理语句列表 UNTIL 表达式 END REPEAT; ","date":"2019-10-03","objectID":"/mysql-simple-faq/:10:2","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"LOOP LOOP 处理语句列表 END LOOP; 在使用 LOOP 时可以使用RETURN语句直接让函数结束就可以达到停止循环的效果，也可以使用LEAVE语句，不过使用LEAVE时，需要先在LOOP语句前边放置一个所谓的标记。 CREATE FUNCTION sum_all(n INT UNSIGNED) RETURNS INT BEGIN DECLARE result INT DEFAULT 0; DECLARE i INT DEFAULT 1; flag:LOOP IF i \u003e n THEN LEAVE flag; END IF; SET result = result + i; SET i = i + 1; END LOOP flag; RETURN result; END ☝️示例中，在LOOP语句前加了一个flag:，相当于为这个循环打了一个名叫flag的标记，然后在对应的END LOOP语句后边也把这个标记名flag 给写上了。在存储函数的函数体中使用LEAVE flag语句来结束flag这个标记所代表的循环。 标记主要是为了可以跳到指定的语句中 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:10:3","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"DUPLICATE KEY UPDATE 对于主键或者有唯一性约束的列或列组合来说，新插入的记录如果和表中已存在的记录重复的话，我们可以选择的策略不仅仅是忽略（INSERT IGNORE）该条记录的插入，也可以选择更新这条重复的旧记录。 CREATETABLE`t`(`idt`int(11)NOTNULLAUTO_INCREMENT,`phone`char(11)DEFAULTNULL,`name`varchar(45)DEFAULTNULL,PRIMARYKEY(`idt`),UNIQUEKEY`idt_UNIQUE`(`idt`),UNIQUEKEY`phone_UNIQUE`(`phone`))ENGINE=InnoDBAUTO_INCREMENT=2DEFAULTCHARSET=utf8 如上表，idt 是唯一主键，phone 是 UNIQUE 唯一约束。 图1图1 \" 图1 表里有条记录 phone = 15212124125，name = '吴彦祖'，现在再添加一条记录，phone 跟 name = '吴彦祖' 是一样的，但是 name='宋江' INSERTINTOt(phone,name)VALUES('15212124125','宋江')ONDUPLICATEKEYUPDATEname='宋江';-- 对于批量插入可以这么写，`VALUES(列名)`的形式来引用待插入记录中对应列的值 INSERTINTOt(phone,name)VALUES('15212124125','宋江'),('15212124126','李逵')ONDUPLICATEKEYUPDATEname=VALUES(`name`); 结果： 图2图2 \" 图2 由结果可知，phone 电话的值没有改变，但是 name 被修改成了宋江。 也就是说，如果 t 表中已经存在 phone 的列值为 15212124125 的记录（因为 phone列具有UNIQUE约束），那么就把该记录的 name列更新为'宋江'。 对于那些是主键或者具有UNIQUE约束的列或者列组合来说，如果表中已存在的记录中有与待插入记录在这些列或者列组合上重复的值，我们可以使用VALUES(列名)的形式来引用待插入记录中对应列的值 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:11:0","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"自定义变量 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:12:0","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"单个变量 设置单个变量可以使用 SET 关键字。 设置单个变量设置单个变量 \" 设置单个变量 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:12:1","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"多个变量 设置多个变量可以使用 INTO 关键字。 设置多个变量设置多个变量 \" 设置多个变量 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:12:2","tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":"xiaobinqt,mysql 数据类型","date":"2019-06-15","objectID":"/mysql-data-type/","tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/"},{"categories":["mysql"],"content":" MySQL 是以字节为单位存储数据的，一个字节拥有8个比特位。如果存储的不足 1 个字节，MySQL 会自动填充成 1 个字节。 字符（Character）是各种文字和符号的总称，包括各国家文字、标点符号、图形符号、数字等。 字符集（Character set）是一个系统支持的所有抽象字符的集合。 字符编码（Character encoding）是把字符集中的字符编码为特定的二进制数，以便在计算机中存储。每个字符集中的字符都对应一个唯一的二进制编码。 ","date":"2019-06-15","objectID":"/mysql-data-type/:0:0","tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/"},{"categories":["mysql"],"content":"字符编码 字符是面向人的概念，字节是面向计算机的概念。如果想在计算机中表示字符，那就需要将该字符与一个特定的字节序列对应起来，这个映射过程称之为编码。但是，这种映射关系并不是唯一的，不同的人制作了不同的编码方案。 根据表示一个字符使用的字节数量是不是固定的，编码方案可以分为以下两种： ","date":"2019-06-15","objectID":"/mysql-data-type/:1:0","tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/"},{"categories":["mysql"],"content":"定长编码 表示不同的字符所需要的字节数量是相同的。比如 ASCII 编码方案采用 1 个字节来编码一个字符，ucs2 采用 2 个字节来编码一个字符。 ","date":"2019-06-15","objectID":"/mysql-data-type/:1:1","tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/"},{"categories":["mysql"],"content":"变长编码 表示不同的字符所需要的字节数量是不同的。比方说 utf8 编码方案采用 1~3 个字节来编码一个字符，gb2312 采用 1~2 个字节来编码一个字符。 对于不同的字符编码方案来说，同一个字符可能被编码成不同的字节序列。比如同样一个字符：我，在 utf8 和 gb2312 这两种编码方案下被映射成如下的字节序列： utf8 编码方案 字符我被编码成 111001101000100010010001，共占用 3 个字节，用十六进制表示就是：0xE68891。 gb2312 编码方案 字符我被编码成 1100111011010010，共占用 2 个字节，用十六进制表示就是：0xCED2。 MySQL 对编码方案和字符集这两个概念并没做什么区分，也就是说 utf8 字符集指的就是 utf8 编码方案，gb2312 字符集指的也就是 gb2312 编码方案。 正宗的 utf8 字符集是使用 1~4 个字节来编码一个字符的，不过MySQL中对utf8字符集做了阉割，编码一个字符最多使用3个字节。 如果有存储使用4个字节来编码的字符的情景，可以使用一种称之为utf8mb4的字符集，它才是正宗的utf8字符集。 ","date":"2019-06-15","objectID":"/mysql-data-type/:1:2","tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/"},{"categories":["mysql"],"content":"整数 类型 占用空间 无符号数取值范围 有符号数取值范围 含义 TINYINT 1 0 ~ 2⁸-1 -2⁷ ~ 2⁷-1 非常小的整数 SMALLINT 2 0 ~ 2¹⁶-1 -2¹⁵ ~ 2¹⁵-1 小的整数 MEDIUMINT 3 0 ~ 2²⁴-1 -2²³ ~ 2²³-1 中等大小的整数 INT（别名：INTEGER） 4 0 ~ 2³²-1 -2³¹ ~ 2³¹-1 标准的整数 BIGINT 8 0 ~ 2⁶⁴-1 -2⁶³ ~ 2⁶³-1 大整数 ","date":"2019-06-15","objectID":"/mysql-data-type/:2:0","tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/"},{"categories":["mysql"],"content":"浮点数 类型 占用空间 绝对值最小非0值 绝对值最大非0值 含义 FLOAT 4 ±1.175494351E-38 ±3.402823466E+38 单精度浮点数 DOUBLE 8 ±2.2250738585072014E-308 ±1.7976931348623157E+308 双精度浮点数 有的十进制小数，比如 1.875 可以被很容易的转换成二进制数 1.111 ，但是更多的小数是无法直接转换成二进制的，比如说 0.3 ，它转换成的二进制小数就是一个无限小数，但是现在只能用 4 个字节或者 8 个字节来表示这个小数，所以只能进行一些舍入来近似的表示，所以说计算机的浮点数表示有时是不精确的。 ","date":"2019-06-15","objectID":"/mysql-data-type/:3:0","tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/"},{"categories":["mysql"],"content":"设置最大最小位数 可以使用 FLOAT(M, D)或者 DOUBLE(M, D) 来限制可以存储到本列中的小数范围。其中： M表示该小数最多需要的十进制有效数字个数。 D表示该小数的小数点后的十进制数字个数。 M的取值范围是1~255，D的取值范围是0~30，而且D的值必须不大于M。 M和D都是可选的，如果省略，它们的值按照机器支持的最大值来存储。 计算十进制有效数字个数时，不计入正负号，不计入最左边的 0，也不计入小数点。所以 -2.3 来说有效数字个数就是 2，0.9 有效数字个数就是 1。 ","date":"2019-06-15","objectID":"/mysql-data-type/:3:1","tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/"},{"categories":["mysql"],"content":"定点数 因为用浮点数表示小数可能会有不精确的情况，在一些情况下我们必须保证小数是精确的，可以使用定点数的数据类型。 与浮点数相比，定点数需要更多的空间来存储数据，所以如果不是在某些需要存储精确小数的场景下，一般的小数用浮点数表示就足够了。 类型 占用空间 取值范围 DECIMAL(M, D) 取决于M和D 取决于M和D DECIMAL 如果不指定精度，默认的 M 的值是 10 ，默认的 D 的值是 0，也就是说下列等式是成立的： DECIMAL = DECIMAL(10) = DECIMAL(10, 0) DECIMAL(n) = DECIMAL(n, 0) M 的范围是 1~65，D的范围是0~30，且D的值不能超过M。 ","date":"2019-06-15","objectID":"/mysql-data-type/:4:0","tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/"},{"categories":["mysql"],"content":"无符号数值 对于数值类型，包括整数、浮点数和定点数，有些情况下只需要用到无符号数（就是非负数）。 MySQL 提供了一个表示无符号数值类型的方式，就是在原数值类型后加一个单词UNSIGNED： 数值类型 UNSIGNED 可以把它当成一种新类型对待，比如INT UNSIGNED就表示无符号整数，FLOAT UNSIGNED表示无符号浮点数，DECIMAL UNSIGNED表示无符号定点数。 在使用的存储空间大小相同的情况下，无符号整数可以表示的正整数范围比有符号整数能表示的正整数范围大一倍。 不过受浮点数和定点数具体的存储格式影响，无符号浮点数和定点数并不能提升正数的表示范围。 ","date":"2019-06-15","objectID":"/mysql-data-type/:5:0","tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/"},{"categories":["mysql"],"content":"时间和日期 类型 存储空间 取值范围 含义 YEAR 1 1901~2155 年份值 DATE 3 1000-01-01~9999-12-31 日期值 TIME 3 -838:59:59~838:59:59 时间值 DATETIME 8 1000-01-01 00:00:00~9999-12-31 23:59:59 日期加时间值 TIMESTAMP 4 1970-01-01 00:00:01~2038-01-19 03:14:07 时间戳 TIME表示时间，格式是hh:mm:ss[.uuuuuu]或者hhh:mm:ss[.uuuuuu] 用时间戳存储时间的好处就是，它展示的值可以随着时区的变化而变化。 DATETIME 中的时间部分表示的是一天内的时间(00:00:00 ~ 23:59:59)，而 TIME 表示的是一段时间，而且可以表示负值。 ","date":"2019-06-15","objectID":"/mysql-data-type/:6:0","tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/"},{"categories":["mysql"],"content":"字符串 类型 最大长度 存储空间要求 含义 CHAR(M) M个字符 M×W个字节 固定长度的字符串 VARCHAR(M) M个字符 L+1 或 L+2 个字节 可变长度的字符串 TINYTEXT 2⁸-1 个字节 L+1个字节 非常小型的字符串 TEXT 2¹⁶-1 个字节 L+2 个字节 小型的字符串 MEDIUMTEXT 2²⁴-1 个字节 L+3个字节 中等大小的字符串 LONGTEXT 2³²-1 个字节 L+4个字节 大型的字符串 M 代表该数据类型最多能存储的字符数量，L 代表我们实际向该类型的属性中存储的字符串在特定字符集下所占的字节数，W代表在该特定字符集下，编码一个字符最多需要的字节数。 VARCHAR(M) 中的M也是代表该类型最多可以存储的字符数量，理论上的取值范围是 1~65535。但是MySQL中还有一个规定，表中某一行包含的所有列中存储的数据大小总共不得超过 65535 个字节，也就是说 VARCHAR(M) 类型实际能够容纳的字符数量是小于 65535 的。 ","date":"2019-06-15","objectID":"/mysql-data-type/:7:0","tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/"},{"categories":["mysql"],"content":"参考 在线进制转换 MySQL数据类型 ","date":"2019-06-15","objectID":"/mysql-data-type/:8:0","tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/"},{"categories":["开发者手册"],"content":"阿里云,SSL,免费证书使用,aliyun,ali,证书,TLS","date":"2019-06-07","objectID":"/ali-ssl/","tags":["SSL","aliyun"],"title":"阿里云 SSL 免费证书使用","uri":"/ali-ssl/"},{"categories":["开发者手册"],"content":"申请 证书申请地址 在这里插入图片描述 \" 申请完成页面 在这里插入图片描述 \" 将主机记录解析 在这里插入图片描述 \" 在这里插入图片描述 \" 在这里插入图片描述 \" 将主机记录和记录值填写 在这里插入图片描述 \" 解析成功后下载证书 在这里插入图片描述 \" 我用的是 Apache ,所以下载的是 Apache 在这里插入图片描述 \" ","date":"2019-06-07","objectID":"/ali-ssl/:1:0","tags":["SSL","aliyun"],"title":"阿里云 SSL 免费证书使用","uri":"/ali-ssl/"},{"categories":["开发者手册"],"content":"上传证书 由于本人使用的是 apache ,以下配置是 apache 的通用配置,具体可参看官方 文档 在 apache 的路径下新建一个 cert 目录,其实该目录建在哪里都可以,但是放在 apache 下方便管理。 \" 在 cert 目录下可以建不同的文件夹放在不同域名或子域名的 ssl 文件。 \" 把我们刚才下载的证书上传到服务器上 \" ","date":"2019-06-07","objectID":"/ali-ssl/:2:0","tags":["SSL","aliyun"],"title":"阿里云 SSL 免费证书使用","uri":"/ali-ssl/"},{"categories":["开发者手册"],"content":"配置 这是基本的配置语句 # 添加 SSL 协议支持协议，去掉不安全的协议 SSLProtocol all -SSLv2 -SSLv3 # 修改加密套件如下 SSLCipherSuite HIGH:!RC4:!MD5:!aNULL:!eNULL:!NULL:!DH:!EDH:!EXP:+MEDIUM SSLHonorCipherOrder on # 证书公钥配置 SSLCertificateFile cert/a_public.crt # 证书私钥配置 SSLCertificateKeyFile cert/a.key # 证书链配置，如果该属性开头有 '#'字符，请删除掉 SSLCertificateChainFile cert/a_chain.crt 我们将默认的配置 copy 一份出来,取一个跟域名有关的文件名 cp /etc/apache2/sites-available/000-default.conf /etc/apache2/sites-available/www.xiaobinqt.cn.conf 具体配置可参考 \u003cVirtualHost *:80\u003e ServerName www.xiaobinqt.cn Redirect permanent / https://www.xiaobinqt.cn/ \u003c/VirtualHost\u003e \u003cVirtualHost *:443\u003e SSLEngine On # 添加 SSL 协议支持协议，去掉不安全的协议 SSLProtocol all -SSLv2 -SSLv3 # 修改加密套件如下 SSLCipherSuite HIGH:!RC4:!MD5:!aNULL:!eNULL:!NULL:!DH:!EDH:!EXP:+MEDIUM SSLHonorCipherOrder on # 证书公钥配置 SSLCertificateFile cert/xiaobinqt.cn/2324042_www.xiaobinqt.cn_public.crt # 证书私钥配置 SSLCertificateKeyFile cert/xiaobinqt.cn/2324042_www.xiaobinqt.cn.key # 证书链配置，如果该属性开头有 '#'字符，请删除掉 SSLCertificateChainFile cert/xiaobinqt.cn/2324042_www.xiaobinqt.cn_chain.crt # etc ServerName www.xiaobinqt.cn ProxyPreserveHost On ProxyRequests Off ProxyPass / http://localhost:30007/ ProxyPassReverse / http://localhost:30007/ \u003c/VirtualHost\u003e 我用的是 docker 服务,如果你的只是项目文件夹可以参考这样配置 \u003cVirtualHost *:80\u003e # The ServerName directive sets the request scheme, hostname and port that # the server uses to identify itself. This is used when creating # redirection URLs. In the context of virtual hosts, the ServerName # specifies what hostname must appear in the request's Host: header to # match this virtual host. For the default virtual host (this file) this # value is not decisive as it is used as a last resort host regardless. # However, you must set it for any further virtual host explicitly. #ServerName www.example.com #ServerAdmin webmaster@localhost ServerName www.xiaobinqt.cn DocumentRoot /var/www/html Redirect permanent / https://www.xiaobinqt.cn/ # Available loglevels: trace8, ..., trace1, debug, info, notice, warn, # error, crit, alert, emerg. # It is also possible to configure the loglevel for particular # modules, e.g. #LogLevel info ssl:warn ErrorLog ${APACHE_LOG_DIR}/error.log CustomLog ${APACHE_LOG_DIR}/access.log combined # For most configuration files from conf-available/, which are # enabled or disabled at a global level, it is possible to # include a line for only one particular virtual host. For example the # following line enables the CGI configuration for this host only # after it has been globally disabled with \"a2disconf\". #Include conf-available/serve-cgi-bin.conf \u003c/VirtualHost\u003e \u003cVirtualHost *:443\u003e SSLEngine On # 添加 SSL 协议支持协议，去掉不安全的协议 SSLProtocol all -SSLv2 -SSLv3 # 修改加密套件如下 SSLCipherSuite HIGH:!RC4:!MD5:!aNULL:!eNULL:!NULL:!DH:!EDH:!EXP:+MEDIUM SSLHonorCipherOrder on # 证书公钥配置 SSLCertificateFile cert/xiaobinqt.cn/public.pem # 证书私钥配置 SSLCertificateKeyFile cert/xiaobinqt.cn/214792197160511.key # 证书链配置，如果该属性开头有 '#'字符，请删除掉 SSLCertificateChainFile cert/xiaobinqt.cn/chain.pem # etc ServerName www.xiaobinqt.cn \u003c/VirtualHost\u003e 以上配置全部基于 apache ,如果你用的不是 apache ,以上配置可能不适合你. 关于 apache 服务的一些其他知识可以参考这篇文章,该文章可能需要翻~墙访问. 配置完成后重启服务,可以利用 curl 命令查看是否配置成功. curl -I localhost:xxx \" 对于 ssl 是否配置成功可以通过浏览器查看. \" 可以看到这是我们最新申请的一年的 ssl 证书. \" ","date":"2019-06-07","objectID":"/ali-ssl/:3:0","tags":["SSL","aliyun"],"title":"阿里云 SSL 免费证书使用","uri":"/ali-ssl/"},{"categories":["开发者手册"],"content":"xiaobinqt,docker compose 使用方法,docker compose 如何映射端口,docker compose 常用命令,dokcer compose 指定镜像,文件挂载,使用数据卷","date":"2019-05-21","objectID":"/docker-compose/","tags":["docker"],"title":"Docker Compose 简单使用","uri":"/docker-compose/"},{"categories":["开发者手册"],"content":"如果说 Dockerfile 是将容器内运行环境的搭建固化下来，那么 Docker Compose 可以理解为将多个容器运行的方式和配置固化下来。 ","date":"2019-05-21","objectID":"/docker-compose/:0:0","tags":["docker"],"title":"Docker Compose 简单使用","uri":"/docker-compose/"},{"categories":["开发者手册"],"content":"常用命令 CMD 说明 docker-compose up 根据 docker-compose.yml 中配置的内容，创建所有的容器、网络、数据卷等等内容，并将它们启动。 docker-compose down 停止所有的容器，并将它们删除，同时消除网络等配置内容。 docker-compose logs 服务名 查看服务日志 docker-compose 命令默认会识别当前控制台所在目录内的 docker-compose.yml 文件，会以这个目录的名字作为组装的应用项目的名称。如果需要改变它们，可以通过选项 -f 来修改识别的 Docker Compose 配置文件，通过 -p 选项来定义项目名👇。 docker-compose -f ./compose/docker-compose.yml -p myapp up -d ","date":"2019-05-21","objectID":"/docker-compose/:1:0","tags":["docker"],"title":"Docker Compose 简单使用","uri":"/docker-compose/"},{"categories":["开发者手册"],"content":"配置项 version:\"3\"services:redis:image:redis:3.2networks:- backendvolumes:- ./redis/redis.conf:/etc/redis.conf:roports:- \"6379:6379\"command:[\"redis-server\",\"/etc/redis.conf\"]database:image:mysql:5.7networks:- backendvolumes:- ./mysql/my.cnf:/etc/mysql/my.cnf:ro- mysql-data:/var/lib/mysqlenvironment:- MYSQL_ROOT_PASSWORD=my-secret-pwports:- \"3306:3306\"webapp:build:./webappnetworks:- frontend- backendvolumes:- ./webapp:/webappdepends_on:- redis- databasenginx:image:nginx:1.12networks:- frontendvolumes:- ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro- ./nginx/conf.d:/etc/nginx/conf.d:ro- ./webapp/html:/webapp/htmldepends_on:- webappports:- \"80:80\"- \"443:443\"networks:frontend:backend:volumes:mysql-data: ☝️如上： version，这个配置是可选的，代表定义的 docker-compose.yml 文件内容所采用的版本，目前 Docker Compose 的配置文件已经迭代至了第三版。 services ，是整个 docker-compose.yml 的核心，services定义了容器的各项细节。 在 Docker Compose 里不直接体现容器这个概念，而是把 service 作为配置的最小单元。虽然看上去每个 service 里的配置内容就像是在配置容器，但其实 service 代表的是一个应用集群的配置。 ","date":"2019-05-21","objectID":"/docker-compose/:2:0","tags":["docker"],"title":"Docker Compose 简单使用","uri":"/docker-compose/"},{"categories":["开发者手册"],"content":"定义服务 在使用 docker compose 时，可以为为每个服务定义一个名称，用以区别不同的服务。在这个例子里，redis、database、webapp、nginx就是服务的名称。 ","date":"2019-05-21","objectID":"/docker-compose/:2:1","tags":["docker"],"title":"Docker Compose 简单使用","uri":"/docker-compose/"},{"categories":["开发者手册"],"content":"指定镜像 容器最基础的就是镜像，而每个服务必须指定镜像。在 Docker Compose 里，可以通过两种方式为服务指定所采用的镜像。 一种是通过 image 这个配置，给出能在镜像仓库中找到镜像的名称即可。 另外一种指定镜像的方式就是直接采用 Dockerfile 来构建镜像，通过 build 这个配置能够定义构建的环境目录，可以通过这种方式指定镜像，Docker Compose 先会帮助我们执行镜像的构建，之后再通过这个镜像启动容器。 在docker build里还能通过选项定义许多内容，这些在 Docker Compose 里依然可以👇，我们能够指定更多的镜像构建参数，例如 Dockerfile 的文件名，构建参数等等。 webapp: build: context: ./webapp dockerfile: webapp-dockerfile args: - JAVA_VERSION=1.6 ","date":"2019-05-21","objectID":"/docker-compose/:2:2","tags":["docker"],"title":"Docker Compose 简单使用","uri":"/docker-compose/"},{"categories":["开发者手册"],"content":"依赖声明 如果服务间有非常强的依赖关系，就必须告知 Docker Compose 容器的先后启动顺序。只有当被依赖的容器完全启动后，Docker Compose 才会创建和启动这个容器。 定义依赖的方式很简单，在上面的例子里可以看到，就是 depends_on 这个配置项，只需要通过它列出这个服务所有依赖的其他服务即可。在 Docker Compose 为我们启动项目的时候，会检查所有依赖，形成正确的启动顺序并按这个顺序来依次启动容器。 ","date":"2019-05-21","objectID":"/docker-compose/:2:3","tags":["docker"],"title":"Docker Compose 简单使用","uri":"/docker-compose/"},{"categories":["开发者手册"],"content":"文件挂载 Docker Compose 里定义文件挂载的方式与 Docker Engine 里也并没有太多的区别，使用 volumes 配置可以像 docker CLI 里的 -v 选项一样来指定外部挂载和数据卷挂载。 在上面的例子里，可以看到几种常用挂载的方式。我们能够直接挂载宿主机文件系统中的目录，也可以通过数据卷的形式挂载内容。 可以直接指定相对目录进行挂载，这里的相对目录是指相对于 docker-compose.yml 文件的目录。 ","date":"2019-05-21","objectID":"/docker-compose/:2:4","tags":["docker"],"title":"Docker Compose 简单使用","uri":"/docker-compose/"},{"categories":["开发者手册"],"content":"使用数据卷 独立于 services 的 volumes 配置就是用来声明数据卷的。定义数据卷最简单的方式仅需要提供数据卷的名称。 如果想把属于 Docker Compose 项目以外的数据卷引入进来直接使用，可以将数据卷定义为外部引入，通过 external 这个配置就能完成这个定义。 volumes: mysql-data: external: true 在加入 external 定义后，Docker Compose 在创建项目时不会直接创建数据卷，而是优先从 Docker Engine 中已有的数据卷里寻找并直接采用。 ","date":"2019-05-21","objectID":"/docker-compose/:2:5","tags":["docker"],"title":"Docker Compose 简单使用","uri":"/docker-compose/"},{"categories":["开发者手册"],"content":"端口映射 ports 这个配置项，它是用来定义端口映射的。可以利用它进行宿主机与容器端口的映射，这个配置与 docker CLI 中 -p 选项的使用方法是近似的。 由于 YAML 格式对 xx:yy 这种格式的解析有特殊性，在设置小于 60 的值时，会被当成时间而不是字符串来处理，所以最好使用引号将端口映射的定义包裹起来，避免歧义。 ","date":"2019-05-21","objectID":"/docker-compose/:2:6","tags":["docker"],"title":"Docker Compose 简单使用","uri":"/docker-compose/"},{"categories":["web"],"content":"xiaobinqt,什么是跨域,如何解决跨域问题,什么是JSONP,什么是 CORS,简单请求,复杂请求,跨域预检机制","date":"2019-03-18","objectID":"/cross-domain/","tags":["web"],"title":"跨域问题","uri":"/cross-domain/"},{"categories":["web"],"content":"同源策略 同源策略是浏览器的一个安全行为，是指浏览器对不同源的脚本或文本的访问方式进行限制。比如，ajax 在进行请求时，浏览器要求当前网页和请求地址必须同源，也就是协议，域名和端口必须相同。 同源指的就是相同的协议，域名，端口号。 浏览器是公共资源，假如没有同源策略，A 网站的接口可以被任意来源的 ajax 请求访问，这样就会出现问题，这是浏览器出于对用户数据的保护。 比如在使用淘宝的过程中，淘宝返回了一个 cookie，下次请求你会带上cookie，这样子服务器就知道你登录过了。 假设，你买东西过程中，又点来了一个链接，由于没有同源策略，他就在后台操作向淘宝发起请求，那么就相当于不法网站利用你的账号为所欲为了。 同源策略限制的不同源之间的交互，主要是针对 JS 中的 XMLHttpRequest 请求。有一些情况是不受影响的如：html 的一些标签的请求，链接 a 标签，图片 img 标签等，这些标签的请求可以为不同源地址。 同源策略限制了 Cookie、LocalStorage 和 IndexDB 无法读取，DOM 和 JS 对象无法获取，Ajax 请求无法发送。 所以，协议，域名，端口号只要有一个不同就存在跨域。 解决跨域问题常用的有 JSONP 和 CORS 这两种方案，以下对他们分别进行介绍。 ","date":"2019-03-18","objectID":"/cross-domain/:1:0","tags":["web"],"title":"跨域问题","uri":"/cross-domain/"},{"categories":["web"],"content":"jsonp jsonp 虽然能解决跨域问题，它只支持GET请求😢。 jsonp 是利用\u003cscript\u003e标签没有跨域限制的“漏洞”来达到与第三方通讯的目的。当需要通讯时，本站脚本创建一个\u003cscript\u003e元素，地址指向第三方的API网址，形如： \u003cscript src=\"http://www.example.net/api?param1=1\u0026param2=2\"\u003e\u003c/script\u003e 并提供一个回调函数来接收数据（函数名可约定，或通过地址参数传递）。 第三方产生的响应为 json 数据的包装，所以称之为 jsonpjson padding，形如： callback({ \"name\": \"吴彦祖\", \"age\": \"28\" }) 这样浏览器会调用 callback 函数，并传递解析后json对象作为参数。本站脚本可在callback函数里处理所传入的数据。 接口返回的函数名一定要跟定义的函数一致 有一个接口http://127.0.0.1:8080/cb返回 jsonp 数据👇 jsonp 接口jsonp 接口 \" jsonp 接口 html 文件： \u003c!doctype html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0\"\u003e \u003cmeta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"\u003e \u003ctitle\u003eDocument\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cscript\u003e let script = document.createElement('script'); script.type = 'text/javascript'; //请求接口地址和参数 script.src = 'http://127.0.0.1:8080/cb'; document.body.appendChild(script); //请求后的回调函数 function callback(res) { console.log(res) } \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e 效果👇 执行效果执行效果 \" 执行效果 ","date":"2019-03-18","objectID":"/cross-domain/:2:0","tags":["web"],"title":"跨域问题","uri":"/cross-domain/"},{"categories":["web"],"content":"CORS 整个CORSCross-origin resource sharing 通信过程，都是浏览器自动完成，不需要用户参与。对于开发者来说，CORS通信与同源的AJAX通信没有差别，代码完全一样。 因此，实现CORS通信的关键是服务器。只要服务器实现了CORS接口，就可以跨源通信。它允许浏览器向跨源服务器，发出XMLHttpRequest 请求，浏览器一旦发现AJAX请求跨源，就会自动添加一些附加的头信息，有时还会多出一次附加的请求，从而克服了 AJAX 只能同源使用的限制。 ","date":"2019-03-18","objectID":"/cross-domain/:3:0","tags":["web"],"title":"跨域问题","uri":"/cross-domain/"},{"categories":["web"],"content":"两种请求 浏览器将CORS请求分成两类：简单请求simple request和非简单请求not-so-simple request。只要同时满足👇两大条件，就属于简单请求。 请求方法是以下三种方法之一： HEAD GET POST HTTP 头信息不超出以下几种字段： Accept Accept-Language Content-Language Last-Event-ID Content-Type：只限于三个值application/x-www-form-urlencoded、multipart/form-data、text/plain 除了由用户代理自动设置的头（如，Connection、 User-Agent） 这是为了兼容表单form，历史上表单一直可以发出跨域请求。AJAX 的跨域设计就是，只要表单可以发，AJAX 就可以直接发。 凡是不同时满足上面两个条件，就属于非简单请求。 浏览器对这两种请求的处理，是不一样的。 ","date":"2019-03-18","objectID":"/cross-domain/:3:1","tags":["web"],"title":"跨域问题","uri":"/cross-domain/"},{"categories":["web"],"content":"简单请求 对于简单请求，浏览器直接发出CORS请求。在头信息之中，增加一个Origin字段。 下面的例子，浏览器发现这次跨源 AJAX 请求是简单请求，就自动在头信息之中，添加一个Origin字段。 GET /cors HTTP/1.1 Origin: http://api.bob.com Host: api.alice.com Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... 上面的头信息中，Origin字段用来说明，本次请求来自哪个源（协议 + 域名 + 端口）。服务器根据这个值，决定是否同意这次请求。 如果Origin 指定的源，不在许可范围内，服务器会返回一个正常的 HTTP回应。浏览器发现，这个回应的头信息没有包含Access-Control-Allow-Origin字段（详见下文），就知道出错了，从而抛出一个错误，被XMLHttpRequest的onerror 回调函数捕获。这种错误无法通过状态码识别，因为HTTP回应的状态码有可能是200。 如果Origin指定的域名在许可范围内，服务器返回的响应，会多出几个头信息字段。 Access-Control-Allow-Origin: http://api.bob.com Access-Control-Allow-Credentials: true Access-Control-Expose-Headers: FooBar Content-Type: text/html; charset=utf-8 上面的头信息之中，有三个与CORS请求相关的字段，都以Access-Control-开头。 Access-Control-Allow-Origin 该字段是必须的。它的值要么是请求时Origin字段的值，要么是一个*，表示接受任意域名的请求。 Access-Control-Allow-Credentials 该字段可选。它的值是一个布尔值，表示是否允许发送Cookie。默认情况下，Cookie不包括在CORS请求之中。设为true，即表示服务器明确许可，Cookie可以包含在请求中，一起发给服务器。这个值也只能设为true，如果服务器不要浏览器发送Cookie，删除该字段即可。 Access-Control-Expose-Headers 该字段可选。CORS请求时，XMLHttpRequest对象的getResponseHeader() 方法只能拿到6个基本字段：Cache-Control、Content-Language、Content-Type、Expires、Last-Modified、Pragma 。如果想拿到其他字段，就必须在Access-Control-Expose-Headers里面指定。上面的例子指定，getResponseHeader('FooBar')可以返回FooBar字段的值。 withCredentials 属性 CORS请求默认不发送Cookie和HTTP认证信息。如果要把Cookie发到服务器，一方面要服务器同意，指定Access-Control-Allow-Credentials字段。 Access-Control-Allow-Credentials: true 另一方面，开发者必须在AJAX请求中打开withCredentials属性。 var xhr = new XMLHttpRequest(); xhr.withCredentials = true; 否则，即使服务器同意发送Cookie，浏览器也不会发送。或者，服务器要求设置Cookie，浏览器也不会处理。 但是，如果省略withCredentials设置，有的浏览器还是会一起发送Cookie。这时，可以显式关闭withCredentials。 xhr.withCredentials = false; 需要注意的是，如果要发送Cookie，Access-Control-Allow-Origin就不能设为星号* ，必须指定明确的、与请求网页一致的域名。同时，Cookie依然遵循同源政策，只有用服务器域名设置的Cookie才会上传，其他域名的Cookie并不会上传，且（跨源）原网页代码中的document.cookie 也无法读取服务器域名下的Cookie。 ","date":"2019-03-18","objectID":"/cross-domain/:3:2","tags":["web"],"title":"跨域问题","uri":"/cross-domain/"},{"categories":["web"],"content":"非简单请求 预检请求 非简单请求是那种对服务器有特殊要求的请求，比如请求方法是PUT或DELETE，或者Content-Type字段的类型是application/json。 非简单请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，称为“预检\"请求preflight request。 浏览器先询问服务器，当前网页所在的域名是否在服务器的许可名单之中，以及可以使用哪些HTTP动词和头信息字段。只有得到肯定答复，浏览器才会发出正式的XMLHttpRequest请求，否则就报错。 下面是一段浏览器的JavaScript脚本。 var url = 'http://api.alice.com/cors'; var xhr = new XMLHttpRequest(); xhr.open('PUT', url, true); xhr.setRequestHeader('X-Custom-Header', 'value'); xhr.send(); 上面代码中，HTTP请求的方法是PUT，并且发送一个自定义头信息X-Custom-Header。 浏览器发现，这是一个非简单请求，就自动发出一个\"预检\"请求，要求服务器确认可以这样请求。下面是这个\"预检\"请求的HTTP头信息。 OPTIONS /cors HTTP/1.1 Origin: http://api.bob.com Access-Control-Request-Method: PUT Access-Control-Request-Headers: X-Custom-Header Host: api.alice.com Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... “预检\"请求用的请求方法是OPTIONS，表示这个请求是用来询问的。头信息里面，关键字段是Origin，表示请求来自哪个源。 除了Origin字段，“预检\"请求的头信息包括两个特殊字段。 Access-Control-Request-Method 该字段是必须的，用来列出浏览器的CORS请求会用到哪些HTTP方法，上例是PUT。 Access-Control-Request-Headers 该字段是一个逗号分隔的字符串，指定浏览器CORS请求会额外发送的头信息字段，上例是X-Custom-Header。 预检请求的回应 服务器收到\"预检\"请求以后，检查了Origin、Access-Control-Request-Method和Access-Control-Request-Headers字段以后，确认允许跨源请求，就可以做出回应。 HTTP/1.1 200 OK Date: Mon, 01 Dec 2008 01:15:39 GMT Server: Apache/2.0.61 (Unix) Access-Control-Allow-Origin: http://api.bob.com Access-Control-Allow-Methods: GET, POST, PUT Access-Control-Allow-Headers: X-Custom-Header Content-Type: text/html; charset=utf-8 Content-Encoding: gzip Content-Length: 0 Keep-Alive: timeout=2, max=100 Connection: Keep-Alive Content-Type: text/plain 上面的HTTP回应中，关键的是Access-Control-Allow-Origin字段，表示http://api.bob.com可以请求数据。该字段也可以设为星号，表示同意任意跨源请求。 Access-Control-Allow-Origin: * 如果服务器否定了\"预检\"请求，会返回一个正常的HTTP回应，但是没有任何CORS相关的头信息字段。这时，浏览器就会认定，服务器不同意预检请求，因此触发一个错误，被XMLHttpRequest对象的onerror 回调函数捕获。控制台会打印出如下的报错信息。 XMLHttpRequest cannot load http://api.alice.com. Origin http://api.bob.com is not allowed by Access-Control-Allow-Origin. 服务器回应的其他CORS相关字段如下。 Access-Control-Allow-Methods: GET, POST, PUT Access-Control-Allow-Headers: X-Custom-Header Access-Control-Allow-Credentials: true Access-Control-Max-Age: 1728000 Access-Control-Allow-Methods 该字段必需，它的值是逗号分隔的一个字符串，表明服务器支持的所有跨域请求的方法。注意，返回的是所有支持的方法，而不单是浏览器请求的那个方法。这是为了避免多次\"预检\"请求。 Access-Control-Allow-Headers 如果浏览器请求包括Access-Control-Request-Headers字段，则Access-Control-Allow-Headers字段是必需的。它也是一个逗号分隔的字符串，表明服务器支持的所有头信息字段，不限于浏览器在” 预检\"中请求的字段。 Access-Control-Allow-Credentials 该字段与简单请求时的含义相同。 Access-Control-Max-Age 该字段可选，用来指定本次预检请求的有效期，单位为秒。上面结果中，有效期是20天（1728000秒），即允许缓存该条回应1728000秒（即20天），在此期间，不用发出另一条预检请求。 浏览器的正常请求和回应 一旦服务器通过了\"预检\"请求，以后每次浏览器正常的CORS请求，就都跟简单请求一样，会有一个Origin头信息字段。服务器的回应，也都会有一个Access-Control-Allow-Origin头信息字段。 下面是\"预检\"请求之后，浏览器的正常CORS请求。 PUT /cors HTTP/1.1 Origin: http://api.bob.com Host: api.alice.com X-Custom-Header: value Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... 上面头信息的Origin字段是浏览器自动添加的。 下面是服务器正常的回应。 Access-Control-Allow-Origin: http://api.bob.com Content-Type: text/html; charset=utf-8 上面头信息中，Access-Control-Allow-Origin字段是每次回应都必定包含的。 ","date":"2019-03-18","objectID":"/cross-domain/:3:3","tags":["web"],"title":"跨域问题","uri":"/cross-domain/"},{"categories":["web"],"content":"参考 跨域及其解决方案 https://developer.mozilla.org/en-US/docs/web/http/cors#simple_requests 跨域资源共享 CORS 详解 ","date":"2019-03-18","objectID":"/cross-domain/:4:0","tags":["web"],"title":"跨域问题","uri":"/cross-domain/"},{"categories":["开发者手册"],"content":"xiaobinqt,wampserver的安装和使用,wampserver,wampserver php","date":"2018-11-12","objectID":"/wampserver-install-usage/","tags":["wampserver","php"],"title":"wampserver 的安装和使用","uri":"/wampserver-install-usage/"},{"categories":["开发者手册"],"content":"本地开发 php 环境推荐使用 wampserver，下载地址为 WampServer download | SourceForge.net 当然国产的 phpStudy 也可以，个人喜好问题。 ","date":"2018-11-12","objectID":"/wampserver-install-usage/:0:0","tags":["wampserver","php"],"title":"wampserver 的安装和使用","uri":"/wampserver-install-usage/"},{"categories":["开发者手册"],"content":"下载 安装 wampserver 之前我们需要先安装 Visual C++ Redistributable for Visual Studio，这是 visual 2015 的下载地址 Visual C++ Redistributable for Visual Studio 2015 from Official Microsoft Download Center ，根据自己的版本下载对应的版本。 Visual StudioVisual Studio \" Visual Studio visual c++ redistributable for visual studio 2012 在很多时候已经不能用了。 我安装的是 64 位的 wampserver，所以下载对应的 visual C++： visual C++ 图 1visual C++ 图 1 \" visual C++ 图 1 visual C++ 图 2visual C++ 图 2 \" visual C++ 图 2 之后安装 wampserver 安装 wampserver安装 wampserver \" 安装 wampserver 在安装过程中会让你选择默认的浏览器和编辑器。 ","date":"2018-11-12","objectID":"/wampserver-install-usage/:1:0","tags":["wampserver","php"],"title":"wampserver 的安装和使用","uri":"/wampserver-install-usage/"},{"categories":["开发者手册"],"content":"配置 如果有需要可以修改 wampserver 的根目录 打开 wampserver 的安装目录，在打开里面的 script 文件夹，用记事本打开里面的 config.inc.php,找到 $wwwDir = $c_installDir.'/www' 改成希望的目录就行了。 比如改成 D:\\website，对应的代码就是 $wwwDir = 'D:/website'；然后关闭 wampserver。 （注意，windows下表示路径的 \\在这里必须改为 /） wwwDirwwwDir \" wwwDir 打开 Apach 下面的 httpd.conf 文件，路径示例 httpd.confhttpd.conf \" httpd.conf 寻找 DocumentRoot，把后面的值改成我们实际网站需要的路径，再寻找 \u003cDirectory \"c:/wamp/www/\"\u003e 修改成相同的路径 DocumentRootDocumentRoot \" DocumentRoot 配置虚拟主机 VirtualHost 示例 \u003cVirtualHost *\u003e ServerAdmin root@localhost DocumentRoot \"D:\\projects\\pc-mes\\src\\public\" ServerName pc-mes.local ErrorLog \"d:\\wamp64\\www\\pc-mes.localhost-error.log\" CustomLog \"d:\\wamp64\\www\\pc-mes.localhost.log\" common \u003c/VirtualHost\u003e 访问示例 访问示例访问示例 \" 访问示例 ","date":"2018-11-12","objectID":"/wampserver-install-usage/:2:0","tags":["wampserver","php"],"title":"wampserver 的安装和使用","uri":"/wampserver-install-usage/"},{"categories":["开发者手册"],"content":"Windows,系统,重做系统,win,win10,U盘安装系统","date":"2018-10-07","objectID":"/redo-system/","tags":["windows"],"title":"windows 重做系统","uri":"/redo-system/"},{"categories":["开发者手册"],"content":"下载系统 首先我们需要一个最小 4G 大的 U 盘 image \" 进入大白菜网站下载大白菜装机版安装到电脑 image \" image \" 将 U 盘插到电脑上,双击打开大白菜装机版,它会自动读到我们插入的 U 盘，自动匹配默认模式,不需要手动选择。 image \" 点击开始制作 –\u003e 确认 image \" 等待写入数据包完成和格式化完成后关掉大白菜软件 image \" image \" image \" 制作完 U 盘启动盘后我们的 U 盘会变成这样说明启动盘已经制作成功 ! image去itellyou下载需要安装的操作系统 ! image \" 比如我们要安装这个版本的 win10 image \" 通过百度网盘下载 ed2k 文件资源,然后再通过百度网盘下载到电脑本地,可以放在除 C 盘外的其他盘中 image \" 下载到本地的文件是这样的光盘映像文件 ! image \" 将我们下载到电脑本地的光盘映像文件复制到刚制作完成的 U 盘启动盘中,复制完成后我们 U 盘启动盘就全部制作完成了 image \" ","date":"2018-10-07","objectID":"/redo-system/:1:0","tags":["windows"],"title":"windows 重做系统","uri":"/redo-system/"},{"categories":["开发者手册"],"content":"重做系统 将我们制作好的 U 盘启动盘插到需要重做系统的电脑上,重启电脑一直按 F12 进入 bios 界面(不同的电脑可能按键不同,可以百度) image \" 选择 usb 方式,回车进入大白菜启动方式,选择 02 方式进入 image \" 进入 u 盘驱动界面,系统默认会选择 C 盘为系统盘,直接点击确定 image \" 确定后进入格式化 C 盘阶段,所有配置为默认配置,直接确定,等待格式化完成! image \" 格式化完成后选择 是 重启电脑,拔出 u 盘 等待电脑重启 ","date":"2018-10-07","objectID":"/redo-system/:2:0","tags":["windows"],"title":"windows 重做系统","uri":"/redo-system/"},{"categories":["开发者手册"],"content":"安装系统完成 image \" image \" ","date":"2018-10-07","objectID":"/redo-system/:3:0","tags":["windows"],"title":"windows 重做系统","uri":"/redo-system/"},{"categories":["mysql"],"content":"xiaobinqt,mysql where 和 having 的区别,what difference between where and having","date":"2018-07-05","objectID":"/what-difference-between-where-and-having/","tags":["mysql"],"title":"mysql where 和 having 的区别","uri":"/what-difference-between-where-and-having/"},{"categories":["mysql"],"content":"总览 where 是一个约束声明，在查询数据库的结果返回之前对数据库中的条件进行约束，where 后面要跟的必须是数据表里真实存在的字段，即在结果返回之前起作用。 那么为什么 where 后面不能写聚合函数这个问题也好理解了，因为聚合函数的列不是数据库表里的字段。 having 是一个过滤声明，是在查询数据库结果返回之后进行过滤，即在结果返回值后起作用。 那么为什么 having 后面可以写聚合函数也好理解了，因为 having 只是根据前面查询出来的是什么就可以后面接什么。 ","date":"2018-07-05","objectID":"/what-difference-between-where-and-having/:1:0","tags":["mysql"],"title":"mysql where 和 having 的区别","uri":"/what-difference-between-where-and-having/"},{"categories":["mysql"],"content":"示例 ","date":"2018-07-05","objectID":"/what-difference-between-where-and-having/:2:0","tags":["mysql"],"title":"mysql where 和 having 的区别","uri":"/what-difference-between-where-and-having/"},{"categories":["mysql"],"content":"示例 1 select*fromedgewhereedge_id='0104932b-8edc-4f2d-9d51-c0450867e373';-- sql1 正确 select*fromedgehavingedge_id='0104932b-8edc-4f2d-9d51-c0450867e373';-- sql2 正确 这 2 句 sql 的效果是一样的，sql1 用 where 过滤相当于在返回结果前过滤，sql2 用 having 过滤相当于在返回结果后过滤。 ","date":"2018-07-05","objectID":"/what-difference-between-where-and-having/:2:1","tags":["mysql"],"title":"mysql where 和 having 的区别","uri":"/what-difference-between-where-and-having/"},{"categories":["mysql"],"content":"示例 2 selecttokennamefromedgewhereedge_id='0104932b-8edc-4f2d-9d51-c0450867e373';-- sql1 正确 selecttokennamefromstudenthavingedge_id='0104932b-8edc-4f2d-9d51-c0450867e373';-- sql2 错误 sql1 是正确的，表中存在 edge_id 这个字段可以过滤。 sql2 是错误的，因为 select 后没有 edge_id 这个字段，所以不能过滤。 ","date":"2018-07-05","objectID":"/what-difference-between-where-and-having/:2:2","tags":["mysql"],"title":"mysql where 和 having 的区别","uri":"/what-difference-between-where-and-having/"},{"categories":["mysql"],"content":"示例 3 selectcount(instance_id)asc,instance_idfromedgegroupbyinstance_idhavingc\u003e10;-- sql1 正确 selectcount(instance_id)asc,instance_idfromedgewherec\u003e10groupbyinstance_id;-- sql2 错误 sql1 是正确的，因为 c 在 select 的字段中是存在，是 count(instance_id) 的别名，所以这里写成 having count(instance_id) \u003e 10 也是正确的。 sql2 是错误的，因为 c 这个字段在表中是不存在的，所以不能过滤，即使写成 count(instance_id) \u003e 10 也是错误的。 ","date":"2018-07-05","objectID":"/what-difference-between-where-and-having/:2:3","tags":["mysql"],"title":"mysql where 和 having 的区别","uri":"/what-difference-between-where-and-having/"},{"categories":["mysql"],"content":"总结 where 只能过滤数据库真实存在的字段，having 可以过滤 select 后的查询字段。 where 子句在分组前进行过滤，作用于每一条记录，where 子句过滤掉的记录将不包括在分组中。而 having 子句在数据分组后进行过滤，作用于整个分组。 ","date":"2018-07-05","objectID":"/what-difference-between-where-and-having/:3:0","tags":["mysql"],"title":"mysql where 和 having 的区别","uri":"/what-difference-between-where-and-having/"},{"categories":["开发者手册"],"content":"xiaobinqt,Docker slim、stretch、buster、jessie、alpine、busyBox、debian、Ubuntu、CentOS、Fedora","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/","tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora","uri":"/docker-slim-stretch-buster-jessie/"},{"categories":["开发者手册"],"content":"版本代号 我太难了，搞这么多代号干啥😢 在写 Dockerfile 引用基础镜像时经常会看到这样的写法： FROMdebian:buster 或是 FROMnode:14.16.1-stretch-slim 那这里的 buster 和 stretch 具体是什么呢？其实 buster、stretch还有jessie针对的是不同 Debian 代号codename，除了 Debian 之外，还有 Ubuntu、CentOS、Fedora，他们每次在更新版本时都会更新代号。 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:1:0","tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora","uri":"/docker-slim-stretch-buster-jessie/"},{"categories":["开发者手册"],"content":"ubuntu 版本代号 ubuntu版本代号ubuntu版本代号 \" ubuntu版本代号 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:1:1","tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora","uri":"/docker-slim-stretch-buster-jessie/"},{"categories":["开发者手册"],"content":"debian 版本代号 debian版本代号debian版本代号 \" debian版本代号 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:1:2","tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora","uri":"/docker-slim-stretch-buster-jessie/"},{"categories":["开发者手册"],"content":"slim slim 可以理解为精简版，跟 Minimal是一样的，仅安装运行特定工具所需的最少软件包。 所以FROM debian:buster 就是把 debian 10 作为基础镜像，FROM node:14.16.1-stretch-slim 就是把 debian 9 的精简版作为基础镜像。 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:2:0","tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora","uri":"/docker-slim-stretch-buster-jessie/"},{"categories":["开发者手册"],"content":"操作系统 alpine、debian、ubuntu、centOS、fedora 这些都是操作系统，是 Linux 的发行版。 Linux发行版Linux发行版 \" Linux发行版 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:3:0","tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora","uri":"/docker-slim-stretch-buster-jessie/"},{"categories":["开发者手册"],"content":"Alpine BusyBox 是一个集成了一百多个最常用 Linux 命令和工具（如cat、echo、grep、mount、telnet等）的精简工具箱，它只有几MB的大小，被誉为“Linux系统的瑞士军刀”。 Alpine 操作系统是一个面向安全的轻型 Linux 发行版。它不同于通常的 Linux 发行版，Alpine 采用了 musl libc 和 BusyBox 以减小系统的体积和运行时资源消耗，但功能上比 BusyBox 又完善得多。在保持瘦身的同时，Alpine 还提供了自己的包管理工具apk ，可以通过 https://pkgs.alpinelinux.org/packages 查询包信息，也可以通过apk命令直接查询和安装各种软件。 相比于其他 Docker 镜像，Alpine Docker 的容量非常小，仅仅只有 5MB 左右，而 Ubuntu 系列镜像接近 200MB，且拥有非常友好的包管理机制。 Alpine 镜像的缺点就在于它实在过于精简，以至于麻雀虽小，也无法做到五脏俱全了。在 Alpine 中缺少很多常见的工具和类库，以至于如果想基于 Alpine 标签的镜像进行二次构建，那搭建的过程会相当烦琐。所以如果想要对软件镜像进行改造，并基于其构建新的镜像，那么 Alpine 镜像不是一个很好的选择。 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:3:1","tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora","uri":"/docker-slim-stretch-buster-jessie/"},{"categories":["开发者手册"],"content":"Debian Debian 是由 GPL 和其他自由软件许可协议授权的自由软件组成的操作系统，由 Debian Project 组织维护。Debian 以其坚守 Unix 和自由软件的精神，以及给予用户的众多选择而闻名。 作为一个大的系统组织框架，Debian下面有多种不同操作系统核心的分支计划，主要为采用 Linux 核心的 Debian GNU/Linux 系统，其他还有采用 GNU Hurd 核心的 Debian GNU/Hurd 系统、采用 FreeBSD 核心的 Debian GNU/kFreeBSD 系统，以及采用 NetBSD 核心的 Debian GNU/NetBSD 系统，甚至还有利用 Debian 的系统架构和工具，采用 OpenSolaris 核心构建而成的 Nexenta OS 系统。在这些 Debian 系统中，以采用 Linux 核心的 Debian GNU/Linux 最为著名。 众多的 Linux 发行版，例如 Ubuntu、Knoppix 和 Linspire 及 Xandros 等，都基于 Debian GNU/Linux。 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:3:2","tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora","uri":"/docker-slim-stretch-buster-jessie/"},{"categories":["开发者手册"],"content":"Ubuntu Ubuntu 是一个以桌面应用为主的 GNU/Linux 操作系统。Ubuntu 基于 Debian 发行版和 GNOME/Unity 桌面环境，与 Debian 的不同在于它每6个月会发布一个新版本，每2年会推出一个长期支持（Long Term Support，LTS）版本，一般支持3年。 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:3:3","tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora","uri":"/docker-slim-stretch-buster-jessie/"},{"categories":["开发者手册"],"content":"CentOS CentOS 是基于 Redhat 的常见 Linux 分支。CentOS 是目前企业级服务器的常用操作系统。 CentOS（Community Enterprise Operating System，社区企业操作系统）是基于 Red Hat Enterprise Linux 源代码编译而成的。由于 CentOS 与 Redhat Linux 源于相同的代码基础，所以很多成本敏感且需要高稳定性的公司就使用 CentOS 来替代商业版 Red Hat Enterprise Linux。CentOS 自身不包含闭源软件。 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:3:4","tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora","uri":"/docker-slim-stretch-buster-jessie/"},{"categories":["开发者手册"],"content":"Fedora Fedora 也是基于 Redhat 的常见 Linux分支。Fedora 则主要面向个人桌面用户。 Fedora 是由 Fedora Project 社区开发，红帽公司赞助的 Linux 发行版。它的目标是创建一套新颖、多功能并且自由和开源的操作系统。对用户而言，Fedora是一套功能完备的、可以更新的免费操作系统，而对赞助商 RedHat 而言，它是许多新技术的测试平台，被认为可用的技术最终会加入到 RedHat Enterprise Linux 中。 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:3:5","tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora","uri":"/docker-slim-stretch-buster-jessie/"},{"categories":["开发者手册"],"content":"参考 趣谈形形色色的 Linux 发行版的代号 Docker运行操作系统环境(BusyBox\u0026Alpine\u0026Debian/Ubuntu\u0026CentOS/Fedora) ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:4:0","tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora","uri":"/docker-slim-stretch-buster-jessie/"},{"categories":["开发者手册"],"content":"xiaobinqt,endpoint 和 cmd 的区别,docker 启动，docker 写时复制,docker 常见命令,虚拟化,docker 文件挂载,数据卷是什么,容器网络,如何写 dockerfile","date":"2018-03-18","objectID":"/docker-glance/","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"虚拟化 ","date":"2018-03-18","objectID":"/docker-glance/:1:0","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"硬件虚拟化 硬件虚拟化，指物理硬件本身就提供虚拟化的支持。 比如，A 平台的 CPU，能够将 B 平台的指令集转换为自身的指令集执行，并给程序完全运行在 B 平台上的感觉。又或者，CPU 能够自身模拟裂变，让程序或者操作系统认为存在多个 CPU，进而能够同时运行多个程序或者操作系统。这些都是硬件虚拟化的体现。 ","date":"2018-03-18","objectID":"/docker-glance/:1:1","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"软件虚拟化 软件虚拟化指的是通过软件的方式来实现虚拟化中关键的指令转换部分。 比如，在软件虚拟化实现中，通过一层夹杂在应用程序和硬件平台上的虚拟化实现软件来进行指令的转换。也就是说，虽然应用程序向操作系统或者物理硬件发出的指令不是当前硬件平台所支持的指令，这个实现虚拟化的软件也会将之转换为当前硬件平台所能识别的。 ","date":"2018-03-18","objectID":"/docker-glance/:1:2","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"浅析 Docker 可以把容器看作一个简易版的Linux系统环境（包括root用户权限、进程空间、用户空间和网络空间等）以及运行在其中的应用程序打包而成的沙盒sandbox。 每个容器内运行着一个应用，不同的容器相互隔离，容器之间也可以通过网络互相通信。容器的创建和停止十分快速，几乎跟创建和终止原生应用一致；另外，容器自身对系统资源的额外需求也十分有限，远远低于传统虚拟机。很多时候，甚至直接把容器当作应用本身也没有任何问题。 Docker 并没有和虚拟机一样利用一个独立的 OS 执行环境的隔离，它利用的是目前当前 Linux 内核本身支持的容器方式，实现了资源和环境的隔离。 支撑 docker 的核心技术有三个：Namespace，Cgroup，UnionFS。 Namespace 提供了虚拟层面的隔离，比如文件隔离，网络隔离等等。每个命名空间中的应用看到的，都是不同的IP地址，用户空间，进程 ID 等。 Cgroup提供了物理资源的隔离，比如 CPU，内存，磁盘等等。 UnionFS 给 docker 镜像提供了技术支撑。在 Docker 中，提供了一种对 UnionFS 的改进实现，也就是 AUFSAdvanced Union File System。 AUFS 将文件的更新挂载到老的文件之上，而不去修改那些不更新的内容，这意味着即使虚拟的文件系统被反复修改，也能保证对真实文件系统的空间占用保持一个较低水平。就像在 Git 中每进行一次提交，Git 并不是将我们所有的内容打包成一个版本，而只是将修改的部分进行记录，这样即使我们提交很多次后，代码库的空间占用也不会倍数增加。 通过 AUFS，Docker 大幅减少了虚拟文件系统对物理存储空间的占用。 ","date":"2018-03-18","objectID":"/docker-glance/:1:3","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"虚拟机和 Docker 虚拟机Virtual Machine，通常来说就是通过一个虚拟机监视器Virtual Machine Monitor 的设施来隔离操作系统与硬件或者应用程序和操作系统，以此达到虚拟化的目的。这个夹在其中的虚拟机监视器，常常被称为 Hypervisor。👇是虚拟机和 Docker 的对比： 虚拟机和容器虚拟机和容器 \" 虚拟机和容器 传统方式是在硬件层面实现虚拟化，需要有额外的虚拟机管理应用和虚拟机操作系统层。Docker容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，因此更加轻量级。 虚拟机更擅长彻底隔离整个运行环境。例如，云服务提供商通常采用虚拟机技术隔离不同的用户。而 Docker 通常用于隔离不同的应用，例如前端，后端以及数据库。 ","date":"2018-03-18","objectID":"/docker-glance/:1:4","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"常用命令 CMD 说明 sudo docker create --name 容器名称 镜像名称 创建容器 docker start 名称 启动容器 sudo docker run --name 容器名称 -d 镜像名称 创建并启动容器且在后台运行,-d=--detach docker ps 列出运行中容器 docker ps -a 列出所有容器, -a=--all docker stop 容器名称/ID 停止容器 docker rm 容器名称/ID 删除容器 docker exec -it 容器名称 bash/sh 进入容器 docker attach --sig-proxy=false 容器名称 将容器转为了前台运行，如果不加--sig-proxy=false Ctrl + C 后会停止容器 docker logs 容器名称 查看容器日志 docker network ls/list 查看已经存在的网络 docker network create -d 网络驱动 网络名 创建新网络 docker volume ls 列出当前已创建的数据卷 docker volume create 名称 创建数据卷 docker volume rm 名称 删除数据卷 docker volume prune 删除没有被容器引用的数据卷 docker build 构建镜像 docker inspect 容器名/ID 查看容器详情 docker exec 命令能帮助我们在正在运行的容器中运行指定的命令。 docker exec [-i] 容器名 命令 docker execdocker exec \" docker exec --rm 选项参数，可以让容器在停止后自动删除，不需要再使用容器删除命令来删除，对临时容器友好👇。 docker run --rm --name mysql2 -e MYSQL_RANDOM_ROOT_PASSWORD=yes mysql:5.7 docker build -t webapp:latest -f ./webapp/a.Dockerfile ./webapp ☝️-t 选项，指定新生成镜像的名称。-f 指定 Dockerfile 文件所在目录，如果不写的话会从 ./webapp 目录中去找，./webapp 可以直接写成. 理解成当前目录，也是镜像构建的上下文，比如 COPY指令执行时就是从这个上下文中去找的。 如果需要禁止缓存可以加上--no-cache参数。 docker build --no-cache ..... ","date":"2018-03-18","objectID":"/docker-glance/:2:0","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"容器网络 在 Docker 网络中，有三个比较核心的概念，就是：沙盒Sandbox 、网络Network、端点Endpoint。 沙盒提供了容器的虚拟网络栈，也就是端口套接字、IP 路由表、防火墙等内容。实现隔离容器网络与宿主机网络，形成了完全独立的容器网络环境。 网络可以理解为 Docker 内部的虚拟子网，网络内的参与者相互可见并能够进行通讯。Docker 的这种虚拟网络也是于宿主机网络存在隔离关系的，其目的主要是形成容器间的安全通讯环境。 端点是位于容器或网络隔离墙之上的洞，其主要目的是形成一个可以控制的突破封闭的网络环境的出入口。当容器的端点与网络的端点形成配对后，就如同在这两者之间搭建了桥梁，便能够进行数据传输了。 这三者形成了 Docker 网络的核心模型，也就是容器网络模型Container Network Model。 容器网络容器网络 \" 容器网络 Docker 官方提供了五种 Docker 网络驱动：Bridge Driver、Host Driver、Overlay Driver、MacLan Driver、None Driver。 网络驱动网络驱动 \" 网络驱动 Bridge 网络是 Docker 容器的默认网络驱动，通过网桥来实现网络通讯。Overlay 网络是借助 Docker 集群模块 Docker Swarm 来搭建的跨 Docker Daemon 网络，可以通过它跨越物理主机的限制，让多个处于不同 Docker daemon 实例中的容器连接到同一个网络，并且让这些容器感觉这个网络与其他类型的网络没有区别。 Overlay networkOverlay network \" Overlay network ","date":"2018-03-18","objectID":"/docker-glance/:3:0","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"创建网络 在 Docker 中，能够创建自定义网络，形成自己定义虚拟子网的目的。创建网络的命令是 docker network create。 docker network create -d bridge individual 通过 -d 选项我们可以为新的网络指定驱动的类型，其值可以默认的 bridge、host、overlay、maclan、none，也可以是其他网络驱动插件所定义的类型。 当不指定网络驱动时，Docker 也会默认采用 Bridge Driver 作为网络驱动。 通过 docker network ls 或是 docker network list 可以查看 Docker 中已经存在的网络。 在创建容器时，可以通过 --network 来指定容器所加入的网络，一旦这个参数被指定，容器便不会默认加入到 bridge 这个网络中，但是仍然可以通过 --network bridge 让其加入。 docker run -d --name mysql -e MYSQL_RANDOM_ROOT_PASSWORD=yes --network individual mysql:5.7 可以通过 docker inspect 容器名/ID 观察此时的容器网络。 Docker 中如果两个容器处于不同的网络，之间是不能相互连接引用的。 ","date":"2018-03-18","objectID":"/docker-glance/:3:1","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"容器互联 要让一个容器连接到另外一个容器，可以在容器通过 docker create 或 docker run 创建时通过 --link 选项进行配置。 例如，创建一个 MySQL 容器，将运行 Web 应用的容器连接到这个 MySQL 容器上，打通两个容器间的网络，实现它们之间的网络互通。 docker run -d --name mysql -e MYSQL_RANDOM_ROOT_PASSWORD=yes mysql docker run -d --name webapp --link mysql webapp:latest 容器间的网络已经打通，如何在 Web 应用中连接到 MySQL 数据库❓ Docker 为容器间连接提供了一种非常友好的方式，只需要将容器的网络命名（容器名）填入到连接地址中，就可以访问需要连接的容器了。 mysql:3306 在这里，连接地址中的 mysql（容器名） 好比常见的域名解析，Docker 会将其指向 MySQL 容器的 IP 地址。 Docker 在容器互通中带来的一项便利就是，不再需要真实的知道另外一个容器的 IP 地址就能进行连接。在以往的开发中，每切换一个环境（例如将程序从开发环境提交到测试环境），都需要重新配置程序中的各项连接地址等参数，而在 Docker 里，并不需要关心这个，只需要程序中配置被连接容器的名称，映射 IP 的工作就可以交给 Docker。 在 Docker 里还支持连接时使用别名来摆脱容器名的限制。 sudo docker run -d --name webapp --link mysql:database webapp:latest 在这里，使用 --link \u003cname\u003e:\u003calias\u003e 的形式，连接到 MySQL 容器，并设置它的别名为 database。当我们要在 Web 应用中使用 MySQL 连接时，我们就可以使用 database 来代替连接地址了。 database:3306 ","date":"2018-03-18","objectID":"/docker-glance/:3:2","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"端口映射 容器直接通过 Docker 网络进行的互相访问，在实际使用中，我们需要在容器外通过网络访问容器中的应用。最简单的一个例子，我们提供了 Web 服务，那么我们就需要提供一种方式访问运行在容器中的 Web 应用。 端口映射端口映射 \" 端口映射 通过 Docker 端口映射功能，可以把容器的端口映射到宿主操作系统的端口上，当我们从外部访问宿主操作系统的端口时，数据请求就会自动发送给与之关联的容器端口。 要映射端口，可以在创建容器时使用 -p 或者是 –publish 选项。 docker run -d --name nginx -p 80:80 -p 443:443 nginx:1.12 使用端口映射选项的格式是 -p \u003cip\u003e:\u003chost-port\u003e:\u003ccontainer-port\u003e，其中 ip 是宿主操作系统的监听 ip，可以用来控制监听的网卡，默认为 0.0.0.0，即是监听所有网卡。host-port 和 container-port 分别表示映射到宿主操作系统的端口和容器的端口，这两者是可以不一样的，比如，可以将容器的 80 端口映射到宿主操作系统的 8080 端口，传入 -p 8080:80 即可。 ","date":"2018-03-18","objectID":"/docker-glance/:3:3","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"文件挂载 ","date":"2018-03-18","objectID":"/docker-glance/:4:0","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"Bind Mount Bind Mount能够直接将宿主操作系统中的目录和文件挂载到容器内的文件系统中，通过指定容器外的路径和容器内的路径，就可以形成挂载映射关系，在容器内外对文件的读写，都是相互可见的。 docker run -d --name nginx -v /webapp/html:/usr/share/nginx/html nginx:1.12 使用 -v 或 --volume 来挂载宿主操作系统目录的形式是 -v \u003chost-path\u003e:\u003ccontainer-path\u003e 或 --volume \u003chost-path\u003e:\u003ccontainer-path\u003e，其中 host-path 和 container-path 分别代表宿主操作系统中的目录和容器中的目录。这里需要注意的是，Docker 这里强制定义目录时必须使用绝对路径，不能使用相对路径。 Docker 还支持以只读的方式挂载，通过只读方式挂载的目录和文件，只能被容器中的程序读取，但不接受容器中程序修改它们的请求。在挂载选项 -v 后再接上 :ro 就可以只读挂载了。 docker run -d --name nginx -v /webapp/html:/usr/share/nginx/html:ro nginx:1.12 ","date":"2018-03-18","objectID":"/docker-glance/:4:1","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"Volume 数据卷Volume是从宿主操作系统中挂载目录到容器内，只不过这个挂载的目录由 Docker 进行管理，只需要指定容器内的目录，不需要关心具体挂载到了宿主操作系统中的哪里。 可以使用 -v 或 --volume 选项来定义数据卷的挂载。 docker run -d --name webapp -v /webapp/storage webapp:latest 数据卷挂载到容器后，可以通过 docker inspect 容器名/ID 看到容器中数据卷挂载的信息👇。 [ { // ...... \"Mounts\": [ { \"Type\": \"volume\", \"Name\": \"2bbd2719b81fbe030e6f446243386d763ef25879ec82bb60c9be7ef7f3a25336\", \"Source\": \"/var/lib/docker/volumes/2bbd2719b81fbe030e6f446243386d763ef25879ec82bb60c9be7ef7f3a25336/_data\", \"Destination\": \"/webapp/storage\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" } ] // ...... } ] Source 是 Docker 为我们分配用于挂载的宿主机目录，其位于 Docker 的资源区域，一般默认为 /var/lib/docker。一版并不需要关心这个目录，一切对它的管理都已经在 Docker 内实现了。 为了方便识别数据卷，可以像命名容器一样为数据卷命名，这里的 Name 是数据卷的命名，在未给出数据卷命名的时候，Docker 会采用数据卷的 ID 命名数据卷。可以通过 -v \u003cname\u003e:\u003ccontainer-path\u003e 这种形式来命名数据卷。 docker run -d --name webapp -v appdata:/webapp/storage webapp:latest -v 在定义Bind Mount时必须使用绝对路径，当不是绝对路径是就是Volume 的定义。 ","date":"2018-03-18","objectID":"/docker-glance/:4:2","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"Tmpfs Mount Tmpfs Mount支持挂载系统内存中的一部分到容器的文件系统里，不过由于内存和容器的特征，它的存储并不是持久的，其中的内容会随着容器的停止而消失。 挂载临时文件目录要通过 --tmpfs 这个选项来完成。由于内存的具体位置不需要指定，在这个选项里只需要传递挂载到容器内的目录即可。 docker run -d --name webapp --tmpfs /webapp/cache webapp:latest ","date":"2018-03-18","objectID":"/docker-glance/:4:3","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"镜像版本管理 工作中，当某个镜像不能满足我们的需求时，我们能够将容器内的修改记录下来，保存为一个新的镜像。 ","date":"2018-03-18","objectID":"/docker-glance/:5:0","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"提交修改生成新镜像 以下以官方的 nginx:1.12 镜像示例，修改后生成一个 nginx-v2 镜像。 先下载镜像👇 # 下载镜像docker pull nginx:1.12# 运行容器docker run --name mynginx -d nginx:1.12 下载镜像并启动容器下载镜像并启动容器 \" 下载镜像并启动容器 通过 docker exec 进入容器，并在 /root 目录下新增 hw.txt 文件，文件内容为 hello world： 添加新文件添加新文件 \" 添加新文件 将这个改动后的容器保存为新的镜像，docker commit提交这次修改，提交容器更新后产生的镜像并没 REPOSITORY 和 TAG 的内容，也就是说，这个新的镜像还没有名字。可以使用 docker tag 给新镜像命名。 提交修改生成新镜像提交修改生成新镜像 \" 提交修改生成新镜像 ","date":"2018-03-18","objectID":"/docker-glance/:5:1","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"导出导入镜像 对于某个镜像我们可以导出成一个 tar 包，也可以将一个 tar 镜像导入到系统中。 docker save -o 命令可以将一个镜像导出为 tar 包👇 导出镜像导出镜像 \" 导出镜像 可以通过docker load 导入一个 tar 包为镜像，以下删除了原有的 nginx-v2 镜像，通过 nginx-v2.tar 成功导入了 nginx-v2 镜像。 导入镜像导入镜像 \" 导入镜像 利用导入的 nginx-v2 镜像启动一个新容器 mynginxv2，在新容器中的 /root 就会有一个 hw.txt 新文件，内容为 hello world： 新镜像新镜像 \" 新镜像 ","date":"2018-03-18","objectID":"/docker-glance/:5:2","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"Dockerfile 利用 Dockerfile 文件可以生成镜像，这对于自定义镜像非常优雅，也利于镜像分享，直接分享 Dockerfile 文件就可以了。 ","date":"2018-03-18","objectID":"/docker-glance/:6:0","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"常用指令 🏆FROM 通过 FROM 指令指定一个基础镜像，接下来所有的指令都是基于这个镜像所展开的。 FROM 指令支持三种形式： FROM \u003cimage\u003e [AS \u003cname\u003e] FROM \u003cimage\u003e[:\u003ctag\u003e] [AS \u003cname\u003e] FROM \u003cimage\u003e[@\u003cdigest\u003e] [AS \u003cname\u003e] Dockerfile 中的第一条指令必须是 FROM 指令，因为没有了基础镜像，一切构建过程都无法开展。 🏆RUN RUN 指令用于向控制台发送命令的指令，在 RUN 指令之后，我们直接拼接上需要执行的命令，在构建时，Docker 就会执行这些命令，并将它们对文件系统的修改记录下来，形成镜像的变化。 RUN \u003ccommand\u003e RUN [\"executable\", \"param1\", \"param2\"] RUN 指令是支持 \\换行的，如果单行的长度过长，建议对内容进行切割，方便阅读。 🏆ENTRYPOINT 和 CMD 在容器启动时会根据镜像所定义的一条命令来启动容器中进程号为 1 的进程。而这个命令的定义，就是通过 Dockerfile 中的 ENTRYPOINT 和 CMD 实现的。 ENTRYPOINT [\"executable\", \"param1\", \"param2\"] ENTRYPOINT command param1 param2 CMD [\"executable\",\"param1\",\"param2\"] CMD [\"param1\",\"param2\"] CMD command param1 param2 当 ENTRYPOINT 与 CMD 同时给出时，CMD 中的内容会作为 ENTRYPOINT 定义命令的参数，最终执行容器启动的还是 ENTRYPOINT 中给出的命令。 🏆EXPOSE 通过 EXPOSE 指令就可以为镜像指定要暴露的端口。 EXPOSE \u003cport\u003e 🏆VOLUME VOLUME [\"/data\"] 在 VOLUME 指令中定义的目录，在基于新镜像创建容器时，会自动建立为数据卷，不需要再单独使用 -v 选项来配置。 🏆 COPY 和 ADD 在制作新的镜像的时候，可能需要将一些软件配置、程序代码、执行脚本等直接导入到镜像内的文件系统里，使用 COPY 或 ADD 指令能够帮助我们直接从宿主机的文件系统里拷贝内容到镜像里的文件系统中。 COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] \u003csrc\u003e... \u003cdest\u003e ADD [--chown=\u003cuser\u003e:\u003cgroup\u003e] \u003csrc\u003e... \u003cdest\u003e COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] [\"\u003csrc\u003e\",... \"\u003cdest\u003e\"] ADD [--chown=\u003cuser\u003e:\u003cgroup\u003e] [\"\u003csrc\u003e\",... \"\u003cdest\u003e\"] COPY 与 ADD 指令的定义方式完全一样，需要注意的仅是当我们的目录中存在空格时，可以使用后两种格式避免空格产生歧义。 ","date":"2018-03-18","objectID":"/docker-glance/:6:1","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"ARG 参数 在 Dockerfile 里，可以用 ARG 指令来建立一个参数变量，可以在构建时通过构建指令传入这个参数变量，并且在 Dockerfile 里使用它。 FROM debian:stretch-slim ## ...... ARG TOMCAT_MAJOR ARG TOMCAT_VERSION ## ...... RUN wget -O tomcat.tar.gz \"https://www.apache.org/dyn/closer.cgi?action=download\u0026filename=tomcat/tomcat-$TOMCAT_MAJOR/v$TOMCAT_VERSION/bin/apache-tomcat-$TOMCAT_VERSION.tar.gz\" ## ...... 在☝️这个例子里，我们将 Tomcat 的版本号通过 ARG 指令定义为参数变量，在调用下载 Tomcat 包时，使用变量替换掉下载地址中的版本号。通过这样的定义，就可以让我们在不对 Dockerfile 进行大幅修改的前提下，轻松实现对 Tomcat 版本的切换并重新构建镜像了。 如果我们需要通过这个 Dockerfile 文件构建 Tomcat 镜像，我们可以在构建时通过 docker build 的 --build-arg 选项来设置参数变量。 docker build --build-arg TOMCAT_MAJOR=8 --build-arg TOMCAT_VERSION=8.0.53 -t tomcat:8.0 ./tomcat ","date":"2018-03-18","objectID":"/docker-glance/:6:2","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"ENV 参数 ENV 环境变量设置的实质，其实就是定义操作系统环境变量，所以在运行的容器里，一样拥有这些变量，而容器中运行的程序也能够得到这些变量的值。 FROM debian:stretch-slim ## ...... ENV TOMCAT_MAJOR 8 ENV TOMCAT_VERSION 8.0.53 ## ...... 环境变量的值不是在构建指令中传入的，而是在 Dockerfile 中编写。由于环境变量在容器运行时依然有效，所以运行容器时我们也可以对其进行覆盖，在创建容器时使用 -e 或是 --env 选项，可以对环境变量的值进行修改或定义新的环境变量。 docker run -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:5.7 ENV 指令所定义的变量，永远会覆盖 ARG 所定义的变量。 ","date":"2018-03-18","objectID":"/docker-glance/:6:3","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"FAQ ","date":"2018-03-18","objectID":"/docker-glance/:7:0","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"ENTRYPOINT 和 CMD 的区别 这 2 个命令都是用来指定基于此镜像所创建容器里主进程的启动命令。 ENTRYPOINT 指令的优先级高于 CMD 指令。当 ENTRYPOINT 和 CMD 同时在镜像中被指定时，CMD 里的内容会作为 ENTRYPOINT 的参数，两者拼接之后，才是最终执行的命令。 NTRYPOINT CMD 实际执行 ENTRYPOINT [\"/bin/ep\", \"arge\"] /bin/ep arge ENTRYPOINT /bin/ep arge /bin/sh -c /bin/ep arge CMD [\"/bin/exec\", \"args\"] /bin/exec args CMD /bin/exec args /bin/sh -c /bin/exec args ENTRYPOINT [\"/bin/ep\", \"arge\"] CMD [\"/bin/exec\", \"argc\"] /bin/ep arge /bin/exec argc ENTRYPOINT [\"/bin/ep\", \"arge\"] CMD /bin/exec args /bin/ep arge /bin/sh -c /bin/exec args ENTRYPOINT /bin/ep arge CMD [\"/bin/exec\", \"argc\"] /bin/sh -c /bin/ep arge /bin/exec argc ENTRYPOINT /bin/ep arge CMD /bin/exec args /bin/sh -c /bin/ep arge /bin/sh -c /bin/exec args ENTRYPOINT 指令主要用于对容器进行一些初始化，而 CMD 指令则用于真正定义容器中主程序的启动命令。 创建容器时可以改写容器主程序的启动命令，而这个覆盖只会覆盖 CMD 中定义的内容，不会影响 ENTRYPOINT 中的内容。 使用脚本文件来作为 ENTRYPOINT 的内容是常见的做法，因为对容器运行初始化的命令相对较多，全部直接放置在 ENTRYPOINT 后会特别复杂。 ","date":"2018-03-18","objectID":"/docker-glance/:7:1","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"COPY 和 ADD 的区别 COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] \u003csrc\u003e... \u003cdest\u003e ADD [--chown=\u003cuser\u003e:\u003cgroup\u003e] \u003csrc\u003e... \u003cdest\u003e COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] [\"\u003csrc\u003e\",... \"\u003cdest\u003e\"] ADD [--chown=\u003cuser\u003e:\u003cgroup\u003e] [\"\u003csrc\u003e\",... \"\u003cdest\u003e\"] 两者的区别主要在于 ADD 能够支持使用网络端的 URL 地址作为 src 源，并且在源文件被识别为压缩包时，自动进行解压，而 COPY 没有这两个能力。 ","date":"2018-03-18","objectID":"/docker-glance/:7:2","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"写时复制 在编程里，写时复制Copy on Write 常常用于对象或数组的拷贝中，当拷贝对象或数组时，复制的过程并不是马上发生在内存中，而只是先让两个变量同时指向同一个内存空间，并进行一些标记，当要对对象或数组进行修改时，才真正进行内存的拷贝。 Docker 的写时复制与编程中的相类似，在通过镜像运行容器时，并不是马上就把镜像里的所有内容拷贝到容器所运行的沙盒文件系统中，而是利用 UnionFS 将镜像以只读的方式挂载到沙盒文件系统中。只有在容器中发生对文件的修改时，修改才会体现到沙盒环境上。 也就是说，容器在创建和启动的过程中，不需要进行任何的文件系统复制操作，也不需要为容器单独开辟大量的硬盘空间，与其他虚拟化方式对这个过程的操作进行对比，Docker 启动的速度可见一斑。 采用写时复制机制来设计的 Docker，既保证了镜像在生成为容器时，以及容器在运行过程中，不会对自身造成修改。又借助剔除常见虚拟化在初始化时需要从镜像中拷贝整个文件系统的过程，大幅提高了容器的创建和启动速度。可以说，Docker 容器能够实现秒级启动速度，写时复制机制在其中发挥了举足轻重的作用。 ","date":"2018-03-18","objectID":"/docker-glance/:7:3","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":"参考 对比Docker和虚拟机 开发者必备的 Docker 实践指南 ","date":"2018-03-18","objectID":"/docker-glance/:8:0","tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"}]