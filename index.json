[{"categories":["生活"],"content":"几年前，一位好朋友去世了，九零后，跟我年纪一样。我跟他从小就认识，我们一起上的小学，一起上的初中，高中之后便联系的少了，后来我去外地读书，联系的就更少了。 那还是二零一九，那时我刚从西安来北京。一天夜里，都很晚了，我妈打电话跟我说，他去世了，好像是心梗，让我在外面多注意身体。天啊，当我听到这个消息的时候，我简直不敢相信，我反复确认了几次，无疑的确是他。 那晚我很难过，因为我不久之前还见过他。二零一九的春节，那天应该是初二的早晨，我骑着电瓶车去外公家拜年，外公家跟他老家离的不远，就几步路，那天早晨我在路边看到了他，我没有停下来，心想就几步路，我回来的时候再去找他，但是等我再往回走的时候他就不在家了。如今听到噩耗，再想起这件事，我真的特别后悔当时应该停下来见他一面。后来，我把这件事说给我女朋友听，她也特别感慨的说，想做什么事一定要赶紧去做。是啊，一定要赶紧去做，毕竟世事无常。 我跟他太久没有联系了，没有他的电话，也没有他的微信，后来在QQ 里找到他的联系方式。我尝试着发了一条消息过去，QQ 的那边，他媳妇回了一条消息，说他人已经不在了。后来有个初中同学联系到了我，是他班上的，建了一个微信群，想尽点绵薄之力，我们一人凑了点钱，由一个在老家的同学给他家里送了去，但他妈妈只是领了我们的心意。他实在是太年轻了，而且新婚不久，孩子才一岁。 这张照片是上初中时我们一起去皖南事变烈士陵园拍的，也是我跟他唯一的一张合影。 2020 年时家里发大水，家里的东西都泡水了，这张照片后来也不知所踪。 左三是他 有一次我回家，我爸还跟我说在一次婚宴上见过他。他过世后，一次在我大舅家吃饭的时候，他家的一个亲戚也在，在聊起他的时候，直夸他在外面干活能吃苦，人不错。 我跟他从小相识，一起在村小学读书，一起在田埂上疯跑，他教我掏鸟窝，网知了，在我眼里，他好像什么都会，他教了我很多技能，带给我很多快乐。上初中的时候，我跟他一起骑车上学，放学也一起回家。他每天早上都是骑着车来我外婆家等我，等我吃完早饭一起走，一路上我们有好几个同学都一起。下午放学他有时也在我外婆家跟我一起做完作业才回家，这些事如今历历在目，但是他却永远不在了。 也许是年纪大了，不知不觉对有些事越来越伤感。几次提笔想写点什么，但是每次都写不出来，心里总记挂这件事，可能是那次我没有停下来见他吧。 2021年10月15日完 ","date":"2022-03-16","objectID":"/old-pal/:0:0","series":null,"tags":["随笔"],"title":"纪念一位老友","uri":"/old-pal/#"},{"categories":["生活"],"content":"xiaobinqt,近视眼手术，近视激光手术，近视，飞秒，半飞秒，全飞秒，晶体植入","date":"2022-11-03","objectID":"/surgery-for-myopia/","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/"},{"categories":["生活"],"content":" 英文简称 英文简称 中文 SMILE 全飞秒激光 FS-LASIK 半飞秒激光 ICL 晶体植入 ","date":"2022-11-03","objectID":"/surgery-for-myopia/:1:0","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#英文简称"},{"categories":["生活"],"content":" 近视度数划分 300 度以下属于低度近视（25-275度） 300 度以上600度以下属于中度近视（300-575度） 600 度以上900以下属于高度近视（600-875度） 900 度以上属于超高度近视（900-∞度） ","date":"2022-11-03","objectID":"/surgery-for-myopia/:2:0","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#近视度数划分"},{"categories":["生活"],"content":" 近视原理 近视屈光 当长时间近距离用眼，导致睫状肌经常处于收缩的状态，导致调节紧张或调节痉挛。晶状体失去睫状肌的牵引就会增厚变凸。☝️ 如上图，当光线进入到眼睛以后聚焦在 D 点，距离视网膜 C 点出现了一段距离。所以这个时候我们就看不清楚了。最真实的表现是看远处的物体变大模糊。 牵引晶状体的红色部分为睫状肌。正常时候，我们的眼睛看远处的时候，睫状肌放松，晶状体变薄，看近处的时候，睫状肌收缩，晶状体变厚变凸。 近视是因为长时间看较近的物体，导致睫状肌痉挛，不能有效的调节牵引晶状体，所以这个时候就出现了假性近视。假性近视患者的眼轴没有发生根本的改变，适当的休息和锻炼是可以恢复过来。 当出现假性近视的情况没有采取合适的治疗手段就会发生以上病变的过程。晶状体收缩变厚可以对眼睛产生两种力。一种力是轴向上由于晶状体变厚产生的压力，这个压力可以压到玻璃体，进而把眼轴撑长。另一种力是垂直于轴向的拉力，相当于晶状体这块肌肉收缩时的拉力，这个拉力可以把眼球拉扁，从而间接使眼轴变长。这两种力共同作用把眼睛从圆球形变成橄榄形，这就是晶状体的调节力造成的近视眼变形。 所谓真性近视就是长期的作用力导致眼轴增长，眼睛变形到难以恢复到圆球形的地步了。近视度数越高的人，其晶状体收缩的越厉害，力量也越大，足以改变眼球形状并产生破坏性，这个时候就形成了真性近视。 眼轴 ","date":"2022-11-03","objectID":"/surgery-for-myopia/:3:0","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#近视原理"},{"categories":["生活"],"content":" 什么是暗瞳暗瞳就是晚上瞳孔（瞳孔的大小是天生的）放大的大小。因为晚上光线弱，瞳孔会生理性放大。 有些人放的大，比如 7.5mm，有些人放的小，比如 6mm，近视手术的光学治疗区一般在 6.5mm 左右，过大的瞳孔会超过光学治疗区，因而会有眩光现象。这个问题在手术前是需要查的。 暗瞳越大切削角膜厚度也就越多，针对角膜大的人最好选择半飞秒、个性化半飞秒，或者不考虑手术。 比如暗瞳为 7.5mm，光区切削为 7.0mm，其中的 0.5mm 因为还是近视状态，就会出现眩光。 有些人说近视手术后给自己带来了后遗症，出现了以上情况可能跟自身情况有关系或者跟自己选择的手术方式有关，不是近视手术的问题。 如果瞳孔过大，晶体或是激光切削的光学区不能完全覆盖瞳孔，就可能会”漏光“，这部分漏过去的光线，是无法聚焦到视网膜上的，相当于还是近视的朦胧的成像，清晰的和近视的两种影像混合在一起，就形成了独特的散光效果，也就是重影虚影，这在发光物体上尤其明显。 如果暗瞳大，角膜厚度理想，度数不高的条件下可以相对的扩大一点光区。但是如果角膜偏薄，度数又高的人光区不建议设计太大，因为本身度数高，角膜组织就切削的多，光区再大角膜就会丢失更多，角膜的剩余力学安全性就会差。 一般大于 7 毫米，基本上就算大，7.5 以上甚至 8 毫米的就是很大的瞳孔了，7 以上其实就要考虑到底要不要做手术了。 ","date":"2022-11-03","objectID":"/surgery-for-myopia/:4:0","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#什么是暗瞳"},{"categories":["生活"],"content":" 什么是光学区光学区或者叫光学治疗区。 近视手术，无论是激光类手术比如半飞秒，还是 ICL 晶体植入类手术，都有一个作用的光学区，当光学区完全覆盖瞳孔最大面积的情况下视力效果才会好。 ","date":"2022-11-03","objectID":"/surgery-for-myopia/:5:0","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#什么是光学区"},{"categories":["生活"],"content":" 光学区的定义角膜激光手术利用波长为 193nm 的激光对角膜进行切削（消融），以改变角膜的折光能力，使得平行光线进入眼屈光系统后能准确的聚集在视网膜上。手术医生可以看到激光消融的范围，这个消融区就是切削区，其大小用直径表示。 为确保激光切削后的角膜有光滑的表面，切削区域分为光学区和过渡区两部分。其中光学区是用来进行屈光矫正的区域，主要起光学成像作用，过渡区起到光学区与非光学区之间的光滑过度作用，可以降低光学区边缘曲率突然改变的几率，有效降低术后球差（角膜Q值与球差）的增加，减少术后角膜上皮的增生和暗环境下的眩光问题。 光学区在临床上是可以调整的，是控制术后视觉质量的参数。正常成年人的角膜横径为 11.5-12mm，垂直径为 10.5-11mm。在角膜屈光手术的临床设计中，光学区大小一般设定在 6～7mm 左右，这也是为什么当暗瞳大与 7mm 时，要综合考虑手术或是否进行手术的原因。 光学区的大小是综合近视同学的角膜直径、角膜厚度、瞳孔直径、角膜曲率及近视散光度数等因素综合设计。 ","date":"2022-11-03","objectID":"/surgery-for-myopia/:5:1","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#光学区的定义"},{"categories":["生活"],"content":" 主流近视手术的光学区范围 手术类型 最大光区 全飞秒 7.0mm 半飞秒激 6.5mm 晶体植入 7.3mm 其实，手术切削区域也受到手术设备的影响而有所差异。 比如，同样是 450 度近视加 100 度散光，鹰视的切削区为 9mm，阿玛仕的切削区为 7.45mm，蔡司的切削区域为 6.50mm，所以，有时手术设备选择也很重要。 ","date":"2022-11-03","objectID":"/surgery-for-myopia/:5:2","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#主流近视手术的光学区范围"},{"categories":["生活"],"content":" 光学区与暗瞳的关系正常人白天亮环境瞳孔直径约 3mm，暗环境下，瞳孔会不由自主的放大，也就是生理性放大；对于中低度近视的同学，如果光学区域做的较大，很少会出现眩光的情况；对于高度近视的朋友，因为度数较高，需要切削掉的组织较多，又不可能把光学区域做的那么大，所以到了晚上光线变暗，瞳孔放大的时候就可能会出现眩光的问题。 光学区与暗瞳的关系 如上☝️ 暗瞳直径大于近视手术光学区直径，则会发生眩光的症状。 若暗瞳超过手术光学区的值过大，为避免出现严重眩光的问题，建议进行保守的矫正方法，采取佩戴眼镜或是隐形眼镜来满足日常用眼需求。 ","date":"2022-11-03","objectID":"/surgery-for-myopia/:6:0","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#光学区与暗瞳的关系"},{"categories":["生活"],"content":" 什么是角膜制瓣近视眼往往伴随着角膜过凸，近视激光手术通过切削角膜组织，将角膜前表面的弧度削平，降低角膜的折光力，从而矫正近视屈光，提高视力。 眼睛的结构 角膜分为五层，由前向后依次为上皮细胞层、前弹力层、基质层、后弹力层、内皮细胞层。角膜中基质层厚度占到整体厚度的 90%，方便切削，手术主要在基质层完成。 角膜瓣是角膜的一部分，是手术过程切开的一层薄薄的角膜组织。 如果要在基质层完成手术，此时必须要掀开上皮细胞层和前弹力层，就需要制作角膜瓣，所以为了达到基质层而进行手术需要掀开的部分就是角膜瓣。 如果将角膜组织比如成一个西瓜，角膜瓣相当于掀起的一块西瓜皮，里面的西瓜瓤就是角膜基质层。手术过程中，先将角膜瓣掀开翻在一边，然后用准分子激光在基质层上进行切削，切削完毕后，再将角膜瓣翻回来盖在原处，手术就完成了。因为角膜厚度和形状发生改变，成像就会准确落到视网膜上，以此达到矫正示例的目的。 如果只拿半飞秒和全飞秒来举例的话，其实可以简单理解为，只有在半飞秒中才会有角膜瓣，全飞秒是不会有角膜瓣的，因为现在激光可以聚焦在角膜的一定深度去处理的，也就是直接在基质层上进行切削，不需要掀开角膜瓣，激光直接对基质层进行切削完成后，在角膜边缘切割一个 2-4mm 的小口，将切割的基质层取出，手术就完成了。 所以，半飞秒需要掀开角膜瓣，全飞秒不需要掀开角膜瓣，在手术都成功的情况下，全飞秒对于角膜的创伤会更小。当然选择那种手术方式，需要更过医生的综合考量，这里只做理论分析。 ","date":"2022-11-03","objectID":"/surgery-for-myopia/:7:0","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#什么是角膜制瓣"},{"categories":["生活"],"content":" 术后眩光症的可能原因 炫光 暗光下瞳孔直径大 即使采用大光区矫正近视难以完全覆盖暗光下的瞳孔，形成眩光，此类人群不宜进行近视手术。 近视度数过高 近视度数过高，若采用激光矫治近视，切削的角膜就越多，激光在角膜表面形成的“凹陷”也就越深，其斜率也越大。 手术过程中太过于紧张 手术者配合度较差，造成偏中心切削。 过度关注引起的心理“假象” 有一小部分人群可能术前就有一定的“眩光”，只是没有在意，而术后对视觉质量的关注度明显提高，往往认为是手术导致的眩光。 ","date":"2022-11-03","objectID":"/surgery-for-myopia/:8:0","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#术后眩光症的可能原因"},{"categories":["生活"],"content":" 夜视力下降问题大部分人黑乌珠的角膜直径是在 11~12 毫米大小，做激光手术时是在角膜的中心，大约 6~7 个毫米的中间进行，平均是在大约 6.5 毫米的区域当中做。由于白天的瞳孔只有 2~3 个毫米，都在 6.5 毫米的覆盖范围之内，所以术后白天视力都是好的；到了晚上，人的瞳孔会慢慢放大，有的人可能放大到 6.7、6.5 毫米，这种人白天和晚上的视觉质量跟手术之前差别一般不会很大。但是有的人瞳孔会放大到很大，比如到 7点几、8.0 毫米以上，这时候不难想象，当8.0毫米的瞳孔大小范围之内，中间部分 6.5 毫米的范围是做过激光手术的，而边上的这一圈还是没有做过激光手术之前的近视状态的角膜，晚上看东西的时候就会有两个屈光界面的图像投射到他的视网膜上，这时候他的大脑会感受到两套清晰度不一样的图像，从而对视觉产生干扰，会觉得有点光晕、光圈。 炫光问题 所以做了激光手术以后，一部分人一开始可能会有夜间视觉质量点下降的感觉。随着时间慢慢的延长，人晚上的瞳孔大小，也就是暗瞳的直径，会逐渐缩小。因为暗瞳直径是受人体的交感神经和副交感神经控制，当交感神经兴奋的时候，瞳孔就会变大；当副交感神经兴奋的时候，瞳孔就会变小。所以激光手术后，当晚上有眩光时，副交感神经就会慢慢兴奋起来，他的瞳孔就会逐渐缩小，缩小到一定程度以后，夜间眩光的现象就会消失。 ","date":"2022-11-03","objectID":"/surgery-for-myopia/:9:0","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#夜视力下降问题"},{"categories":["生活"],"content":" 为什么会出现干眼我们的眼睛最表面有一层泪膜，来维持眼表面的湿润。泪膜是由眼睛表面的睑板腺、泪腺、副泪腺分泌出来的，这些腺体分泌的泪液由角膜表面的神经反馈所控制。 当做了激光手术以后，会损伤一部分角膜表面的神经，使得反馈的机制减弱，泪腺分泌的泪液量就会减少。 做了近视激光手术之后，都会或多或少的有一点干眼症。但是随着术后的恢复，大部分人在术后 6 个月的时候，角膜表面神经的功能会逐步恢复起来，干眼症也会逐步减轻。 但是，但是，但是，我觉得干眼可能跟医生的水平和设备有关，很多都只是理论上没问题，比如我老表很早之前在我们市里做的手术，到现在了还一直干眼👇 可能干眼会一直存在 因为全飞秒的对角膜的损伤是非常小的，所以对角膜的神经的损伤也是非常小的，相对而言，全飞秒对干眼的症状理论上会小很多。 ","date":"2022-11-03","objectID":"/surgery-for-myopia/:10:0","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#为什么会出现干眼"},{"categories":["生活"],"content":" 飞蚊症在我查近视手术后遗症时，很多人发帖说在术手会出现飞蚊症。 ","date":"2022-11-03","objectID":"/surgery-for-myopia/:11:0","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#飞蚊症"},{"categories":["生活"],"content":" 飞蚊表现 视野中出现一些小图形，可以表现为色暗的斑点或圆球、透明的线状漂浮物 转动眼球时，这些斑点会移动，但是当你想要看清斑点时，它们又迅速从您的视野中消失 在你看着明亮的背景（例如蓝天或白墙）时，这些斑点最明显 小图形或线状物最终会下降并漂出您的视线 ","date":"2022-11-03","objectID":"/surgery-for-myopia/:11:1","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#飞蚊表现"},{"categories":["生活"],"content":" 出现原因 与年龄有关的眼部变化。 随着年龄的增长，充满眼球并帮助其保持圆形的玻璃体或胶状物质会发生变化。随着时间的推移，玻璃体部分液化，这一过程导致其脱离眼球内表面。当玻璃体收缩和下垂时，它会结块并变得黏稠。这些结块碎片阻挡了一些通过眼睛的光线，在您的视网膜上投射出微小的阴影，被视为漂浮物。 眼睛后部发炎。后葡萄膜炎是眼睛后部葡萄膜层的炎症。这种病症会导致炎症碎片释放到玻璃体中而被视为漂浮物。后葡萄膜炎可能时由于感染、炎症性疾病或其他原因所致。 眼睛出血。玻璃体出血有很多原因，包括糖尿病、高血压、血管阻塞和损伤。血细胞被视为漂浮物。 视网膜撕裂。当下垂的玻璃体产生足够的力拉扯视网膜时，就会发生视网膜撕裂。如果不治疗，视网膜撕裂可能导致视网膜脱离，也就是视网膜后面的液体积聚，导致视网膜从您眼睛后方脱离。视网膜脱离不经治疗可导致永久性视力丧失。 眼科手术和眼科药物。某些药物注入玻璃体会导致气泡形成。这些气泡会被视为阴影，直至您的眼睛将其吸收。某些玻璃体视网膜手术会在玻璃体中加入硅油，这也可能被看作是漂浮物。 ","date":"2022-11-03","objectID":"/surgery-for-myopia/:11:2","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#出现原因"},{"categories":["生活"],"content":" 近视手术是否会导致飞蚊 玻璃体图 在知乎很多这样的回答中，出现了 2 个比较高赞的观点，一个是手术过程损伤了玻璃体，导致了飞蚊现象，比如这个👇 有问题的解释 近视手术是在角膜上进行的手术，由【玻璃体图】可知，角膜和玻璃体之间还隔了一个水晶体或是叫晶状体，在手术过程中根本不会接触到玻璃体，所以近视手术破坏晶状体的一些回答应该是不可靠的。这个回答还说了会导致夜盲症，这里的夜盲应该是夜视力下降，而夜视力下降是光学区和暗瞳不一定导致的，具体可以看光学区和暗瞳的关系。当然这只是我的个人分析。 还有一个导致飞蚊的观点是负压吸引导致的，比如这个回答： 负压吸引 负压是手术过程中，负压环对眼球产生的压力。负压高，会导致眼压升高，视神经和视网膜受到压迫，术中出现短暂的黑朦情况。我查到的资料是，蔡司的机器，半飞秒负压极低，术中不会出现眼压升高，全程可视，所以我觉得这个可能跟机器有关系，跟手术本身关系不大。 ","date":"2022-11-03","objectID":"/surgery-for-myopia/:11:3","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#近视手术是否会导致飞蚊"},{"categories":["生活"],"content":" 手术失败眼会瞎吗飞秒和全飞秒近视手术是在眼角膜的基质层上进行的，对眼内组织没有影响，直接上医生的科普： 但是要注意术后感染，比如这个视频： ","date":"2022-11-03","objectID":"/surgery-for-myopia/:12:0","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#手术失败眼会瞎吗"},{"categories":["生活"],"content":" 是否可以通过训练逆转近视近视，一个可以逆转的现代疾病！ ","date":"2022-11-03","objectID":"/surgery-for-myopia/:13:0","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#是否可以通过训练逆转近视"},{"categories":["生活"],"content":" 相关视频 我不做近視雷射手術的原因? 該擔心後遺症嗎? Smile全飛秒是什麼? | 蒼藍鴿聊醫學EP179 眼睛裡面是什麼樣的呢 激光近视术后十年回顾！本人遭遇无法避免的后遗症并发症大盘点！ ","date":"2022-11-03","objectID":"/surgery-for-myopia/:14:0","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#相关视频"},{"categories":["生活"],"content":" 关于我我觉得我可以不用考虑手术了😢，毕竟我暗瞳已经大与 7mm 了。以下是我在同仁医院的挂号医生跟我说的我的暗瞳。 我的暗瞳 而且我眼底也有问题，做过次全视网膜凝光术。 凝光术 其实这几天查了一些资料，包括跟朋友的交流，不知道为啥，我对做手术的欲望已经不是很大了，甚至觉得可以通过一些锻炼来恢复自己的视力，😂 个人觉得理论上，只要反向训练，就可以恢复视力，但是这个过程可能会很漫长🙈。 ","date":"2022-11-03","objectID":"/surgery-for-myopia/:15:0","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#关于我"},{"categories":["生活"],"content":" 参考 人民网-科普中国-近视激光手术安全吗？十个问题了解近视激光手术 想做近视手术，却被各种并发症“劝退”？眼科专家破解五大传言 暗瞳大做近视手术会有什么后遗症？ - 眼科医生陈鼎的回答 - 知乎 详解近视手术中手术光学区问题 近视手术靠谱吗？只是能矫正，但无法根治近视 近视手术中的角膜瓣是什么 飞蚊症 近视真的可以自愈吗？ ","date":"2022-11-03","objectID":"/surgery-for-myopia/:16:0","series":null,"tags":["近视激光手术","半飞秒","全飞秒"],"title":"近视手术调研笔记","uri":"/surgery-for-myopia/#参考"},{"categories":[""],"content":"xiaobinqt,萧十一郎,mysql 面试题,redis,golang,go,BAT 面试题,资料库,工具,www.xiaobinqt.cn,xiaobinqt.github.io","date":"2022-10-01","objectID":"/memo/","series":null,"tags":[""],"title":"收藏夹","uri":"/memo/"},{"categories":[""],"content":" 面试题 Redis面试题（总结最全面的面试题） 很用心的为你写了 9 道 MySQL 面试题 史上最详细的一线大厂Mysql面试题详解 面试BAT前先搞定这18道MySQL经典面试题（含答案解析） MySQL面试题（总结最全面的面试题） mysql索引相关面试题 Golang 常见面试题目解析 化身一个请求感受浏览器输入URL后奇妙的网络之旅 TCP和UDP协议的区别以及原理 ","date":"2022-10-01","objectID":"/memo/:1:0","series":null,"tags":[""],"title":"收藏夹","uri":"/memo/#面试题"},{"categories":[""],"content":" 视频 一条视频讲清楚TCP协议与UDP协议-什么是三次握手与四次挥手 【尚硅谷】Kubernetes（k8s）入门到实战教程丨全新升级完整版 ","date":"2022-10-01","objectID":"/memo/:2:0","series":null,"tags":[""],"title":"收藏夹","uri":"/memo/#视频"},{"categories":[""],"content":" 在线文档 计算机教育中缺失的一课 Docker-从入门到实战 区块链技术指南 build-web-application-with-golang Mastering_Go Go语言101 PHP扩展开发及内核应用 JavaScript 标准参考教程 ES6 入门教程 JavaScript 教程 网道 PHP编程之道 Kubernetes 文档 Uber Go 语言编码规范 地鼠文档 go Standard library alblue 床长人工智能教程 Golang开发手记 Git飞行规则(Flight Rules) Pro Git（中文版） ","date":"2022-10-01","objectID":"/memo/:3:0","series":null,"tags":[""],"title":"收藏夹","uri":"/memo/#在线文档"},{"categories":[""],"content":" 博主 刘丹冰aceld gairuo 面向信仰编程 煎鱼 xargin 竹子爱熊猫 技术印记 abcdocker运维博客 ","date":"2022-10-01","objectID":"/memo/:4:0","series":null,"tags":[""],"title":"收藏夹","uri":"/memo/#博主"},{"categories":[""],"content":" 工具 HTTP Cats loading.io Data Structure Visualizations learn git branching navicat premium15破解教程 Bit Calculator YouTube Video Download 冷熊简历 身份证号码生成器 PlantUML简述 ","date":"2022-10-01","objectID":"/memo/:5:0","series":null,"tags":[""],"title":"收藏夹","uri":"/memo/#工具"},{"categories":[""],"content":" 资料库 国内多家权威出版社：10000多本电子书合集 ahhhhfs｜A姐分享 首发整理！【编程开发全套资料】 百度网盘资源 ","date":"2022-10-01","objectID":"/memo/:6:0","series":null,"tags":[""],"title":"收藏夹","uri":"/memo/#资料库"},{"categories":["golang"],"content":"xiaobinqt,Go sync.Map 解读,golang sync.map 源码解析,golang sync.map 并发 map 的使用","date":"2022-09-14","objectID":"/go-sync-map/","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/"},{"categories":["golang"],"content":" 背景项目中遇到了需要使用高并发的 map 的场景，众所周知 Go 官方的原生 map 是不支持并发读写的，直接并发的读写很容易触发 panic。 解决的办法有两个： 自己配一把锁sync.Mutex，或者更加考究一点配一把读写锁sync.RWMutex。这种方案简约直接，但是缺点也明显，就是性能不会太高。 使用 Go 语言在 2017 年发布的 Go 1.9 中正式加入了并发安全的字典类型sync.Map。 很显然，方案 2 是优雅且实用的。但是，为什么官方的sync.Map能够在 lock free无锁并发 的前提下，保证足够高的性能？本文结合源码进行简单的分析。 ","date":"2022-09-14","objectID":"/go-sync-map/:1:0","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#背景"},{"categories":["golang"],"content":" 核心思想＆架构如果要保证并发的安全，最朴素的想法就是使用锁，但是这意味着要把一些并发的操作强制串行化，性能自然就会下降。 事实上，除了使用锁，还有一个办法，也可以达到类似并发安全的目的，就是原子操作atomic。 sync.Map的设计非常巧妙，充分利用了atmoic和mutex的配合。 ","date":"2022-09-14","objectID":"/go-sync-map/:2:0","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#核心思想架构"},{"categories":["golang"],"content":" 核心思想核心原则就是，尽量使用原子操作，最大程度上减少了锁的使用，从而接近了 lock free 的效果。 核心点： 使用了两个原生的 map 作为存储介质，分别是 read map只读字典 和 dirty map脏字典。 只读字典使用atomic.Value来承载，保证原子性和高性能；脏字典则需要用互斥锁来保护，保证了互斥。 只读字典和脏字典中的键值对集合并不是实时同步的，它们在某些时间段内可能会有不同。 无论是 read 还是 dirty，本质上都是map[interface{}]*entry类型，这里的 entry 其实就是 Map 的 value 的容器。 entry 的本质，是一层封装，可以表示具体值的指针，也可以表示 key 已删除的状态（即逻辑假删除）。 通过这种设计，规避了原生 map 无法并发安全 delete 的问题，同时在变更某个键所对应的值的时候，就也可以使用原子操作了。 这里列一下 Map 的源码定义。篇幅问题，我去除了大量的英文原版注释，换成融合自身理解的直观解释。如果有需要可以结合原版的注释对比着看。 type Map struct { mu sync.Mutex // read contains .... 省略原版的注释 // read map是被atomic包托管的，这意味着它本身Load是并发安全的（但是它的Store操作需要锁mu的保护） // read map中的entries可以安全地并发更新，但是对于expunged entry，在更新前需要经它unexpunge化并存入dirty //（这句话，在Store方法的第一种特殊情况中，使用e.unexpungeLocked处有所体现） read atomic.Value // readOnly // dirty contains .... 省略原版的注释 // 关于dirty map必须要在锁mu的保护下，进行操作。它仅仅存储 non-expunged entries // 如果一个 expunged entries需要存入dirty，需要先进行unexpunged化处理 // 如果dirty map是nil的，则对dirty map的写入之前，需要先根据read map对dirty map进行浅拷贝初始化 dirty map[interface{}]*entry // misses counts .... 省略原版的注释 // 每当读取的是时候，read中不存在，需要去dirty查看，miss自增，到一定程度会触发dirty=\u003eread升级转储 // 升级完毕之后，dirty置空 \u0026miss清零 \u0026read.amended置false misses int } // 这是一个被原子包atomic.Value托管了的结构，内部仍然是一个map[interface{}]*entry // 以及一个amended标记位，如果为真，则说明dirty中存在新增的key，还没升级转储，不存在于read中 type readOnly struct { m map[interface{}]*entry amended bool // true if the dirty map contains some key not in m. } // An entry is a slot in the map corresponding to a particular key. // 这是一个容器，可以存储任意的东西，因为成员p是unsafe.Pointer(*interface{}) // sync.Map中的值都不是直接存入map的，都是在entry的包裹下存入的 type entry struct { // p points .... 省略原版的注释 // entry的p可能的状态： // e.p == nil：entry已经被标记删除，不过此时还未经过read=\u003edirty重塑，此时可能仍然属于dirty（如果dirty非nil） // e.p == expunged：entry已经被标记删除，经过read=\u003edirty重塑，不属于dirty，仅仅属于read，下一次dirty=\u003eread升级，会被彻底清理 // e.p == 普通指针：此时entry是一个不同的存在状态，属于read，如果dirty非nil，也属于dirty p unsafe.Pointer // *interface{} } ","date":"2022-09-14","objectID":"/go-sync-map/:2:1","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#核心思想"},{"categories":["golang"],"content":" 架构设计图初看这个结构的设计，会觉得复杂，不理解为什么要设计成这样，这里画了一个图，力求更加直观的说明 read 和 dirty 之间的配合关系。 设计架构图 架构的进一步解释说明： read map 由于是原子包托管，主要负责高性能，但是无法保证拥有全量的 key（因为对于新增 key，会首先加到 dirty 中），所以 read 某种程度上，类似于一个 key 的快照。 dirty map 拥有全量的 key，当 Store 操作要新增一个之前不存在的 key 的时候，预先是增加自 dirty 中的。 在查找指定的 key 的时候，总会先去只读字典中寻找，并不需要锁定互斥锁。只有当 read 中没有，但 dirty 中可能会有这个 key 的时候，才会在锁的保护下去访问 dirty。 在存储键值对的时候，只要 read 中已存有这个 key，并且该键值对未被标记为expunged，就会把新值存到里面并直接返回，这种情况下也不需要用到锁。 expunged 和 nil，都表示标记删除，但是它们是有区别的，简单说 expunged 是 read 独有的，而 nil 则是 read 和 dirty 共有的，具体这么设计的原因，最后统一总结。 read 和 map 的关系，是一直在动态变化的，可能存在重叠，也可能是某某一方为空；重叠的公共部分，由分为两种情况，nil 和 normal，它们分别的意义，会在最后统一总结。 read 和 dirty 之间是会互相转换的，在 dirty 中查找 key 对次数足够多的时候，sync.Map会把 dirty 直接作为 read，即触发 dirty=\u003eread升级。同时在某些情况，也会出现read=\u003edirty的重塑，具体方式和这么设计的原因，最后详述。 ","date":"2022-09-14","objectID":"/go-sync-map/:2:2","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#架构设计图"},{"categories":["golang"],"content":" 源码细节梳理通过上面的分析，可以对sync.Map有一个初步的整体认知，这里再列出 CURD 几个关键操作的源码，进一步加深理解。同样的由于篇幅原因，我去除了大段冗长的英文注释，换成了提炼之后更加通俗的理解，有需要可以对比原文注释。 ","date":"2022-09-14","objectID":"/go-sync-map/:3:0","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#源码细节梳理"},{"categories":["golang"],"content":" Store操作（对应C/U） // Store sets the value for a key. func (m *Map) Store(key, value interface{}) { // 首先把readonly字段原子地取出来 // 如果key在readonly里面，则先取出key对应的entry，然后尝试对这个entry存入value的指针 read, _ := m.read.Load().(readOnly) if e, ok := read.m[key]; ok \u0026\u0026 e.tryStore(\u0026value) { return } // 如果readonly里面不存在key或者是对应的key是被擦除掉了的，则继续。。。 m.mu.Lock() // 上锁 // 锁的惯用模式：再次检查readonly，防止在上锁前的时间缝隙出现存储 read, _ = m.read.Load().(readOnly) if e, ok := read.m[key]; ok { // 这里有两种情况： // 1. 上面的时间缝隙里面，出现了key的存储过程（可能是normal值，也可能是expunge值） // 此时先校验e.p，如果是普通值，说明read和dirty里都有相同的entry，则直接设置entry // 如果是expunge值，则说明dirty里面已经不存在key了，需要先在dirty里面种上key，然后设置entry // 2. 本来read里面就存在，只不过对应的entry是expunge的状态 // 这种情况和上面的擦除情况一样，说明dirty里面已经不存在key了，需要先在dirty里面种上key，然后设置entry if e.unexpungeLocked() { // The entry was previously expunged, which implies that there is a // non-nil dirty map and this entry is not in it. m.dirty[key] = e } e.storeLocked(\u0026value) // 将value存入容器e } else if e, ok := m.dirty[key]; ok { // readonly里面不存在，则查看dirty里面是否存在 // 如果dirty里面存在，则直接设置dirty的对应key e.storeLocked(\u0026value) } else { // dirty里面也不存在（或者dirty为nil），则应该先设置在ditry里面 // 此时要检查read.amended，如果为假（标识dirty中没有自己独有的key or 两者均是初始化状态） // 此时要在dirty里面设置新的key，需要确保dirty是初始化的且需要设置amended为true（表示自此dirty多出了一些独有key） if !read.amended { // We're adding the first new key to the dirty map. // Make sure it is allocated and mark the read-only map as incomplete. m.dirtyLocked() m.read.Store(readOnly{m: read.m, amended: true}) } m.dirty[key] = newEntry(value) } // 解锁 m.mu.Unlock() } // 这是一个自旋乐观锁：只有key是非expunged的情况下，会得到set操作 func (e *entry) tryStore(i *interface{}) bool { for { p := atomic.LoadPointer(\u0026e.p) // 如果p是expunged就不可以了set了 // 因为expunged状态是read独有的，这种情况下说明这个key已经删除（并且发生过了read=\u003edirty重塑过）了 // 此时要新增只能在dirty中，不能在read中 if p == expunged { return false } // 如果非expunged，则说明是normal的entry或者nil的entry，可以直接替换 if atomic.CompareAndSwapPointer(\u0026e.p, p, unsafe.Pointer(i)) { return true } } } // 利用了go的CAS，如果e.p是 expunged，则将e.p置为空，从而保证她是read和dirty共有的 func (e *entry) unexpungeLocked() (wasExpunged bool) { return atomic.CompareAndSwapPointer(\u0026e.p, expunged, nil) } // 真正的set操作，从这里也可以看出来2点：1是set是原子的 2是封装的过程 func (e *entry) storeLocked(i *interface{}) { atomic.StorePointer(\u0026e.p, unsafe.Pointer(i)) } // 利用read重塑dirty！ // 如果dirty为nil，则利用当前的read来初始化dirty（包括read本身也为空的情况） // 此函数是在锁的保护下进行，所以不用担心出现不一致 func (m *Map) dirtyLocked() { if m.dirty != nil { return } // 经过这么一轮操作: // dirty里面存储了全部的非expunged的entry // read里面存储了dirty的全集，以及所有expunged的entry // 且read中不存在e.p == nil的entry（已经被转成了expunged） read, _ := m.read.Load().(readOnly) m.dirty = make(map[interface{}]*entry, len(read.m)) for k, e := range read.m { if !e.tryExpungeLocked() { // 只有非擦除的key，能够重塑到dirty里面 m.dirty[k] = e } } } // 利用乐观自旋锁， // 如果e.p是nil，尽量将e.p置为expunged // 返回最终e.p是否是expunged func (e *entry) tryExpungeLocked() (isExpunged bool) { p := atomic.LoadPointer(\u0026e.p) for p == nil { if atomic.CompareAndSwapPointer(\u0026e.p, nil, expunged) { return true } p = atomic.LoadPointer(\u0026e.p) } return p == expunged } ","date":"2022-09-14","objectID":"/go-sync-map/:3:1","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#store操作对应cu"},{"categories":["golang"],"content":" Store操作（对应R） func (m *Map) Load(key interface{}) (value interface{}, ok bool) { // 把readonly字段原子地取出来 read, _ := m.read.Load().(readOnly) e, ok := read.m[key] // 如果readonly没找到，且dirty包含了read没有的key，则尝试去dirty里面找 if !ok \u0026\u0026 read.amended { m.mu.Lock() // 锁的惯用套路 read, _ = m.read.Load().(readOnly) e, ok = read.m[key] if !ok \u0026\u0026 read.amended { e, ok = m.dirty[key] // Regardless of ... 省略英文 // 记录miss次数，并在满足阈值后，触发dirty=\u003emap的升级 m.missLocked() } m.mu.Unlock() } // readonly和dirty的key列表，都没找到，返回nil if !ok { return nil, false } // 找到了对应entry，随即取出对应的值 return e.load() } // 自增miss计数器 // 如果增加到一定程度，dirty会升级成为readonly（dirty自身清空 \u0026 read.amended置为false） func (m *Map) missLocked() { m.misses++ if m.misses \u003c len(m.dirty) { return } // 直接用dirty覆盖到了read上（那也就是意味着dirty的值是必然是read的父集合，当然这不包括read中的expunged entry） m.read.Store(readOnly{m: m.dirty}) // 这里有一个隐含操作，read.amended再次变成false m.dirty = nil m.misses = 0 } // entry是一个容器，从entry里面取出实际存储的值（以指针提取的方式） func (e *entry) load() (value interface{}, ok bool) { p := atomic.LoadPointer(\u0026e.p) if p == nil || p == expunged { return nil, false } return *(*interface{})(p), true } ","date":"2022-09-14","objectID":"/go-sync-map/:3:2","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#store操作对应r"},{"categories":["golang"],"content":" Delete操作（对应D） // Delete deletes the value for a key. func (m *Map) Delete(key interface{}) { m.LoadAndDelete(key) } // 删除的逻辑和Load的逻辑，基本上是一致的 func (m *Map) LoadAndDelete(key interface{}) (value interface{}, loaded bool) { read, _ := m.read.Load().(readOnly) e, ok := read.m[key] if !ok \u0026\u0026 read.amended { m.mu.Lock() read, _ = m.read.Load().(readOnly) e, ok = read.m[key] if !ok \u0026\u0026 read.amended { e, ok = m.dirty[key] delete(m.dirty, key) // Regardless of ...省略 m.missLocked() } m.mu.Unlock() } if ok { return e.delete() } return nil, false } // 如果e.p == expunged 或者nil，则返回false // 否则，设置e.p = nil，返回删除的值得指针 func (e *entry) delete() (value interface{}, ok bool) { for { p := atomic.LoadPointer(\u0026e.p) if p == nil || p == expunged { return nil, false } if atomic.CompareAndSwapPointer(\u0026e.p, p, nil) { return *(*interface{})(p), true } } } ","date":"2022-09-14","objectID":"/go-sync-map/:3:3","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#delete操作对应d"},{"categories":["golang"],"content":" 整体思考第一次读 Map 的源码，会觉得很晦涩，虽然整体思路是明确的，但是细节却很多，困惑于为什么做这样的设计，多读几遍之后，很多问题能够略窥门径。这里列出一些开始觉得困惑的问题： ","date":"2022-09-14","objectID":"/go-sync-map/:4:0","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#整体思考"},{"categories":["golang"],"content":" dirty 和 read 互转，分别在什么样的时机下会进行？ dirty=\u003eread：随着 load 的 miss 不断自增，达到阈值后触发升级转储（完毕之后，dirty 置空\u0026miss清零\u0026read.amended置false） read=\u003edirty ：当有 read 中不存在的新 key 需要增加且 read 和 dirty 一致的时候，触发重塑，且read.amended置 true（然后再在 dirty 新增）。重塑的过程，会将 nil 状态的 entry，全部挤压到 expunged 状态中，同时将非 expunged 的 entry 浅拷贝到 dirty 中，这样可以避免 read 的 key 无限的膨胀（存在大量逻辑删除的 key）。最终，在 dirty 再次升级为 read 的时候，这些逻辑删除的 key 就可以一次性丢弃释放了（因为是直接覆盖上去） read=\u003edirty ","date":"2022-09-14","objectID":"/go-sync-map/:4:1","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#dirty-和-read-互转分别在什么样的时机下会进行"},{"categories":["golang"],"content":" read 从何而来，存在的意义又是什么？ read 是由 dirty 升级而来，是利用了atomic.Store一次性覆盖，而不是一点点的 set 操作出来的。所以，read 更像是一个快照，read 中 key 的集合不能被改变（注意，这里说的 read 的 key 不可改变，不代表指定的 key 的 value 不可改变，value 是可以通过原子CAS 来进行更改的），所以其中的键的集合有时候可能是不全的。 相反，脏字典中的键值对集合总是完全的，但是其中不会包含expunged的键值对。 read 的存在价值，在于加速读性能（通过原子操作避免了锁） ","date":"2022-09-14","objectID":"/go-sync-map/:4:2","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#read-从何而来存在的意义又是什么"},{"categories":["golang"],"content":" entry 的 p 可能的状态，有哪些？ e.p==nil：entry 已经被标记删除，不过此时还未经过read=\u003edirty重塑，此时可能仍然属于 dirty（如果 dirty 非 nil） e.p==expunged：entry 已经被标记删除，经过read=\u003edirty重塑，不属于 dirty，仅仅属于 read，下一次dirty=\u003eread 升级，会被彻底清理（因为升级的操作是直接覆盖，read 中的 expunged 会被自动释放回收） e.p==普通指针：此时 entry 是一个普通的存在状态，属于 read，如果 dirty 非 nil，也属于 dirty。对应架构图中的 normal 状态。 ","date":"2022-09-14","objectID":"/go-sync-map/:4:3","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#entry-的-p-可能的状态有哪些"},{"categories":["golang"],"content":" 删除操作的细节，e.p到底是设置成了 nil 还是 expunged？ 如果 key 不在 read 中，但是在 dirty 中，则直接 delete。 如果 key 在 read 中，则逻辑删除，e.p赋值为 nil(后续在重塑的时候，nil 会变成 expunged ) ","date":"2022-09-14","objectID":"/go-sync-map/:4:4","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#删除操作的细节ep到底是设置成了-nil-还是-expunged"},{"categories":["golang"],"content":" 什么时候 e.p 由 nil 变成 expunged？ read=\u003edirty重塑的时候，此时 read 中仍然是 nil 的，会变成 expunged，表示这部分 key 等待被最终丢弃（expunged 是最终态，等待被丢弃，除非又出现了重新 store 的情况） 最终丢弃的时机：就是dirty=\u003eread升级的时候，dirty 的直接粗暴覆盖，会使得 read 中的所有成员都被丢弃，包括 expunged。 ","date":"2022-09-14","objectID":"/go-sync-map/:4:5","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#什么时候-ep-由-nil-变成-expunged"},{"categories":["golang"],"content":" 既然 nil 也表示标记删除，那么再设计出一个 expunged 的意义是什么？expunged 是有存在意义的，它作为删除的最终状态（待释放），这样 nil 就可以作为一种中间状态。如果仅仅使用 nil，那么，在read=\u003edirty重塑的时候，可能会出现如下的情况： 如果 nil 在 read 浅拷贝至 dirty 的时候仍然保留 entry 的指针（即拷贝完成后，对应键值下 read 和 dirty 中都有对应键下 entry e 的指针，且e.p=nil）那么之后在dirty=\u003eread升级 key 的时候对应 entry 的指针仍然会保留。那么最终；的合集会越来越大，存在大量 nil 的状态，永远无法得到清理的机会。 如果 nil 在 read 浅拷贝时不进入 dirty，那么之后 store 某个 Key 键的时候，可能会出现 read 和 dirty 不同步的情况，即此时 read 中包含 dirty 不包含的键，那么之后用 dirty 替换 read 的时候就会出现数据丢失的问题。 如果 nil 在 read 浅拷贝时直接把 read 中对应键删除（从而避免了不同步的问题），但这又必须对 read 加锁，违背了 read 读写不加锁的初衷。 综上，为了保证 read 作为快照的性质（不能单独删除或新增key），同时要避免 Map 中 nil 的 key 不断膨胀等多个前提要求，才设计成了 expungd 的状态。 ","date":"2022-09-14","objectID":"/go-sync-map/:4:6","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#既然-nil-也表示标记删除那么再设计出一个-expunged-的意义是什么"},{"categories":["golang"],"content":" 对于一个 entry，从生到死的状态机图 entry 从生到死的状态机图 ","date":"2022-09-14","objectID":"/go-sync-map/:4:7","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#对于一个-entry从生到死的状态机图"},{"categories":["golang"],"content":" 注释中关于 slow path 和 fast path 的解释 慢路径其实就是经过了锁的代码路径。 快路径就是不经过锁的。 ","date":"2022-09-14","objectID":"/go-sync-map/:4:8","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#注释中关于-slow-path-和-fast-path-的解释"},{"categories":["golang"],"content":" 总结sync.Map的源码并不长，但是里面的很多细节都非常的考究，比如对于原子和锁的使用、利用状态机的变化标记来代替 map 的 delete 从而提高性能和安全性等等。 ","date":"2022-09-14","objectID":"/go-sync-map/:5:0","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#总结"},{"categories":["golang"],"content":" 参考 不得不知道的Golang之sync.Map解读！ ","date":"2022-09-14","objectID":"/go-sync-map/:6:0","series":null,"tags":["golang"],"title":"Go sync.Map 解读","uri":"/go-sync-map/#参考"},{"categories":["golang"],"content":"xiaobinqt,","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/"},{"categories":["golang"],"content":" 原文链接：https://segmentfault.com/a/1190000018626163 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:0:0","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#"},{"categories":["golang"],"content":" 0. 目录和说明文章在介绍一些基本概念后，按照以下过程阐述了整个架构的演进过程： 单机架构 第一次演进：Tomcat 与数据库分开部署 第二次演进：引入本地缓存和分布式缓存 第三次演进：引入反向代理实现负载均衡 第四次演进：数据库读写分离 第五次演进：数据库按业务分库 第六次演进：把大表拆分为小表 第七次演进：使用 LVS 或 F5 来使多个 Nginx 负载均衡 第八次演进：通过 DNS 轮询实现机房间的负载均衡 第九次演进：引入 NoSQL 数据库和搜索引擎等技术 第十次演进：大应用拆分为小应用 第十一次演进：复用的功能抽离成微服务 第十二次演进：引入企业服务总线 ESB 屏蔽服务接口的访问差异 第十三次演进：引入容器化技术实现运行环境隔离与动态服务管理 第十四次演进：以云平台承载系统 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:1:0","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#0-目录和说明"},{"categories":["golang"],"content":" 1. 概述本文以淘宝作为例子，介绍从一百个到千万级并发情况下服务端的架构的演进过程，同时列举出每个演进阶段会遇到的相关技术，让大家对架构的演进有一个整体的认知，文章最后汇总了一些架构设计的原则。 特别说明：本文以淘宝为例仅仅是为了便于说明演进过程可能遇到的问题，并非是淘宝真正的技术演进路径 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:2:0","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#1-概述"},{"categories":["golang"],"content":" 2. 基本概念在介绍架构之前，为了避免部分读者对架构设计中的一些概念不了解，下面对几个最基础的概念进行介绍。 分布式系统中的多个模块在不同服务器上部署，即可称为分布式系统，如 Tomcat 和数据库分别部署在不同的服务器上，或两个相同功能的 Tomcat 分别部署在不同服务器上 高可用系统中部分节点失效时，其他节点能够接替它继续提供服务，则可认为系统具有高可用性 集群一个特定领域的软件部署在多台服务器上并作为一个整体提供一类服务，这个整体称为集群。如 Zookeeper 中的 Master 和 Slave 分别部署在多台服务器上，共同组成一个整体提供集中配置服务。在常见的集群中，客户端往往能够连接任意一个节点获得服务，并且当集群中一个节点掉线时，其他节点往往能够自动的接替它继续提供服务，这时候说明集群具有高可用性 负载均衡请求发送到系统时，通过某些方式把请求均匀分发到多个节点上，使系统中每个节点能够均匀的处理请求负载，则可认为系统是负载均衡的 正向代理和反向代理 系统内部要访问外部网络时，统一通过一个代理服务器把请求转发出去，在外部网络看来就是代理服务器发起的访问，此时代理服务器实现的是正向代理；当外部请求进入系统时，代理服务器把该请求转发到系统中的某台服务器上，对外部请求来说，与之交互的只有代理服务器，此时代理服务器实现的是反向代理。简单来说， 正向代理是代理服务器代替系统内部来访问外部网络的过程，反向代理是外部请求访问系统时通过代理服务器转发到内部服务器的过程。 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:3:0","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#2-基本概念"},{"categories":["golang"],"content":" 3. 架构演进","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:4:0","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#3-架构演进"},{"categories":["golang"],"content":" 3.1 单机架构 单机架构 以淘宝作为例子。在网站最初时，应用数量与用户数都较少，可以把 Tomcat 和数据库部署在同一台服务器上。浏览器往 www.taobao.com 发起请求时，首先经过 DNS 服务器（域名系统）把域名转换为实际 IP 地址 10.102.4.1，浏览器转而访问该 IP 对应的 Tomcat。 随着用户数的增长，Tomcat 和数据库之间竞争资源，单机性能不足以支撑业务 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:4:1","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#31-单机架构"},{"categories":["golang"],"content":" 3.2 第一次演进：Tomcat 与数据库分开部署 第一次演进 Tomcat 和数据库分别独占服务器资源，显著提高两者各自性能。 随着用户数的增长，并发读写数据库成为瓶颈 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:4:2","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#32-第一次演进tomcat-与数据库分开部署"},{"categories":["golang"],"content":" 3.3 第二次演进：引入本地缓存和分布式缓存 第二次演进 在 Tomcat 同服务器上或同 JVM 中增加本地缓存，并在外部增加分布式缓存，缓存热门商品信息或热门商品的 html 页面等。通过缓存能把绝大多数请求在读写数据库前拦截掉，大大降低数据库压力。其中涉及的技术包括：使用 memcached 作为本地缓存，使用 Redis 作为分布式缓存，还会涉及缓存一致性、缓存穿透/击穿、缓存雪崩、热点数据集中失效等问题。 缓存抗住了大部分的访问请求，随着用户数的增长，并发压力主要落在单机的 Tomcat 上，响应逐渐变慢 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:4:3","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#33-第二次演进引入本地缓存和分布式缓存"},{"categories":["golang"],"content":" 3.4 第三次演进：引入反向代理实现负载均衡 第三次演进 在多台服务器上分别部署 Tomcat，使用反向代理软件（Nginx）把请求均匀分发到每个 Tomcat 中。此处假设 Tomcat 最多支持 100 个并发，Nginx 最多支持 50000 个并发，那么理论上 Nginx 把请求分发到 500 个 Tomcat 上，就能抗住 50000 个并发。其中涉及的技术包括：Nginx、HAProxy，两者都是工作在网络第七层的反向代理软件，主要支持 http 协议，还会涉及 session 共享、文件上传下载的问题。 反向代理使应用服务器可支持的并发量大大增加，但并发量的增长也意味着更多请求穿透到数据库，单机的数据库最终成为瓶颈 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:4:4","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#34-第三次演进引入反向代理实现负载均衡"},{"categories":["golang"],"content":" 3.5 第四次演进：数据库读写分离 第四次演进 把数据库划分为读库和写库，读库可以有多个，通过同步机制把写库的数据同步到读库，对于需要查询最新写入数据场景，可通过在缓存中多写一份，通过缓存获得最新数据。其中涉及的技术包括：Mycat，它是数据库中间件，可通过它来组织数据库的分离读写和分库分表，客户端通过它来访问下层数据库，还会涉及数据同步，数据一致性的问题。 业务逐渐变多，不同业务之间的访问量差距较大，不同业务直接竞争数据库，相互影响性能 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:4:5","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#35-第四次演进数据库读写分离"},{"categories":["golang"],"content":" 3.6 第五次演进：数据库按业务分库 第五次演进 把不同业务的数据保存到不同的数据库中，使业务之间的资源竞争降低，对于访问量大的业务，可以部署更多的服务器来支撑。这样同时导致跨业务的表无法直接做关联分析，需要通过其他途径来解决，但这不是本文讨论的重点，有兴趣的可以自行搜索解决方案。 随着用户数的增长，单机的写库会逐渐会达到性能瓶颈 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:4:6","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#36-第五次演进数据库按业务分库"},{"categories":["golang"],"content":" 3.7 第六次演进：把大表拆分为小表 第六次演进 比如针对评论数据，可按照商品 ID 进行 hash，路由到对应的表中存储；针对支付记录，可按照小时创建表，每个小时表继续拆分为小表，使用用户 ID 或记录编号来路由数据。只要实时操作的表数据量足够小，请求能够足够均匀的分发到多台服务器上的小表，那数据库就能通过水平扩展的方式来提高性能。其中前面提到的 Mycat 也支持在大表拆分为小表情况下的访问控制。 这种做法显著的增加了数据库运维的难度，对 DBA 的要求较高。数据库设计到这种结构时，已经可以称为分布式数据库，但是这只是一个逻辑的数据库整体，数据库里不同的组成部分是由不同的组件单独来实现的，如分库分表的管理和请求分发，由 Mycat 实现，SQL 的解析由单机的数据库实现，读写分离可能由网关和消息队列来实现，查询结果的汇总可能由数据库接口层来实现等等，这种架构其实是 MPPMassively Parallel Processing （大规模并行处理）架构的一类实现。 目前开源和商用都已经有不少 MPP 数据库，开源中比较流行的有 Greenplum、TiDB、Postgresql XC、HAWQ 等，商用的如南大通用的 GBase、睿帆科技的雪球 DB、华为的 LibrA 等等，不同的 MPP 数据库的侧重点也不一样，如 TiDB 更侧重于分布式 OLTP 场景，Greenplum 更侧重于分布式 OLAP 场景，这些 MPP 数据库基本都提供了类似 Postgresql、Oracle、MySQL 那样的 SQL 标准支持能力，能把一个查询解析为分布式的执行计划分发到每台机器上并行执行，最终由数据库本身汇总数据进行返回，也提供了诸如权限管理、分库分表、事务、数据副本等能力，并且大多能够支持 100 个节点以上的集群，大大降低了数据库运维的成本，并且使数据库也能够实现水平扩展。 数据库和 Tomcat 都能够水平扩展，可支撑的并发大幅提高，随着用户数的增长，最终单机的 Nginx 会成为瓶颈 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:4:7","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#37-第六次演进把大表拆分为小表"},{"categories":["golang"],"content":" 3.8 第七次演进：使用 LVS 或 F5 来使多个 Nginx 负载均衡 第七次演进 由于瓶颈在 Nginx，因此无法通过两层的 Nginx 来实现多个 Nginx 的负载均衡。图中的 LVSLinux Virtual Server 和 F5 是工作在网络第四层的负载均衡解决方案，其中 LVS 是软件，运行在操作系统内核态，可对 TCP 请求或更高层级的网络协议进行转发，因此支持的协议更丰富，并且性能也远高于 Nginx，可假设单机的 LVS 可支持几十万个并发的请求转发；F5 是一种负载均衡硬件，与 LVS 提供的能力类似，性能比 LVS 更高，但价格昂贵。 由于 LVS 是单机版的软件，若 LVS 所在服务器宕机则会导致整个后端系统都无法访问，因此需要有备用节点。可使用 keepalived 软件模拟出虚拟 IP，然后把虚拟 IP 绑定到多台 LVS 服务器上，浏览器访问虚拟 IP 时，会被路由器重定向到真实的 LVS 服务器，当主 LVS 服务器宕机时，keepalived 软件会自动更新路由器中的路由表，把虚拟 IP 重定向到另外一台正常的 LVS 服务器，从而达到 LVS 服务器高可用的效果。 此处需要注意的是，上图中从 Nginx 层到 Tomcat 层这样画并不代表全部 Nginx 都转发请求到全部的 Tomcat，在实际使用时，可能会是几个 Nginx 下面接一部分的 Tomcat，这些 Nginx 之间通过 keepalived 实现高可用，其他的 Nginx 接另外的 Tomcat，这样可接入的 Tomcat 数量就能成倍的增加。 由于 LVS 也是单机的，随着并发数增长到几十万时，LVS 服务器最终会达到瓶颈，此时用户数达到千万甚至上亿级别，用户分布在不同的地区，与服务器机房距离不同，导致了访问的延迟会明显不同 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:4:8","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#38-第七次演进使用-lvs-或-f5-来使多个-nginx-负载均衡"},{"categories":["golang"],"content":" 3.9 第八次演进：通过 DNS 轮询实现机房间的负载均衡 第八次演进 在 DNS 服务器中可配置一个域名对应多个 IP 地址，每个 IP 地址对应到不同的机房里的虚拟 IP。当用户访问 www.taobao.com 时，DNS 服务器会使用轮询策略或其他策略，来选择某个 IP 供用户访问。此方式能实现机房间的负载均衡，至此，系统可做到机房级别的水平扩展，千万级到亿级的并发量都可通过增加机房来解决，系统入口处的请求并发量不再是问题。 随着数据的丰富程度和业务的发展，检索、分析等需求越来越丰富，单单依靠数据库无法解决如此丰富的需求 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:4:9","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#39-第八次演进通过-dns-轮询实现机房间的负载均衡"},{"categories":["golang"],"content":" 3.10 第九次演进：引入 NoSQL 数据库和搜索引擎等技术 第九次演进 当数据库中的数据多到一定规模时，数据库就不适用于复杂的查询了，往往只能满足普通查询的场景。对于统计报表场景，在数据量大时不一定能跑出结果，而且在跑复杂查询时会导致其他查询变慢，对于全文检索、可变数据结构等场景，数据库天生不适用。因此需要针对特定的场景，引入合适的解决方案。如对于海量文件存储，可通过分布式文件系统 HDFS 解决，对于 key value 类型的数据，可通过 HBase 和 Redis 等方案解决，对于全文检索场景，可通过搜索引擎如 ElasticSearch 解决，对于多维分析场景，可通过 Kylin 或 Druid 等方案解决。 当然，引入更多组件同时会提高系统的复杂度，不同的组件保存的数据需要同步，需要考虑一致性的问题，需要有更多的运维手段来管理这些组件等。 引入更多组件解决了丰富的需求，业务维度能够极大扩充，随之而来的是一个应用中包含了太多的业务代码，业务的升级迭代变得困难 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:4:10","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#310-第九次演进引入-nosql-数据库和搜索引擎等技术"},{"categories":["golang"],"content":" 3.11 第十次演进：大应用拆分为小应用 第十次演进 按照业务板块来划分应用代码，使单个应用的职责更清晰，相互之间可以做到独立升级迭代。这时候应用之间可能会涉及到一些公共配置，可以通过分布式配置中心 Zookeeper 来解决。 不同应用之间存在共用的模块，由应用单独管理会导致相同代码存在多份，导致公共功能升级时全部应用代码都要跟着升级 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:4:11","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#311-第十次演进大应用拆分为小应用"},{"categories":["golang"],"content":" 3.12 第十一次演进：复用的功能抽离成微服务 第十一次演进 如用户管理、订单、支付、鉴权等功能在多个应用中都存在，那么可以把这些功能的代码单独抽取出来形成一个单独的服务来管理，这样的服务就是所谓的微服务，应用和服务之间通过 HTTP、TCP 或 RPC 请求等多种方式来访问公共服务，每个单独的服务都可以由单独的团队来管理。此外，可以通过 Dubbo、SpringCloud 等框架实现服务治理、限流、熔断、降级等功能，提高服务的稳定性和可用性。 不同服务的接口访问方式不同，应用代码需要适配多种访问方式才能使用服务，此外，应用访问服务，服务之间也可能相互访问，调用链将会变得非常复杂，逻辑变得混乱 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:4:12","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#312-第十一次演进复用的功能抽离成微服务"},{"categories":["golang"],"content":" 3.13 第十二次演进：引入企业服务总线 ESB 屏蔽服务接口的访问差异 第十二次演进 通过 ESB 统一进行访问协议转换，应用统一通过 ESB 来访问后端服务，服务与服务之间也通过 ESB 来相互调用，以此降低系统的耦合程度。这种单个应用拆分为多个应用，公共服务单独抽取出来来管理，并使用企业消息总线来解除服务之间耦合问题的架构，就是所谓的 SOAService-Oriented Architecture（面向服务）架构，这种架构与微服务架构容易混淆，因为表现形式十分相似。 个人理解，微服务架构更多是指把系统里的公共服务抽取出来单独运维管理的思想，而 SOA 架构则是指一种拆分服务并使服务接口访问变得统一的架构思想，SOA 架构中包含了微服务的思想。 业务不断发展，应用和服务都会不断变多，应用和服务的部署变得复杂，同一台服务器上部署多个服务还要解决运行环境冲突的问题， 此外，对于如大促这类需要动态扩缩容的场景，需要水平扩展服务的性能，就需要在新增的服务上准备运行环境，部署服务等，运维将变得十分困难 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:4:13","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#313-第十二次演进引入企业服务总线-esb-屏蔽服务接口的访问差异"},{"categories":["golang"],"content":" 3.14 第十三次演进：引入容器化技术实现运行环境隔离与动态服务管理 第十三次演进 目前最流行的容器化技术是 Docker，最流行的容器管理服务是 Kubernetes(K8S)，应用/服务可以打包为 Docker 镜像，通过 K8S 来动态分发和部署镜像。Docker 镜像可理解为一个能运行你的应用/服务的最小的操作系统，里面放着应用/服务的运行代码，运行环境根据实际的需要设置好。把整个“操作系统”打包为一个镜像后，就可以分发到需要部署相关服务的机器上，直接启动 Docker 镜像就可以把服务起起来，使服务的部署和运维变得简单。 在大促的之前，可以在现有的机器集群上划分出服务器来启动 Docker 镜像，增强服务的性能，大促过后就可以关闭镜像，对机器上的其他服务不造成影响（在 3.14 节之前，服务运行在新增机器上需要修改系统配置来适配服务，这会导致机器上其他服务需要的运行环境被破坏）。 使用容器化技术后服务动态扩缩容问题得以解决，但是机器还是需要公司自身来管理，在非大促的时候，还是需要闲置着大量的机器资源来应对大促，机器自身成本和运维成本都极高，资源利用率低 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:4:14","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#314-第十三次演进引入容器化技术实现运行环境隔离与动态服务管理"},{"categories":["golang"],"content":" 3.15 第十四次演进：以云平台承载系统 第十四次演进 系统可部署到公有云上，利用公有云的海量机器资源，解决动态硬件资源的问题，在大促的时间段里，在云平台中临时申请更多的资源，结合 Docker 和 K8S 来快速部署服务，在大促结束后释放资源，真正做到按需付费，资源利用率大大提高，同时大大降低了运维成本。 所谓的云平台，就是把海量机器资源，通过统一的资源管理，抽象为一个资源整体，在之上可按需动态申请硬件资源（如 CPU、内存、网络等），并且之上提供通用的操作系统，提供常用的技术组件（如 Hadoop 技术栈，MPP 数据库等）供用户使用，甚至提供开发好的应用，用户不需要关系应用内部使用了什么技术，就能够解决需求（如音视频转码服务、邮件服务、个人博客等）。在云平台中会涉及如下几个概念： IaaS基础设施即服务。对应于上面所说的机器资源统一为资源整体，可动态申请硬件资源的层面； PaaS平台即服务。对应于上面所说的提供常用的技术组件方便系统的开发和维护； SaaS软件即服务。对应于上面所说的提供开发好的应用或服务，按功能或性能要求付费。 至此，以上所提到的从高并发访问问题，到服务的架构和系统实施的层面都有了各自的解决方案，但同时也应该意识到，在上面的介绍中，其实是有意忽略了诸如跨机房数据同步、分布式事务实现等等的实际问题，这些问题以后有机会再拿出来单独讨论 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:4:15","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#315-第十四次演进以云平台承载系统"},{"categories":["golang"],"content":" 4. 架构设计总结 架构的调整是否必须按照上述演变路径进行？ 不是的，以上所说的架构演变顺序只是针对某个侧面进行单独的改进，在实际场景中，可能同一时间会有几个问题需要解决，或者可能先达到瓶颈的是另外的方面，这时候就应该按照实际问题实际解决。如在政府类的并发量可能不大，但业务可能很丰富的场景，高并发就不是重点解决的问题，此时优先需要的可能会是丰富需求的解决方案。 对于将要实施的系统，架构应该设计到什么程度？ 对于单次实施并且性能指标明确的系统，架构设计到能够支持系统的性能指标要求就足够了，但要留有扩展架构的接口以便不备之需。对于不断发展的系统，如电商平台，应设计到能满足下一阶段用户量和性能指标要求的程度，并根据业务的增长不断的迭代升级架构，以支持更高的并发和更丰富的业务。 服务端架构和大数据架构有什么区别? 所谓的“大数据”其实是海量数据采集清洗转换、数据存储、数据分析、数据服务等场景解决方案的一个统称，在每一个场景都包含了多种可选的技术，如数据采集有 Flume、Sqoop、Kettle 等，数据存储有分布式文件系统 HDFS、FastDFS，NoSQL 数据库 HBase、MongoDB 等，数据分析有 Spark 技术栈、机器学习算法等。总的来说大数据架构就是根据业务的需求，整合各种大数据组件组合而成的架构，一般会提供分布式存储、分布式计算、多维分析、数据仓库、机器学习算法等能力。而服务端架构更多指的是应用组织层面的架构，底层能力往往是由大数据架构来提供。 有没有一些架构设计的原则？ N+1 设计。系统中的每个组件都应做到没有单点故障； 回滚设计。确保系统可以向前兼容，在系统升级时应能有办法回滚版本； 禁用设计。应该提供控制具体功能是否可用的配置，在系统出现故障时能够快速下线功能； 监控设计。在设计阶段就要考虑监控的手段； 多活数据中心设计。若系统需要极高的高可用，应考虑在多地实施数据中心进行多活，至少在一个机房断电的情况下系统依然可用； 采用成熟的技术。刚开发的或开源的技术往往存在很多隐藏的 bug，出了问题没有商业支持可能会是一个灾难； 资源隔离设计。应避免单一业务占用全部资源； 架构应能水平扩展。系统只有做到能水平扩展，才能有效避免瓶颈问题； 非核心则购买。非核心功能若需要占用大量的研发资源才能解决，则考虑购买成熟的产品； 使用商用硬件。商用硬件能有效降低硬件故障的机率； 快速迭代。系统应该快速开发小功能模块，尽快上线进行验证，早日发现问题大大降低系统交付的风险； 无状态设计。服务接口应该做成无状态的，当前接口的访问不依赖于接口上次访问的状态。 ","date":"2022-08-29","objectID":"/highly-concurrent-architecture-evolution/:5:0","series":null,"tags":["golang","高并发"],"title":"高并发分布式架构演进","uri":"/highly-concurrent-architecture-evolution/#4-架构设计总结"},{"categories":["web"],"content":"xiaobinqt,session,cookie,token 的区别,jwt token,JWT,token","date":"2022-06-11","objectID":"/session-cookie-token-difference/","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/"},{"categories":["web"],"content":" Cookie 和 SessionHTTP 协议是一种无状态协议，即每次服务端接收到客户端的请求时，都是一个全新的请求，服务器并不知道客户端的历史请求记录；Session 和 Cookie 的主要目的就是为了弥补 HTTP 的无状态特性。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:1:0","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#cookie-和-session"},{"categories":["web"],"content":" Session 是什么客户端请求服务端，服务端会为这次请求开辟一块内存空间，这个对象便是 Session 对象，存储结构为 ConcurrentHashMap。Session 弥补了 HTTP 无状态特性，服务器可以利用 Session 存储客户端在同一个会话期间的一些操作记录。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:1:1","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#session-是什么"},{"categories":["web"],"content":" Session 如何判断是否是同一会话服务器第一次接收到请求时，开辟了一块 Session 空间（创建了Session对象），同时生成一个 sessionId ，并通过响应头的 Set-Cookie：JSESSIONID=XXXXXXX 命令，向客户端发送要求设置 Cookie 的响应；客户端收到响应后，在本机客户端设置了一个 JSESSIONID=XXXXXXX 的 Cookie 信息，该 Cookie 的过期时间为浏览器会话结束。 会话 接下来客户端每次向同一个网站发送请求时，请求头都会带上该 Cookie信息（包含 sessionId ）， 然后，服务器通过读取请求头中的 Cookie 信息，获取名称为 JSESSIONID 的值，得到此次请求的 sessionId。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:1:2","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#session-如何判断是否是同一会话"},{"categories":["web"],"content":" Session 的缺点Session 机制有个缺点，比如 A 服务器存储了 Session，就是做了负载均衡后，假如一段时间内 A 的访问量激增，会转发到 B 进行访问，但是 B 服务器并没有存储 A 的 Session，会导致 Session 的失效。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:1:3","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#session-的缺点"},{"categories":["web"],"content":" Cookies 是什么HTTP 协议中的 Cookie 包括 Web Cookie 和浏览器 Cookie，它是服务器发送到 Web 浏览器的一小块数据。服务器发送到浏览器的 Cookie，浏览器会进行存储，并与下一个请求一起发送到服务器。通常，它用于判断两个请求是否来自于同一个浏览器，例如用户保持登录状态。 HTTP Cookie 机制是 HTTP 协议无状态的一种补充和改良 Cookie 主要用于下面三个目的 会话管理 登陆、购物车、游戏得分或者服务器应该记住的其他内容 个性化 用户偏好、主题或者其他设置 追踪 记录和分析用户行为 Cookie 曾经用于一般的客户端存储。虽然这是合法的，因为它们是在客户端上存储数据的唯一方法，但如今建议使用现代存储 API。Cookie 随每个请求一起发送，因此它们可能会降低性能（尤其是对于移动数据连接而言）。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:1:4","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#cookies-是什么"},{"categories":["web"],"content":" 创建 Cookie当接收到客户端发出的 HTTP 请求时，服务器可以发送带有响应的 Set-Cookie 标头，Cookie 通常由浏览器存储，然后将 Cookie 与 HTTP 标头一同向服务器发出请求。 参数名 作用 Max-Age 设置cookie的过期时间，单位为秒 Domain 指定了Cookie所属的域名 Path 指定了Cookie所属的路径 HttpOnly 告诉浏览器此Cookie只能靠浏览器Http协议传输,禁止其他方式访问 Secure 告诉浏览器此Cookie只能在Https安全协议中传输,如果是Http则禁止传输 Set-Cookie 和 Cookie 标头Set-Cookie HTTP 响应标头将 cookie 从服务器发送到用户代理。下面是一个发送 Cookie 的例子 此标头告诉客户端存储 Cookie 现在，随着对服务器的每个新请求，浏览器将使用 Cookie 头将所有以前存储的 Cookie 发送回服务器。 有两种类型的 Cookies，一种是 Session Cookies会话 cookie，一种是 Persistent Cookies永久 cookie，如果 Cookie 不包含到期日期，则将其视为会话 Cookie。会话 Cookie 存储在内存中，永远不会写入磁盘，当浏览器关闭时，此后 Cookie 将永久丢失。如果 Cookie 包含有效期 ，则将其视为持久性 Cookie。在到期指定的日期，Cookie 将从磁盘中删除。 还有一种是 Cookie 的 Secure 和 HttpOnly 标记。 会话 Cookies上面的示例创建的是会话 Cookie ，会话 Cookie 有个特征，客户端关闭时 Cookie 会删除，因为它没有指定Expires或 Max-Age 指令。 但是，Web 浏览器可能会使用会话还原，这会使大多数会话 Cookie 保持永久状态，就像从未关闭过浏览器一样。 永久性 Cookies永久性 Cookie 不会在客户端关闭时过期，而是在特定日期（Expires）或特定时间长度（Max-Age）外过期。例如 Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; Cookie 的 Secure 和 HttpOnly 标记安全的 Cookie 需要经过 HTTPS 协议通过加密的方式发送到服务器。即使是安全的，也不应该将敏感信息存储在cookie 中，因为它们本质上是不安全的，并且此标志不能提供真正的保护。 HttpOnly 的作用 会话 Cookie 中缺少 HttpOnly 属性会导致攻击者可以通过程序( JS 脚本、Applet 等)获取到用户的 Cookie 信息，造成用户 Cookie 信息泄露，增加攻击者的跨站脚本攻击威胁。 HttpOnly 是微软对 Cookie 做的扩展，该值指定 Cookie 是否可通过客户端脚本访问。 如果在 Cookie 中没有设置 HttpOnly 属性为 true，可能导致 Cookie 被窃取。窃取的 Cookie 可以包含标识站点用户的敏感信息，如 ASP.NET 会话 ID 或 Forms 身份验证票证，攻击者可以重播窃取的 Cookie，以便伪装成用户或获取敏感信息，进行跨站脚本攻击等。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:1:5","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#创建-cookie"},{"categories":["web"],"content":" 创建 Cookie当接收到客户端发出的 HTTP 请求时，服务器可以发送带有响应的 Set-Cookie 标头，Cookie 通常由浏览器存储，然后将 Cookie 与 HTTP 标头一同向服务器发出请求。 参数名 作用 Max-Age 设置cookie的过期时间，单位为秒 Domain 指定了Cookie所属的域名 Path 指定了Cookie所属的路径 HttpOnly 告诉浏览器此Cookie只能靠浏览器Http协议传输,禁止其他方式访问 Secure 告诉浏览器此Cookie只能在Https安全协议中传输,如果是Http则禁止传输 Set-Cookie 和 Cookie 标头Set-Cookie HTTP 响应标头将 cookie 从服务器发送到用户代理。下面是一个发送 Cookie 的例子 此标头告诉客户端存储 Cookie 现在，随着对服务器的每个新请求，浏览器将使用 Cookie 头将所有以前存储的 Cookie 发送回服务器。 有两种类型的 Cookies，一种是 Session Cookies会话 cookie，一种是 Persistent Cookies永久 cookie，如果 Cookie 不包含到期日期，则将其视为会话 Cookie。会话 Cookie 存储在内存中，永远不会写入磁盘，当浏览器关闭时，此后 Cookie 将永久丢失。如果 Cookie 包含有效期 ，则将其视为持久性 Cookie。在到期指定的日期，Cookie 将从磁盘中删除。 还有一种是 Cookie 的 Secure 和 HttpOnly 标记。 会话 Cookies上面的示例创建的是会话 Cookie ，会话 Cookie 有个特征，客户端关闭时 Cookie 会删除，因为它没有指定Expires或 Max-Age 指令。 但是，Web 浏览器可能会使用会话还原，这会使大多数会话 Cookie 保持永久状态，就像从未关闭过浏览器一样。 永久性 Cookies永久性 Cookie 不会在客户端关闭时过期，而是在特定日期（Expires）或特定时间长度（Max-Age）外过期。例如 Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; Cookie 的 Secure 和 HttpOnly 标记安全的 Cookie 需要经过 HTTPS 协议通过加密的方式发送到服务器。即使是安全的，也不应该将敏感信息存储在cookie 中，因为它们本质上是不安全的，并且此标志不能提供真正的保护。 HttpOnly 的作用 会话 Cookie 中缺少 HttpOnly 属性会导致攻击者可以通过程序( JS 脚本、Applet 等)获取到用户的 Cookie 信息，造成用户 Cookie 信息泄露，增加攻击者的跨站脚本攻击威胁。 HttpOnly 是微软对 Cookie 做的扩展，该值指定 Cookie 是否可通过客户端脚本访问。 如果在 Cookie 中没有设置 HttpOnly 属性为 true，可能导致 Cookie 被窃取。窃取的 Cookie 可以包含标识站点用户的敏感信息，如 ASP.NET 会话 ID 或 Forms 身份验证票证，攻击者可以重播窃取的 Cookie，以便伪装成用户或获取敏感信息，进行跨站脚本攻击等。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:1:5","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#set-cookie-和-cookie-标头"},{"categories":["web"],"content":" 创建 Cookie当接收到客户端发出的 HTTP 请求时，服务器可以发送带有响应的 Set-Cookie 标头，Cookie 通常由浏览器存储，然后将 Cookie 与 HTTP 标头一同向服务器发出请求。 参数名 作用 Max-Age 设置cookie的过期时间，单位为秒 Domain 指定了Cookie所属的域名 Path 指定了Cookie所属的路径 HttpOnly 告诉浏览器此Cookie只能靠浏览器Http协议传输,禁止其他方式访问 Secure 告诉浏览器此Cookie只能在Https安全协议中传输,如果是Http则禁止传输 Set-Cookie 和 Cookie 标头Set-Cookie HTTP 响应标头将 cookie 从服务器发送到用户代理。下面是一个发送 Cookie 的例子 此标头告诉客户端存储 Cookie 现在，随着对服务器的每个新请求，浏览器将使用 Cookie 头将所有以前存储的 Cookie 发送回服务器。 有两种类型的 Cookies，一种是 Session Cookies会话 cookie，一种是 Persistent Cookies永久 cookie，如果 Cookie 不包含到期日期，则将其视为会话 Cookie。会话 Cookie 存储在内存中，永远不会写入磁盘，当浏览器关闭时，此后 Cookie 将永久丢失。如果 Cookie 包含有效期 ，则将其视为持久性 Cookie。在到期指定的日期，Cookie 将从磁盘中删除。 还有一种是 Cookie 的 Secure 和 HttpOnly 标记。 会话 Cookies上面的示例创建的是会话 Cookie ，会话 Cookie 有个特征，客户端关闭时 Cookie 会删除，因为它没有指定Expires或 Max-Age 指令。 但是，Web 浏览器可能会使用会话还原，这会使大多数会话 Cookie 保持永久状态，就像从未关闭过浏览器一样。 永久性 Cookies永久性 Cookie 不会在客户端关闭时过期，而是在特定日期（Expires）或特定时间长度（Max-Age）外过期。例如 Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; Cookie 的 Secure 和 HttpOnly 标记安全的 Cookie 需要经过 HTTPS 协议通过加密的方式发送到服务器。即使是安全的，也不应该将敏感信息存储在cookie 中，因为它们本质上是不安全的，并且此标志不能提供真正的保护。 HttpOnly 的作用 会话 Cookie 中缺少 HttpOnly 属性会导致攻击者可以通过程序( JS 脚本、Applet 等)获取到用户的 Cookie 信息，造成用户 Cookie 信息泄露，增加攻击者的跨站脚本攻击威胁。 HttpOnly 是微软对 Cookie 做的扩展，该值指定 Cookie 是否可通过客户端脚本访问。 如果在 Cookie 中没有设置 HttpOnly 属性为 true，可能导致 Cookie 被窃取。窃取的 Cookie 可以包含标识站点用户的敏感信息，如 ASP.NET 会话 ID 或 Forms 身份验证票证，攻击者可以重播窃取的 Cookie，以便伪装成用户或获取敏感信息，进行跨站脚本攻击等。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:1:5","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#会话-cookies"},{"categories":["web"],"content":" 创建 Cookie当接收到客户端发出的 HTTP 请求时，服务器可以发送带有响应的 Set-Cookie 标头，Cookie 通常由浏览器存储，然后将 Cookie 与 HTTP 标头一同向服务器发出请求。 参数名 作用 Max-Age 设置cookie的过期时间，单位为秒 Domain 指定了Cookie所属的域名 Path 指定了Cookie所属的路径 HttpOnly 告诉浏览器此Cookie只能靠浏览器Http协议传输,禁止其他方式访问 Secure 告诉浏览器此Cookie只能在Https安全协议中传输,如果是Http则禁止传输 Set-Cookie 和 Cookie 标头Set-Cookie HTTP 响应标头将 cookie 从服务器发送到用户代理。下面是一个发送 Cookie 的例子 此标头告诉客户端存储 Cookie 现在，随着对服务器的每个新请求，浏览器将使用 Cookie 头将所有以前存储的 Cookie 发送回服务器。 有两种类型的 Cookies，一种是 Session Cookies会话 cookie，一种是 Persistent Cookies永久 cookie，如果 Cookie 不包含到期日期，则将其视为会话 Cookie。会话 Cookie 存储在内存中，永远不会写入磁盘，当浏览器关闭时，此后 Cookie 将永久丢失。如果 Cookie 包含有效期 ，则将其视为持久性 Cookie。在到期指定的日期，Cookie 将从磁盘中删除。 还有一种是 Cookie 的 Secure 和 HttpOnly 标记。 会话 Cookies上面的示例创建的是会话 Cookie ，会话 Cookie 有个特征，客户端关闭时 Cookie 会删除，因为它没有指定Expires或 Max-Age 指令。 但是，Web 浏览器可能会使用会话还原，这会使大多数会话 Cookie 保持永久状态，就像从未关闭过浏览器一样。 永久性 Cookies永久性 Cookie 不会在客户端关闭时过期，而是在特定日期（Expires）或特定时间长度（Max-Age）外过期。例如 Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; Cookie 的 Secure 和 HttpOnly 标记安全的 Cookie 需要经过 HTTPS 协议通过加密的方式发送到服务器。即使是安全的，也不应该将敏感信息存储在cookie 中，因为它们本质上是不安全的，并且此标志不能提供真正的保护。 HttpOnly 的作用 会话 Cookie 中缺少 HttpOnly 属性会导致攻击者可以通过程序( JS 脚本、Applet 等)获取到用户的 Cookie 信息，造成用户 Cookie 信息泄露，增加攻击者的跨站脚本攻击威胁。 HttpOnly 是微软对 Cookie 做的扩展，该值指定 Cookie 是否可通过客户端脚本访问。 如果在 Cookie 中没有设置 HttpOnly 属性为 true，可能导致 Cookie 被窃取。窃取的 Cookie 可以包含标识站点用户的敏感信息，如 ASP.NET 会话 ID 或 Forms 身份验证票证，攻击者可以重播窃取的 Cookie，以便伪装成用户或获取敏感信息，进行跨站脚本攻击等。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:1:5","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#永久性-cookies"},{"categories":["web"],"content":" 创建 Cookie当接收到客户端发出的 HTTP 请求时，服务器可以发送带有响应的 Set-Cookie 标头，Cookie 通常由浏览器存储，然后将 Cookie 与 HTTP 标头一同向服务器发出请求。 参数名 作用 Max-Age 设置cookie的过期时间，单位为秒 Domain 指定了Cookie所属的域名 Path 指定了Cookie所属的路径 HttpOnly 告诉浏览器此Cookie只能靠浏览器Http协议传输,禁止其他方式访问 Secure 告诉浏览器此Cookie只能在Https安全协议中传输,如果是Http则禁止传输 Set-Cookie 和 Cookie 标头Set-Cookie HTTP 响应标头将 cookie 从服务器发送到用户代理。下面是一个发送 Cookie 的例子 此标头告诉客户端存储 Cookie 现在，随着对服务器的每个新请求，浏览器将使用 Cookie 头将所有以前存储的 Cookie 发送回服务器。 有两种类型的 Cookies，一种是 Session Cookies会话 cookie，一种是 Persistent Cookies永久 cookie，如果 Cookie 不包含到期日期，则将其视为会话 Cookie。会话 Cookie 存储在内存中，永远不会写入磁盘，当浏览器关闭时，此后 Cookie 将永久丢失。如果 Cookie 包含有效期 ，则将其视为持久性 Cookie。在到期指定的日期，Cookie 将从磁盘中删除。 还有一种是 Cookie 的 Secure 和 HttpOnly 标记。 会话 Cookies上面的示例创建的是会话 Cookie ，会话 Cookie 有个特征，客户端关闭时 Cookie 会删除，因为它没有指定Expires或 Max-Age 指令。 但是，Web 浏览器可能会使用会话还原，这会使大多数会话 Cookie 保持永久状态，就像从未关闭过浏览器一样。 永久性 Cookies永久性 Cookie 不会在客户端关闭时过期，而是在特定日期（Expires）或特定时间长度（Max-Age）外过期。例如 Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; Cookie 的 Secure 和 HttpOnly 标记安全的 Cookie 需要经过 HTTPS 协议通过加密的方式发送到服务器。即使是安全的，也不应该将敏感信息存储在cookie 中，因为它们本质上是不安全的，并且此标志不能提供真正的保护。 HttpOnly 的作用 会话 Cookie 中缺少 HttpOnly 属性会导致攻击者可以通过程序( JS 脚本、Applet 等)获取到用户的 Cookie 信息，造成用户 Cookie 信息泄露，增加攻击者的跨站脚本攻击威胁。 HttpOnly 是微软对 Cookie 做的扩展，该值指定 Cookie 是否可通过客户端脚本访问。 如果在 Cookie 中没有设置 HttpOnly 属性为 true，可能导致 Cookie 被窃取。窃取的 Cookie 可以包含标识站点用户的敏感信息，如 ASP.NET 会话 ID 或 Forms 身份验证票证，攻击者可以重播窃取的 Cookie，以便伪装成用户或获取敏感信息，进行跨站脚本攻击等。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:1:5","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#cookie-的-secure-和-httponly-标记"},{"categories":["web"],"content":" Cookie 的作用域Domain 和 Path 标识定义了 Cookie 的作用域：即 Cookie 应该发送给哪些 URL。 Domain 标识指定了哪些主机可以接受 Cookie。如果不指定，默认为当前主机(不包含子域名）。如果指定了Domain，则一般包含子域名。 例如，如果设置 Domain=mozilla.org，则 Cookie 也包含在子域名中（如developer.mozilla.org）。 例如，设置 Path=/docs，则以下地址都会匹配： /docs /docs/Web/ /docs/Web/HTTP ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:1:6","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#cookie-的作用域"},{"categories":["web"],"content":" JSON Web Token 和 Session Cookies 的对比JSON Web Token，简称 JWT，它和 Session都可以为网站提供用户的身份认证，但是它们不是一回事。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:2:0","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#json-web-token-和-session-cookies-的对比"},{"categories":["web"],"content":" JWT 和 Session Cookies 的相同之处在探讨 JWT 和 Session Cookies 之前，有必要需要先去理解一下它们的相同之处。 它们既可以对用户进行身份验证，也可以用来在用户单击进入不同页面时以及登陆网站或应用程序后进行身份验证。 如果没有这两者，那你可能需要在每个页面切换时都需要进行登录了。因为 HTTP 是一个无状态的协议。这也就意味着当你访问某个网页，然后单击同一站点上的另一个页面时，服务器的内存中将不会记住你之前的操作。 因此，如果你登录并访问了你有权访问的另一个页面，由于 HTTP 不会记录你刚刚登录的信息，因此你将再次登录。 JWT 和 Session Cookies 就是用来处理在不同页面之间切换，保存用户登录信息的机制。 也就是说，这两种技术都是用来保存你的登录状态，能够让你在浏览任意受密码保护的网站。通过在每次产生新的请求时对用户数据进行身份验证来解决此问题。 所以 JWT 和 Session Cookies 的相同之处是什么？那就是它们能够支持你在发送不同请求之间，记录并验证你的登录状态的一种机制。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:2:1","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#jwt-和-session-cookies-的相同之处"},{"categories":["web"],"content":" 什么是 Session CookiesSession Cookies 也称为会话 Cookies，在 Session Cookies 中，用户的登录状态会保存在服务器的内存中。当用户登录时，Session 就被服务端安全的创建。 在每次请求时，服务器都会从会话 Cookie 中读取 SessionId，如果服务端的数据和读取的 SessionId 相同，那么服务器就会发送响应给浏览器，允许用户登录。 ⚠️ 下图有点问题，域名应该一致。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:2:2","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#什么是-session-cookies"},{"categories":["web"],"content":" 什么是 Json Web TokensJson Web Token 的简称就是 JWT，通常可以称为 Json 令牌。它是RFC 7519 中定义的用于安全的将信息作为 Json 对象进行传输的一种形式。JWT 中存储的信息是经过数字签名 的，因此可以被信任和理解。可以使用 HMAC 算法或使用 RSA/ECDSA 的公用/专用密钥对 JWT 进行签名。 使用 JWT 主要用来下面两点 认证(Authorization)：这是使用 JWT 最常见的一种情况，一旦用户登录，后面每个请求都会包含 JWT，从而允许用户访问该令牌所允许的路由、服务和资源。单点登录是当今广泛使用 JWT 的一项功能，因为它的开销很小。 信息交换(Information Exchange)：JWT 是能够安全传输信息的一种方式。通过使用公钥/私钥对 JWT 进行签名认证。此外，由于签名是使用 head 和 payload 计算的，因此你还可以验证内容是否遭到篡改。 JWT 的格式下面，我们会探讨一下 JWT 的组成和格式是什么 JWT 主要由三部分组成，每个部分用 . 进行分割，各个部分分别是 Header Payload Signature 因此，一个非常简单的 JWT 组成会是下面这样 Header Header 是 JWT 的标头，它通常由两部分组成：令牌的类型(即 JWT)和使用的 签名算法，例如 HMAC SHA256（写成 HS256） 或 RSA。 例如 { \"alg\": \"HS256\", \"typ\": \"JWT\" } 指定类型和签名算法后，Json 块被 Base64Url 编码形成 JWT 的第一部分。 Payload Token 的第二部分是 Payload，Payload 中包含一个声明。声明是有关实体（通常是用户）和其他数据的声明。共有三种类型的声明：registered, public 和 private 声明。 registered 声明： 包含一组建议使用的预定义声明，主要包括 ISS 签发人 iss (issuer) 签发人 exp (expiration time) 过期时间 sub (subject) 主题 aud (audience) 受众 nbf (Not Before) 生效时间 iat (Issued At) 签发时间 jti (JWT ID) 编号 public 声明：公共的声明，可以添加任何的信息，一般添加用户的相关信息或其他业务需要的必要信息，但不建议添加敏感信息，因为该部分在客户端可解密。 private 声明：自定义声明，旨在在同意使用它们的各方之间共享信息，既不是注册声明也不是公共声明。 例如 { \"sub\": \"1234567890\", \"name\": \"John Doe\", \"admin\": true } 然后 payload Json 块会被Base64Url 编码形成 JWT 的第二部分。 signature JWT 的第三部分是一个签证信息，这个签证信息由三部分组成 header (base64后的) payload (base64后的) secret 比如我们需要 HMAC SHA256 算法进行签名 HMACSHA256( base64UrlEncode(header) + \".\" + base64UrlEncode(payload), secret) 签名用于验证消息在此过程中没有更改，并且对于使用私钥进行签名的令牌，它还可以验证 JWT 的发送者的真实身份 拼凑在一起现在我们把上面的三个由点分隔的 Base64-URL 字符串部分组成在一起，这个字符串可以在 HTML 和 HTTP 环境中轻松传递这些字符串。 下面是一个完整的 JWT 示例，它对 header 和 payload 进行编码，然后使用 signature 进行签名 eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 如果想自己测试编写的话，可以访问 JWT 官网 https://jwt.io/#debugger-io ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:2:3","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#什么是-json-web-tokens"},{"categories":["web"],"content":" 什么是 Json Web TokensJson Web Token 的简称就是 JWT，通常可以称为 Json 令牌。它是RFC 7519 中定义的用于安全的将信息作为 Json 对象进行传输的一种形式。JWT 中存储的信息是经过数字签名 的，因此可以被信任和理解。可以使用 HMAC 算法或使用 RSA/ECDSA 的公用/专用密钥对 JWT 进行签名。 使用 JWT 主要用来下面两点 认证(Authorization)：这是使用 JWT 最常见的一种情况，一旦用户登录，后面每个请求都会包含 JWT，从而允许用户访问该令牌所允许的路由、服务和资源。单点登录是当今广泛使用 JWT 的一项功能，因为它的开销很小。 信息交换(Information Exchange)：JWT 是能够安全传输信息的一种方式。通过使用公钥/私钥对 JWT 进行签名认证。此外，由于签名是使用 head 和 payload 计算的，因此你还可以验证内容是否遭到篡改。 JWT 的格式下面，我们会探讨一下 JWT 的组成和格式是什么 JWT 主要由三部分组成，每个部分用 . 进行分割，各个部分分别是 Header Payload Signature 因此，一个非常简单的 JWT 组成会是下面这样 Header Header 是 JWT 的标头，它通常由两部分组成：令牌的类型(即 JWT)和使用的 签名算法，例如 HMAC SHA256（写成 HS256） 或 RSA。 例如 { \"alg\": \"HS256\", \"typ\": \"JWT\" } 指定类型和签名算法后，Json 块被 Base64Url 编码形成 JWT 的第一部分。 Payload Token 的第二部分是 Payload，Payload 中包含一个声明。声明是有关实体（通常是用户）和其他数据的声明。共有三种类型的声明：registered, public 和 private 声明。 registered 声明： 包含一组建议使用的预定义声明，主要包括 ISS 签发人 iss (issuer) 签发人 exp (expiration time) 过期时间 sub (subject) 主题 aud (audience) 受众 nbf (Not Before) 生效时间 iat (Issued At) 签发时间 jti (JWT ID) 编号 public 声明：公共的声明，可以添加任何的信息，一般添加用户的相关信息或其他业务需要的必要信息，但不建议添加敏感信息，因为该部分在客户端可解密。 private 声明：自定义声明，旨在在同意使用它们的各方之间共享信息，既不是注册声明也不是公共声明。 例如 { \"sub\": \"1234567890\", \"name\": \"John Doe\", \"admin\": true } 然后 payload Json 块会被Base64Url 编码形成 JWT 的第二部分。 signature JWT 的第三部分是一个签证信息，这个签证信息由三部分组成 header (base64后的) payload (base64后的) secret 比如我们需要 HMAC SHA256 算法进行签名 HMACSHA256( base64UrlEncode(header) + \".\" + base64UrlEncode(payload), secret) 签名用于验证消息在此过程中没有更改，并且对于使用私钥进行签名的令牌，它还可以验证 JWT 的发送者的真实身份 拼凑在一起现在我们把上面的三个由点分隔的 Base64-URL 字符串部分组成在一起，这个字符串可以在 HTML 和 HTTP 环境中轻松传递这些字符串。 下面是一个完整的 JWT 示例，它对 header 和 payload 进行编码，然后使用 signature 进行签名 eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 如果想自己测试编写的话，可以访问 JWT 官网 https://jwt.io/#debugger-io ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:2:3","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#jwt-的格式"},{"categories":["web"],"content":" 什么是 Json Web TokensJson Web Token 的简称就是 JWT，通常可以称为 Json 令牌。它是RFC 7519 中定义的用于安全的将信息作为 Json 对象进行传输的一种形式。JWT 中存储的信息是经过数字签名 的，因此可以被信任和理解。可以使用 HMAC 算法或使用 RSA/ECDSA 的公用/专用密钥对 JWT 进行签名。 使用 JWT 主要用来下面两点 认证(Authorization)：这是使用 JWT 最常见的一种情况，一旦用户登录，后面每个请求都会包含 JWT，从而允许用户访问该令牌所允许的路由、服务和资源。单点登录是当今广泛使用 JWT 的一项功能，因为它的开销很小。 信息交换(Information Exchange)：JWT 是能够安全传输信息的一种方式。通过使用公钥/私钥对 JWT 进行签名认证。此外，由于签名是使用 head 和 payload 计算的，因此你还可以验证内容是否遭到篡改。 JWT 的格式下面，我们会探讨一下 JWT 的组成和格式是什么 JWT 主要由三部分组成，每个部分用 . 进行分割，各个部分分别是 Header Payload Signature 因此，一个非常简单的 JWT 组成会是下面这样 Header Header 是 JWT 的标头，它通常由两部分组成：令牌的类型(即 JWT)和使用的 签名算法，例如 HMAC SHA256（写成 HS256） 或 RSA。 例如 { \"alg\": \"HS256\", \"typ\": \"JWT\" } 指定类型和签名算法后，Json 块被 Base64Url 编码形成 JWT 的第一部分。 Payload Token 的第二部分是 Payload，Payload 中包含一个声明。声明是有关实体（通常是用户）和其他数据的声明。共有三种类型的声明：registered, public 和 private 声明。 registered 声明： 包含一组建议使用的预定义声明，主要包括 ISS 签发人 iss (issuer) 签发人 exp (expiration time) 过期时间 sub (subject) 主题 aud (audience) 受众 nbf (Not Before) 生效时间 iat (Issued At) 签发时间 jti (JWT ID) 编号 public 声明：公共的声明，可以添加任何的信息，一般添加用户的相关信息或其他业务需要的必要信息，但不建议添加敏感信息，因为该部分在客户端可解密。 private 声明：自定义声明，旨在在同意使用它们的各方之间共享信息，既不是注册声明也不是公共声明。 例如 { \"sub\": \"1234567890\", \"name\": \"John Doe\", \"admin\": true } 然后 payload Json 块会被Base64Url 编码形成 JWT 的第二部分。 signature JWT 的第三部分是一个签证信息，这个签证信息由三部分组成 header (base64后的) payload (base64后的) secret 比如我们需要 HMAC SHA256 算法进行签名 HMACSHA256( base64UrlEncode(header) + \".\" + base64UrlEncode(payload), secret) 签名用于验证消息在此过程中没有更改，并且对于使用私钥进行签名的令牌，它还可以验证 JWT 的发送者的真实身份 拼凑在一起现在我们把上面的三个由点分隔的 Base64-URL 字符串部分组成在一起，这个字符串可以在 HTML 和 HTTP 环境中轻松传递这些字符串。 下面是一个完整的 JWT 示例，它对 header 和 payload 进行编码，然后使用 signature 进行签名 eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 如果想自己测试编写的话，可以访问 JWT 官网 https://jwt.io/#debugger-io ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:2:3","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#拼凑在一起"},{"categories":["web"],"content":" JWT 和 Session Cookies 的不同JWT 和 Session Cookies 都提供安全的用户身份验证，但是它们有以下几点不同 密码签名JWT 具有加密签名，而 Session Cookies 则没有。 JSON 是无状态的JWT 是无状态的，因为声明被存储在客户端，而不是服务端内存中。 身份验证可以在本地进行，而不是在请求必须通过服务器数据库或类似位置中进行。 这意味着可以对用户进行多次身份验证，而无需与站点或应用程序的数据库进行通信，也无需在此过程中消耗大量资源。 可扩展性Session Cookies 是存储在服务器内存中，这就意味着如果网站或者应用很大的情况下会耗费大量的资源。由于 JWT 是无状态的，在许多情况下，它们可以节省服务器资源。因此 JWT 要比 Session Cookies 具有更强的可扩展性。 JWT 支持跨域认证Session Cookies 只能用在单个节点的域或者它的子域中有效。如果它们尝试通过第三个节点访问，就会被禁止。如果你希望自己的网站和其他站点建立安全连接时，这是一个问题。 使用 JWT 可以解决这个问题，使用 JWT 能够通过多个节点进行用户认证，也就是我们常说的跨域认证。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:2:4","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#jwt-和-session-cookies-的不同"},{"categories":["web"],"content":" JWT 和 Session Cookies 的不同JWT 和 Session Cookies 都提供安全的用户身份验证，但是它们有以下几点不同 密码签名JWT 具有加密签名，而 Session Cookies 则没有。 JSON 是无状态的JWT 是无状态的，因为声明被存储在客户端，而不是服务端内存中。 身份验证可以在本地进行，而不是在请求必须通过服务器数据库或类似位置中进行。 这意味着可以对用户进行多次身份验证，而无需与站点或应用程序的数据库进行通信，也无需在此过程中消耗大量资源。 可扩展性Session Cookies 是存储在服务器内存中，这就意味着如果网站或者应用很大的情况下会耗费大量的资源。由于 JWT 是无状态的，在许多情况下，它们可以节省服务器资源。因此 JWT 要比 Session Cookies 具有更强的可扩展性。 JWT 支持跨域认证Session Cookies 只能用在单个节点的域或者它的子域中有效。如果它们尝试通过第三个节点访问，就会被禁止。如果你希望自己的网站和其他站点建立安全连接时，这是一个问题。 使用 JWT 可以解决这个问题，使用 JWT 能够通过多个节点进行用户认证，也就是我们常说的跨域认证。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:2:4","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#密码签名"},{"categories":["web"],"content":" JWT 和 Session Cookies 的不同JWT 和 Session Cookies 都提供安全的用户身份验证，但是它们有以下几点不同 密码签名JWT 具有加密签名，而 Session Cookies 则没有。 JSON 是无状态的JWT 是无状态的，因为声明被存储在客户端，而不是服务端内存中。 身份验证可以在本地进行，而不是在请求必须通过服务器数据库或类似位置中进行。 这意味着可以对用户进行多次身份验证，而无需与站点或应用程序的数据库进行通信，也无需在此过程中消耗大量资源。 可扩展性Session Cookies 是存储在服务器内存中，这就意味着如果网站或者应用很大的情况下会耗费大量的资源。由于 JWT 是无状态的，在许多情况下，它们可以节省服务器资源。因此 JWT 要比 Session Cookies 具有更强的可扩展性。 JWT 支持跨域认证Session Cookies 只能用在单个节点的域或者它的子域中有效。如果它们尝试通过第三个节点访问，就会被禁止。如果你希望自己的网站和其他站点建立安全连接时，这是一个问题。 使用 JWT 可以解决这个问题，使用 JWT 能够通过多个节点进行用户认证，也就是我们常说的跨域认证。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:2:4","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#json-是无状态的"},{"categories":["web"],"content":" JWT 和 Session Cookies 的不同JWT 和 Session Cookies 都提供安全的用户身份验证，但是它们有以下几点不同 密码签名JWT 具有加密签名，而 Session Cookies 则没有。 JSON 是无状态的JWT 是无状态的，因为声明被存储在客户端，而不是服务端内存中。 身份验证可以在本地进行，而不是在请求必须通过服务器数据库或类似位置中进行。 这意味着可以对用户进行多次身份验证，而无需与站点或应用程序的数据库进行通信，也无需在此过程中消耗大量资源。 可扩展性Session Cookies 是存储在服务器内存中，这就意味着如果网站或者应用很大的情况下会耗费大量的资源。由于 JWT 是无状态的，在许多情况下，它们可以节省服务器资源。因此 JWT 要比 Session Cookies 具有更强的可扩展性。 JWT 支持跨域认证Session Cookies 只能用在单个节点的域或者它的子域中有效。如果它们尝试通过第三个节点访问，就会被禁止。如果你希望自己的网站和其他站点建立安全连接时，这是一个问题。 使用 JWT 可以解决这个问题，使用 JWT 能够通过多个节点进行用户认证，也就是我们常说的跨域认证。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:2:4","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#可扩展性"},{"categories":["web"],"content":" JWT 和 Session Cookies 的不同JWT 和 Session Cookies 都提供安全的用户身份验证，但是它们有以下几点不同 密码签名JWT 具有加密签名，而 Session Cookies 则没有。 JSON 是无状态的JWT 是无状态的，因为声明被存储在客户端，而不是服务端内存中。 身份验证可以在本地进行，而不是在请求必须通过服务器数据库或类似位置中进行。 这意味着可以对用户进行多次身份验证，而无需与站点或应用程序的数据库进行通信，也无需在此过程中消耗大量资源。 可扩展性Session Cookies 是存储在服务器内存中，这就意味着如果网站或者应用很大的情况下会耗费大量的资源。由于 JWT 是无状态的，在许多情况下，它们可以节省服务器资源。因此 JWT 要比 Session Cookies 具有更强的可扩展性。 JWT 支持跨域认证Session Cookies 只能用在单个节点的域或者它的子域中有效。如果它们尝试通过第三个节点访问，就会被禁止。如果你希望自己的网站和其他站点建立安全连接时，这是一个问题。 使用 JWT 可以解决这个问题，使用 JWT 能够通过多个节点进行用户认证，也就是我们常说的跨域认证。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:2:4","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#jwt-支持跨域认证"},{"categories":["web"],"content":" JWT 和 Session Cookies 的选型我们上面探讨了 JWT 和 Cookies 的不同点，相信你也会对选型有了更深的认识，大致来说 对于只需要登录用户并访问存储在站点数据库中的一些信息的中小型网站来说，Session Cookies 通常就能满足。 如果你有企业级站点，应用程序或附近的站点，并且需要处理大量的请求，尤其是第三方或很多第三方（包括位于不同域的API），则 JWT 显然更适合。 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:2:5","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#jwt-和-session-cookies-的选型"},{"categories":["web"],"content":" FAQ 如何禁用 Cookies 后，使用 Session 如果禁用了 Cookies，服务器仍会将 sessionId 以 cookie 的方式发送给浏览器，但是，浏览器不再保存这个cookie (即sessionId) 了。 如果想要继续使用 session，需要采用 URL 重写 的方式来实现，可以参考 https://www.cnblogs.com/Renyi-Fan/p/11012086.html ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:3:0","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#faq"},{"categories":["web"],"content":" FAQ 如何禁用 Cookies 后，使用 Session 如果禁用了 Cookies，服务器仍会将 sessionId 以 cookie 的方式发送给浏览器，但是，浏览器不再保存这个cookie (即sessionId) 了。 如果想要继续使用 session，需要采用 URL 重写 的方式来实现，可以参考 https://www.cnblogs.com/Renyi-Fan/p/11012086.html ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:3:0","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#如何禁用-cookies-后使用-session"},{"categories":["web"],"content":" JWT 前端解码在线解密工具https://www.box3.cn/tools/jwt.html npm 包https://www.npmjs.com/package/jwt-decode ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:3:1","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#jwt-前端解码"},{"categories":["web"],"content":" 参考 看完这篇 Session、Cookie、Token，和面试官扯皮就没问题了 JSON Web Token 入门教程 ","date":"2022-06-11","objectID":"/session-cookie-token-difference/:4:0","series":null,"tags":["web"],"title":"session cookie token 的区别","uri":"/session-cookie-token-difference/#参考"},{"categories":["mysql"],"content":"xiaobinqt,MySQL 为什么使用 B+ 树索引,B+ 树索引有什么优点？","date":"2022-05-20","objectID":"/why-mysql-index-use-btree/","series":null,"tags":["mysql"],"title":"MySQL 为什么使用 B+ 树索引","uri":"/why-mysql-index-use-btree/"},{"categories":["mysql"],"content":" 原文地址：https://juejin.cn/post/7081065180301361183 ","date":"2022-05-20","objectID":"/why-mysql-index-use-btree/:0:0","series":null,"tags":["mysql"],"title":"MySQL 为什么使用 B+ 树索引","uri":"/why-mysql-index-use-btree/#"},{"categories":["mysql"],"content":" 前言为什么 MySQL 采用 B+ 树作为索引？ 如果纯粹的猜测 MySQL 数据库索引为什么使用 B+ 树？那么围绕这个问题的回答通常一定是围绕 B+ 树本身是什么，有什么优势这两点去解释这个问题。 这不是我开始这么去想的，看了很多文章都是从这一维度问答，这些回答让我失望啊。直到那天问了坐在我旁边那个整天摸鱼的5年程序员；他慵懒的回答：你想为什么是使用的是树结构呢？咦，听到这回答，一下打开了我的思绪，有点意思！ 先抛开 B+ 树是什么，有什么优势，这些先入为主的答案。 我并不想要往我脑袋硬塞硬邦邦的答案。 我想要的是为什么？ 为什么 MySQL 的索引有那么多的数据结构可选，偏偏选树结构？为什么那么多的树结构？为什么又偏偏采用 B+ 树作为索引? 这才是我要想明白的！我想要的是不只是答案，还要答案背后的脉络！ 我想要的不仅是要知其然，更想要知其所以然。 ","date":"2022-05-20","objectID":"/why-mysql-index-use-btree/:1:0","series":null,"tags":["mysql"],"title":"MySQL 为什么使用 B+ 树索引","uri":"/why-mysql-index-use-btree/#前言"},{"categories":["mysql"],"content":" 那么多数据结构，为什么选树结构？众多的数据结构在逻辑层面可分为：线性结构 和 非线性结构。 线性结构有：数组、链表，基于它们衍生出的有哈希表（哈希表也称散列表）、栈、队列等。 非线性结构有：树、图。 还有其他数据结构如：跳表、位图 也都由基础数据结构演化而来，不同的数据结构存在即都是为了解决某些场景问题。 如果要知道索引适合什么数据结构，那我们得先来回答索引需要来解决什么样的问题（痛点）？和发挥着什么样的作用？其次再才是选择什么样的数据结构；后者只是果，前者才是因。 我们都知道 MySQL 存储的数据是在磁盘里，因为即使设备断电，放在磁盘的数据是不会有影响的，保障了数据不丢失，这意味着 MySQL 在磁盘上的数据是持久化的。 但数据存储在磁盘得到保障的同时也是有代价的，这代价就是磁盘的处理速度是毫秒级别的，相比内存纳秒级别的速度，简直是小巫见大巫。 你可能对时间单位没什么概念，可以看 1 毫秒能慢上纳秒几万倍。 图片 索引虽然存储在磁盘上，但使用索引查找数据时，可以从磁盘先读取索引放到内存中，再通过索引从磁盘找到数据；再然后将磁盘读取到的数据也放到内存里。 索引就让磁盘和内存强强联手，趁机搭上了内存的车，感受了一把纳秒级别速度的推背感。 图片 但是不管查询的过程中怎么优化，只要根还在磁盘，就避免不了会发生多次磁盘 I/O ，而磁盘 I/O 次数越多，消耗的时间也越多。 （聪明的同学这会可以看出这其实就是个需要考虑解决的痛点了） 要尽少在磁盘做 I/O 操作。 但还有那么多的数据结构可选呢。 其实索引需要发挥的目的已经决定了有哪些数据结构可选，那么就可以缩小选择其他数据结构的范围。 从为什么要建索引本身的首要目的出发。 要能尽快的按照区间高效地范围查找。 当然索引首要目的能支持高效范围查询，还要有插入更新等操作的动态数据结构。 所以有满足以这两条主要条件的除了树结构你还会想到其他什么数据结构？ 哈希表、跳表 ","date":"2022-05-20","objectID":"/why-mysql-index-use-btree/:2:0","series":null,"tags":["mysql"],"title":"MySQL 为什么使用 B+ 树索引","uri":"/why-mysql-index-use-btree/#那么多数据结构为什么选树结构"},{"categories":["mysql"],"content":" 哈希表先看哈希表，哈希表对于我们来说太熟悉不过，哈希表的物理存储是一个数组⚠️，而数组在内存中是连续地址的空间。 数据以Key、Value的方式存储。哈希表拥有精确的查询，所以时间复杂度是 O(1)。 而哈希表之所以能这么快是通过Key计算数组下标来快速找到Value。 最简单的计算的方式是 余数法，通过先计算key的 HashCode，再通过哈希表的数组长度对 HashCode 求余，求余得出的余数就是数组下标，最后由下标访问到哈希表中存的Key、Value。 图片 但是 Key 计算出的下标可能会有相同的情况，例如 HashCode 1010 对 6 取余是 2，但是 HashCode 1112 对 6 取余也是 2。 哈希算法随机计算出 HashCode 取余数组长度可能出现数组下标相同的情况，就是所谓的 哈希冲突。 而 哈希冲突 常用 链表 的方法解决。当发生 哈希冲突，相同下标的数据元素会替换成存储指针，而不同Key 的数据元素添加到链表中。查找时通过指针遍历这个链表，再匹配出正确的 Key 就可以。 图片 如上图所示，Key 是 “一颗剽悍的种子” 的字符串 ，Value 是 “不要忘了关注、点赞、评论”。我们通过计算key为 HashCode（1010） 的整数型值int。然后用 HashCode（1010） 对长度为 6 的哈希表数组长度做取余得出 2，这 2 的值元素就是 ( Key = “一颗剽悍的种子”,Value = “不要忘了关注、点赞、评论”) 。 虽然哈希表虽然可以高效的等值查询。例如SQL： select * from weixin where username = \"一颗剽悍的种子\" 但是不支持区间查询。例如SQL： select * from weixin where age \u003c 18 那么如果哈希表用来做成索引，当进行范围查询时意味着要全部扫描。但类似 Redis 存储形式是 KV 的 NoSQL 数据库，会适合等值查询的场景。 ","date":"2022-05-20","objectID":"/why-mysql-index-use-btree/:2:1","series":null,"tags":["mysql"],"title":"MySQL 为什么使用 B+ 树索引","uri":"/why-mysql-index-use-btree/#哈希表"},{"categories":["mysql"],"content":" 跳表跳表似乎对于我们来说是一个比较陌生的数据结构，但是在 Redis 中却是比较常用的数据结构之一。跳表底层实质就是可以进行二分查找的有序链表，他在链表基础加上索引层，即能支持插入、删除等动态操作，也支持按区间高效查询。而且不管是查找、插入、删除对应的时间复杂度都是 O(logn)。 要理解跳表，先来看链表，假设链表存储是有序的数据，我们要想查询某一个数据，在最差的情况下要从头全遍历整个链表，时间复杂度是 O(n)。 图片 如下图所示，跳表是在链表基础上加了索引层。可以起到支持区间查询的作用。 图片 从上图所示，我们如果要查询一个 26 的节点，跳表就可以先从索引层遍历，当遍历到在索引层的 21 节点，会发现下一个索引层的节点是 36 节点时，很明显要找的 26 的节点就在这区间。此时我们只要再通过索引层指向原始链表的指针往下移到原始链这一层遍历，只要遍历 2 个节点即可找到 26 了。如果用原来的链表需要遍历 10 个节点，现在只要遍历 8 个节点。 如下图中，一图胜千言。当数据量大时，一个包含多个结点的链表，在建立了五级索引后可以突显的看到索引层的优势。同时注意道这样一个规律 “加一层索引，查询所需要遍历的节点个数减少，查询效率也就提高了。” 从用户的角度就是，跳表这家伙其实就是在告诉链表从什么地方开始找比较快。 图片 看到这，跳表似乎也很适合用来作为索引的数据结构。但是不要忘了还有首个条件没满足，就是 “要尽少在磁盘做 I/O 操作。” 而跳表显然没能满足这一条件，跳表在随数据量增多的情况，索引层也会随着增高，相应的就会增加读取I/O的次数，从而影响性能。 那么我们回到 “那么多数据结构，为什么选树结构的问题？”我们发现哈希表和跳表并不能很好的满足解决磁盘痛点和索引目的的这两个主要条件。那么我们来看为什么要来选树结构。 ","date":"2022-05-20","objectID":"/why-mysql-index-use-btree/:2:2","series":null,"tags":["mysql"],"title":"MySQL 为什么使用 B+ 树索引","uri":"/why-mysql-index-use-btree/#跳表"},{"categories":["mysql"],"content":" 树结构我们先来看现实中一颗树都有哪些部分组成，首先要有根、树枝、还有树叶。那抽象成树结构也是一样的，树结构的顶端是 根节点（root），左侧的节点称为 左子树，右子树对应的在右侧的节点，树的末端没有节点的称为 叶子节点。 图片 从树的层级关系可以分为上下级和同级节点，如下图，D、E是B节点的子节点，那么B 节点就是它们的父节点，跟B节点在同一层级的C节点是B节点的兄弟节点。 同时树的最大层级数，称为树的高度（深度），图下的树高度是3。 图片 从树结构的层级角度看，其实树结构是不是跟前面的跳表还有点相似。而跳表之所以这么快是因为有能按区间高效查询的索引层。 而树结构其特性决定了遍历数据方式本身就纯天然的支持按区间查询。再加上树是非线性结构的优势相比于线性结构的数组，不必像数组的数据是连续存放的。那么当树结构在插入新数据时就不用像数组插入数据前时，需要将数据所在往后的所有数据节点都得往后挪动的开销。所以树结构更适合插入更新等动态操作的数据结构。 树结构在满足了索引目的和其他条件的情况下，至于减少磁盘查询操作的痛点其实我们就可以在基于树结构的数据结构中去选择。 ","date":"2022-05-20","objectID":"/why-mysql-index-use-btree/:2:3","series":null,"tags":["mysql"],"title":"MySQL 为什么使用 B+ 树索引","uri":"/why-mysql-index-use-btree/#树结构"},{"categories":["mysql"],"content":" 那么多的树结构？为什么偏偏采用 B+ 树作为索引?那么多的树结构中，除了B+树，你还会想到哪些树结构？二叉树查找树、自平衡二叉树、B树。 ","date":"2022-05-20","objectID":"/why-mysql-index-use-btree/:3:0","series":null,"tags":["mysql"],"title":"MySQL 为什么使用 B+ 树索引","uri":"/why-mysql-index-use-btree/#那么多的树结构为什么偏偏采用-b-树作为索引"},{"categories":["mysql"],"content":" 二叉树在了解二叉查找树或者自平衡二叉树前需要先简单知道什么是二叉树，什么是二分查找树。因为你看二叉查找树不就是这两棵树的合并吗。 我们先来看看二叉树，二叉树的树结构中定义的是每个节点的可以是0个子节或1个子节点，但是最多不超2个子节点。 而二叉树还有两个形式：满二叉树、完全二叉树 满二叉树 满二叉树的定义是一棵二叉树的所有非叶子节点都存在左右子节点，并且所有子节点在同一层级。 图片 完全二叉树 完全二叉树的定义是如果这颗树的所有节点和同深度的满二叉树的的节点位置相同则这二叉树是完全二叉树。如下图。 图片 ","date":"2022-05-20","objectID":"/why-mysql-index-use-btree/:3:1","series":null,"tags":["mysql"],"title":"MySQL 为什么使用 B+ 树索引","uri":"/why-mysql-index-use-btree/#二叉树"},{"categories":["mysql"],"content":" 二叉查找树接下来我们来简单看一下二叉查找树，此二叉查找树的“二”非彼二，因为此 “二” 即可以说是表示二叉的树，也可以表示二分查找，因为二叉查找树即是二叉也融合了二分查找。 先简单的看看二分查找，二分查找可以避免有序的数组从头依次的遍历查询，因为我们知道这种情况如果要查找一个数最差的情况时间复杂就是O(n) ，整体查询效率不高。而如果数组是有序的，就可以通过二分查找将每次的查询范围减半，时间复杂度自然就是O(logn)。如下图所示。 图片 所以说，二叉查找树不同于普通二叉查找树，是将小于根节点的元素放在左子树，而右子树正好相反是放大于根节点的元素。（说白了就是根节点是左子树和右子树的中位数，左边放小于中位数的，右边放大于中位数，这不就是二分查找算法的奥义） 图片 如上动图所示，二分查找树在查找数据时，只需要将需要查找的元素与树节点元素进行比较，当元素大于根节点则往右子树中查找，元素小于根节点则往左子树中查找，元素如果正好是中位数那么就是正好是根节点，所以二叉查找树具备高效查询。 但是二叉树也有明显弊端，在极端情况下，如果每次插入的数据都是最小或者都是最大的元素，那么树结构会退化成一条链表。查询数据是的时间复杂度就会是O(n)，如下图所示。 图片 当二分查找树退化成链表时，我们都知道链表不仅不能高效查询而且也增加了磁盘 IO 操作，所以我们得划向下一个树型数据结构。 ","date":"2022-05-20","objectID":"/why-mysql-index-use-btree/:3:2","series":null,"tags":["mysql"],"title":"MySQL 为什么使用 B+ 树索引","uri":"/why-mysql-index-use-btree/#二叉查找树"},{"categories":["mysql"],"content":" 自平衡二叉树自平衡二叉树就是来解决二叉查找树极端下退化成链表的问题，自平衡二叉树也称 平衡二叉查找树（AVL树）。 可以看到从简单的二叉树，一步步演化到二分查找树再到现在的自平衡二叉树。一个简单的东西慢慢的逐渐走向复杂。如果只知道答案，我们是不会知道来龙去脉的。 平衡二叉查找树其实主要就是在二叉查找树的基础上加上约束：让每个节点的左右子树高度差不能超过 1。那么这样让可以让左右子树都保持平衡，让查询数据操作的时间复杂度在 O(logn)。 如下图所示，平衡二叉查找树将每次插入的元素数据都会维持自平衡。 图片 如下图所示，普通非二叉树和平衡二叉树的对比。 图片 当然还有在 Java中集合类常见的红黑树，也是自平衡二叉树中的一种。 但是不管自平衡树是平衡二叉查找树还是红黑树，都会随着插入的元素增多，而导致树的高度变高，这同样意味着磁盘 I/O 操作次数多，影响到整体查询的效率。 ","date":"2022-05-20","objectID":"/why-mysql-index-use-btree/:3:3","series":null,"tags":["mysql"],"title":"MySQL 为什么使用 B+ 树索引","uri":"/why-mysql-index-use-btree/#自平衡二叉树"},{"categories":["mysql"],"content":" B树我们平时看到 B+树 还有 B-树，不免就会将 B-树 读成 “B减树” ，但 B-树 其 - 横线只是连接符，所以 B-树 就是称为 B树。 自平衡二叉树虽然查找的时间复杂度在O(logn)，前面也说过它本身是一个二叉树，每个节点只能有2个子节点，那么随着数据量增大的时候，节点个数越多，树高度也会增高（也就是树的深度越深），增加磁盘I/O次数，影响查询效率。 那么你如果从树形结构的二叉树这一路的进阶过程中可以看到，二叉树每一次为了解决一个新的问题都会创造出新的 bug （或者创造一个又个的痛点）。 看到这就不难猜到，B树的出现可以解决树高度的问题。之所以是B树，而并不是名称中\"xxx二叉树\"，就是它不再限制一个父节点中只能有两个子节点，而是允许 M 个子节点（M \u003e 2） 。不仅如此，B树的一个节点可以存储多个元素，相比较于前面的那些二叉树数据结构又将整体的树高度降低了。 B 树的节点可以包含有多个子节点，所以 B树是一棵多叉树，它的每一个节点包含的最多子节点数量的称为B树的阶。如下图是一颗 3 阶的 B树。 图片 上图中每一个节点称为页，在 mysql 中数据读取的基本单位是页，而页就是我们上面所说的磁盘块。磁盘块中的 p 节点是指向子节点的指针。指针在树结构中都有，在前面的二叉树中也都是有的。 那我们来看一下上图所示，当一颗 3 阶的B树查找 90 这个的元素时的流程是怎么样的？ 先从根节点出发，也就是 磁盘块1，判断 90 在17 ~ 35之间，通过磁盘块1中的指针 p3 找到磁盘块4。还是按照原来的步骤，在磁盘块4中的65 ~ 87之间相比较，最后磁盘4的指针p3 找到磁盘块11。也就找到有匹配90的键值。 可以发现一颗3阶的B树在查找叶子节点时，由于树高度只有 3，所以查找过程最多只需要3次的磁盘 I/O 操作。 数据量不大时可能不太真切。但当数据量大时，节点也会随着增多；此时如果还是前面的自平衡二叉树的场景下，由于二叉树只能最多2 个叶子节点的约束，也只能纵向去的去扩展子节点，树的高度会很高，意味着需要更多的操作磁盘I/O次数。而B树则可以通过横向扩展节点从而降低树的高度，所以效率自然要比二叉树效率更高。（直白说就是变矮胖了） 看到这，相信你也知道如果 B树 这么适合，也就没有接下来 B+ 树的什么事了。 接着，那为什么不用B树，而用了B+树呢？ 你看，B树其实已经满足了我们最前面所要满足的条件，减少磁盘I/O操作，同时支持按区间查找。但注意，虽然B树支持按区间查找，但并不高效。 例如上面的例子中，B树能高效的通过等值查询 90这个值，但不方便查询出一个区间内比如，3 ~ 10 区间内所有数的结果。因为当B树做范围查询时需要使用中序遍历，那么父节点和子节点也就需要不断的来回切换涉及了多个节点会给磁盘 I/O 带来很多负担。 ","date":"2022-05-20","objectID":"/why-mysql-index-use-btree/:3:4","series":null,"tags":["mysql"],"title":"MySQL 为什么使用 B+ 树索引","uri":"/why-mysql-index-use-btree/#b树"},{"categories":["mysql"],"content":" B+树B+树从 + 的符号可以看出是B树的升级版，MySQL 中 innoDB 引擎中的索引底层数据结构采用的正是 B+树。 B+树相比于B树，做了这样的升级：B+树中的非叶子节点都不存储数据，而是只作为索引。由叶子节点存放整棵树的所有数据。而叶子节点之间构成一个从小到大有序的链表互相指向相邻的叶子节点，也就是叶子节点之间形成了有序的双向链表。如下图B+树的结构。 图片 （B+树是不是有点像前面的跳表，数据底层是数据，上层都是按底层区间构成的索引层，只不过它不像跳表是纵向扩展，而是横向扩展的“跳表”。这么做的好处即减少磁盘的IO操作又提高了范围查找的效率。） 接着再来看B+树的插入和删除，B+树做了大量冗余节点，从上面可以发现父节点的所有元素都会在子节点中出现，这样当删除一个节点时，可以直接从叶子节点中删除，这样效率更快。 B树相比于B+树，B树没有冗余节点，删除节点时会发生复杂的树变形，而B+树有冗余节点，不会涉及到复杂的树变形。而且B+树的插入也是如此，最多只涉及树的一条分支路径。B+树也不用更多复杂算法，可以类似黑红树的旋转去自动平衡。 ","date":"2022-05-20","objectID":"/why-mysql-index-use-btree/:3:5","series":null,"tags":["mysql"],"title":"MySQL 为什么使用 B+ 树索引","uri":"/why-mysql-index-use-btree/#b树-1"},{"categories":["mysql"],"content":" 总结从文章的题目开始就是一个问题，我们并没有直接回答为什么MySQL 采用B+树作为索引的答案，而是相反的问出了两个疑惑我，但不知道有没有疑惑到你的问题，一个问题是那么多数据结构，为什么选树结构？另一个问题是那么多的树结构，又为什么偏偏采用B+树？ 要得到果，得先知道因，我们从两个方面开始出发，因为MySQL的数据是放在磁盘的，而磁盘的处理速度是毫秒级别的，如果在磁盘IO做过多查询操作，会给查询带来负担，所以要尽少在磁盘 I/O 操作中做查询。另一个是从索引本身的首要目的，要能按区间高效地范围查找。 有了因，我们就开始去探索果，我们就可以先来回答第一个问题，“那么多数据结构，为什么选树结构？” 在其他数据结构中按逻辑结构的线性结构有哈希表和跳表。哈希表底层基于数组，虽然可以高效查询，但是只能等值查询，而不能支持范围查询。而跳表底层是链表，通过索引层可以实现高效的区间查询，但是随着数据量的递增，索引层也随着数据量的增多而增加。所以采用树的数据结构，树结构其特性决定了遍历数据方式本身就纯天然的支持按区间查询。树结构在插入等操作不用线性结构数组的开销，所以更适合插入更新等动态操作的数据结构。 接着我们另一个问题，“那么多的树结构，又为什么偏偏采用B+树？” 我们从树结构中父节点最多只能有两个子节点的二叉树，再从二叉树加上二分查找的二叉查找树，二叉树展现了高效的查询能力；但二叉查找树在极端情况下会退化成链表。所以进阶到自平衡二叉树，自平衡二叉树约束了每个节点的左右子树相差不能大于1。但是二叉树因为只能最多是两个子节点，所以树的高度过高会导致磁盘做过多I/O的查询操作负担。 所以最后真正到了B树，B树是多叉树，但只能高效单查询，并不能高效区间查询。所以才有B+树，B+树是B树的升级，所有非叶子节点都用来做索引，只有叶子节点存储数据而且是有序双向的链表，树节点做了冗余，相比于B树既能支持高效的区间查询，插入和删除都比B树更加出色。 ","date":"2022-05-20","objectID":"/why-mysql-index-use-btree/:4:0","series":null,"tags":["mysql"],"title":"MySQL 为什么使用 B+ 树索引","uri":"/why-mysql-index-use-btree/#总结"},{"categories":["mysql"],"content":" 参考 MySQL索引为什么要用B+树实现 MySQL 为什么使用 B+ 树来作索引？MySQL 为什么使用 B+ 树来作索引？ B+树详解 ","date":"2022-05-20","objectID":"/why-mysql-index-use-btree/:5:0","series":null,"tags":["mysql"],"title":"MySQL 为什么使用 B+ 树索引","uri":"/why-mysql-index-use-btree/#参考"},{"categories":["mysql"],"content":"xiaobinqt,mysql workbench 查看 Triggers 触发器","date":"2022-04-20","objectID":"/mysql-workbench-show-triggers/","series":null,"tags":["mysql"],"title":"mysql workbench 查看触发器","uri":"/mysql-workbench-show-triggers/"},{"categories":["mysql"],"content":" mysql workbench 是官方推荐的数据库工具，用了很长时间却一直不知道触发器在哪儿😢。 触发器是对单个表的操作，而不是整个数据库的操作，所以 Alter Table 就可以看到触发器： 图1 图2 点这个扳手图标也可以看到触发器，跟 Alter Table 效果一样： 图3 ","date":"2022-04-20","objectID":"/mysql-workbench-show-triggers/:0:0","series":null,"tags":["mysql"],"title":"mysql workbench 查看触发器","uri":"/mysql-workbench-show-triggers/#"},{"categories":["mysql"],"content":" 参考 MySQL Workbench : How to Configure Triggers in MySQL ","date":"2022-04-20","objectID":"/mysql-workbench-show-triggers/:1:0","series":null,"tags":["mysql"],"title":"mysql workbench 查看触发器","uri":"/mysql-workbench-show-triggers/#参考"},{"categories":["开发者手册"],"content":"xiaobinqt,Redis 缓存击穿,缓存穿透,缓存雪崩,Redis","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/"},{"categories":["开发者手册"],"content":" 缓存击穿高并发流量，访问的这个数据是热点数据，请求的数据在 DB 中存在，但是 Redis 存的那一份已经过期，后端需要从 DB 从加载数据并写到 Redis。 总结起来就是：单一热点数据、高并发、数据失效。 缓存击穿 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:1:0","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/#缓存击穿"},{"categories":["开发者手册"],"content":" 解决方案 过期时间➕随机值对于热点数据，我们不设置过期时间，这样就可以把请求都放在缓存中处理，充分把 Redis 高吞吐量性能利用起来。或者过期时间再加一个随机值。 设计缓存的过期时间时，使用公式：过期时间 = baes 时间 + 随机时间。 即相同业务数据写缓存时，在基础过期时间之上，再加一个随机的过期时间，让数据在未来一段时间内慢慢过期，避免瞬时全部过期，对 DB 造成过大压力。 预热预先把热门数据提前存入 Redis 中，并设热门数据的过期时间超大值。 使用锁当发现缓存失效的时候，不是立即从数据库加载数据。 而是先获取分布式锁，获取锁成功才执行数据库查询和写数据到缓存的操作，获取锁失败，则说明当前有线程在执行数据库查询操作，当前线程睡眠一段时间再重试。这样只让一个请求去数据库读取数据。 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:1:1","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/#解决方案"},{"categories":["开发者手册"],"content":" 解决方案 过期时间➕随机值对于热点数据，我们不设置过期时间，这样就可以把请求都放在缓存中处理，充分把 Redis 高吞吐量性能利用起来。或者过期时间再加一个随机值。 设计缓存的过期时间时，使用公式：过期时间 = baes 时间 + 随机时间。 即相同业务数据写缓存时，在基础过期时间之上，再加一个随机的过期时间，让数据在未来一段时间内慢慢过期，避免瞬时全部过期，对 DB 造成过大压力。 预热预先把热门数据提前存入 Redis 中，并设热门数据的过期时间超大值。 使用锁当发现缓存失效的时候，不是立即从数据库加载数据。 而是先获取分布式锁，获取锁成功才执行数据库查询和写数据到缓存的操作，获取锁失败，则说明当前有线程在执行数据库查询操作，当前线程睡眠一段时间再重试。这样只让一个请求去数据库读取数据。 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:1:1","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/#过期时间随机值"},{"categories":["开发者手册"],"content":" 解决方案 过期时间➕随机值对于热点数据，我们不设置过期时间，这样就可以把请求都放在缓存中处理，充分把 Redis 高吞吐量性能利用起来。或者过期时间再加一个随机值。 设计缓存的过期时间时，使用公式：过期时间 = baes 时间 + 随机时间。 即相同业务数据写缓存时，在基础过期时间之上，再加一个随机的过期时间，让数据在未来一段时间内慢慢过期，避免瞬时全部过期，对 DB 造成过大压力。 预热预先把热门数据提前存入 Redis 中，并设热门数据的过期时间超大值。 使用锁当发现缓存失效的时候，不是立即从数据库加载数据。 而是先获取分布式锁，获取锁成功才执行数据库查询和写数据到缓存的操作，获取锁失败，则说明当前有线程在执行数据库查询操作，当前线程睡眠一段时间再重试。这样只让一个请求去数据库读取数据。 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:1:1","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/#预热"},{"categories":["开发者手册"],"content":" 解决方案 过期时间➕随机值对于热点数据，我们不设置过期时间，这样就可以把请求都放在缓存中处理，充分把 Redis 高吞吐量性能利用起来。或者过期时间再加一个随机值。 设计缓存的过期时间时，使用公式：过期时间 = baes 时间 + 随机时间。 即相同业务数据写缓存时，在基础过期时间之上，再加一个随机的过期时间，让数据在未来一段时间内慢慢过期，避免瞬时全部过期，对 DB 造成过大压力。 预热预先把热门数据提前存入 Redis 中，并设热门数据的过期时间超大值。 使用锁当发现缓存失效的时候，不是立即从数据库加载数据。 而是先获取分布式锁，获取锁成功才执行数据库查询和写数据到缓存的操作，获取锁失败，则说明当前有线程在执行数据库查询操作，当前线程睡眠一段时间再重试。这样只让一个请求去数据库读取数据。 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:1:1","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/#使用锁"},{"categories":["开发者手册"],"content":" 缓存穿透数据库本就没有这个数据，请求直奔数据库，缓存系统形同虚设。 大量请求的 key 根本不存在于缓存中也不存在数据库，导致请求直接到了数据库上，根本没有经过缓存这一层，对数据库造成压力而影响正常服务。 缓存穿透 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:2:0","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/#缓存穿透"},{"categories":["开发者手册"],"content":" 解决方案最基本的首先就是做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。 缓存无效的 key如果缓存和数据库都查不到某个 key 的数据就写一个到 Redis 中去并设置过期时间。这种方式可以解决请求的 key 变化不频繁的情况，如果黑客恶意攻击，每次构建不同的请求 key，会导致 Redis 中缓存大量无效的 key 。这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，应该尽量将无效的 key 的过期时间设置短一点比如 1 分钟。 布隆过滤器布隆过滤器是一个非常神奇的数据结构，通过它我们可以非常方便地判断一个给定数据是否存在于海量数据中。我们需要的仅仅就是判断 key 是否合法。 具体是这样做的：把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话再走其他的判断流程。 布隆过滤器 需要注意的是布隆过滤器可能会存在误判的情况。 布隆过滤器说某个元素存在，小概率会误判。布隆过滤器说某个元素不在，这个元素一定不在。 这是因为，当一个元素加入布隆过滤器中的时候，会进行如下操作： 使用布隆过滤器中的哈希函数对元素值进行计算，得到哈希值（有几个哈希函数得到几个哈希值）。 根据得到的哈希值，在位数组中把对应下标的值置为 1。 当需要判断一个元素是否存在于布隆过滤器的时候，会进行如下操作： 对给定元素再次进行相同的哈希计算； 得到值之后判断位数组中的每个元素是否都为 1，如果值都为 1，那么说明这个值在布隆过滤器中，如果存在一个值不为 1，说明该元素不在布隆过滤器中。 然而，一定会出现这样一种情况：不同的字符串可能哈希出来的位置相同。我们可以适当增加位数组大小或者调整我们的哈希函数来降低概率。 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:2:1","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/#解决方案-1"},{"categories":["开发者手册"],"content":" 解决方案最基本的首先就是做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。 缓存无效的 key如果缓存和数据库都查不到某个 key 的数据就写一个到 Redis 中去并设置过期时间。这种方式可以解决请求的 key 变化不频繁的情况，如果黑客恶意攻击，每次构建不同的请求 key，会导致 Redis 中缓存大量无效的 key 。这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，应该尽量将无效的 key 的过期时间设置短一点比如 1 分钟。 布隆过滤器布隆过滤器是一个非常神奇的数据结构，通过它我们可以非常方便地判断一个给定数据是否存在于海量数据中。我们需要的仅仅就是判断 key 是否合法。 具体是这样做的：把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话再走其他的判断流程。 布隆过滤器 需要注意的是布隆过滤器可能会存在误判的情况。 布隆过滤器说某个元素存在，小概率会误判。布隆过滤器说某个元素不在，这个元素一定不在。 这是因为，当一个元素加入布隆过滤器中的时候，会进行如下操作： 使用布隆过滤器中的哈希函数对元素值进行计算，得到哈希值（有几个哈希函数得到几个哈希值）。 根据得到的哈希值，在位数组中把对应下标的值置为 1。 当需要判断一个元素是否存在于布隆过滤器的时候，会进行如下操作： 对给定元素再次进行相同的哈希计算； 得到值之后判断位数组中的每个元素是否都为 1，如果值都为 1，那么说明这个值在布隆过滤器中，如果存在一个值不为 1，说明该元素不在布隆过滤器中。 然而，一定会出现这样一种情况：不同的字符串可能哈希出来的位置相同。我们可以适当增加位数组大小或者调整我们的哈希函数来降低概率。 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:2:1","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/#缓存无效的-key"},{"categories":["开发者手册"],"content":" 解决方案最基本的首先就是做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。 缓存无效的 key如果缓存和数据库都查不到某个 key 的数据就写一个到 Redis 中去并设置过期时间。这种方式可以解决请求的 key 变化不频繁的情况，如果黑客恶意攻击，每次构建不同的请求 key，会导致 Redis 中缓存大量无效的 key 。这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，应该尽量将无效的 key 的过期时间设置短一点比如 1 分钟。 布隆过滤器布隆过滤器是一个非常神奇的数据结构，通过它我们可以非常方便地判断一个给定数据是否存在于海量数据中。我们需要的仅仅就是判断 key 是否合法。 具体是这样做的：把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话再走其他的判断流程。 布隆过滤器 需要注意的是布隆过滤器可能会存在误判的情况。 布隆过滤器说某个元素存在，小概率会误判。布隆过滤器说某个元素不在，这个元素一定不在。 这是因为，当一个元素加入布隆过滤器中的时候，会进行如下操作： 使用布隆过滤器中的哈希函数对元素值进行计算，得到哈希值（有几个哈希函数得到几个哈希值）。 根据得到的哈希值，在位数组中把对应下标的值置为 1。 当需要判断一个元素是否存在于布隆过滤器的时候，会进行如下操作： 对给定元素再次进行相同的哈希计算； 得到值之后判断位数组中的每个元素是否都为 1，如果值都为 1，那么说明这个值在布隆过滤器中，如果存在一个值不为 1，说明该元素不在布隆过滤器中。 然而，一定会出现这样一种情况：不同的字符串可能哈希出来的位置相同。我们可以适当增加位数组大小或者调整我们的哈希函数来降低概率。 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:2:1","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/#布隆过滤器"},{"categories":["开发者手册"],"content":" 缓存雪崩缓存在同一时间大面积的失效，后面的请求都直接落到了数据库上，造成数据库短时间内承受大量请求。或是有一些被大量访问数据（热点缓存）在某一时刻大面积失效，导致对应的请求直接落到了数据库上。 而出现该原因主要有两种： 大量热点数据同时过期，导致大量请求需要查询数据库并写到缓存。 Redis 故障宕机，缓存系统异常。 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:3:0","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/#缓存雪崩"},{"categories":["开发者手册"],"content":" 缓存大量数据同时过期数据保存在缓存系统并设置了过期时间，但是由于在同时一刻，大量数据同时过期。系统就把请求全部打到数据库获取数据，并发量大的话就会导致数据库压力激增。 缓存雪崩是发生在大量数据同时失效的场景，而缓存击穿是在某个热点数据失效的场景，这是他们最大的区别。 缓存大量数据同时过期 过期时间添加随机值要避免给大量的数据设置一样的过期时间，过期时间 = baes 时间+ 随机时间（较小的随机数，比如随机增加 1~5 分钟）。 这样一来，就不会导致同一时刻热点数据全部失效，同时过期时间差别也不会太大，既保证了相近时间失效，又能满足业务需求。 接口限流当访问的不是核心数据的时候，在查询的方法上加上接口限流保护。比如设置 10000 req/s。如果访问的是核心数据接口，缓存不存在允许从数据库中查询并设置到缓存中。这样的话，只有部分请求会发送到数据库，减少了压力。 限流，就是指，我们在业务系统的请求入口前端控制每秒进入系统的请求数，避免过多的请求被发送到数据库。 限流 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:3:1","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/#缓存大量数据同时过期"},{"categories":["开发者手册"],"content":" 缓存大量数据同时过期数据保存在缓存系统并设置了过期时间，但是由于在同时一刻，大量数据同时过期。系统就把请求全部打到数据库获取数据，并发量大的话就会导致数据库压力激增。 缓存雪崩是发生在大量数据同时失效的场景，而缓存击穿是在某个热点数据失效的场景，这是他们最大的区别。 缓存大量数据同时过期 过期时间添加随机值要避免给大量的数据设置一样的过期时间，过期时间 = baes 时间+ 随机时间（较小的随机数，比如随机增加 1~5 分钟）。 这样一来，就不会导致同一时刻热点数据全部失效，同时过期时间差别也不会太大，既保证了相近时间失效，又能满足业务需求。 接口限流当访问的不是核心数据的时候，在查询的方法上加上接口限流保护。比如设置 10000 req/s。如果访问的是核心数据接口，缓存不存在允许从数据库中查询并设置到缓存中。这样的话，只有部分请求会发送到数据库，减少了压力。 限流，就是指，我们在业务系统的请求入口前端控制每秒进入系统的请求数，避免过多的请求被发送到数据库。 限流 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:3:1","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/#过期时间添加随机值"},{"categories":["开发者手册"],"content":" 缓存大量数据同时过期数据保存在缓存系统并设置了过期时间，但是由于在同时一刻，大量数据同时过期。系统就把请求全部打到数据库获取数据，并发量大的话就会导致数据库压力激增。 缓存雪崩是发生在大量数据同时失效的场景，而缓存击穿是在某个热点数据失效的场景，这是他们最大的区别。 缓存大量数据同时过期 过期时间添加随机值要避免给大量的数据设置一样的过期时间，过期时间 = baes 时间+ 随机时间（较小的随机数，比如随机增加 1~5 分钟）。 这样一来，就不会导致同一时刻热点数据全部失效，同时过期时间差别也不会太大，既保证了相近时间失效，又能满足业务需求。 接口限流当访问的不是核心数据的时候，在查询的方法上加上接口限流保护。比如设置 10000 req/s。如果访问的是核心数据接口，缓存不存在允许从数据库中查询并设置到缓存中。这样的话，只有部分请求会发送到数据库，减少了压力。 限流，就是指，我们在业务系统的请求入口前端控制每秒进入系统的请求数，避免过多的请求被发送到数据库。 限流 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:3:1","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/#接口限流"},{"categories":["开发者手册"],"content":" Redis 故障一旦 Redis 故障或宕机，会导致大量请求打到数据库，从而发生缓存雪崩。对于 Redis 故障，主要有以下两种解决方案。 服务熔断和限流在业务系统中，针对高并发的使用服务熔断来有损提供服务从而保证系统的可用性。 服务熔断就是当从缓存获取数据发现异常，则直接返回错误数据给前端，防止所有流量打到数据库导致宕机。 服务熔断和限流属于在发生了缓存雪崩，如何降低雪崩对数据库造成的影响的方案。 高可用缓存集群缓存系统一定要构建一套 Redis 高可用集群，比如 Redis 哨兵集群 TODO 或者 Redis Cluster 集群 TODO，如果 Redis 的主节点故障宕机了，从节点还可以切换成为主节点，继续提供缓存服务，避免了由于缓存实例宕机而导致的缓存雪崩问题。 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:3:2","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/#redis-故障"},{"categories":["开发者手册"],"content":" Redis 故障一旦 Redis 故障或宕机，会导致大量请求打到数据库，从而发生缓存雪崩。对于 Redis 故障，主要有以下两种解决方案。 服务熔断和限流在业务系统中，针对高并发的使用服务熔断来有损提供服务从而保证系统的可用性。 服务熔断就是当从缓存获取数据发现异常，则直接返回错误数据给前端，防止所有流量打到数据库导致宕机。 服务熔断和限流属于在发生了缓存雪崩，如何降低雪崩对数据库造成的影响的方案。 高可用缓存集群缓存系统一定要构建一套 Redis 高可用集群，比如 Redis 哨兵集群 TODO 或者 Redis Cluster 集群 TODO，如果 Redis 的主节点故障宕机了，从节点还可以切换成为主节点，继续提供缓存服务，避免了由于缓存实例宕机而导致的缓存雪崩问题。 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:3:2","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/#服务熔断和限流"},{"categories":["开发者手册"],"content":" Redis 故障一旦 Redis 故障或宕机，会导致大量请求打到数据库，从而发生缓存雪崩。对于 Redis 故障，主要有以下两种解决方案。 服务熔断和限流在业务系统中，针对高并发的使用服务熔断来有损提供服务从而保证系统的可用性。 服务熔断就是当从缓存获取数据发现异常，则直接返回错误数据给前端，防止所有流量打到数据库导致宕机。 服务熔断和限流属于在发生了缓存雪崩，如何降低雪崩对数据库造成的影响的方案。 高可用缓存集群缓存系统一定要构建一套 Redis 高可用集群，比如 Redis 哨兵集群 TODO 或者 Redis Cluster 集群 TODO，如果 Redis 的主节点故障宕机了，从节点还可以切换成为主节点，继续提供缓存服务，避免了由于缓存实例宕机而导致的缓存雪崩问题。 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:3:2","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/#高可用缓存集群"},{"categories":["开发者手册"],"content":" 总结 缓存穿透指的是数据库本就没有这个数据，请求直奔数据库，缓存系统形同虚设。 缓存击穿（失效）指的是数据库有数据，缓存本应该也有数据，但是缓存过期了，Redis 这层流量防护屏障被击穿了，请求直奔数据库。 缓存雪崩指的是大量的热点数据无法在 Redis 缓存中处理（大面积热点数据缓存失效、Redis 宕机），流量全部打到数据库，导致数据库极大压力。 ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:4:0","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/#总结"},{"categories":["开发者手册"],"content":" 参考 Redis 缓存击穿（失效）、缓存穿透、缓存雪崩怎么解决？ ","date":"2022-04-13","objectID":"/redis-break-pierce-avalanche/:5:0","series":null,"tags":["redis"],"title":"Redis 缓存击穿、缓存穿透、缓存雪崩","uri":"/redis-break-pierce-avalanche/#参考"},{"categories":["golang"],"content":"golang GC 原理,Go 垃圾回收,Go 垃圾标记清除算法,golang 三色标记法,golang 屏障机制,go 垃圾回收算法,go 插入/删除屏障,golang 混合写屏障,垃圾回收 STW,STW","date":"2022-04-06","objectID":"/go-gc/","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/"},{"categories":["golang"],"content":"垃圾回收（Garbage Collection，GC）是编程语言中提供的自动的内存管理机制，自动释放不需要的内存对象，让出存储器资源。GC 过程中无需程序员手动执行。 GC 机制在现代很多编程语言都支持，GC 能力的性能与优劣也是不同语言之间对比度指标之一。 ","date":"2022-04-06","objectID":"/go-gc/:0:0","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#"},{"categories":["golang"],"content":" 堆和栈栈：由操作系统自动分配释放，存放函数的参数值，局部变量的值等。 堆：一般由程序员分配和释放，若程序员不释放，程序结束时可能由 OS 回收。 栈使用的是一级缓存，他们通常都是被调用时处于存储空间中，调用完毕立即释放。 堆则是存放在二级缓存中，生命周期由虚拟机的垃圾回收算法来决定，但并不是一旦成为孤儿对象就能被回收。 申请到栈内存好处：函数返回直接释放，不会引起垃圾回收，对性能没有影响。 ","date":"2022-04-06","objectID":"/go-gc/:1:0","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#堆和栈"},{"categories":["golang"],"content":" 标记清除算法Go v1.3 之前使用普通的标记-清除（mark and sweep）算法，主要有两个主要的步骤： 标记(Mark phase)，找出不可达的对象，然后做上标记。 清除(Sweep phase)，回收标记好的对象。 ","date":"2022-04-06","objectID":"/go-gc/:2:0","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#标记清除算法"},{"categories":["golang"],"content":" 第一步暂停程序业务逻辑， 分类出可达和不可达的对象，然后做上标记。 程序与对象的可达关系 👆图中表示是程序与对象的可达关系，目前程序的可达对象有对象 1-2-3，对象 4-7 等五个对象。 ","date":"2022-04-06","objectID":"/go-gc/:2:1","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#第一步"},{"categories":["golang"],"content":" 第二步开始标记，程序找出它所有可达的对象，并做上标记👇。 找出可达对象 对象 1-2-3 、对象 4-7 等五个对象被做上标记。 ","date":"2022-04-06","objectID":"/go-gc/:2:2","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#第二步"},{"categories":["golang"],"content":" 第三步标记完了之后，然后开始清除未标记的对象。 清除对象 对象 5，6 不可达，被 GC 清除。 操作简单，但是，mark and sweep 算法在执行的时候，需要程序暂停！即 STW（stop the world），STW 的过程中，CPU 不执行用户代码，全部用于垃圾回收，这个过程的影响很大，所以 STW 也是一些回收机制最大的难题和希望优化的点。 在执行第三步的这段时间，程序会暂定停止任何工作，卡在那等待回收执行完毕。 ","date":"2022-04-06","objectID":"/go-gc/:2:3","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#第三步"},{"categories":["golang"],"content":" 第四步停止暂停，让程序继续执行。然后循环重复这个过程，直到 process 程序生命周期结束。 ","date":"2022-04-06","objectID":"/go-gc/:2:4","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#第四步"},{"categories":["golang"],"content":" 缺点与优化标记清除算法明了，过程鲜明干脆，但是也有非常严重的问题，就是 STW，让程序暂停，程序出现卡顿。 Go V1.3版本之前就是用这种方式来实施的。执行 GC 的基本流程就是首先启动 STW 暂停，然后执行标记，再执行数据回收，最后停止 STW ，如下图👇 STW 从👆来看，全部的 GC 时间都是包裹在 STW 范围之内的，这样貌似程序暂停的时间过长，影响程序的运行性能。所以Go v1.3 做了简单的优化，将 STW 的步骤提前，减少 STW 暂停的时间范围 👇 STW优化 主要是将 STW 的步骤提前了一步，因为在 Sweep 清除的时候，可以不需要 STW 停止，因为这些对象已经是不可达对象了，不会出现回收写冲突等问题，清除操作和用户逻辑可以并发。 但是无论怎么优化，Go v1.3 都面临这个一个重要问题，就是 mark-and-sweep 算法会暂停整个程序 。 ","date":"2022-04-06","objectID":"/go-gc/:2:5","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#缺点与优化"},{"categories":["golang"],"content":" 有STW的三色标记法Go 中的垃圾回收主要应用三色标记法，GC 过程和其他用户 goroutine 可并发运行，但需要一定时间的 STW。 所谓三色标记法实际上就是通过三个阶段的标记来确定需要清除的对象有哪些。 ","date":"2022-04-06","objectID":"/go-gc/:3:0","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#有stw的三色标记法"},{"categories":["golang"],"content":" 第一步每次新创建的对象，默认的颜色都是标记为“白色”👇 白色对象 上图所示，我们的程序可抵达的内存对象关系如左图所示，右边的标记表，是用来记录目前每个对象的标记颜色分类。 这里所谓“程序”，是一些对象的根节点集合。如果我们将“程序”展开，会得到类似如下的表现形式： 程序的根节点集合展开 ","date":"2022-04-06","objectID":"/go-gc/:3:1","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#第一步-1"},{"categories":["golang"],"content":" 第二步每次 GC 回收开始， 会从根节点开始遍历所有对象，把遍历到的对象从白色集合放入“灰色”集合： 遍历根对象 本次遍历是一次遍历，非递归形式，是从程序初次可抵达的对象遍历一层，如上图所示，当前可抵达的对象是对象1和对象4，那么自然本轮遍历结束，对象1和对象4就会被标记为灰色，灰色标记表就会多出这两个对象。 ","date":"2022-04-06","objectID":"/go-gc/:3:2","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#第二步-1"},{"categories":["golang"],"content":" 第三步遍历灰色集合，将灰色对象引用的对象从白色集合放入灰色集合，之后将此灰色对象放入黑色集合： 遍历_2 这一次遍历是只扫描灰色对象，将灰色对象的第一层遍历可抵达的对象由白色变为灰色，如：对象2、对象7。 而之前的灰色对象1 和对象4 则会被标记为黑色，同时由灰色标记表移动到黑色标记表中。 ","date":"2022-04-06","objectID":"/go-gc/:3:3","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#第三步-1"},{"categories":["golang"],"content":" 第四步重复第三步， 直到灰色中无任何对象，如图👇所示： 遍历_3 遍历_4 当我们全部的可达对象都遍历完后，灰色标记表将不再存在灰色对象。 目前全部内存的数据只有两种颜色，黑色和白色。那么，黑色对象就是我们程序逻辑可达（需要的）对象，这些数据是目前支撑程序正常业务运行的，是合法的有用数据，不可删除。白色的对象是全部不可达对象，目前程序逻辑并不依赖他们，那么白色对象就是内存中目前的垃圾数据，需要被清除。 ","date":"2022-04-06","objectID":"/go-gc/:3:4","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#第四步-1"},{"categories":["golang"],"content":" 第五步回收所有的白色标记表的对象，也就是回收垃圾，如图所示👇： GC 回收 将全部的白色对象进行删除回收，剩下的就是全部依赖的黑色对象。 这里面可能会有很多并发流程均会被扫描，执行并发流程的内存可能相互依赖，为了在 GC 过程中保证数据的安全，在开始三色标记之前就会加上 STW，在扫描确定黑白对象之后再放开 STW。但是很明显这样的 GC 扫描的性能实在是太低了。 所以现在的三色标记法还是会 STW。 ","date":"2022-04-06","objectID":"/go-gc/:3:5","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#第五步"},{"categories":["golang"],"content":" 没有STW的三色标记法如果没有 STW，那么也就不会再存在性能上的问题，那么假设如果三色标记法不加入STW会发生什么事情❓ 还是基于上述的三色标记法来分析，他是一定要依赖 STW 的，因为如果不暂停程序，程序的逻辑可能会改变对象的引用关系，这种动作如果在标记阶段做了修改，会影响标记结果的正确性。 来看看一个场景，如果三色标记法，标记过程不使用 STW 将会发生什么事情❓ 我们把初始状态设置为已经经历了第一轮扫描，目前黑色的有对象1和对象4，灰色的有对象2和对象7，其他的为白色对象，且对象2是通过指针 p 指向对象3的，如下图所示。 no ST2 01 现在如果三色标记过程不启动 STW，那么在 GC 扫描过程中，任意的对象均可能发生读写操作，如下图所示，在还没有扫描到对象2的时候，已经标记为黑色的对象4，此时创建指针 q，并且指向白色的对象3。 no ST2 02 与此同时灰色的对象2将指针 p 移除，那么白色的对象3实则是被挂在了已经扫描完成的黑色的对象4下，如下图所示。 no ST2 03 然后我们正常执行三色标记的算法逻辑，将所有灰色的对象标记为黑色，那么对象2和对象7就被标记成了黑色，如下图所示。 no ST2 04 那么就执行了三色标记的最后一步，将所有白色对象当做垃圾进行回收，如图所示。 no ST2 05 但是最后我们才发现，本来是对象4合法引用的对象3，却被GC给“误杀”回收掉了。 可以看出，有两种情况，在三色标记法中，是不希望被发生的。 👉 一个白色对象被黑色对象引用 （白色被挂在黑色下） 👉 灰色对象与它之间可达关系的白色对象遭到破坏 （灰色同时丢了该白色） 如果当以上两个条件同时满足时，就会出现对象丢失现象！ 并且，上面所示的场景中，如果示例中的白色对象3还有很多下游对象的话， 也会一并都清理掉。 为了防止这种现象的发生，最简单的方式就是 STW，直接禁止掉其他用户程序对对象引用关系的干扰，但是 STW的过程有明显的资源浪费，对所有的用户程序都有很大影响 。 那么是否可以在保证对象不丢失的情况下合理的尽可能的提高 GC 效率，减少 STW 时间呢❓ 答案是可以的，我们只要使用一种机制，尝试去破坏上面的两个必要条件就可以了。 ","date":"2022-04-06","objectID":"/go-gc/:4:0","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#没有stw的三色标记法"},{"categories":["golang"],"content":" 屏障机制如果让 GC 回收器，满足下面两种情况之一时，即可保证对象不丢失。这两种方式就是强三色不变式和弱三色不变式。 ","date":"2022-04-06","objectID":"/go-gc/:5:0","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#屏障机制"},{"categories":["golang"],"content":" 强三色不变式强三色不变式实际上是强制性的不允许黑色对象引用白色对象，这样就不会出现有白色对象被误删的情况。 强三色不变式 ","date":"2022-04-06","objectID":"/go-gc/:5:1","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#强三色不变式"},{"categories":["golang"],"content":" 弱三色不变式所有被黑色对象引用的白色对象都处于灰色保护状态。 弱三色不变式 弱三色不变式强调，黑色对象可以引用白色对象，但是这个白色对象必须存在其他灰色对象对它的引用，或者可达它的链路上游存在灰色对象。 这样实则是黑色对象引用白色对象，白色对象处于一个危险被删除的状态，但是由于上游灰色对象的引用，可以保护该白色对象，使其安全。 为了遵循上述的两个方式，GC 算法演进到两种屏障方式，分别是插入屏障和删除屏障。 ","date":"2022-04-06","objectID":"/go-gc/:5:2","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#弱三色不变式"},{"categories":["golang"],"content":" 插入屏障具体操作： 在 A 对象引用 B 对象的时候，B 对象被标记为灰色。将 B 挂在 A 下游，B 必须被标记为灰色。 满足：强三色不变式。不存在黑色对象引用白色对象的情况了， 因为白色会强制变成灰色。 伪代码如下： 添加下游对象(当前下游对象slot, 新下游对象ptr) { //1 标记灰色(新下游对象ptr) //2 当前下游对象slot = 新下游对象ptr } 场景： A.添加下游对象(nil, B) //A 之前没有下游， 新添加一个下游对象B， B被标记为灰色 A.添加下游对象(C, B) //A 将下游对象C 更换为B， B被标记为灰色 这段伪代码逻辑就是写屏障，我们知道，黑色对象的内存槽有两种位置，栈和堆。 栈空间的特点是容量小，但是要求响应速度快，因为函数调用弹出频繁使用，所以插入屏障机制，在栈空间的对象操作中不使用，而仅仅使用在堆空间对象的操作中。 接下来，我们用几张图，来模拟一下整个详细的过程，希望能更可观的看清整体流程。 插入屏障01 插入屏障02 插入屏障03 插入屏障04 插入屏障05 插入屏障06 但是如果栈不添加，当全部三色标记扫描之后，栈上有可能依然存在白色对象被引用的情况（如上图的对象9）。 所以要对栈重新进行三色标记扫描，但这次为了对象不丢失，要对本次标记扫描启动 STW 暂停，直到栈空间的三色标记结束。 插入屏障07 插入屏障08 插入屏障09 最后将栈和堆空间扫描剩余的全部白色节点清除，这次 STW 大约的时间在 10~100ms 间。 插入屏障10 ","date":"2022-04-06","objectID":"/go-gc/:5:3","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#插入屏障"},{"categories":["golang"],"content":" 删除屏障具体操作：被删除的对象，如果自身为灰色或者白色，那么被标记为灰色。 满足：弱三色不变式，保护灰色对象到白色对象的路径不会断。 伪代码： 添加下游对象(当前下游对象slot， 新下游对象ptr) { //1 if (当前下游对象slot是灰色 || 当前下游对象slot是白色) { 标记灰色(当前下游对象slot) //slot为被删除对象， 标记为灰色 } //2 当前下游对象slot = 新下游对象ptr } 场景： A.添加下游对象(B, nil) //A对象，删除B对象的引用。 B被A删除，被标记为灰(如果B之前为白) A.添加下游对象(B, C) //A对象，更换下游B变成C。 B被A删除，被标记为灰(如果B之前为白) 接下来，我们用几张图，来模拟一个详细的过程，希望能够更可观的看清楚整体流程。 删除屏障01 删除屏障02 删除屏障03 删除屏障04 删除屏障05 删除屏障06 删除屏障07 这种方式的回收精度低，一个对象即使被删除了最后一个指向它的指针也依旧可以活过这一轮，在下一轮 GC 中被清理掉。 ","date":"2022-04-06","objectID":"/go-gc/:5:4","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#删除屏障"},{"categories":["golang"],"content":" 混合写屏障插入写屏障和删除写屏障的短板： 插入写屏障：结束时需要 STW 来重新扫描栈，标记栈上引用的白色对象的存活。 删除写屏障：回收精度低，GC 开始时 STW 扫描堆栈来记录初始快照（监控对象的内存修改，判断对象是否删除），这个过程会保护开始时刻的所有存活对象。 Go v1.8 版本引入了混合写屏障机制（hybrid write barrier），避免了对栈 re-scan 的过程，极大的减少了 STW 的时间，结合了两者的优点。 ","date":"2022-04-06","objectID":"/go-gc/:6:0","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#混合写屏障"},{"categories":["golang"],"content":" 规则具体操作： GC 开始将栈上的对象全部扫描并标记为黑色（之后不再进行第二次重复扫描，无需 STW ）。 GC 期间，任何在栈上创建的新对象，均为黑色。 被删除的对象标记为灰色。 被添加的对象标记为灰色。 满足：变形的弱三色不变式。 伪代码： 添加下游对象(当前下游对象slot, 新下游对象ptr) { //1 标记灰色(当前下游对象slot) //只要当前下游对象被移走，就标记灰色 //2 标记灰色(新下游对象ptr) //3 当前下游对象slot = 新下游对象ptr } 屏障技术是不在栈上应用的，因为要保证栈的运行效率。 ","date":"2022-04-06","objectID":"/go-gc/:6:1","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#规则"},{"categories":["golang"],"content":" 具体场景我们用几张图，来模拟一个详细的过程，希望能够更可观的看清楚整体流程。 混合写屏障是 GC 的一种屏障机制，所以只是当程序执行 GC 的时候，才会触发这种机制。 GC开始：优先扫描栈区，将可达对象全部标记为黑 混合写屏障01 混合写屏障02 场景一对象被一个堆对象删除引用，成为栈对象的下游。 伪代码 //前提：堆对象4-\u003e对象7 = 对象7； //对象7 被 对象4引用 栈对象1-\u003e对象7 = 堆对象7； //将堆对象7 挂在 栈对象1 下游 堆对象4-\u003e对象7 = null； //对象4 删除引用 对象7 场景1-01 场景1-02 场景二对象被一个栈对象删除引用，成为另一个栈对象的下游。 伪代码： new 栈对象9； 对象8-\u003e对象3 = 对象3； //将栈对象3 挂在 栈对象9 下游 对象2-\u003e对象3 = null； //对象2 删除引用 对象3 场景2-01 场景2-02 场景2-03 场景三对象被一个堆对象删除引用，成为另一个堆对象的下游。 伪代码： 堆对象10-\u003e对象7 = 堆对象7； //将堆对象7 挂在 堆对象10 下游 堆对象4-\u003e对象7 = null； //对象4 删除引用 对象7 场景3-01 场景3-02 场景3-03 场景四对象从一个栈对象删除引用，成为另一个堆对象的下游。 伪代码： 堆对象10-\u003e对象7 = 堆对象7； //将堆对象7 挂在 堆对象10 下游 堆对象4-\u003e对象7 = null； //对象4 删除引用 对象7 场景4-01 场景4-02 场景4-03 Go 中的混合写屏障满足弱三色不变式，结合了删除写屏障和插入写屏障的优点，只需要在开始时并发扫描各个 goroutine 的栈，使其变黑并一直保持，这个过程不需要 STW，而标记结束后，因为栈在扫描后始终是黑色的，也无需再进行 re-scan 操作了，减少了 STW 的时间。 ","date":"2022-04-06","objectID":"/go-gc/:6:2","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#具体场景"},{"categories":["golang"],"content":" 具体场景我们用几张图，来模拟一个详细的过程，希望能够更可观的看清楚整体流程。 混合写屏障是 GC 的一种屏障机制，所以只是当程序执行 GC 的时候，才会触发这种机制。 GC开始：优先扫描栈区，将可达对象全部标记为黑 混合写屏障01 混合写屏障02 场景一对象被一个堆对象删除引用，成为栈对象的下游。 伪代码 //前提：堆对象4-\u003e对象7 = 对象7； //对象7 被 对象4引用 栈对象1-\u003e对象7 = 堆对象7； //将堆对象7 挂在 栈对象1 下游 堆对象4-\u003e对象7 = null； //对象4 删除引用 对象7 场景1-01 场景1-02 场景二对象被一个栈对象删除引用，成为另一个栈对象的下游。 伪代码： new 栈对象9； 对象8-\u003e对象3 = 对象3； //将栈对象3 挂在 栈对象9 下游 对象2-\u003e对象3 = null； //对象2 删除引用 对象3 场景2-01 场景2-02 场景2-03 场景三对象被一个堆对象删除引用，成为另一个堆对象的下游。 伪代码： 堆对象10-\u003e对象7 = 堆对象7； //将堆对象7 挂在 堆对象10 下游 堆对象4-\u003e对象7 = null； //对象4 删除引用 对象7 场景3-01 场景3-02 场景3-03 场景四对象从一个栈对象删除引用，成为另一个堆对象的下游。 伪代码： 堆对象10-\u003e对象7 = 堆对象7； //将堆对象7 挂在 堆对象10 下游 堆对象4-\u003e对象7 = null； //对象4 删除引用 对象7 场景4-01 场景4-02 场景4-03 Go 中的混合写屏障满足弱三色不变式，结合了删除写屏障和插入写屏障的优点，只需要在开始时并发扫描各个 goroutine 的栈，使其变黑并一直保持，这个过程不需要 STW，而标记结束后，因为栈在扫描后始终是黑色的，也无需再进行 re-scan 操作了，减少了 STW 的时间。 ","date":"2022-04-06","objectID":"/go-gc/:6:2","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#场景一"},{"categories":["golang"],"content":" 具体场景我们用几张图，来模拟一个详细的过程，希望能够更可观的看清楚整体流程。 混合写屏障是 GC 的一种屏障机制，所以只是当程序执行 GC 的时候，才会触发这种机制。 GC开始：优先扫描栈区，将可达对象全部标记为黑 混合写屏障01 混合写屏障02 场景一对象被一个堆对象删除引用，成为栈对象的下游。 伪代码 //前提：堆对象4-\u003e对象7 = 对象7； //对象7 被 对象4引用 栈对象1-\u003e对象7 = 堆对象7； //将堆对象7 挂在 栈对象1 下游 堆对象4-\u003e对象7 = null； //对象4 删除引用 对象7 场景1-01 场景1-02 场景二对象被一个栈对象删除引用，成为另一个栈对象的下游。 伪代码： new 栈对象9； 对象8-\u003e对象3 = 对象3； //将栈对象3 挂在 栈对象9 下游 对象2-\u003e对象3 = null； //对象2 删除引用 对象3 场景2-01 场景2-02 场景2-03 场景三对象被一个堆对象删除引用，成为另一个堆对象的下游。 伪代码： 堆对象10-\u003e对象7 = 堆对象7； //将堆对象7 挂在 堆对象10 下游 堆对象4-\u003e对象7 = null； //对象4 删除引用 对象7 场景3-01 场景3-02 场景3-03 场景四对象从一个栈对象删除引用，成为另一个堆对象的下游。 伪代码： 堆对象10-\u003e对象7 = 堆对象7； //将堆对象7 挂在 堆对象10 下游 堆对象4-\u003e对象7 = null； //对象4 删除引用 对象7 场景4-01 场景4-02 场景4-03 Go 中的混合写屏障满足弱三色不变式，结合了删除写屏障和插入写屏障的优点，只需要在开始时并发扫描各个 goroutine 的栈，使其变黑并一直保持，这个过程不需要 STW，而标记结束后，因为栈在扫描后始终是黑色的，也无需再进行 re-scan 操作了，减少了 STW 的时间。 ","date":"2022-04-06","objectID":"/go-gc/:6:2","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#场景二"},{"categories":["golang"],"content":" 具体场景我们用几张图，来模拟一个详细的过程，希望能够更可观的看清楚整体流程。 混合写屏障是 GC 的一种屏障机制，所以只是当程序执行 GC 的时候，才会触发这种机制。 GC开始：优先扫描栈区，将可达对象全部标记为黑 混合写屏障01 混合写屏障02 场景一对象被一个堆对象删除引用，成为栈对象的下游。 伪代码 //前提：堆对象4-\u003e对象7 = 对象7； //对象7 被 对象4引用 栈对象1-\u003e对象7 = 堆对象7； //将堆对象7 挂在 栈对象1 下游 堆对象4-\u003e对象7 = null； //对象4 删除引用 对象7 场景1-01 场景1-02 场景二对象被一个栈对象删除引用，成为另一个栈对象的下游。 伪代码： new 栈对象9； 对象8-\u003e对象3 = 对象3； //将栈对象3 挂在 栈对象9 下游 对象2-\u003e对象3 = null； //对象2 删除引用 对象3 场景2-01 场景2-02 场景2-03 场景三对象被一个堆对象删除引用，成为另一个堆对象的下游。 伪代码： 堆对象10-\u003e对象7 = 堆对象7； //将堆对象7 挂在 堆对象10 下游 堆对象4-\u003e对象7 = null； //对象4 删除引用 对象7 场景3-01 场景3-02 场景3-03 场景四对象从一个栈对象删除引用，成为另一个堆对象的下游。 伪代码： 堆对象10-\u003e对象7 = 堆对象7； //将堆对象7 挂在 堆对象10 下游 堆对象4-\u003e对象7 = null； //对象4 删除引用 对象7 场景4-01 场景4-02 场景4-03 Go 中的混合写屏障满足弱三色不变式，结合了删除写屏障和插入写屏障的优点，只需要在开始时并发扫描各个 goroutine 的栈，使其变黑并一直保持，这个过程不需要 STW，而标记结束后，因为栈在扫描后始终是黑色的，也无需再进行 re-scan 操作了，减少了 STW 的时间。 ","date":"2022-04-06","objectID":"/go-gc/:6:2","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#场景三"},{"categories":["golang"],"content":" 具体场景我们用几张图，来模拟一个详细的过程，希望能够更可观的看清楚整体流程。 混合写屏障是 GC 的一种屏障机制，所以只是当程序执行 GC 的时候，才会触发这种机制。 GC开始：优先扫描栈区，将可达对象全部标记为黑 混合写屏障01 混合写屏障02 场景一对象被一个堆对象删除引用，成为栈对象的下游。 伪代码 //前提：堆对象4-\u003e对象7 = 对象7； //对象7 被 对象4引用 栈对象1-\u003e对象7 = 堆对象7； //将堆对象7 挂在 栈对象1 下游 堆对象4-\u003e对象7 = null； //对象4 删除引用 对象7 场景1-01 场景1-02 场景二对象被一个栈对象删除引用，成为另一个栈对象的下游。 伪代码： new 栈对象9； 对象8-\u003e对象3 = 对象3； //将栈对象3 挂在 栈对象9 下游 对象2-\u003e对象3 = null； //对象2 删除引用 对象3 场景2-01 场景2-02 场景2-03 场景三对象被一个堆对象删除引用，成为另一个堆对象的下游。 伪代码： 堆对象10-\u003e对象7 = 堆对象7； //将堆对象7 挂在 堆对象10 下游 堆对象4-\u003e对象7 = null； //对象4 删除引用 对象7 场景3-01 场景3-02 场景3-03 场景四对象从一个栈对象删除引用，成为另一个堆对象的下游。 伪代码： 堆对象10-\u003e对象7 = 堆对象7； //将堆对象7 挂在 堆对象10 下游 堆对象4-\u003e对象7 = null； //对象4 删除引用 对象7 场景4-01 场景4-02 场景4-03 Go 中的混合写屏障满足弱三色不变式，结合了删除写屏障和插入写屏障的优点，只需要在开始时并发扫描各个 goroutine 的栈，使其变黑并一直保持，这个过程不需要 STW，而标记结束后，因为栈在扫描后始终是黑色的，也无需再进行 re-scan 操作了，减少了 STW 的时间。 ","date":"2022-04-06","objectID":"/go-gc/:6:2","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#场景四"},{"categories":["golang"],"content":" 总结GoV1.3：普通标记清除法，整体过程需要启动 STW，效率极低。 GoV1.5：三色标记法，堆空间启动写屏障，栈空间不启动，全部扫描之后，需要重新扫描一次栈(需要 STW )，效率普通。 GoV1.8：三色标记法，混合写屏障机制， 栈空间不启动，堆空间启动。整个过程几乎不需要STW，效率较高。 ","date":"2022-04-06","objectID":"/go-gc/:7:0","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#总结"},{"categories":["golang"],"content":" 参考 golang 的GC原理 Golang三色标记+混合写屏障GC模式全分析 ","date":"2022-04-06","objectID":"/go-gc/:8:0","series":null,"tags":["golang"],"title":"Go GC 原理","uri":"/go-gc/#参考"},{"categories":["开发者手册"],"content":"github actions replace env vars in file, 将配置文件中的变量替换为环境变量,github actions,github actions 替换配置文件","date":"2022-04-02","objectID":"/github-actions-replace-env-vars-in-file/","series":null,"tags":["github-actions"],"title":"Github Actions replace env vars in file","uri":"/github-actions-replace-env-vars-in-file/"},{"categories":["开发者手册"],"content":"Github Actions 是个好东西😀，最近在使用的时候有个需求是，我项目不想把设置成私有的，但是有些配置又比较私密，比如 github 的 Personal access token，这种配置就不能暴露出来。 呃，这种需求前辈们估计也遇到过，github actions marketplace 是个好地方，我去里面搜了搜，果然有很多轮子，但是不知道能不能满足需求。 marketplace ","date":"2022-04-02","objectID":"/github-actions-replace-env-vars-in-file/:0:0","series":null,"tags":["github-actions"],"title":"Github Actions replace env vars in file","uri":"/github-actions-replace-env-vars-in-file/#"},{"categories":["开发者手册"],"content":" replace-env-vars-in-fileReplace env vars in file 是我选中的一个轮子。 Replace env vars in file Replace env vars in file 的文档就一句话， Replaces __TOKENS__ with environment variables in file. 我刚开始还不太理解。 好吧，其实是所有的环境变量都必须以__开头，然后以__结尾，这样才能被替换。 我在项目中是这样使用的： 配置文件 action 中替换 具体可以参看 config.toml 配置文件 ，workflows 工作流 ","date":"2022-04-02","objectID":"/github-actions-replace-env-vars-in-file/:1:0","series":null,"tags":["github-actions"],"title":"Github Actions replace env vars in file","uri":"/github-actions-replace-env-vars-in-file/#replace-env-vars-in-file"},{"categories":["开发者手册"],"content":" simple-template-renderersimple-template-renderer 相比 Replace env vars in file 有个明显的优势，simple-template-renderer 支持 html 格式，replace-env-vars-in-file 不支持😢。 比如源文件中有 icp = \"${ICP}\" 这个变量，需要把 ${ICP} 替换成一个 html 的变量： ICP: \"\u003ca href=https://beian.miit.gov.cn/ target=_blank\u003e京ICP备16062974号-1\u003c/a\u003e\" simple-template-renderer ","date":"2022-04-02","objectID":"/github-actions-replace-env-vars-in-file/:2:0","series":null,"tags":["github-actions"],"title":"Github Actions replace env vars in file","uri":"/github-actions-replace-env-vars-in-file/#simple-template-renderer"},{"categories":["开发者手册"],"content":"Gitalk 初始化 issue,Gitalk,自动化,init issue,python 脚本自动初始 gitalk issue,hugo 主题,hugo theme","date":"2022-04-01","objectID":"/gitalk-init-issue/","series":null,"tags":["gitalk","python","hugo"],"title":"Gitalk 初始化 issue","uri":"/gitalk-init-issue/"},{"categories":["开发者手册"],"content":"在用 Gitalk 作为个人博客评论系统时，发现有个恶心的点是，每篇文章必须手动初始化一个 issue 或是登录 github 后，把文章一个一个点开界面去初始化 issue，不然就会出现以下的提示 no issus 个人觉得这件事情非常麻烦，Gitalk 使用 labels 来映射 issuse，可以看下我用的主题 Gitalk 在初始化评论时发出的网络请求 创建 issue 的请求 labels 第一个参数是 Gitalk，第二个参数是文章的发布时间，呃，感觉改成文章的 path 会更好，但是 github label 的最大长度是 50 个字符，所以把 path md5 会更好。我看了下源码修改成了 URL path 的 md5 格式 themes/LoveIt/layouts/partials/comment.html comment id 初始工作做完，就可以写脚本了。 ","date":"2022-04-01","objectID":"/gitalk-init-issue/:0:0","series":null,"tags":["gitalk","python","hugo"],"title":"Gitalk 初始化 issue","uri":"/gitalk-init-issue/#"},{"categories":["开发者手册"],"content":" 分析我们要做的事其实就是给每篇新文章初始化一个 issue，可以用 github Actions 来做这件事。 初始化 issue 大致逻辑 这里有几个稍微麻烦的地方，以下是我的实现方案，仅仅是提供一个思路。 ","date":"2022-04-01","objectID":"/gitalk-init-issue/:1:0","series":null,"tags":["gitalk","python","hugo"],"title":"Gitalk 初始化 issue","uri":"/gitalk-init-issue/#分析"},{"categories":["开发者手册"],"content":" 获取所有文章信息怎么获取所有的文章❓，我用的 LoveIt 主题在 build 时在 public 目录里会有一个 index.json 文件，里面包含了所有的文章的信息。 public index.json 其他的主题可以使用 sitemap.xml 来获取所有的文章信息，hugo 在 build 时会生成 sitemap.xml 文件。 sitemap.xml ","date":"2022-04-01","objectID":"/gitalk-init-issue/:1:1","series":null,"tags":["gitalk","python","hugo"],"title":"Gitalk 初始化 issue","uri":"/gitalk-init-issue/#获取所有文章信息"},{"categories":["开发者手册"],"content":" issue 如何初始化 issue内容 如上截图👆是我创建的 issue 内容。body 是文章的 URL，title 是文章标题，labels 有 Gitalk 和文章的 URL path 的 md5 两个。那么问题就简单了，我们只需要给每篇文章初始化一个这样的 issue 就可以了。 固定文章的 URL 为唯一标识，组成两个 map ，map 键就是文章的 URL。一个 map 是 github 已存在的 issue 暂定为 issue_map，一个 map 是我们所有文章的 map 暂定为 posts_map ，URL 在 posts_map 中存在但是 issue_map 不存在的就是新增 。URL 在 posts_map 和 issue_map 中都存在但是 posts_map 中的标题跟 issue_map 中的标题不相同可能就是文章标题被修改了。 对于新的 URL 我的做法是承认它是新文章，或是旧文章的 URL 被修改了那只能去 github 手动修改 issue body 为新的 URL, label 为新的 uri 的 md5 值。 ","date":"2022-04-01","objectID":"/gitalk-init-issue/:1:2","series":null,"tags":["gitalk","python","hugo"],"title":"Gitalk 初始化 issue","uri":"/gitalk-init-issue/#issue-如何初始化"},{"categories":["开发者手册"],"content":" python 脚本实现 import hashlib import json import sys import time import requests site_url = \"https://xiaobinqt.github.io\" if len(sys.argv) != 4: print(\"Usage:\") print(sys.argv[0], \"token username repo_name\") sys.exit(1) # issue 的 body 就是文章的 URL token = sys.argv[1] username = sys.argv[2] repo_name = sys.argv[3] issue_map = dict() ## [issue_body] = {\"issue_number\": issue_number, \"issue_title\": issue_title} posts_map = dict() # [post_url] = {\"post_uri\":uri,\"post_date\":date,\"post_title\":title} def get_all_gitalk_issues(token, username, repo_name): for i in range(1, 150): # 15000 个 issue 基本够用了,不够可以再加 _, ret = get_issues_page(i) time.sleep(5) if ret == -1: break ## 删除的文章不管.... ## 文章 title 修改了的文章该怎么处理？ 标题可能修改,但是 uri 不变,issue 的 body 是文章地址,只要文章地址不变，就可以直接 update issue title ## uri 如果也变了，相当于是文件的重命名了，这时只能去手动 update issue title 了?..... def update_issue(issue_number, title): if title == \"\": return url = 'https://api.github.com/repos/%s/%s/issues/%d' % (username, repo_name, issue_number) print(\"update_issue url: %s\" % url) data = { 'title': title, } print(\"create_issue req json: %s\" % json.dumps(data)) r = requests.patch(url, data=json.dumps(data), headers={ \"Authorization\": \"token %s\" % token, }, verify=False) if r.status_code == 200: print(\"update_issue success\") else: print(\"update_issue fail, status_code: %d,title: %s,issue_number: %d\" % (r.status_code, title, issue_number)) # 获取所有 label 为 gitalk 的 issue def get_issues_page(page=1): url = 'https://api.github.com/repos/%s/%s/issues?labels=Gitalk\u0026per_page=100\u0026page=%d' % (username, repo_name, page) print(\"get_issues url: %s\" % url) r = requests.get(url, headers={ \"Authorization\": \"token %s\" % token, \"Accept\": \"application/vnd.github.v3+json\" }) if r.status_code != 200: print(\"get_issues_page fail, status_code: %d\" % r.status_code) sys.exit(2) if r.json() == []: return (issue_map, -1) for issue in r.json(): if issue['body'] not in issue_map and issue[\"body\"] != \"\": issue_map[issue['body']] = { \"issue_number\": issue['number'], \"issue_title\": issue['title'] } return (issue_map, 0) # 通过 public/index.json 获取所有的文章 def get_post_titles(): with open(file='public/index.json', mode='r', encoding='utf-8') as f: file_data = f.read() if file_data == \"\" or file_data == [] or file_data == {}: return posts_map file_data = json.loads(file_data) for data in file_data: key = \"%s%s\" % (site_url, data['uri']) if key not in posts_map: posts_map[key] = { \"post_uri\": data['uri'], \"post_date\": data['date'], \"post_title\": data['title'] } return posts_map def create_issue(title=\"\", uri=\"\", date=\"\"): if title == \"\": return url = 'https://api.github.com/repos/%s/%s/issues' % (username, repo_name) print(\"create_issue title: %s uri: %s date: %s\" % (title, uri, date)) m = hashlib.md5() m.update(uri.encode('utf-8')) urlmd5 = m.hexdigest() data = { 'title': title, 'body': '%s%s' % (site_url, uri), 'labels': [ 'Gitalk', urlmd5 ] } print(\"create_issue req json: %s\" % json.dumps(data)) r = requests.post(url, data=json.dumps(data), headers={ \"Authorization\": \"token %s\" % token, }) if r.status_code == 201: print(\"create_issue success\") else: print(\"create_issue fail, status_code: %d,title: %s,req url: %s \\n\" % (r.status_code, title, url)) # 创建 gitalk 创建 issue,如果 issue 已经存在，则不创建 def init_gitalk(): for post_url, item in posts_map.items(): ## 标题被修改了 if post_url in issue_map and item['post_title'] != issue_map[post_url]['issue_title']: update_issue(issue_map[post_url][\"issue_number\"], item['post_title']) elif post_url not in issue_map: # 新增的文章 print(\"title: [%s] , body [%s] issue 不存在,创建...\" % (item[\"post_title\"], post_url)) create_issue(item[\"post_title\"], item[\"post_uri\"], item[\"post_date\"]) # 延迟 5 秒，防止 github api 请求过于频繁： https://docs.github.com/en/rest/guides/best-practices-for-integrators#dealing-with-secondary-rate-limits time.sleep(5) def get_uri_md5(uri): m = hashlib.md5() m.update(uri.encode('utf-8')) return m.hexdigest() if __name__ == \"__main__\": # print(get_uri_md5(\"/gmp-model/\")) ## 执行.... get_all_gitalk_issues(token, userna","date":"2022-04-01","objectID":"/gitalk-init-issue/:2:0","series":null,"tags":["gitalk","python","hugo"],"title":"Gitalk 初始化 issue","uri":"/gitalk-init-issue/#python-脚本实现"},{"categories":["开发者手册"],"content":" 参考 自动初始化 Gitalk 和 Gitment 评论 利用 Github Action 自动初始化 Gitalk 评论之Python篇 ","date":"2022-04-01","objectID":"/gitalk-init-issue/:3:0","series":null,"tags":["gitalk","python","hugo"],"title":"Gitalk 初始化 issue","uri":"/gitalk-init-issue/#参考"},{"categories":["开发者手册"],"content":"Node-red,Low-code,自定义nodered节点,nodered,节点开发,how to create node-red node","date":"2022-04-01","objectID":"/node-red-glance/","series":null,"tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/"},{"categories":["开发者手册"],"content":" 概述Node-RED 是构建物联网 (IOT,Internet of Things) 应用程序的一个强大工具，其重点是简化代码块的“连接\"以执行任务。它使用可视 化编程方法，允许开发人员将预定义的代码块（称为“节点”，Node) 连接起来执行任务。连接的节点，通常是输入节点、处理节点和输出节点的组合，当它们连接在一起时，构成一个“流”(Flows)。 ","date":"2022-04-01","objectID":"/node-red-glance/:1:0","series":null,"tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/#概述"},{"categories":["开发者手册"],"content":" 安装node-red安装 node-red 的方式大致有 2 种，使用 docker 和 npm ，docker 安装可以参考。这里使用 npm 安装。个人觉得在本地调试 npm 比 docker 更方便一点，源码都在本地，docker 的话还需要把目录映射出来。 npm 安装直接一行命令就可以搞定，具体可以参考 npm i node-red 安装成功后，会在用户目录下生成一个 .node-red 目录，我用的是 Windows 系统，所以这里的目录是 C:\\Users\\weibin\\.node-red，这个目录下有配置文件 settings.js，里面有一些 node-red 配置项，比如默认端口等。 node-red 目录 ","date":"2022-04-01","objectID":"/node-red-glance/:2:0","series":null,"tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/#安装node-red"},{"categories":["开发者手册"],"content":" 启动安装完成后，直接执行 node-red 就可以启动服务。 cmd 启动 node-red node-red 的默认端口是 1880，直接用浏览器访问 http://127.0.0.1:1880 就可以看到 node-red 的页面。 node-red 界面 ","date":"2022-04-01","objectID":"/node-red-glance/:3:0","series":null,"tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/#启动"},{"categories":["开发者手册"],"content":" 创建自定义节点每一个 node-red 节点都是一个 npm 包，开发 npm 节点跟开发 npm 组件包是一样。 一个 node-red 节点主要包括两个文件，一个是 html 文件，一个是 js 文件。html 是界面配置，js 处理逻辑，加上 npm 的 package.json 文件，正常三个文件就可以实现一个 node-red 节点。 ","date":"2022-04-01","objectID":"/node-red-glance/:4:0","series":null,"tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/#创建自定义节点"},{"categories":["开发者手册"],"content":" 加法器节点开发我们创建一个自定义节点实现一个加法器，输入两个数字，输出两个数字的和。 ","date":"2022-04-01","objectID":"/node-red-glance/:4:1","series":null,"tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/#加法器节点开发"},{"categories":["开发者手册"],"content":" 新建项目我们新建一个节点项目 node-sum，这个项目随便放在那个目录下都行，这里我的目录是 D:\\tmp\\node-sum。 新建项目 ","date":"2022-04-01","objectID":"/node-red-glance/:4:2","series":null,"tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/#新建项目"},{"categories":["开发者手册"],"content":" npm 初始化切到项目目录下，执行 npm init 将项目进行 npm 初始化，然后根据提示填写即可。 npm init 用 IDE 打开 node-sum 项目就可以看到已经给我们初始化好了 package.json 文件。 package.json ","date":"2022-04-01","objectID":"/node-red-glance/:4:3","series":null,"tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/#npm-初始化"},{"categories":["开发者手册"],"content":" 功能实现sum.html \u003cscript type=\"text/javascript\"\u003e RED.nodes.registerType('sum', { // 这个值 必须和 js 中 RED.nodes.registerType 的值一致 category: '自定义节点', // 分类 color: '#a6bbcf', // 节点颜色 defaults: { name: {value: \"\"}, // name 默认是空 add1: {value: 0}, // add1 默认值 0 add2: {value: 0}, // add2 默认值 0 }, inputs: 0, // 节点有多少输入 0 或者多个 outputs: 1, // 节点有多少输出 0 或者多个 icon: \"file.png\", // 节点使用的图标 paletteLabel: \"加法器\", // 节点显示的名称 label: function () { // 节点的工作区的标签 return this.name || \"加法器\"; }, // 钩子函数,双节节点调出 template 时触发 oneditprepare: function () { console.log(\"oneditprepare 被调用\"); }, // 钩子函数,点击 template 中的完成按钮时触发 oneditsave: function () { console.log(\"oneditsave 被调用\"); } }); \u003c/script\u003e \u003c!--data-template-name 必须和 js 中 RED.nodes.registerType 的值一致 --\u003e \u003c!--template 是模板，可以理解成表单，节点需要的信息可以从这里输入--\u003e \u003cscript type=\"text/html\" data-template-name=\"sum\"\u003e \u003cdiv class=\"form-row\"\u003e \u003clabel for=\"node-input-name\"\u003e\u003ci class=\"fa fa-tag\"\u003e\u003c/i\u003e Name\u003c/label\u003e \u003cinput type=\"text\" id=\"node-input-name\" placeholder=\"Name\"\u003e \u003c/div\u003e \u003cdiv class=\"form-row\"\u003e \u003clabel for=\"node-input-add1\"\u003e\u003ci class=\"fa fa-tag\"\u003e\u003c/i\u003e加数1\u003c/label\u003e \u003cinput type=\"text\" id=\"node-input-add1\" placeholder=\"加数1\"\u003e \u003c/div\u003e \u003cdiv class=\"form-row\"\u003e \u003clabel for=\"node-input-add2\"\u003e\u003ci class=\"fa fa-tag\"\u003e\u003c/i\u003e加数2\u003c/label\u003e \u003cinput type=\"text\" id=\"node-input-add2\" placeholder=\"加数2\"\u003e \u003c/div\u003e \u003c/script\u003e \u003c!--data-help-name 必须和 js 中 RED.nodes.registerType 的值一致 --\u003e \u003c!--help 是节点的帮助文档--\u003e \u003cscript type=\"text/html\" data-help-name=\"sum\"\u003e \u003cp\u003e一个简单的加法器\u003c/p\u003e \u003c/script\u003e sum.js module.exports = function (RED) { function Sum(config) { RED.nodes.createNode(this, config); var node = this; // 获取输入的参数 let add1 = parseInt(config.add1) let add2 = parseInt(config.add2) node.send({ // 向下一个节点输出信息 payload: `${add1} + ${add2} 结果为 ` + (add1 + add2) }); node.on('input', function (msg) { // 接收上游节点接收消息 }); } // 注册一个节点 sum,注册的节点不能重复也就是说同一个 node-red 项目不能有 2 个 registerType sum 节点 RED.nodes.registerType(\"sum\", Sum); } 需要在 package.json 文件里添加 node-red 信息，完整的 package.json 如下： { \"name\": \"node-sum\", \"version\": \"1.0.0\", \"description\": \"node-red 加法器\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" \u0026\u0026 exit 1\" }, \"keywords\": [ \"node-red\", \"add\" ], \"author\": \"xiaobinqt@163.com\", \"license\": \"ISC\", \"node-red\": { \"nodes\": { \"sum\": \"sum.js\" } } } 在 package.json 中添加的 node-red 信息是固定写法，可以理解成向 node-red 中注册了 nodes 的名称为 sum，注册的 js 文件为 sum.js。 \"node-red\": { \"nodes\": { \"sum\": \"sum.js\" } } ","date":"2022-04-01","objectID":"/node-red-glance/:4:4","series":null,"tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/#功能实现"},{"categories":["开发者手册"],"content":" 本地安装可以通过 npm i 安装刚才的 sum 节点到 node-red 中。切到.node-red 目录下，执行 npm i d:\\tmp\\node-sum 安转本地节点并重启 然后重启 node-red 就可以看到刚才安装的节点了。 节点安装成功 ","date":"2022-04-01","objectID":"/node-red-glance/:4:5","series":null,"tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/#本地安装"},{"categories":["开发者手册"],"content":" 测试功能把节点拖到工作区，双击节点（会触发oneditprepare函数）打开编辑区 双节节点填写编辑区 填写完编辑区内容后点击完成（会触发oneditsave函数），点击部署就会在调试窗口输出 node.send 信息。 部署 ","date":"2022-04-01","objectID":"/node-red-glance/:4:6","series":null,"tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/#测试功能"},{"categories":["开发者手册"],"content":" 参考 Creating your first node Design: i18n ","date":"2022-04-01","objectID":"/node-red-glance/:5:0","series":null,"tags":["low-code","node-red"],"title":"Node-RED 节点开发","uri":"/node-red-glance/#参考"},{"categories":["开发者手册"],"content":"https,google强制跳到https,ERR_SSL_PROTOCOL_ERROR,How to Stop Chrome from Automatically Redirecting to https","date":"2022-03-29","objectID":"/stop-chrome-auto-redirect-2-https/","series":null,"tags":["chrome","http/https"],"title":"禁止Google浏览器强制跳转https","uri":"/stop-chrome-auto-redirect-2-https/"},{"categories":["开发者手册"],"content":"这几天在使用 google 浏览器打开公司的一个网站时，发现总是自动跳转到 https，以至于出现下面这个页面： ERR_SSL_PROTOCOL_ERROR 有时候浏览器太智能了也不是一件好事🤣。 ","date":"2022-03-29","objectID":"/stop-chrome-auto-redirect-2-https/:0:0","series":null,"tags":["chrome","http/https"],"title":"禁止Google浏览器强制跳转https","uri":"/stop-chrome-auto-redirect-2-https/#"},{"categories":["开发者手册"],"content":" 解决方法复制链接 chrome://net-internals/#hsts用 Google 浏览器打开，这个页面，在最下面的 Delete domain security policies 填上需要禁止跳转的网站，然后点击Delete。 Delete domain security policies 然后重启浏览器，重启浏览器，重启浏览器，不然可能不生效。 这里有个需要注意的地方是，如果我们的网址是 http://g.xiaobinqt.cn:8000，那么Domain 的值填的是 xiaobinqt.cn。 ","date":"2022-03-29","objectID":"/stop-chrome-auto-redirect-2-https/:1:0","series":null,"tags":["chrome","http/https"],"title":"禁止Google浏览器强制跳转https","uri":"/stop-chrome-auto-redirect-2-https/#解决方法"},{"categories":["开发者手册"],"content":" 参考 How to Stop Chrome from Automatically Redirecting to https ","date":"2022-03-29","objectID":"/stop-chrome-auto-redirect-2-https/:2:0","series":null,"tags":["chrome","http/https"],"title":"禁止Google浏览器强制跳转https","uri":"/stop-chrome-auto-redirect-2-https/#参考"},{"categories":["开发者手册"],"content":"hugo,algolia,algoliasearch,exceptions.AlgoliaUnreachableHostException: Unreachable hosts, algolia索引","date":"2022-03-28","objectID":"/hugo-algolia/","series":null,"tags":["hugo","algolia"],"title":"hugo algolia Unreachable hosts","uri":"/hugo-algolia/"},{"categories":["开发者手册"],"content":"最近在使用 hugo algolia 时，在 github actions 同步索引到 algolia 时总是出现这样的错误： action error list Unreachable hosts 我用的 action 插件是Algolia Index Uploader，找了半天发现是参数 algolia_index_id 写的有问题😥： algolia_index_id 填的值 上传成功后可以去 algolia 官网查看效果： Settings -\u003e Applications -\u003e 进入到应用 -\u003e Search -\u003e Browse 上传索引效果 ","date":"2022-03-28","objectID":"/hugo-algolia/:0:0","series":null,"tags":["hugo","algolia"],"title":"hugo algolia Unreachable hosts","uri":"/hugo-algolia/#"},{"categories":["开发者手册"],"content":" 参考 Algolia Hosts unreachable ","date":"2022-03-28","objectID":"/hugo-algolia/:1:0","series":null,"tags":["hugo","algolia"],"title":"hugo algolia Unreachable hosts","uri":"/hugo-algolia/#参考"},{"categories":["理解计算机"],"content":"TCP,UDP,网络模型,实体层,链接层,网络层,传输层,应用层,网络数据包,以太网协议,MAC地址,广播,Physical Layer,Application Layer,Transport Layer,Network Layer,Internet Protocol Suite,Ethernet,subnet mask,IPv4,IPv6,互联网协议的通信过程","date":"2022-03-27","objectID":"/net-protocol-glance/","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/"},{"categories":["理解计算机"],"content":" 概述","date":"2022-03-27","objectID":"/net-protocol-glance/:1:0","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#概述"},{"categories":["理解计算机"],"content":" 五层模型互联网的实现，分成好几层。每一层都有自己的功能，就像建筑物一样，每一层都靠下一层支持。 用户接触到的，只是最上面的一层，根本没有感觉到下面的层。理解互联网，需要从最下层开始，自下而上理解每一层的功能。 如何分层有不同的模型，有的模型分七层，有的分四层。把互联网分成五层，比较容易解释。 五层模型 如上图所示，最底下的一层叫做\"实体层\"（Physical Layer），最上面的一层叫做\"应用层\"（Application Layer），中间的三层（自下而上）分别是\"链接层\"（Link Layer）、“网络层”（Network Layer）和\"传输层\"（Transport Layer）。越下面的层，越靠近硬件；越上面的层，越靠近用户。 名字只是一个代号，它们叫什么名字，其实并不重要。只需要知道，互联网分成若干层就可以了。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:1:1","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#五层模型"},{"categories":["理解计算机"],"content":" 层与协议每一层都是为了完成一种功能。为了实现这些功能，就需要大家都遵守共同的规则。 大家都遵守的规则，就叫做\"协议\"（protocol）。 互联网的每一层，都定义了很多协议。这些协议的总称，就叫做\"互联网协议\"（Internet Protocol Suite），它们是互联网的核心。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:1:2","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#层与协议"},{"categories":["理解计算机"],"content":" 实体层电脑要组网，第一件事是先把电脑连起来，可以用光缆、电缆、双绞线、无线电波等方式。 这就叫做\"实体层\"，它就是把电脑连接起来的物理手段。它主要规定了网络的一些电气特性，作用是负责传送 0 和 1 的电信号。 实体层 ","date":"2022-03-27","objectID":"/net-protocol-glance/:2:0","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#实体层"},{"categories":["理解计算机"],"content":" 链接层单纯的 0 和 1 没有任何意义，必须规定解读方式：多少个电信号算一组？每个信号位有何意义？ 这就是\"链接层\"的功能，它在\"实体层\"的上方，确定了 0 和 1 的分组方式。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:3:0","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#链接层"},{"categories":["理解计算机"],"content":" 以太网协议早期的时候，每家公司都有自己的电信号分组方式。逐渐地，一种叫做\"以太网\"（Ethernet）的协议，占据了主导地位。 以太网规定，一组电信号构成一个数据包，叫做\"帧\"（Frame）。每一帧分成两个部分：标头（Head）和数据（Data）。 head-data “标头\"包含数据包的一些说明项，比如发送者、接受者、数据类型等等；“数据\"则是数据包的具体内容。 “标头\"的长度，固定为 18 字节。“数据\"的长度，最短为 46 字节，最长为 1500 字节。因此，整个\"帧\"最短为 64 字节，最长为 1518 字节。如果数据很长，就必须分割成多个帧进行发送。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:3:1","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#以太网协议"},{"categories":["理解计算机"],"content":" MAC 地址以太网数据包的\"标头”，包含了发送者和接受者的信息。那么，发送者和接受者是如何标识呢？ 以太网规定，连入网络的所有设备，都必须具有\"网卡\"接口。数据包必须是从一块网卡，传送到另一块网卡。网卡的地址，就是数据包的发送地址和接收地址，这叫做 MAC 地址。 每块网卡出厂的时候，都有一个全世界独一无二的 MAC 地址，长度是 48 个二进制位，通常用 12 个十六进制数表示。 前 6 个十六进制数是厂商编号，后 6 个是该厂商的网卡流水号。有了 MAC 地址，就可以定位网卡和数据包的路径了。 MAC address 上图的 MAC 地址的二进制位为 00000000-10110000-11010000-10000110-10111011-11110111。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:3:2","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#mac-地址"},{"categories":["理解计算机"],"content":" 广播以太网数据包必须知道接收方的 MAC 地址，然后才能发送，那么问题来了， 一块网卡怎么会知道另一块网卡的MAC地址？ 就算有了 MAC 地址，系统怎样才能把数据包准确送到接收方？ 回答是以太网采用了一种很\"原始\"的方式，它不是把数据包准确送到接收方，而是向本网络内所有计算机发送，让每台计算机自己判断，是否为接收方。 广播 上图中，1 号计算机向 2 号计算机发送一个数据包，同一个子网络的 3 号、4 号、5 号计算机都会收到这个包。它们读取这个包的\"标头”，找到接收方的 MAC 地址，然后与自身的 MAC 地址相比较，如果两者相同，就接受这个包，做进一步处理，否则就丢弃这个包。这种发送方式就叫做广播broadcasting。 有了数据包的定义、网卡的 MAC 地址、广播的发送方式，“链接层\"就可以在多台计算机之间传送数据了。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:3:3","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#广播"},{"categories":["理解计算机"],"content":" 网络层","date":"2022-03-27","objectID":"/net-protocol-glance/:4:0","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#网络层"},{"categories":["理解计算机"],"content":" 网络层的由来以太网协议，依靠 MAC 地址发送数据。理论上，单单依靠 MAC 地址，上海的网卡就可以找到洛杉矶的网卡了，技术上是可以实现的。 但是，这样做有一个重大的缺点。以太网采用广播方式发送数据包，所有成员人手一\"包”，不仅效率低，而且局限在发送者所在的子网络。也就是说，如果两台计算机不在同一个子网络，广播是传不过去的 。这种设计是合理的，否则互联网上每一台计算机都会收到所有包，那会引起灾难。 互联网是无数子网络共同组成的一个巨型网络，很像想象上海和洛杉矶的电脑会在同一个子网络，这几乎是不可能的。 子网络 因此，必须找到一种方法，能够区分哪些 MAC 地址属于同一个子网络，哪些不是。如果是同一个子网络，就采用广播方式发送，否则就采用\"路由\"方式发送。（“路由\"的意思，就是指如何向不同的子网络分发数据包。），MAC 地址本身无法做到这一点，它只与厂商有关，与所处网络无关。 这就导致了\"网络层\"的诞生。它的作用是引进一套新的地址，使得我们能够区分不同的计算机是否属于同一个子网络。这套地址就叫做\"网络地址”，简称\"网址”。 于是，“网络层\"出现以后，每台计算机有了两种地址，一种是 MAC 地址，另一种是网络地址。两种地址之间没有任何联系，MAC 地址是绑定在网卡上的，网络地址则是管理员分配的，它们只是随机组合在一起。 网络地址帮助我们确定计算机所在的子网络，MAC 地址则将数据包送到该子网络中的目标网卡。因此，从逻辑上可以推断，必定是先处理网络地址，然后再处理 MAC 地址。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:4:1","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#网络层的由来"},{"categories":["理解计算机"],"content":" IP协议和子网掩码规定网络地址的协议，叫做 IP 协议。它所定义的地址，就被称为 IP 地址。 目前，广泛采用的是 IP 协议第四版，简称 IPv4。这个版本规定，网络地址由 32 个二进制位组成。 IP协议 习惯上，我们用分成四段的十进制数表示 IP 地址，从 0.0.0.0 一直到 255.255.255.255。 互联网上的每一台计算机，都会分配到一个 IP 地址。 IP 地址分成两个部分，前一部分代表网络，后一部分代表主机。 比如，IP 地址 172.16.254.1，这是一个 32 位的地址，假定它的网络部分是前 24 位（172.16.254），那么主机部分就是后 8 位（最后的那个 1 ）。处于同一个子网络的电脑，它们 IP 地址的网络部分必定是相同的，也就是说 172.16.254.2 应该与 172.16.254.1 处在同一个子网络。 单单从 IP 地址，我们无法判断网络部分。还是以 172.16.254.1 为例，它的网络部分，到底是前 24 位，还是前 16 位，甚至前 28 位，从 IP 地址上是看不出来的。 那么，怎样才能从IP地址，判断两台计算机是否属于同一个子网络呢？这就要用到另一个参数子网掩码subnet mask。 所谓 “子网掩码”，就是表示子网络特征的一个参数。它在形式上等同于 IP 地址，也是一个 32 位二进制数字，它的网络部分全部为 1，主机部分全部为 0 。比如，IP 地址 172.16.254.1 ，如果已知网络部分是前 24 位，主机部分是后 8 位，那么子网络掩码就是 11111111.11111111.11111111.00000000，写成十进制就是 255.255.255.0。 知道\"子网掩码”，我们就能判断，任意两个 IP 地址是否处在同一个子网络。方法是将两个 IP 地址与子网掩码分别进行 AND 运算（两个数位都为 1 ，运算结果为 1，否则为 0），然后比较结果是否相同，如果是的话，就表明它们在同一个子网络中，否则就不是。 比如，已知IP地址 172.16.254.1 和 172.16.254.233 的子网掩码都是 255.255.255.0，请问它们是否在同一个子网络？两者与子网掩码分别进行 AND 运算，结果都是 172.16.254.0，因此它们在同一个子网络。 10101100.00010000.11111110.00000001 # 172.16.254.1 11111111.11111111.11111111.00000000 # 255.255.255.0 10101100.00010000.11111110.00000000 # AND 结果二进制位 172.16.254.0 # AND 结果转成十进制 所以，IP 协议的作用主要有两个，一个是为每一台计算机分配 IP 地址，另一个是确定哪些地址在同一个子网络。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:4:2","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#ip协议和子网掩码"},{"categories":["理解计算机"],"content":" IP数据包根据 IP 协议发送的数据，就叫做 IP 数据包。不难想象，其中必定包括 IP 地址信息。 但是前面说过，以太网数据包只包含 MAC 地址，并没有 IP 地址的栏位。那么是否需要修改数据定义，再添加一个栏位呢？ 回答是不需要，我们可以把 IP 数据包直接放进以太网数据包的\"数据\"部分，因此完全不用修改以太网的规格。这就是互联网分层结构的好处：上层的变动完全不涉及下层的结构。 具体来说，IP 数据包也分为\"标头\"和\"数据\"两个部分。 IP数据包1 “标头\"部分主要包括版本、长度、IP 地址等信息，“数据\"部分则是 IP 数据包的具体内容。它放进以太网数据包后，以太网数据包就变成了下面这样。 IP数据包2 IP 数据包的“标头”部分的长度为 20 到 60 字节，整个数据包的总长度最大为 65,535 字节。因此，理论上，一个 IP 数据包的\"数据\"部分，最长为 65,515 字节。前面说过，以太网数据包的\"数据\"部分，最长只有 1500 字节。因此，如果 IP 数据包超过了 1500 字节（上图红色部分），它就需要分割成几个以太网数据包，分开发送了。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:4:3","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#ip数据包"},{"categories":["理解计算机"],"content":" ARP协议由于 IP 数据包是放在以太网数据包里发送的，所以我们必须同时知道两个地址，一个是对方的 MAC 地址，另一个是对方的 IP 地址。通常情况下，对方的 IP 地址是已知的，但是我们不知道它的 MAC 地址。 所以，我们需要一种机制，能够从 IP 地址得到 MAC 地址。 这里又可以分成两种情况。 第一种情况，如果两台主机不在同一个子网络，那么事实上没有办法得到对方的 MAC 地址，只能把数据包传送到两个子网络连接处的网关gateway，让网关去处理。 第二种情况，如果两台主机在同一个子网络，那么我们可以用 ARP 协议，得到对方的 MAC 地址。ARP 协议也是发出一个数据包（包含在以太网数据包中），其中包含它所要查询主机的 IP 地址，在对方的 MAC 地址这一栏，填的是FF:FF:FF:FF:FF:FF，表示这是一个\"广播” 地址。它所在子网络的每一台主机，都会收到这个数据包，从中取出 IP 地址，与自身的 IP 地址进行比较。如果两者相同，都做出回复，向对方报告自己的 MAC 地址，否则就丢弃这个包。 有了 ARP 协议之后，我们就可以得到同一个子网络内的主机 MAC 地址，可以把数据包发送到任意一台主机之上。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:4:4","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#arp协议"},{"categories":["理解计算机"],"content":" 传输层","date":"2022-03-27","objectID":"/net-protocol-glance/:5:0","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#传输层"},{"categories":["理解计算机"],"content":" 传输层的由来有了 MAC 地址和 IP 地址，我们已经可以在互联网上任意两台主机上建立通信。 接下来的问题是，同一台主机上有许多程序都需要用到网络，比如，你一边浏览网页，一边与朋友在线聊天。当一个数据包从互联网上发来的时候，你怎么知道，它是表示网页的内容，还是表示在线聊天的内容？ 也就是说，我们还需要一个参数，表示这个数据包到底供哪个程序（进程）使用。这个参数就叫做端口port，它其实是每一个使用网卡的程序的编号。每个数据包都发到主机的特定端口，所以不同的程序就能取到自己所需要的数据。 “端口\"是 0 到 65535 之间的一个整数，正好 16 个二进制位。0 到 1023 的端口被系统占用，用户只能选用大于 1023 的端口。不管是浏览网页还是在线聊天，应用程序会随机选用一个端口，然后与服务器的相应端口联系。 “传输层\"的功能，就是建立\"端口到端口\"的通信。相比之下，“网络层\"的功能是建立\"主机到主机\"的通信。只要确定主机和端口，我们就能实现程序之间的交流。因此，Unix系统就把主机+端口，叫做套接字socket。有了它，就可以进行网络应用程序开发了。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:5:1","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#传输层的由来"},{"categories":["理解计算机"],"content":" UDP 协议我们必须在数据包中加入端口信息，这就需要新的协议。最简单的实现叫做 UDP 协议，它的格式几乎就是在数据前面，加上端口号。 UDP 数据包，也是由\"标头\"和\"数据\"两部分组成。 UDP数据格式_1 “标头\"部分主要定义了发出端口和接收端口，“数据\"部分就是具体的内容。然后，把整个 UDP 数据包放入 IP 数据包的\"数据\"部分，而前面说过，IP 数据包又是放在以太网数据包之中的，所以整个以太网数据包现在变成了下面这样： UDP数据格式_2 UDP 数据包非常简单，“标头\"部分一共只有 8 个字节，总长度不超过 65,535 字节，正好放进一个IP数据包。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:5:2","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#udp-协议"},{"categories":["理解计算机"],"content":" TCP 协议UDP 协议的优点是比较简单，容易实现，但是缺点是可靠性较差，一旦数据包发出，无法知道对方是否收到。 为了解决这个问题，提高网络可靠性，TCP 协议就诞生了。这个协议非常复杂，但可以近似认为，它就是有确认机制的 UDP 协议，每发出一个数据包都要求确认。如果有一个数据包遗失，就收不到确认，发出方就知道有必要重发这个数据包了。 因此，TCP 协议能够确保数据不会遗失。它的缺点是过程复杂、实现困难、消耗较多的资源。 TCP 数据包和 UDP 数据包一样，都是内嵌在 IP 数据包的“数据”部分。TCP 数据包没有长度限制，理论上可以无限长，但是为了保证网络的效率，通常 TCP 数据包的长度不会超过 IP 数据包的长度，以确保单个 TCP 数据包不必再分割。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:5:3","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#tcp-协议"},{"categories":["理解计算机"],"content":" 应用层应用程序收到\"传输层\"的数据，接下来就要进行解读。由于互联网是开放架构，数据来源五花八门，必须事先规定好格式，否则根本无法解读。 “应用层”的作用，就是规定应用程序的数据格式。 举例来说，TCP 协议可以为各种各样的程序传递数据，比如 Email、WWW、FTP 等等。那么，必须有不同协议（比如 http 协议）规定电子邮件、网页、FTP 数据的格式，这些应用程序协议就构成了\"应用层”。 这是最高的一层，直接面对用户。它的数据就放在 TCP 数据包的\"数据\"部分。因此，现在的以太网的数据包就变成下面这样。 应用层数据包 ","date":"2022-03-27","objectID":"/net-protocol-glance/:6:0","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#应用层"},{"categories":["理解计算机"],"content":" 小结网络通信就是交换数据包。电脑 A 向电脑 B 发送一个数据包，后者收到了，回复一个数据包，从而实现两台电脑之间的通信。数据包的结构，基本上是下面这样： 数据包 发送这个包，需要知道两个地址： 对方的 MAC 地址 对方的 IP 地址 有了这两个地址，数据包才能准确送到接收者手中。但是，MAC 地址有局限性，如果两台电脑不在同一个子网络，就无法知道对方的 MAC 地址，必须通过网关（gateway）转发。 网关 上图☝️中，1 号电脑要向 4 号电脑发送一个数据包。它先判断 4 号电脑是否在同一个子网络，结果发现不是，于是就把这个数据包发到网关 A。网关 A 通过路由协议，发现 4 号电脑位于子网络 B，又把数据包发给网关 B，网关 B 再转发到 4 号电脑。 1 号电脑把数据包发到网关 A，必须知道网关 A 的 MAC 地址。所以，数据包的目标地址，实际上分成两种情况： 场景 数据包地址 同一个子网络 对方的MAC地址，对方的IP地址 非同一个子网络 网关的MAC地址，对方的IP地址 发送数据包之前，电脑必须判断对方是否在同一个子网络，然后选择相应的 MAC 地址。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:7:0","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#小结"},{"categories":["理解计算机"],"content":" 用户的上网设置","date":"2022-03-27","objectID":"/net-protocol-glance/:8:0","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#用户的上网设置"},{"categories":["理解计算机"],"content":" 静态IP地址 new computer 通常你必须做一些设置。有时，管理员会告诉你下面四个参数，你把它们填入操作系统，计算机就能连上网了： 本机的IP地址 子网掩码 网关的IP地址 DNS的IP地址 下图是Windows系统的设置窗口。 系统设置 这四个参数缺一不可。由于它们是给定的，计算机每次开机，都会分到同样的IP地址，所以这种情况被称作\"静态IP地址上网”。 但是，这样的设置很专业，普通用户望而生畏，而且如果一台电脑的IP地址保持不变，其他电脑就不能使用这个地址，不够灵活。出于这两个原因，大多数用户使用\"动态IP地址上网”。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:8:1","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#静态ip地址"},{"categories":["理解计算机"],"content":" 动态IP地址所谓\"动态IP地址”，指计算机开机后，会自动分配到一个IP地址，不用人为设定。它使用的协议叫做DHCP协议。 这个协议规定，每一个子网络中，有一台计算机负责管理本网络的所有IP地址，它叫做“DHCP服务器”。新的计算机加入网络，必须向“DHCP服务器”发送一个“DHCP请求”数据包，申请IP地址和相关的网络参数。 前面说过，如果两台计算机在同一个子网络，必须知道对方的 MAC 地址和 IP 地址，才能发送数据包。但是，新加入的计算机不知道这两个地址，怎么发送数据包呢？ DHCP协议做了一些巧妙的规定。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:8:2","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#动态ip地址"},{"categories":["理解计算机"],"content":" DHCP协议首先，它是一种应用层协议，建立在UDP协议之上，所以整个数据包是这样的： HDCP协议数据包 最前面的\"以太网标头”，设置发出方（本机）的MAC地址和接收方（DHCP服务器）的MAC地址。前者就是本机网卡的MAC地址，后者这时不知道，就填入一个广播地址：FF-FF-FF-FF-FF-FF。 后面的\"IP标头”，设置发出方的IP地址和接收方的IP地址。这时，对于这两者，本机都不知道。于是，发出方的IP地址就设为0.0.0.0，接收方的IP地址设为255.255.255.255。 最后的\"UDP标头”，设置发出方的端口和接收方的端口。这一部分是DHCP协议规定好的，发出方是68端口，接收方是67端口。 这个数据包构造完成后，就可以发出了。以太网是广播发送，同一个子网络的每台计算机都收到了这个包。因为接收方的MAC地址是FF-FF-FF-FF-FF-FF ，看不出是发给谁的，所以每台收到这个包的计算机，还必须分析这个包的IP地址，才能确定是不是发给自己的。当看到发出方IP地址是0.0.0.0，接收方是255.255.255.255，于是DHCP服务器知道” 这个包是发给我的\"，而其他计算机就可以丢弃这个包。 接下来，DHCP服务器读出这个包的数据内容，分配好IP地址，发送回去一个\"DHCP响应\" 数据包。这个响应包的结构也是类似的，以太网标头的MAC地址是双方的网卡地址，IP标头的IP地址是DHCP服务器的IP地址（发出方）和255.255.255.255 （接收方），UDP标头的端口是67（发出方）和68（接收方），分配给请求端的IP地址和本网络的具体参数则包含在Data部分。 新加入的计算机收到这个响应包，于是就知道了自己的IP地址、子网掩码、网关地址、DNS服务器等等参数。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:8:3","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#dhcp协议"},{"categories":["理解计算机"],"content":" 小结不管是\"静态IP地址\"还是\"动态IP地址\"，电脑上网的首要步骤，是确定四个参数。这四个值很重要，值得重复一遍： 本机的IP地址 子网掩码 网关的IP地址 DNS的IP地址 有了这几个数值，电脑就可以上网\"冲浪\"了。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:8:4","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#小结-1"},{"categories":["理解计算机"],"content":" 一个实例","date":"2022-03-27","objectID":"/net-protocol-glance/:9:0","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#一个实例"},{"categories":["理解计算机"],"content":" 本机参数我们假定，用户设置好了自己的网络参数： 本机的IP地址：192.168.1.100 子网掩码：255.255.255.0 网关的IP地址：192.168.1.1 DNS的IP地址：8.8.8.8 然后他打开浏览器，想要访问Google，在地址栏输入了网址：www.google.com。 访问google 这意味着，浏览器要向Google发送一个网页请求的数据包。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:9:1","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#本机参数"},{"categories":["理解计算机"],"content":" DNS协议我们知道，发送数据包，必须要知道对方的IP地址。但是，现在，我们只知道网址www.google.com，不知道它的IP地址。 DNS协议可以帮助我们，将这个网址转换成IP地址。已知DNS服务器为8.8.8.8，于是我们向这个地址发送一个DNS数据包（53端口）。 DNS数据包 然后，DNS服务器做出响应，告诉我们Google的IP地址是172.194.72.105。于是，我们知道了对方的IP地址。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:9:2","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#dns协议"},{"categories":["理解计算机"],"content":" 子网掩码接下来，我们要判断，这个IP地址是不是在同一个子网络，这就要用到子网掩码。 已知子网掩码是255.255.255.0，本机用它对自己的IP地址192.168.1.100，做一个二进制的AND运算（两个数位都为1，结果为1，否则为0），计算结果为192.168.1.0 ；然后对Google的IP地址172.194.72.105也做一个AND运算，计算结果为172.194.72.0。这两个结果不相等，所以结论是，Google与本机不在同一个子网络。 因此，我们要向Google发送数据包，必须通过网关192.168.1.1转发，也就是说，接收方的MAC地址将是网关的MAC地址。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:9:3","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#子网掩码"},{"categories":["理解计算机"],"content":" 应用层协议浏览网页用的是HTTP协议，它的整个数据包构造是这样的： HTTP协议数据包 HTTP部分的内容，类似于下面这样： GET / HTTP/1.1 Host: www.google.com Connection: keep-alive User-Agent: Mozilla/5.0 (Windows NT 6.1) ...... Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Encoding: gzip,deflate,sdch Accept-Language: zh-CN,zh;q=0.8 Accept-Charset: GBK,utf-8;q=0.7,*;q=0.3 Cookie: ... ... 我们假定这个部分的长度为 4960 字节，它会被嵌在TCP数据包之中。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:9:4","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#应用层协议"},{"categories":["理解计算机"],"content":" TCP协议TCP数据包需要设置端口，接收方（Google）的HTTP端口默认是80，发送方（本机）的端口是一个随机生成的1024-65535之间的整数，假定为51775。 TCP数据包的标头长度为20字节，加上嵌入HTTP的数据包，总长度变为4980字节。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:9:5","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#tcp协议"},{"categories":["理解计算机"],"content":" IP协议然后，TCP数据包再嵌入IP数据包。IP数据包需要设置双方的IP地址，这是已知的，发送方是192.168.1.100（本机），接收方是172.194.72.105（Google）。 IP数据包的标头长度为20字节，加上嵌入的TCP数据包，总长度变为5000字节。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:9:6","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#ip协议"},{"categories":["理解计算机"],"content":" 以太网协议最后，IP数据包嵌入以太网数据包。以太网数据包需要设置双方的MAC地址，发送方为本机的网卡MAC地址，接收方为网关192.168.1.1的MAC地址（通过ARP协议得到）。 以太网数据包的数据部分，最大长度为1500字节，而现在的IP数据包长度为5000字节。因此，IP数据包必须分割成四个包。因为每个包都有自己的IP标头（20字节），所以四个包的IP数据包的长度分别为1500、1500、1500、560。 以太网协议 ","date":"2022-03-27","objectID":"/net-protocol-glance/:9:7","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#以太网协议-1"},{"categories":["理解计算机"],"content":" 服务器端响应经过多个网关的转发，Google的服务器172.194.72.105，收到了这四个以太网数据包。 根据IP标头的序号，Google将四个包拼起来，取出完整的TCP数据包，然后读出里面的\"HTTP请求\"，接着做出\"HTTP响应\"，再用TCP协议发回来。 本机收到HTTP响应以后，就可以将网页显示出来，完成一次网络通信。 服务器相应 上面的例子，虽然经过了简化，但它大致上反映了互联网协议的整个通信过程。 ","date":"2022-03-27","objectID":"/net-protocol-glance/:9:8","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#服务器端响应"},{"categories":["理解计算机"],"content":" 参考 互联网协议入门 ","date":"2022-03-27","objectID":"/net-protocol-glance/:10:0","series":null,"tags":["network"],"title":"互联网协议简述","uri":"/net-protocol-glance/#参考"},{"categories":null,"content":" 我们已经在一起 ","date":"2022-03-25","objectID":"/love/:0:0","series":null,"tags":null,"title":"Since 2019/09/17","uri":"/love/#我们已经在一起"},{"categories":["开发者手册"],"content":"OAuth2.0,第三方登录,令牌,TOKEN,授权码,权限","date":"2022-03-22","objectID":"/oauth2.0/","series":null,"tags":["web","oauth"],"title":"OAuth2.0的理解与应用","uri":"/oauth2.0/"},{"categories":["开发者手册"],"content":" 什么是 OAuth2.0OAuth 的核心就是向第三方应用颁发令牌，比如网站A想用Github的信息，那么对于Github来说，网站A就是第三方应用。 第三方应用申请令牌之前，都必须先到系统备案，比如申请Github的令牌，得先到github备案登记， 说明自己的身份，然后会拿到两个身份识别码：客户端 ID（client ID）和客户端密钥（client secret）。这是为了防止令牌被滥用，没有备案过的第三方应用，是不会拿到令牌的。 关于 OAuth2.0 是什么可以参考一下文章： OAuth 2.0 的一个简单解释 [简易图解]『 OAuth2.0』 『进阶』 授权模式总结 ","date":"2022-03-22","objectID":"/oauth2.0/:1:0","series":null,"tags":["web","oauth"],"title":"OAuth2.0的理解与应用","uri":"/oauth2.0/#什么是-oauth20"},{"categories":["开发者手册"],"content":" 第三方登录Github所谓第三方登录，实质就是 OAuth 授权。用户想要登录 A 网站，A 网站让用户提供第三方网站的数据，证明自己的身份。获取第三方网站的身份数据，就需要 OAuth 授权。 比如，A 网站允许 GitHub 登录，背后就是下面的流程： A 网站让用户跳转到 GitHub。 GitHub 要求用户登录，然后询问\"A 网站要求获得 xx 权限，你是否同意？\" 用户同意，GitHub 就会重定向回 A 网站，同时发回一个授权码。 A 网站使用授权码，向 GitHub 请求令牌。 GitHub 返回令牌. A 网站使用令牌，向 GitHub 请求用户数据。 ","date":"2022-03-22","objectID":"/oauth2.0/:2:0","series":null,"tags":["web","oauth"],"title":"OAuth2.0的理解与应用","uri":"/oauth2.0/#第三方登录github"},{"categories":["开发者手册"],"content":" 注册 OAuth 应用现在在 Github 上注册一个 OAuth 应用。 字段 描述 Application name 应用名称 Homepage URL 首页URL，如https://www.xiaobinqt.cn Authorization callback URL 用户在 Github 登录成功后重定向回的 URL 注册成功后会生成 Client ID 和 Client Secret，这两个是用来请求令牌的。 ","date":"2022-03-22","objectID":"/oauth2.0/:2:1","series":null,"tags":["web","oauth"],"title":"OAuth2.0的理解与应用","uri":"/oauth2.0/#注册-oauth-应用"},{"categories":["开发者手册"],"content":" 通过 OAuth 获取用户信息前端界面 oauth.html \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003eTitle\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ca href=\"https://github.com/login/oauth/authorize?client_id={{.ClientId}}\u0026redirect_uri={{.RedirectUrl}}\"\u003e Github 第三方授权登录\u003c/a\u003e \u003c/body\u003e \u003c/html\u003e go 代码通过OAuth获取用户信息 package main import ( \"encoding/json\" \"flag\" \"fmt\" \"html/template\" \"io/ioutil\" \"log\" \"net/http\" \"os\" ) var ( clientSecret = flag.String(\"cs\", \"\", \"github oauth client secret\") clientID = flag.String(\"ci\", \"\", \"github oauth client id\") ) type Conf struct { ClientId string ClientSecret string RedirectUrl string } type Token struct { AccessToken string `json:\"access_token\"` } // 认证并获取用户信息 func OAuth(w http.ResponseWriter, r *http.Request) { var ( err error ) // 获取 code code := r.URL.Query().Get(\"code\") // 通过 code, 获取 token var tokenAuthUrl = GetTokenAuthURL(code) var token *Token if token, err = GetToken(tokenAuthUrl); err != nil { fmt.Println(err) return } // 通过token，获取用户信息 var userInfo map[string]interface{} if userInfo, err = GetUserInfo(token); err != nil { fmt.Println(\"获取用户信息失败，错误信息为:\", err) return } // 将用户信息返回前端 var userInfoBytes []byte if userInfoBytes, err = json.Marshal(userInfo); err != nil { fmt.Println(\"在将用户信息(map)转为用户信息([]byte)时发生错误，错误信息为:\", err) return } w.Header().Set(\"Content-Type\", \"application/json\") if _, err = w.Write(userInfoBytes); err != nil { fmt.Println(\"在将用户信息([]byte)返回前端时发生错误，错误信息为:\", err) return } } // 通过code获取token认证url func GetTokenAuthURL(code string) string { return fmt.Sprintf( \"https://github.com/login/oauth/access_token?client_id=%s\u0026client_secret=%s\u0026code=%s\", *clientID, *clientSecret, code, ) } // 获取 token func GetToken(url string) (*Token, error) { // 形成请求 var req *http.Request var err error if req, err = http.NewRequest(http.MethodGet, url, nil); err != nil { return nil, err } req.Header.Set(\"accept\", \"application/json\") // 发送请求并获得响应 var ( httpClient = http.Client{} res *http.Response respBody = make([]byte, 0) token Token ) if res, err = httpClient.Do(req); err != nil { return nil, err } respBody, err = ioutil.ReadAll(res.Body) if err != nil { return nil, err } log.Printf(\"token: %s\", string(respBody)) // 将响应体解析为 token，并返回 err = json.Unmarshal(respBody, \u0026token) if err != nil { return nil, err } return \u0026token, nil } // 获取用户信息 func GetUserInfo(token *Token) (map[string]interface{}, error) { // 形成请求 var userInfoUrl = \"https://api.github.com/user\" // github用户信息获取接口 var req *http.Request var err error if req, err = http.NewRequest(http.MethodGet, userInfoUrl, nil); err != nil { return nil, err } req.Header.Set(\"accept\", \"application/json\") req.Header.Set(\"Authorization\", fmt.Sprintf(\"token %s\", token.AccessToken)) // 发送请求并获取响应 var client = http.Client{} var res *http.Response if res, err = client.Do(req); err != nil { return nil, err } // 将响应的数据写入 userInfo 中，并返回 var userInfo = make(map[string]interface{}) if err = json.NewDecoder(res.Body).Decode(\u0026userInfo); err != nil { return nil, err } return userInfo, nil } func Html(w http.ResponseWriter, r *http.Request) { // 解析指定文件生成模板对象 var ( temp *template.Template err error ) dir, _ := os.Getwd() if temp, err = template.ParseFiles(dir + \"/oauth.html\"); err != nil { fmt.Println(\"读取文件失败，错误信息为:\", err) return } // 利用给定数据渲染模板(html页面)，并将结果写入w，返回给前端 if err = temp.Execute(w, Conf{ ClientId: *clientID, ClientSecret: *clientSecret, RedirectUrl: \"http://127.0.0.1:9000/oauth/callback\", }); err != nil { fmt.Println(\"读取渲染html页面失败，错误信息为:\", err) return } } func UserInfo(w http.ResponseWriter, r *http.Request) { token := r.URL.Query().Get(\"token\") log.Printf(\"UserInfo token: %s\", token) var ( err error userInfo map[string]interface{} ) if userInfo, err = GetUserInfo(\u0026Token{AccessToken: token}); err != nil { fmt.Println(\"获取用户信息失败，错误信息为:\", err) return } // 将用户信息返回前端 var userInfoBytes []byte if userInfoBytes, err = json.Marshal(userInfo); err != nil { fmt.Println(\"在将用户信息(map)转为用户信息([]byte)时发生错误，错误信息为:\", err) return } w.Header().Set(\"Content-Type\", \"application/json\") if _, err = w.Write(u","date":"2022-03-22","objectID":"/oauth2.0/:2:2","series":null,"tags":["web","oauth"],"title":"OAuth2.0的理解与应用","uri":"/oauth2.0/#通过-oauth-获取用户信息"},{"categories":["开发者手册"],"content":" 效果 前端界面 授权页面 github返回的用户信息 ","date":"2022-03-22","objectID":"/oauth2.0/:3:0","series":null,"tags":["web","oauth"],"title":"OAuth2.0的理解与应用","uri":"/oauth2.0/#效果"},{"categories":["开发者手册"],"content":" 源码源码地址 ","date":"2022-03-22","objectID":"/oauth2.0/:4:0","series":null,"tags":["web","oauth"],"title":"OAuth2.0的理解与应用","uri":"/oauth2.0/#源码"},{"categories":["开发者手册"],"content":" 参考 Building OAuth Apps OAuth 2.0 的一个简单解释 Go语言实现第三方登录Github (通过OAuth2.0) basics of authentication [简易图解]『 OAuth2.0』 『进阶』 授权模式总结 ","date":"2022-03-22","objectID":"/oauth2.0/:5:0","series":null,"tags":["web","oauth"],"title":"OAuth2.0的理解与应用","uri":"/oauth2.0/#参考"},{"categories":["开发者手册"],"content":"HUGO,hugo主题标题支持emoji,emoji表情","date":"2022-03-21","objectID":"/hugo-title-support-emoji/","series":null,"tags":["hugo","emoji"],"title":"hugo主题标题支持emoji:smile:","uri":"/hugo-title-support-emoji/"},{"categories":["开发者手册"],"content":" 解决方法hugo 在渲染时默认是不支持标题中的emoji的（有的主题也许是支持的），可以通过修改主题源码来支持。 我用的主题是LoveIt，找到 simple.html 文件，路径为 themes/LoveIt/layouts/posts/single.html 修改标题的渲染方式为 {{ .Title | emojify }}，如下： 这样就可以支持 emoji 了。 此时列表中还不支持 emoji，同样的修改方式。 修改 themes/LoveIt/layouts/_default/summary.html 文件文件中的 title 的渲染方式为 {{ .Title | emojify }}。 ","date":"2022-03-21","objectID":"/hugo-title-support-emoji/:1:0","series":null,"tags":["hugo","emoji"],"title":"hugo主题标题支持emoji:smile:","uri":"/hugo-title-support-emoji/#解决方法"},{"categories":["开发者手册"],"content":" 参考 Hugo should render emojis in page titles if enableEmoji = true ","date":"2022-03-21","objectID":"/hugo-title-support-emoji/:2:0","series":null,"tags":["hugo","emoji"],"title":"hugo主题标题支持emoji:smile:","uri":"/hugo-title-support-emoji/#参考"},{"categories":["理解计算机"],"content":" tcp,tcp连接管理,三次握手,四次挥手,为什么建立连接需要三次握手,为什么不能用两次握手进行连接,SYN,FIN,ACK,PSH,SYN_SENT,SYN_RECV,ESTABLISHED,FIN_WAIT_1,FIN_WAIT_2,CLOSE_WAIT,LAST_ACK,TIME_WAIT,CLOSE","date":"2022-03-21","objectID":"/tcp-handshark/","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/"},{"categories":["理解计算机"],"content":" 名词解释 名词 解释 SYN 同步序号，用于建立连接过程，在连接请求中，SYN=1 和 ACK=0 表示该数据段没有使用捎带的确认域，而连接应答捎带一个确认，即 SYN=1 和 ACK=1 FIN finish 标志，用于释放连接，为 1 时表示发送方已经没有数据发送了，即关闭本方数据流 ACK 确认序号标志，为 1 时表示确认号有效，为 0 表示报文中不含确认信息，忽略确认号字段 PSH push 标志，为 1 表示是带有 push 标志的数据，指示接收方在接收到该报文段以后，应尽快将这个报文段交给应用程序，而不是在缓冲区排队 RST 重置连接标志，用于重置由于主机崩溃或其他原因而出现错误的连接。或者用于拒绝非法的报文段和拒绝连接请求 序列号 seq 占 4 个字节，用来标记数据段的顺序，TCP 把连接中发送的所有数据字节都编上一个序号，第一个字节的编号由本地随机产生；给字节编上序号后，就给每一个报文段指派一个序号；序列号 seq 就是这个报文段中的第一个字节的数据编号 确认号 ack 占 4 个字节，期待收到对方下一个报文段的第一个数据字节的序号；序列号表示报文段携带数据的第一个字节的编号；而确认号指的是期望接收到下一个字节的编号；因此当前报文段最后一个字节的编号 +1 （ACK会占一个序号）即为确认号 Info ACK、SYN 和 FIN 这些大写的单词表示标志位，其值要么是 1，要么是 0；ack、seq 小写的单词表示序号。 ACK 是可能与 SYN，FIN 等同时使用的。比如 SYN和ACK可能同时为 1，它表示的就是建立连接之后的响应搜索 如果只是单个的一个SYN，它表示的只是建立连接。 SYN与FIN是不会同时为 1 的，因为前者表示的是建立连接，而后者表示的是断开连接。 RST一般是在FIN之后才会出现为 1 的情况，表示的是连接重置。 一般，当出现FIN包或RST包时，便认为客户端与服务器端断开了连接；而当出现SYN和SYN＋ACK包时，我们认为客户端与服务器建立了一个连接。 PSH为 1 的情况，一般只出现在DATA内容不为 0 的包中，也就是说PSH为1表示的是有真正的 TCP 数据包内容被传递。 ","date":"2022-03-21","objectID":"/tcp-handshark/:1:0","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#名词解释"},{"categories":["理解计算机"],"content":" 三次握手 三次握手 ","date":"2022-03-21","objectID":"/tcp-handshark/:2:0","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#三次握手"},{"categories":["理解计算机"],"content":" 第一次握手 客户端主动打开（active open），向服务端发送 SYN 报文段SYN=1, SN=client_isn, OPT=client_mss，请求建立连接。 client_isn 是客户端初始序号，动态生成，用于实现可靠传输，client_sn-client_isn 等于客户端已发送字节数。 SYN 报文段虽然不能携带数据，但是会消耗一个序号（相当于发送了1个字节的有效数据），下次客户端再向服务端发送的报文段中 SN=client_isn+1。 除了 SYN 报文段和 ACK-SYN 报文段，其他所有后续报文段的序号 SN 值都等于上次接收的 ACK 报文段中的确认号 AN 值。 client_mss 是客户端最大报文段长度，在 TCP 首部的选项和填充部分，会在客户端与服务端的 MSS 中选择一个较小值使用。 客户端变为 SYN_SENT 状态，然后等待服务端 ACK 报文段。 ","date":"2022-03-21","objectID":"/tcp-handshark/:2:1","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#第一次握手"},{"categories":["理解计算机"],"content":" 第一次握手 客户端主动打开（active open），向服务端发送 SYN 报文段SYN=1, SN=client_isn, OPT=client_mss，请求建立连接。 client_isn 是客户端初始序号，动态生成，用于实现可靠传输，client_sn-client_isn 等于客户端已发送字节数。 SYN 报文段虽然不能携带数据，但是会消耗一个序号（相当于发送了1个字节的有效数据），下次客户端再向服务端发送的报文段中 SN=client_isn+1。 除了 SYN 报文段和 ACK-SYN 报文段，其他所有后续报文段的序号 SN 值都等于上次接收的 ACK 报文段中的确认号 AN 值。 client_mss 是客户端最大报文段长度，在 TCP 首部的选项和填充部分，会在客户端与服务端的 MSS 中选择一个较小值使用。 客户端变为 SYN_SENT 状态，然后等待服务端 ACK 报文段。 ","date":"2022-03-21","objectID":"/tcp-handshark/:2:1","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#客户端"},{"categories":["理解计算机"],"content":" 第二次握手 服务端接收来自客户端的 SYN 报文段，得知客户端发送能力正常。 被动打开passive open，向客户端发送 SYN-ACK 报文段ACK=1, AN=client_isn+1, SYN=1, SN=server_isn, OPT=server_mss ，应答来自客户端的建立连接请求并向客户端发起建立连接请求。 SN=server_isn 是服务端初始序号，ACK-SYN 报文段虽然不能携带数据，但是会消耗一个序号（相当于发送了1个字节的有效数据），下次服务端再向客户端发送的报文中 SN=server_isn+1 。 OPT=server_mss 是服务端最大报文段长度。 AN=client_isn+1 是确认号，表明服务端接下来要开始接收来自客户端的第 client_isn+1 个字节的有效数据。 服务端变为 SYN_RCVD 状态，并等待客户端 ACK 报文段。 ","date":"2022-03-21","objectID":"/tcp-handshark/:2:2","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#第二次握手"},{"categories":["理解计算机"],"content":" 第二次握手 服务端接收来自客户端的 SYN 报文段，得知客户端发送能力正常。 被动打开passive open，向客户端发送 SYN-ACK 报文段ACK=1, AN=client_isn+1, SYN=1, SN=server_isn, OPT=server_mss ，应答来自客户端的建立连接请求并向客户端发起建立连接请求。 SN=server_isn 是服务端初始序号，ACK-SYN 报文段虽然不能携带数据，但是会消耗一个序号（相当于发送了1个字节的有效数据），下次服务端再向客户端发送的报文中 SN=server_isn+1 。 OPT=server_mss 是服务端最大报文段长度。 AN=client_isn+1 是确认号，表明服务端接下来要开始接收来自客户端的第 client_isn+1 个字节的有效数据。 服务端变为 SYN_RCVD 状态，并等待客户端 ACK 报文段。 ","date":"2022-03-21","objectID":"/tcp-handshark/:2:2","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#服务端"},{"categories":["理解计算机"],"content":" 第三次握手 客户端接收来自服务端的 SYN-ACK 报文段，得知服务端发送能力和接收能力都正常。 向客户端发送 ACK 报文段ACK=1, AN=server_isn+1, SN=client_isn+1, MESSAGE=message，应答来自服务端的建立连接请求。 SN=client_isn+1 是序号，表明当前报文段发送的有效数据首字节是从请求建立连接到现在为止客户端向服务端发送的第 (client_isn+1)-clien_isn+1=2 个字节的有效数据。 有效数据：一般有效数据指的是应用层的报文数据，不过 SYN 报文段、 ACK-SYN 报文段和 FIN 报文段虽然没有携带报文数据，但认为发送了1个字节的有效数据。 AN=server_isn+1 是确认号，表明客户端接下来要开始接收来自服务端的第 server_isn+1 个字节的有效数据。 MESSAGE=message 此时可以在报文段中携带客户端到服务端的报文数据；该 ACK 报文段消耗的序号个数等于 message_length（注意 message_length 可以等于0，即不携带有效数据，此时 ACK报文段不消耗序号），下次客户端再向服务端发送的报文段中 SN=client_isn+1+message_length 。 客户端变为 ESTABLISHED 状态，client——\u003eserver 数据流建立。 服务端接收来自客户端的 ACK 报文段，得知客户端接收能力正常。 变为 ESTABLISHED 状态，server——\u003eclient 数据流也建立。 ","date":"2022-03-21","objectID":"/tcp-handshark/:2:3","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#第三次握手"},{"categories":["理解计算机"],"content":" 第三次握手 客户端接收来自服务端的 SYN-ACK 报文段，得知服务端发送能力和接收能力都正常。 向客户端发送 ACK 报文段ACK=1, AN=server_isn+1, SN=client_isn+1, MESSAGE=message，应答来自服务端的建立连接请求。 SN=client_isn+1 是序号，表明当前报文段发送的有效数据首字节是从请求建立连接到现在为止客户端向服务端发送的第 (client_isn+1)-clien_isn+1=2 个字节的有效数据。 有效数据：一般有效数据指的是应用层的报文数据，不过 SYN 报文段、 ACK-SYN 报文段和 FIN 报文段虽然没有携带报文数据，但认为发送了1个字节的有效数据。 AN=server_isn+1 是确认号，表明客户端接下来要开始接收来自服务端的第 server_isn+1 个字节的有效数据。 MESSAGE=message 此时可以在报文段中携带客户端到服务端的报文数据；该 ACK 报文段消耗的序号个数等于 message_length（注意 message_length 可以等于0，即不携带有效数据，此时 ACK报文段不消耗序号），下次客户端再向服务端发送的报文段中 SN=client_isn+1+message_length 。 客户端变为 ESTABLISHED 状态，client——\u003eserver 数据流建立。 服务端接收来自客户端的 ACK 报文段，得知客户端接收能力正常。 变为 ESTABLISHED 状态，server——\u003eclient 数据流也建立。 ","date":"2022-03-21","objectID":"/tcp-handshark/:2:3","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#客户端-1"},{"categories":["理解计算机"],"content":" 第三次握手 客户端接收来自服务端的 SYN-ACK 报文段，得知服务端发送能力和接收能力都正常。 向客户端发送 ACK 报文段ACK=1, AN=server_isn+1, SN=client_isn+1, MESSAGE=message，应答来自服务端的建立连接请求。 SN=client_isn+1 是序号，表明当前报文段发送的有效数据首字节是从请求建立连接到现在为止客户端向服务端发送的第 (client_isn+1)-clien_isn+1=2 个字节的有效数据。 有效数据：一般有效数据指的是应用层的报文数据，不过 SYN 报文段、 ACK-SYN 报文段和 FIN 报文段虽然没有携带报文数据，但认为发送了1个字节的有效数据。 AN=server_isn+1 是确认号，表明客户端接下来要开始接收来自服务端的第 server_isn+1 个字节的有效数据。 MESSAGE=message 此时可以在报文段中携带客户端到服务端的报文数据；该 ACK 报文段消耗的序号个数等于 message_length（注意 message_length 可以等于0，即不携带有效数据，此时 ACK报文段不消耗序号），下次客户端再向服务端发送的报文段中 SN=client_isn+1+message_length 。 客户端变为 ESTABLISHED 状态，client——\u003eserver 数据流建立。 服务端接收来自客户端的 ACK 报文段，得知客户端接收能力正常。 变为 ESTABLISHED 状态，server——\u003eclient 数据流也建立。 ","date":"2022-03-21","objectID":"/tcp-handshark/:2:3","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#服务端-1"},{"categories":["理解计算机"],"content":" 四次挥手 TCP四次挥手 断开连接前，客户端和服务端都处于 ESTABLISHED 状态，两者谁都可以先发起断开连接请求。以下假设客户端先发起断开连接请求。 ","date":"2022-03-21","objectID":"/tcp-handshark/:3:0","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#四次挥手"},{"categories":["理解计算机"],"content":" 第一次挥手 客户端向服务端发送 FIN 报文段FIN=1, SN=client_sn，请求断开连接。 SN=client_sn是序号，表明当前报文段发送的有效数据首字节是从请求建立连接到现在为止客户端向服务端发送的第 client_sn-clien_isn+1 个字节的有效数据。 FIN 报文段虽然不能携带数据，但是会消耗一个序号（相当于发送了1个字节的有效数据），下次客户端再向服务端发送的报文中 SN=client_isn+1 。 客户端变为 FIN_WAIT1 状态，等待服务端 ACK 报文段。 ","date":"2022-03-21","objectID":"/tcp-handshark/:3:1","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#第一次挥手"},{"categories":["理解计算机"],"content":" 第一次挥手 客户端向服务端发送 FIN 报文段FIN=1, SN=client_sn，请求断开连接。 SN=client_sn是序号，表明当前报文段发送的有效数据首字节是从请求建立连接到现在为止客户端向服务端发送的第 client_sn-clien_isn+1 个字节的有效数据。 FIN 报文段虽然不能携带数据，但是会消耗一个序号（相当于发送了1个字节的有效数据），下次客户端再向服务端发送的报文中 SN=client_isn+1 。 客户端变为 FIN_WAIT1 状态，等待服务端 ACK 报文段。 ","date":"2022-03-21","objectID":"/tcp-handshark/:3:1","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#客户端-2"},{"categories":["理解计算机"],"content":" 第二次挥手 服务端接收来自客户端的 FIN 报文段。 向客户端发送 ACK 报文段ACK=1, AN=client_sn+1, SN=server_sn_wave2，应答客户端的断开连接请求。 SN=server_sn_wave2 是序号，表明当前报文段发送的有效数据首字节是从请求建立连接到现在为止服务端向客户端发送的第 server_sn_wave2-client_isn+1 个字节的有效数据。 AN=client_sn+1 是确认号，表明服务端接下来要开始接收来自客户端的第 client_sn+1 个字节的有效数据。 此时服务端变为 CLOSE_WAIT 状态。 客户端接收来自服务端的 ACK 包。 变为 FIN_WAIT2 状态，等待服务端关闭连接请求FIN报文段。 ","date":"2022-03-21","objectID":"/tcp-handshark/:3:2","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#第二次挥手"},{"categories":["理解计算机"],"content":" 第二次挥手 服务端接收来自客户端的 FIN 报文段。 向客户端发送 ACK 报文段ACK=1, AN=client_sn+1, SN=server_sn_wave2，应答客户端的断开连接请求。 SN=server_sn_wave2 是序号，表明当前报文段发送的有效数据首字节是从请求建立连接到现在为止服务端向客户端发送的第 server_sn_wave2-client_isn+1 个字节的有效数据。 AN=client_sn+1 是确认号，表明服务端接下来要开始接收来自客户端的第 client_sn+1 个字节的有效数据。 此时服务端变为 CLOSE_WAIT 状态。 客户端接收来自服务端的 ACK 包。 变为 FIN_WAIT2 状态，等待服务端关闭连接请求FIN报文段。 ","date":"2022-03-21","objectID":"/tcp-handshark/:3:2","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#服务端-2"},{"categories":["理解计算机"],"content":" 第二次挥手 服务端接收来自客户端的 FIN 报文段。 向客户端发送 ACK 报文段ACK=1, AN=client_sn+1, SN=server_sn_wave2，应答客户端的断开连接请求。 SN=server_sn_wave2 是序号，表明当前报文段发送的有效数据首字节是从请求建立连接到现在为止服务端向客户端发送的第 server_sn_wave2-client_isn+1 个字节的有效数据。 AN=client_sn+1 是确认号，表明服务端接下来要开始接收来自客户端的第 client_sn+1 个字节的有效数据。 此时服务端变为 CLOSE_WAIT 状态。 客户端接收来自服务端的 ACK 包。 变为 FIN_WAIT2 状态，等待服务端关闭连接请求FIN报文段。 ","date":"2022-03-21","objectID":"/tcp-handshark/:3:2","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#客户端-3"},{"categories":["理解计算机"],"content":" 第三次挥手 服务端（服务端想断开连接时）向客户端发送 FIN 报文段FIN=1, SN=server_sn，请求断开连接。 SN=server_sn 是序号，表明当前报文段发送的有效数据首字节是从请求建立连接到现在为止服务端向客户端发送的第 server_sn-clien_isn+1 个字节的有效数据。 FIN 报文段虽然不能携带数据，但是会消耗一个序号（相当于发送了1个字节的有效数据），下次服务端再向客户端发送的报文中 SN=client_isn+2 （若断开连接成功，则服务端不会再向客户端发送下一个报文段）。 第二次挥手和第三次挥手之间，服务端又向客户端发送了 server_sn - server_sn_wave2 个字节的有效数据。 服务端变为 LAST_ACK 状态，等待客户端的 ACK 报文段。 ","date":"2022-03-21","objectID":"/tcp-handshark/:3:3","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#第三次挥手"},{"categories":["理解计算机"],"content":" 第三次挥手 服务端（服务端想断开连接时）向客户端发送 FIN 报文段FIN=1, SN=server_sn，请求断开连接。 SN=server_sn 是序号，表明当前报文段发送的有效数据首字节是从请求建立连接到现在为止服务端向客户端发送的第 server_sn-clien_isn+1 个字节的有效数据。 FIN 报文段虽然不能携带数据，但是会消耗一个序号（相当于发送了1个字节的有效数据），下次服务端再向客户端发送的报文中 SN=client_isn+2 （若断开连接成功，则服务端不会再向客户端发送下一个报文段）。 第二次挥手和第三次挥手之间，服务端又向客户端发送了 server_sn - server_sn_wave2 个字节的有效数据。 服务端变为 LAST_ACK 状态，等待客户端的 ACK 报文段。 ","date":"2022-03-21","objectID":"/tcp-handshark/:3:3","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#服务端-3"},{"categories":["理解计算机"],"content":" 第四次挥手 客户端接收来自服务端的 FIN 报文段。 向服务端发送 ACK 报文段ACK=1, AN=server_sn+1, SN=client_sn+1，应答服务端断开连接请求。 client_sn+1 是序号，表明当前报文段发送的有效数据首字节是从请求建立连接到现在为止客户端向客户端发送的第 client_isn+1)-clien_isn+1 个字节的有效数据。AN=server_sn+1 是确认号，表明服务端接下来要开始接收来自客户端的第 client_sn+1 个字节的有效数据。 客户端变为 TIME_WAIT 状态，等待2MSL时间后进入 CLOSED 状态，至此 client——\u003eserver 数据流被关闭。 服务端接收来自客户端的 ACK 报文段。 变为 CLOSED 状态，至此 server——\u003eclient 数据流被关闭。 Tips 当收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据。 ","date":"2022-03-21","objectID":"/tcp-handshark/:3:4","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#第四次挥手"},{"categories":["理解计算机"],"content":" 第四次挥手 客户端接收来自服务端的 FIN 报文段。 向服务端发送 ACK 报文段ACK=1, AN=server_sn+1, SN=client_sn+1，应答服务端断开连接请求。 client_sn+1 是序号，表明当前报文段发送的有效数据首字节是从请求建立连接到现在为止客户端向客户端发送的第 client_isn+1)-clien_isn+1 个字节的有效数据。AN=server_sn+1 是确认号，表明服务端接下来要开始接收来自客户端的第 client_sn+1 个字节的有效数据。 客户端变为 TIME_WAIT 状态，等待2MSL时间后进入 CLOSED 状态，至此 client——\u003eserver 数据流被关闭。 服务端接收来自客户端的 ACK 报文段。 变为 CLOSED 状态，至此 server——\u003eclient 数据流被关闭。 Tips 当收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据。 ","date":"2022-03-21","objectID":"/tcp-handshark/:3:4","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#客户端-4"},{"categories":["理解计算机"],"content":" 第四次挥手 客户端接收来自服务端的 FIN 报文段。 向服务端发送 ACK 报文段ACK=1, AN=server_sn+1, SN=client_sn+1，应答服务端断开连接请求。 client_sn+1 是序号，表明当前报文段发送的有效数据首字节是从请求建立连接到现在为止客户端向客户端发送的第 client_isn+1)-clien_isn+1 个字节的有效数据。AN=server_sn+1 是确认号，表明服务端接下来要开始接收来自客户端的第 client_sn+1 个字节的有效数据。 客户端变为 TIME_WAIT 状态，等待2MSL时间后进入 CLOSED 状态，至此 client——\u003eserver 数据流被关闭。 服务端接收来自客户端的 ACK 报文段。 变为 CLOSED 状态，至此 server——\u003eclient 数据流被关闭。 Tips 当收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据。 ","date":"2022-03-21","objectID":"/tcp-handshark/:3:4","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#服务端-4"},{"categories":["理解计算机"],"content":" 常见问题 ❓ 为什么建立连接需要“三次”握手 客户端和服务端之间建立的TCP是全双工通信，双方都要确保对方发送能力和接收能力正常。 一次握手后，服务端得知客户端发送能力正常。 二次握手后，客户端得知服务端接收能力和发送能力正常。 三次握手后，服务端得知客户端接收能力正常。 ❓ 为什么第四次挥手时要等待2MSL的时间再进入CLOSED状态 MSL（Maximum Segment Lifetime，报文段最大生存时间）是一个未被接受的报文段在网络中被丢弃前存活的最大时间。 保证建立新连接时网络中不存在上次连接时发送的数据包，进入 CLOSED 状态意味着可以建立新连接，等待 \u003eMSL 的时间再进入 CLOSED 状态可以保证建立新连接后，网络中不会存在上次连接时发送出去的数据包。若网络中同时存在发送端在两次连接中发出的数据包，对接收端接收数据可能会有影响。 保证第四次挥手发送的 ACK 能到达接收端，第四次挥手发送的 ACK 可能会出现丢包，另一端接收不到 ACK 会重新发送 FIN。等待 2MSL 的时间可以应对该情况，重发 ACK ，保证另一端能正常关闭连接。 ❓ 已经建立了连接，客户端突然出现故障怎么办 TCP 设有一个保活计时器，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为 2 小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔 75 秒钟发送一次。若一连发送 10 个探测报文仍然没反应， 服务器就认为客户端出了故障，接着就关闭连接。 ❓ 为什么不能用两次握手进行连接 三次握手完成两个重要的功能，既要双方做好发送数据的准备工作（双方都知道彼此已准备好），也要允许双方就初始序列号进行协商，这个序列号在握手过程中被发送和确认。 现在把三次握手改成仅需要两次握手，死锁是可能发生的。 比如，计算机 S 和 C 之间的通信，假定 C 给 S 发送一个连接请求分组，S 收到了这个分组，并发送了确认应答分组。按照两次握手的协定，S 认为连接已经成功地建立了，可以开始发送数据分组。 可是，C 在 S 的应答分组在传输中被丢失的情况下，将不知道 S 是否已准备好，不知道 S 建立什么样的序列号，C 甚至怀疑 S 是否收到自己的连接请求分组。在这种情况下，C 认为连接还未建立成功，将忽略 S 发来的任何数据分 组，只等待连接确认应答分组，而 S 在发出的分组超时后，重复发送同样的分组，这样就形成了死锁。 ","date":"2022-03-21","objectID":"/tcp-handshark/:4:0","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#常见问题"},{"categories":["理解计算机"],"content":" 参考 计算机网络——TCP连接管理（三次握手和四次挥手） “三次握手，四次挥手”你真的懂吗？ TCP的三次握手与四次挥手理解及面试题（很全面） TCP报文格式详解 面试官，不要再问我三次握手和四次挥手 ","date":"2022-03-21","objectID":"/tcp-handshark/:5:0","series":null,"tags":["tcp"],"title":"TCP连接管理","uri":"/tcp-handshark/#参考"},{"categories":[""],"content":" 前小端 技术为本，创意为先 MakerLi 向着全栈工程师前进 西瓜 liupray xiaowuneng ","date":"2022-03-19","objectID":"/links/:0:0","series":null,"tags":[""],"title":"友情链接","uri":"/links/#"},{"categories":["web"],"content":"JS运行机制,javascript异步运行机制,js异步同步,js Promise,js catch reject,js执行顺讯,async/await,js微任务和宏任务","date":"2022-03-18","objectID":"/js-cb-asyn/","series":null,"tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/"},{"categories":["web"],"content":" 执行模式JS的执行模式是单线程的，当有多个任务时必须排队执行，优点是执行环境简单，缺点是性能低下，当有多个任务时，需要等待上一个任务执行完成才能执行下一个任务， 如果某个任务出现了死循环，那么就会导致程序崩溃。 所以JS出现了同步和异步的概念。 ","date":"2022-03-18","objectID":"/js-cb-asyn/:1:0","series":null,"tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/#执行模式"},{"categories":["web"],"content":" 同步后一个任务等待前一个任务结束，然后再执行，程序的执行顺序与任务的排列顺序是一致的。 ","date":"2022-03-18","objectID":"/js-cb-asyn/:1:1","series":null,"tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/#同步"},{"categories":["web"],"content":" 异步每一个任务有一个或多个回调函数（callback），前一个任务结束后，不是执行后一个任务，而是执行回调函数，后一个任务则是不等前一个任务结束就执行，所以程序的执行顺序与任务的排列顺序可能是不一致的。 ","date":"2022-03-18","objectID":"/js-cb-asyn/:1:2","series":null,"tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/#异步"},{"categories":["web"],"content":" Event Loop// TODO ","date":"2022-03-18","objectID":"/js-cb-asyn/:2:0","series":null,"tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/#event-loop"},{"categories":["web"],"content":" PromisePromise 对象代表一个异步操作，then() 第一个参数是成功resolve的回调函数，第二个参数是失败reject的回调函数，当不写第二个 then() 参数时，可以用 catch() 捕获 reject 异常。 ","date":"2022-03-18","objectID":"/js-cb-asyn/:3:0","series":null,"tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/#promise"},{"categories":["web"],"content":" 使用 var p1 = new Promise(function (resolve, reject) { // resolve('成功'); reject(\"失败\") }); p1.then(function (res) { console.log(\"第一个fn: \", res) }, function (res) { console.log(\"第二个 fn: \", res) }); resolve和reject除了正常的值外，还可能是另一个promise实例。 const p1 = new Promise(function (resolve, reject) { resolve(1) }); const p2 = new Promise(function (resolve, reject) { // ... resolve(p1); }) p2.then(function (res) { console.log(res) }, function (res) { }) 用 catch 捕获 reject 异常 var p1 = new Promise(function (resolve, reject) { // todo... reject(111111) }); p1.then(function (res) { console.log(\"第一个fn: \", res) }).catch(function (err) { console.log(\"err :\", err) }).finally(function () { console.log(\"finally exec...\") }) ","date":"2022-03-18","objectID":"/js-cb-asyn/:3:1","series":null,"tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/#使用"},{"categories":["web"],"content":" 执行顺序","date":"2022-03-18","objectID":"/js-cb-asyn/:3:2","series":null,"tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/#执行顺序"},{"categories":["web"],"content":" async/await的用法和理解async 函数是非常新的语法功能，在 ES7 中可用。 async 函数返回一个 Promise 对象，可以使用 then 方法添加回调函数。await 作为修饰符，只能放在 async 内部使用。 当函数执行的时候，一旦遇到 await 就会先返回，等到触发的异步操作完成，再接着执行函数体内后面的语句。 await 等待右侧表达式的结果。 如果等到的不是一个 promise 对象，那 await 表达式的运算结果就是它等到的东西。 如果它等到的是一个 promise 对象，它会阻塞后面的代码，等着 promise 对象 resolve，然后得到 resolve 的值，作为 await 表达式的运算结果。 async function test() { let promise = new Promise(resolve =\u003e { setTimeout(() =\u003e resolve(\"test\"), 2000); }); await promise.then((ret) =\u003e { console.log(ret) }) let test1Ret = await test1() console.log(test1Ret) console.log(\"test end...\") } function test1() { return \"test1_return\" } test(); console.log('end') ","date":"2022-03-18","objectID":"/js-cb-asyn/:4:0","series":null,"tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/#asyncawait的用法和理解"},{"categories":["web"],"content":" 宏任务和微任务// TODO ","date":"2022-03-18","objectID":"/js-cb-asyn/:5:0","series":null,"tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/#宏任务和微任务"},{"categories":["web"],"content":" 参考 Javascript异步编程的4种方法 JavaScript 运行机制详解：再谈Event Loop async 函数的含义和用法 JS执行——Promise 你真的了解回调? 回调地狱 js中微任务和宏任务的区别 Javascript单线程和事件循环 ","date":"2022-03-18","objectID":"/js-cb-asyn/:6:0","series":null,"tags":["js"],"title":"JS运行机制","uri":"/js-cb-asyn/#参考"},{"categories":["理解计算机"],"content":"HTTP,HTTP协议,超文本传输协议,互联网,TCP/IP,Transmission Control Protocol,传输控制协议,ISO","date":"2022-03-17","objectID":"/http-glance/","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/"},{"categories":["理解计算机"],"content":"该笔记是在学习《透视 HTTP 协议》时整理，还参考了网上的其他资料。鄙人只是网络世界的搬运整理工😂。 ","date":"2022-03-17","objectID":"/http-glance/:0:0","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#"},{"categories":["理解计算机"],"content":" 总览 ","date":"2022-03-17","objectID":"/http-glance/:1:0","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#总览"},{"categories":["理解计算机"],"content":" http 协议http（超文本传输协议）是一个用在计算机世界里的协议。它使用计算机能够理解的语言确立了一种计算机之间交流通信的规范，以及相关的各种控制和错误处理方式。 http 是一个在计算机世界里专门在两点之间传输文字、图片、音频、视频等超文本数据的约定和规范。 http 不是编程语言，但是可以用编程语言去实现 HTTP，告诉计算机如何用 HTTP 来与外界通信。 在互联网世界里，HTTP 通常跑在 TCP/IP 协议栈之上，依靠 IP 协议实现寻址和路由、TCP 协议实现可靠数据传输、DNS 协议实现域名查找、SSL/TLS 协议实现安全通信。此外，还有一些协议依赖于 HTTP，例如 WebSocket、HTTPDNS 等。这些协议相互交织，构成了一个协议网，而 HTTP 则处于中心地位。 HTTP 传输的不是 TCP/UDP 这些底层协议里被切分的杂乱无章的二进制包（datagram），而是完整的、有意义的数据，可以被浏览器、服务器这样的上层应用程序处理。 ","date":"2022-03-17","objectID":"/http-glance/:2:0","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#http-协议"},{"categories":["理解计算机"],"content":" 互联网和万维网的区别我们通常所说的“上网”实际上访问的只是互联网的一个子集“万维网”（World Wide Web），它基于 HTTP 协议，传输 HTML 等超文本资源，能力被限制在 HTTP 协议之内。 互联网上还有许多万维网之外的资源，例如常用的电子邮件、BT 和 Magnet 点对点下载、FTP 文件下载、SSH 安全登录、各种即时通信服务等等，它们需要用各自的专有协议来访问。 不过由于 HTTP 协议非常灵活、易于扩展，而且“超文本”的表述能力很强，所以很多其他原本不属于 HTTP 的资源也可以“包装”成 HTTP 来访问，这就是我们为什么能够总看到各种“网页应用”——例如“微信网页版”“邮箱网页版”——的原因。 ","date":"2022-03-17","objectID":"/http-glance/:2:1","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#互联网和万维网的区别"},{"categories":["理解计算机"],"content":" TCP/IPTCP/IP 协议实际上是一系列网络通信协议的统称， 其中最核心的两个协议是TCP（Transmission Control Protocol/传输控制协议）和IP（Internet Protocol），其他的还有 UDP、ICMP、ARP 等等，共同构成了一个复杂但有层次的协议栈。 HTTP 是超文本传输协议，TCP 是传输控制协议，都是传输，区别是，HTTP 传输的是完整的、有意义的数据，可以被浏览器、 服务器这样的上层应用程序处理，HTTP 不关心寻址、路由、数据完整性等传输细节，而要求这些工作都由下层（基本都由 TCP）来处理。 TCP 传输的是可靠的、字节流和二进制包。 TCP 是 HTTP 得以实现的基础，HTTP 协议运行在 TCP/IP 上，HTTP 可以更准确地称为 “HTTP over TCP/IP”。 ","date":"2022-03-17","objectID":"/http-glance/:2:2","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#tcpip"},{"categories":["理解计算机"],"content":" URI/URLURI（Uniform Resource Identifier），中文名称是 统一资源标识符，使用它就能够唯一地标记互联网上资源。 URI 另一个更常用的表现形式是 URL（Uniform Resource Locator）， 统一资源定位符，也就是我们俗称的“网址”，它实际上是 URI 的一个子集，这两者几乎是相同的，差异不大，除非写论文，否则不用特意区分。 ","date":"2022-03-17","objectID":"/http-glance/:2:3","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#uriurl"},{"categories":["理解计算机"],"content":" SSL/TSLSSL 的全称是“Secure Socket Layer”，网景公司发明，当发展到 3.0 时被标准化，改名为 TLS，即“Transport Layer Security”。 所以 TLS 跟 SSL 是一个东西，相当于张君宝的 2.0 版本是张三丰。 SSL 是一个负责加密通信的安全协议，建立在 TCP/IP 之上，在 HTTP 协议之下。 ","date":"2022-03-17","objectID":"/http-glance/:2:4","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#ssltsl"},{"categories":["理解计算机"],"content":" Proxy 代理 匿名代理：完全“隐匿”了被代理的机器，外界看到的只是代理服务器； 透明代理：顾名思义，它在传输过程中是“透明开放”的，外界既知道代理，也知道客户端； 正向代理：靠近客户端，代表客户端向服务器发送请求； 正向代理 反向代理：靠近服务器端，代表服务器响应客户端的请求； 反向代理 Tip 如何理解反向代理服务器 ","date":"2022-03-17","objectID":"/http-glance/:2:5","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#proxy-代理"},{"categories":["理解计算机"],"content":" http 版本万维网关键技术 URI：即统一资源标识符，作为互联网上资源的唯一身份； HTML：即超文本标记语言，描述超文本文档； HTTP：即超文本传输协议，用来传输超文本。 基于这三项关键技术就可以把超文本系统完美地运行在互联网上，让各地的人们能够自由地共享信息，这个系统称为“万维网”（World Wide Web），也就是我们现在所熟知的 Web。 ","date":"2022-03-17","objectID":"/http-glance/:3:0","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#http-版本"},{"categories":["理解计算机"],"content":" http/0.9结构简单，设置之初设想系统里的文档都是只读的，所以只允许用 GET 动作从服务器上获取 HTML 纯文本格式的文档，并且在响应请求之后立即关闭连接，功能非常有限。 ","date":"2022-03-17","objectID":"/http-glance/:3:1","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#http09"},{"categories":["理解计算机"],"content":" http/1.0HTTP/1.0 并不是一个标准，只是记录已有实践和模式的一份参考文档，不具有实际的约束力，相当于一个备忘录。 在多方面增强了 0.9 版，形式上已经和我们现在的 HTTP 差别不大了，例如： 增加了 HEAD、POST 等新方法； 增加了响应状态码，标记可能的错误原因； 引入了协议版本号概念； 引入了 HTTP Header（头部）的概念，让 HTTP 处理请求和响应更加灵活； 传输的数据不再仅限于文本。 ","date":"2022-03-17","objectID":"/http-glance/:3:2","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#http10"},{"categories":["理解计算机"],"content":" http/1.1是一个正式的标准，而不是一份可有可无的参考文档，只要用到 HTTP 协议，就必须严格遵守这个标准。 主要变更： 增加了 PUT、DELETE 等新的方法； 增加了缓存管理和控制； 明确了连接管理，允许持久连接； 允许响应数据分块（chunked），利于传输大文件； 强制要求 Host 头，让互联网主机托管成为可能。 ","date":"2022-03-17","objectID":"/http-glance/:3:3","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#http11"},{"categories":["理解计算机"],"content":" http/2由 google 主导，基于 google 的 SPDY 协议为基础开始制定新版本的 HTTP 协议，最终在 2015 年发布了 HTTP/2。 主要特点： 二进制协议，不再是纯文本； 可发起多个请求，废弃了 1.1 里的管道； 使用专用算法压缩头部，减少数据传输量； 允许服务器主动向客户端推送数据； 增强了安全性，“事实上”要求加密通信。 ","date":"2022-03-17","objectID":"/http-glance/:3:4","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#http2"},{"categories":["理解计算机"],"content":" http/3由 google 主导，基于 google 的 QUIC 协议为基础开始制定新版本的 HTTP 协议。 ","date":"2022-03-17","objectID":"/http-glance/:3:5","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#http3"},{"categories":["理解计算机"],"content":" 网络分层模型","date":"2022-03-17","objectID":"/http-glance/:4:0","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#网络分层模型"},{"categories":["理解计算机"],"content":" TCP/IP TCP/IP分层模型 这里的层次顺序是“从下往上”数的，所以第一层就是最下面的一层。 链接层第一层叫“链接层”（link layer），负责在以太网、WiFi 这样的底层网络上发送原始数据包，工作在网卡这个层次，使用 MAC 地址来标记网络上的设备，所以有时候也叫 MAC 层。 网络互联层第二层叫“网际层”或者“网络互连层”（internet layer），IP 协议就处在这一层。因为 IP 协议定义了“IP 地址”的概念，所以就可以在“链接层”的基础上，用 IP 地址取代 MAC 地址，把许许多多的局域网、广域网连接成一个虚拟的巨大网络，在这个网络里找设备时只要把 IP 地址再“翻译”成 MAC 地址就可以了。 传输层第三层叫“传输层”（transport layer），这个层次协议的职责是保证数据在 IP 地址标记的两点之间“可靠”地传输，是 TCP 协议工作的层次，另外还有它的一个“小伙伴”UDP。 TCP 是一个有状态的协议，需要先与对方建立连接然后才能发送数据，而且保证数据不丢失不重复。而 UDP 则比较简单，它无状态，不用事先建立连接就可以任意发送数据，但不保证数据一定会发到对方。两个协议的另一个重要区别在于数据的形式。TCP 的数据是连续的“字节流”，有先后顺序，而 UDP 则是分散的小数据包，是顺序发，乱序收。 应用层协议栈的第四层叫“应用层”（application layer），由于下面的三层把基础打得非常好，所以在这一层就“百花齐放”了，有各种面向具体应用的协议。例如 Telnet、SSH、FTP、SMTP，HTTP 等等。 Tip MAC 层（链接层）的传输单位是帧（frame），IP 层（网络互联层）的传输单位是包（packet），TCP 层传输层的传输单位是段（segment）， HTTP （应用层）的传输单位则是消息或报文（message）。这些名词并没有什么本质的区分，可以统称为数据包。 ","date":"2022-03-17","objectID":"/http-glance/:4:1","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#tcpip-1"},{"categories":["理解计算机"],"content":" TCP/IP TCP/IP分层模型 这里的层次顺序是“从下往上”数的，所以第一层就是最下面的一层。 链接层第一层叫“链接层”（link layer），负责在以太网、WiFi 这样的底层网络上发送原始数据包，工作在网卡这个层次，使用 MAC 地址来标记网络上的设备，所以有时候也叫 MAC 层。 网络互联层第二层叫“网际层”或者“网络互连层”（internet layer），IP 协议就处在这一层。因为 IP 协议定义了“IP 地址”的概念，所以就可以在“链接层”的基础上，用 IP 地址取代 MAC 地址，把许许多多的局域网、广域网连接成一个虚拟的巨大网络，在这个网络里找设备时只要把 IP 地址再“翻译”成 MAC 地址就可以了。 传输层第三层叫“传输层”（transport layer），这个层次协议的职责是保证数据在 IP 地址标记的两点之间“可靠”地传输，是 TCP 协议工作的层次，另外还有它的一个“小伙伴”UDP。 TCP 是一个有状态的协议，需要先与对方建立连接然后才能发送数据，而且保证数据不丢失不重复。而 UDP 则比较简单，它无状态，不用事先建立连接就可以任意发送数据，但不保证数据一定会发到对方。两个协议的另一个重要区别在于数据的形式。TCP 的数据是连续的“字节流”，有先后顺序，而 UDP 则是分散的小数据包，是顺序发，乱序收。 应用层协议栈的第四层叫“应用层”（application layer），由于下面的三层把基础打得非常好，所以在这一层就“百花齐放”了，有各种面向具体应用的协议。例如 Telnet、SSH、FTP、SMTP，HTTP 等等。 Tip MAC 层（链接层）的传输单位是帧（frame），IP 层（网络互联层）的传输单位是包（packet），TCP 层传输层的传输单位是段（segment）， HTTP （应用层）的传输单位则是消息或报文（message）。这些名词并没有什么本质的区分，可以统称为数据包。 ","date":"2022-03-17","objectID":"/http-glance/:4:1","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#链接层"},{"categories":["理解计算机"],"content":" TCP/IP TCP/IP分层模型 这里的层次顺序是“从下往上”数的，所以第一层就是最下面的一层。 链接层第一层叫“链接层”（link layer），负责在以太网、WiFi 这样的底层网络上发送原始数据包，工作在网卡这个层次，使用 MAC 地址来标记网络上的设备，所以有时候也叫 MAC 层。 网络互联层第二层叫“网际层”或者“网络互连层”（internet layer），IP 协议就处在这一层。因为 IP 协议定义了“IP 地址”的概念，所以就可以在“链接层”的基础上，用 IP 地址取代 MAC 地址，把许许多多的局域网、广域网连接成一个虚拟的巨大网络，在这个网络里找设备时只要把 IP 地址再“翻译”成 MAC 地址就可以了。 传输层第三层叫“传输层”（transport layer），这个层次协议的职责是保证数据在 IP 地址标记的两点之间“可靠”地传输，是 TCP 协议工作的层次，另外还有它的一个“小伙伴”UDP。 TCP 是一个有状态的协议，需要先与对方建立连接然后才能发送数据，而且保证数据不丢失不重复。而 UDP 则比较简单，它无状态，不用事先建立连接就可以任意发送数据，但不保证数据一定会发到对方。两个协议的另一个重要区别在于数据的形式。TCP 的数据是连续的“字节流”，有先后顺序，而 UDP 则是分散的小数据包，是顺序发，乱序收。 应用层协议栈的第四层叫“应用层”（application layer），由于下面的三层把基础打得非常好，所以在这一层就“百花齐放”了，有各种面向具体应用的协议。例如 Telnet、SSH、FTP、SMTP，HTTP 等等。 Tip MAC 层（链接层）的传输单位是帧（frame），IP 层（网络互联层）的传输单位是包（packet），TCP 层传输层的传输单位是段（segment）， HTTP （应用层）的传输单位则是消息或报文（message）。这些名词并没有什么本质的区分，可以统称为数据包。 ","date":"2022-03-17","objectID":"/http-glance/:4:1","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#网络互联层"},{"categories":["理解计算机"],"content":" TCP/IP TCP/IP分层模型 这里的层次顺序是“从下往上”数的，所以第一层就是最下面的一层。 链接层第一层叫“链接层”（link layer），负责在以太网、WiFi 这样的底层网络上发送原始数据包，工作在网卡这个层次，使用 MAC 地址来标记网络上的设备，所以有时候也叫 MAC 层。 网络互联层第二层叫“网际层”或者“网络互连层”（internet layer），IP 协议就处在这一层。因为 IP 协议定义了“IP 地址”的概念，所以就可以在“链接层”的基础上，用 IP 地址取代 MAC 地址，把许许多多的局域网、广域网连接成一个虚拟的巨大网络，在这个网络里找设备时只要把 IP 地址再“翻译”成 MAC 地址就可以了。 传输层第三层叫“传输层”（transport layer），这个层次协议的职责是保证数据在 IP 地址标记的两点之间“可靠”地传输，是 TCP 协议工作的层次，另外还有它的一个“小伙伴”UDP。 TCP 是一个有状态的协议，需要先与对方建立连接然后才能发送数据，而且保证数据不丢失不重复。而 UDP 则比较简单，它无状态，不用事先建立连接就可以任意发送数据，但不保证数据一定会发到对方。两个协议的另一个重要区别在于数据的形式。TCP 的数据是连续的“字节流”，有先后顺序，而 UDP 则是分散的小数据包，是顺序发，乱序收。 应用层协议栈的第四层叫“应用层”（application layer），由于下面的三层把基础打得非常好，所以在这一层就“百花齐放”了，有各种面向具体应用的协议。例如 Telnet、SSH、FTP、SMTP，HTTP 等等。 Tip MAC 层（链接层）的传输单位是帧（frame），IP 层（网络互联层）的传输单位是包（packet），TCP 层传输层的传输单位是段（segment）， HTTP （应用层）的传输单位则是消息或报文（message）。这些名词并没有什么本质的区分，可以统称为数据包。 ","date":"2022-03-17","objectID":"/http-glance/:4:1","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#传输层"},{"categories":["理解计算机"],"content":" TCP/IP TCP/IP分层模型 这里的层次顺序是“从下往上”数的，所以第一层就是最下面的一层。 链接层第一层叫“链接层”（link layer），负责在以太网、WiFi 这样的底层网络上发送原始数据包，工作在网卡这个层次，使用 MAC 地址来标记网络上的设备，所以有时候也叫 MAC 层。 网络互联层第二层叫“网际层”或者“网络互连层”（internet layer），IP 协议就处在这一层。因为 IP 协议定义了“IP 地址”的概念，所以就可以在“链接层”的基础上，用 IP 地址取代 MAC 地址，把许许多多的局域网、广域网连接成一个虚拟的巨大网络，在这个网络里找设备时只要把 IP 地址再“翻译”成 MAC 地址就可以了。 传输层第三层叫“传输层”（transport layer），这个层次协议的职责是保证数据在 IP 地址标记的两点之间“可靠”地传输，是 TCP 协议工作的层次，另外还有它的一个“小伙伴”UDP。 TCP 是一个有状态的协议，需要先与对方建立连接然后才能发送数据，而且保证数据不丢失不重复。而 UDP 则比较简单，它无状态，不用事先建立连接就可以任意发送数据，但不保证数据一定会发到对方。两个协议的另一个重要区别在于数据的形式。TCP 的数据是连续的“字节流”，有先后顺序，而 UDP 则是分散的小数据包，是顺序发，乱序收。 应用层协议栈的第四层叫“应用层”（application layer），由于下面的三层把基础打得非常好，所以在这一层就“百花齐放”了，有各种面向具体应用的协议。例如 Telnet、SSH、FTP、SMTP，HTTP 等等。 Tip MAC 层（链接层）的传输单位是帧（frame），IP 层（网络互联层）的传输单位是包（packet），TCP 层传输层的传输单位是段（segment）， HTTP （应用层）的传输单位则是消息或报文（message）。这些名词并没有什么本质的区分，可以统称为数据包。 ","date":"2022-03-17","objectID":"/http-glance/:4:1","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#应用层"},{"categories":["理解计算机"],"content":" OSI 网络分层模型OSI 分层模型在发布的时候就明确地表明是一个“参考”，不是强制标准。这是因为 TCP/IP 等协议已经在许多网络上实际运行，不可能推翻重来。 OSI网络模型 第一层：物理层，网络的物理形式，例如电缆、光纤、网卡、集线器等等； 第二层：数据链路层，它基本相当于 TCP/IP 的链接层； 第三层：网络层，相当于 TCP/IP 里的网际层； 第四层：传输层，相当于 TCP/IP 里的传输层； 第五层：会话层，维护网络中的连接状态，即保持会话和同步； 第六层：表示层，把数据转换为合适、可理解的语法和语义； 第七层：应用层，面向具体的应用传输数据。 对比一下就可以发现，TCP/IP 是一个纯软件的栈，没有网络应有的最根基的电缆、网卡等物理设备的位置。而 OSI 则补足了这个缺失， 在理论层面上描述网络更加完整。 OSI 还为每一层标记了明确了编号，最底层是一层，最上层是七层，而 TCP/IP 的层次从来只有名字而没有编号。 ","date":"2022-03-17","objectID":"/http-glance/:4:2","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#osi-网络分层模型"},{"categories":["理解计算机"],"content":" 两个分层模型的对应关系 两个分层模型的对应关系 所谓的“四层负载均衡”就是指工作在传输层上，基于 TCP/IP 协议的特性，例如 IP 地址、端口号等实现对后端服务器的负载均衡。 所谓的“七层负载均衡”就是指工作在应用层上，看到的是 HTTP 协议，解析 HTTP 报文里的 URI、主机名、资源类型等数据，再用适当的策略转发给后端服务器。 有一个辨别四层和七层比较好的（但不是绝对的）小窍门，“两个凡是”：凡是由操作系统负责处理的就是四层或四层以下，否则，凡是需要由应用程序（也就是你自己写代码）负责处理的就是七层。 ","date":"2022-03-17","objectID":"/http-glance/:4:3","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#两个分层模型的对应关系"},{"categories":["理解计算机"],"content":" http协议核心由于 HTTP 是在 TCP/IP 协议之上的，而 TCP/IP 协议负责底层的具体传输工作，所以 http 在传输方面不用太操心，TCP/IP 会去解决，所以 HTTP 关心的就只有他所传输的报文内容，又因为 HTTP 是“纯文本”的，包括头信息都是 ASCII 码的文本，不用借助程序解析可以直接阅读。 ","date":"2022-03-17","objectID":"/http-glance/:5:0","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#http协议核心"},{"categories":["理解计算机"],"content":" 常用头字段 注意事项 字段名不区分大小写，例如“Host”也可以写成“host”，但首字母大写的可读性更好； 字段名里不允许出现空格，可以使用连字符“-”，但不能使用下划线“_”。例如，“test-name”是合法的字段名，而“test name”“test_name”是不正确的字段名； 字段名后面必须紧接着“:”，不能有空格，而“:”后的字段值前可以有多个空格； 分类 通用字段：在请求头和响应头里都可以出现； 请求字段：仅能出现在请求头里，进一步说明请求信息或者额外的附加条件； 响应字段：仅能出现在响应头里，补充说明响应报文的信息； 实体字段：它实际上属于通用字段，但专门描述 body 的额外信息。 字段 类型 说明 Host 请求字段 唯一一个 HTTP/1.1 规范里要求必须出现的字段，如果请求头里没有 Host，那这就是一个错误的报文。Host 字段告诉服务器这个请求应该由哪个主机来处理 User-Agent 请求字段 描述发起 HTTP 请求的客户端，服务器可以依据它来返回最合适此浏览器显示的页面 Date 通用字段 表示 HTTP 报文创建的时间，客户端可以使用这个时间再搭配其他字段决定缓存策略 Server 响应字段 告诉客户端当前正在提供 Web 服务的软件名称和版本号 Content-Length 实体字段 报文里 body 的长度，也就是请求头或响应头空行后面数据的长度。服务器看到这个字段，就知道了后续有多少数据，可以直接接收。如果没有这个字段，那么 body 就是不定长的，需要使用 chunked 方式分段传输 ","date":"2022-03-17","objectID":"/http-glance/:5:1","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#常用头字段"},{"categories":["理解计算机"],"content":" 常用头字段 注意事项 字段名不区分大小写，例如“Host”也可以写成“host”，但首字母大写的可读性更好； 字段名里不允许出现空格，可以使用连字符“-”，但不能使用下划线“_”。例如，“test-name”是合法的字段名，而“test name”“test_name”是不正确的字段名； 字段名后面必须紧接着“:”，不能有空格，而“:”后的字段值前可以有多个空格； 分类 通用字段：在请求头和响应头里都可以出现； 请求字段：仅能出现在请求头里，进一步说明请求信息或者额外的附加条件； 响应字段：仅能出现在响应头里，补充说明响应报文的信息； 实体字段：它实际上属于通用字段，但专门描述 body 的额外信息。 字段 类型 说明 Host 请求字段 唯一一个 HTTP/1.1 规范里要求必须出现的字段，如果请求头里没有 Host，那这就是一个错误的报文。Host 字段告诉服务器这个请求应该由哪个主机来处理 User-Agent 请求字段 描述发起 HTTP 请求的客户端，服务器可以依据它来返回最合适此浏览器显示的页面 Date 通用字段 表示 HTTP 报文创建的时间，客户端可以使用这个时间再搭配其他字段决定缓存策略 Server 响应字段 告诉客户端当前正在提供 Web 服务的软件名称和版本号 Content-Length 实体字段 报文里 body 的长度，也就是请求头或响应头空行后面数据的长度。服务器看到这个字段，就知道了后续有多少数据，可以直接接收。如果没有这个字段，那么 body 就是不定长的，需要使用 chunked 方式分段传输 ","date":"2022-03-17","objectID":"/http-glance/:5:1","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#注意事项"},{"categories":["理解计算机"],"content":" 常用头字段 注意事项 字段名不区分大小写，例如“Host”也可以写成“host”，但首字母大写的可读性更好； 字段名里不允许出现空格，可以使用连字符“-”，但不能使用下划线“_”。例如，“test-name”是合法的字段名，而“test name”“test_name”是不正确的字段名； 字段名后面必须紧接着“:”，不能有空格，而“:”后的字段值前可以有多个空格； 分类 通用字段：在请求头和响应头里都可以出现； 请求字段：仅能出现在请求头里，进一步说明请求信息或者额外的附加条件； 响应字段：仅能出现在响应头里，补充说明响应报文的信息； 实体字段：它实际上属于通用字段，但专门描述 body 的额外信息。 字段 类型 说明 Host 请求字段 唯一一个 HTTP/1.1 规范里要求必须出现的字段，如果请求头里没有 Host，那这就是一个错误的报文。Host 字段告诉服务器这个请求应该由哪个主机来处理 User-Agent 请求字段 描述发起 HTTP 请求的客户端，服务器可以依据它来返回最合适此浏览器显示的页面 Date 通用字段 表示 HTTP 报文创建的时间，客户端可以使用这个时间再搭配其他字段决定缓存策略 Server 响应字段 告诉客户端当前正在提供 Web 服务的软件名称和版本号 Content-Length 实体字段 报文里 body 的长度，也就是请求头或响应头空行后面数据的长度。服务器看到这个字段，就知道了后续有多少数据，可以直接接收。如果没有这个字段，那么 body 就是不定长的，需要使用 chunked 方式分段传输 ","date":"2022-03-17","objectID":"/http-glance/:5:1","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#分类"},{"categories":["理解计算机"],"content":" 请求方式 方式 说明 GET 获取资源，可以理解为读取或者下载数据 HEAD 获取资源的元信息，不会返回请求的实体数据，只会传回响应头 POST 向资源提交数据，相当于写入或上传数据 PUT 类似 POST DELETE 删除资源 CONNECT 建立特殊的连接隧道 OPTIONS 列出可对资源实行的方法 TRACE 追踪请求 - 响应的传输路径 ","date":"2022-03-17","objectID":"/http-glance/:5:2","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#请求方式"},{"categories":["理解计算机"],"content":" 状态码 状态码 含义 1×x 提示信息，表示目前是协议处理的中间状态，还需要后续的操作 2×× 成功，报文已经收到并被正确处理 3×× 重定向，资源位置发生变动，需要客户端重新发送请求 4×× 客户端错误，请求报文有误，服务器无法处理 5×× 服务器错误，服务器在处理请求时内部发生了错误 一些常用状态码说明 status code 说明 301 永久重定向，含义是此次请求的资源已经不存在了，需要改用改用新的 URI 再次访问 302 临时重定向，意思是请求的资源还在，但需要暂时用另一个 URI 来访问。比如，你的网站升级到了 HTTPS，原来的 HTTP 不打算用了，这就是“永久”的，所以要配置 301 跳转，把所有的 HTTP 流量都切换到 HTTPS。 再比如，今天夜里网站后台要系统维护，服务暂时不可用，这就属于“临时”的，可以配置成 302 跳转，把流量临时切换到一个静态通知页面，浏览器看到这个 302 就知道这只是暂时的情况，不会做缓存优化，第二天还会访问原来的地址。 304 Not Modified，它用于 If-Modified-Since 等条件请求，表示资源未修改，用于缓存控制。它不具有通常的跳转含义，但可以理解成“重定向已到缓存的文件”（即“缓存重定向”） 405 不允许使用某些方法操作资源，例如不允许 POST 只能 GET 406 Not Acceptable 资源无法满足客户端请求的条件，例如请求中文但只有英文 408 Request Timeout：请求超时，服务器等待了过长的时间 409 Conflict：多个请求发生了冲突，可以理解为多线程并发时的竞态 413 Request Entity Too Large：请求报文里的 body 太大 414 Request-URI Too Long：请求行里的 URI 太大 429 Too Many Requests 客户端发送了太多的请求，通常是由于服务器的限连策略 431 Request Header Fields Too Large 请求头某个字段或总体太大 500 Internal Server Error 与 400 类似，也是一个通用的错误码，服务器究竟发生了什么错误我们是不知道的。不过对于服务器来说这应该算是好事，通常不应该把服务器内部的详细信息，例如出错的函数调用栈告诉外界。虽然不利于调试，但能够防止黑客的窥探或者分析 501 Not Implemented 表示客户端请求的功能还不支持，这个错误码比 500 要温和一些，和“即将开业，敬请期待”的意思差不多，不过具体什么时候“开业”就不好说了 502 Bad Gateway”通常是服务器作为网关或者代理时返回的错误码，表示服务器自身工作正常，访问后端服务器时发生了错误，但具体的错误原因也是不知道的 503 Service Unavailable 表示服务器当前很忙，暂时无法响应服务，我们上网时有时候遇到的“网络服务正忙，请稍后重试”的提示信息就是状态码 503 ","date":"2022-03-17","objectID":"/http-glance/:5:3","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#状态码"},{"categories":["理解计算机"],"content":" http 特点 HTTP 是灵活可扩展的，可以任意添加头字段实现任意功能。 HTTP 是可靠传输协议，基于 TCP/IP 协议“尽量”保证数据的送达。 HTTP 是应用层协议，比 FTP、SSH 等更通用功能更多，能够传输任意数据。 HTTP 使用了请求 - 应答模式，客户端主动发起请求，服务器被动回复请求。 HTTP 本质上是无状态的，每个请求都是互相独立、毫无关联的，协议不要求客户端或服务器记录请求相关的信息。 ","date":"2022-03-17","objectID":"/http-glance/:5:4","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#http-特点"},{"categories":["理解计算机"],"content":" 参考 http cats ","date":"2022-03-17","objectID":"/http-glance/:6:0","series":null,"tags":["http/https"],"title":"http入门笔记","uri":"/http-glance/#参考"},{"categories":["golang"],"content":"golang GMP 模型,go 数学模型,GMP,进程,线程,协程,goroutine,go 调度器,golang调度器,go协程调度模型","date":"2022-03-16","objectID":"/gmp-model/","series":null,"tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/"},{"categories":["golang"],"content":" 进程、线程、协程的区别","date":"2022-03-16","objectID":"/gmp-model/:1:0","series":null,"tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/#进程线程协程的区别"},{"categories":["golang"],"content":" 进程进程是操作系统对一个正在运行的程序的一种抽象，进程是资源分配的最小单位。 进程在操作系统中的抽象表现 进程存在的意义是为了合理压榨 CPU 的性能和分配运行的时间片，不能 “闲着“。 在计算机中，其计算核心是 CPU，负责所有计算相关的工作和资源。单个 CPU 一次只能运行一个任务。如果一个进程跑着，就把唯一一个 CPU 给完全占住，那是非常不合理的。 如果总是在运行一个进程上的任务，就会出现一个现象。就是任务不一定总是在执行 ”计算型“ 的任务，会有很大可能是在执行网络调用，阻塞了，CPU 岂不就浪费了？ 进程上下文切换 所以出现了多进程，多个 CPU，多个进程。多进程就是指计算机系统可以同时执行多个进程，从一个进程到另外一个进程的转换是由操作系统内核管理的，一般是同时运行多个软件。 ","date":"2022-03-16","objectID":"/gmp-model/:1:1","series":null,"tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/#进程"},{"categories":["golang"],"content":" 线程有了多进程，在操作系统上可以同时运行多个进程。那么为什么有了进程，还要线程呢？这是因为， 进程间的信息难以共享数据，父子进程并未共享内存，需要通过进程间通信（IPC），在进程间进行信息交换，性能开销较大。 创建进程（一般是调用 fork 方法）的性能开销较大。 大家又把目光转向了进程内，能不能在进程里做点什么呢？ 进程由多个线程组成 一个进程可以由多个称为线程的执行单元组成。每个线程都运行在进程的上下文中，共享着同样的代码和全局数据。 多个进程，就可以有更多的线程。多线程比多进程之间更容易共享数据，在上下文切换中线程一般比进程更高效。这是因为， 线程之间能够非常方便、快速地共享数据。 只需将数据复制到进程中的共享区域就可以了，但需要注意避免多个线程修改同一份内存。 创建线程比创建进程要快 10 倍甚至更多。 线程都是同一个进程下自家的孩子，像是内存页、页表等就不需要了。 ","date":"2022-03-16","objectID":"/gmp-model/:1:2","series":null,"tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/#线程"},{"categories":["golang"],"content":" 协程协程（Coroutine）是用户态的线程。通常创建协程时，会从进程的堆中分配一段内存作为协程的栈。 线程的栈有 8 MB，而协程栈的大小通常只有 KB，而 Go 语言的协程更夸张，只有 2-4KB，非常的轻巧。 协程有以下优势👋： 👉节省 CPU：避免系统内核级的线程频繁切换，造成的 CPU 资源浪费。好钢用在刀刃上。而协程是用户态的线程，用户可以自行控制协程的创建于销毁，极大程度避免了系统级线程上下文切换造成的资源浪费。 👉节约内存：在 64 位的Linux中，一个线程需要分配 8MB 栈内存和 64MB 堆内存，系统内存的制约导致我们无法开启更多线程实现高并发。而在协程编程模式下，可以轻松有十几万协程，这是线程无法比拟的。 👉稳定性：前面提到线程之间通过内存来共享数据，这也导致了一个问题，任何一个线程出错时，进程中的所有线程都会跟着一起崩溃。 👉开发效率：使用协程在开发程序之中，可以很方便的将一些耗时的IO操作异步化，例如写文件、耗时 IO 请求等。 ","date":"2022-03-16","objectID":"/gmp-model/:1:3","series":null,"tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/#协程"},{"categories":["golang"],"content":" goroutine 是什么Goroutine 是一个由 Go 运行时管理的轻量级线程，我们称为 “协程”。 go f(x, y, z) 操作系统本身是无法明确感知到 Goroutine 的存在的，Goroutine 的操作和切换归属于 “用户态” 中。 Goroutine 由特定的调度模式来控制，以 “多路复用” 的形式运行在操作系统为 Go 程序分配的几个系统线程上。 同时创建 Goroutine 的开销很小，初始只需要 2-4k 的栈空间。Goroutine 本身会根据实际使用情况进行自伸缩，非常轻量。 Tips Go程序中没有语言级的关键字让你去创建一个内核线程，你只能创建 goroutine，内核线程只能由 runtime 根据实际情况去创建。 Go运行时系统并没有内核调度器的中断能力，内核调度器会发起抢占式调度将长期运行的线程中断并让出CPU资源，让其他线程获得执行机会。 ","date":"2022-03-16","objectID":"/gmp-model/:2:0","series":null,"tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/#goroutine-是什么"},{"categories":["golang"],"content":" 什么是调度用户态的 Goroutine，操作系统看不到它。必然需要有某个东西去管理他，才能更好的运作起来。 这就是 Go 语言中的调度，也就是 GMP 模型。 Go scheduler /ˈskedʒuːlər/ 的主要功能是针对在处理器上运行的 OS 线程分发可运行的 Goroutine，而我们一提到调度器，就离不开三个经常被提到的缩写，分别是： G：Goroutine，实际上我们每次调用 go func 就是生成了一个 G。 P：Processor，处理器，一般 P 的数量就是处理器的核数，可以通过 GOMAXPROCS 进行修改。 M：Machine，系统线程。 这三者交互实际来源于 Go 的 M: N 调度模型。也就是 M 必须与 P 进行绑定，然后不断地在 M 上循环寻找可运行的 G 来执行相应的任务。 ","date":"2022-03-16","objectID":"/gmp-model/:3:0","series":null,"tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/#什么是调度"},{"categories":["golang"],"content":" 调度流程 调度流程 当我们执行 go func() 时，实际上就是创建一个全新的 Goroutine，我们称它为 G。 新创建的 G 会被放入 P 的本地队列（Local Queue）或全局队列（Global Queue）中，准备下一步的动作。需要注意的一点，这里的 P 指的是创建 G 的 P。 唤醒或创建 M 以便执行 G。 不断地进行事件循环 寻找在可用状态下的 G 进行执行任务 清除后，重新进入事件循环 在描述中有提到全局和本地这两类队列，其实在功能上来讲都是用于存放正在等待运行的 G，但是不同点在于，本地队列有数量限制，不允许超过 256 个。 并且在新建 G 时，会优先选择 P 的本地队列，如果本地队列满了，则将 P 的本地队列的一半的 G 移动到全局队列。 可以理解为调度资源的共享和再平衡。 ","date":"2022-03-16","objectID":"/gmp-model/:4:0","series":null,"tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/#调度流程"},{"categories":["golang"],"content":" 窃取行为可以看到图上有 steal 行为，这是用来做什么的呢，我们都知道当你创建新的 G 或者 G 变成可运行状态时，它会被推送加入到当前 P 的本地队列中。 其实当 P 执行 G 完毕后，它也会 “干活”，它会将其从本地队列中弹出 G，同时会检查当前本地队列是否为空，如果为空会随机的从其他 P 的本地队列中尝试窃取一半可运行的 G 到自己的名下。 窃取行为 上图中👆，P2 在本地队列中找不到可以运行的 G，它会执行 work-stealing 调度算法，随机选择其它的处理器 P1，并从 P1 的本地队列中窃取了三个 G 到它自己的本地队列中去。 至此，P1、P2 都拥有了可运行的 G，P1 多余的 G 也不会被浪费，调度资源将会更加平均的在多个处理器中流转。 ","date":"2022-03-16","objectID":"/gmp-model/:5:0","series":null,"tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/#窃取行为"},{"categories":["golang"],"content":" 限制条件","date":"2022-03-16","objectID":"/gmp-model/:6:0","series":null,"tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/#限制条件"},{"categories":["golang"],"content":" M 的限制在协程的执行中，真正干活的是 GPM 中的 M（系统线程） ，因为 G 是用户态上的东西，最终执行都是得映射，对应到 M 这一个系统线程上去运行。 那么 M 有没有限制呢？ 答案是：有的。在 Go 语言中，M 的默认数量限制是 10000，如果超出则会报错： GO: runtime: program exceeds 10000-thread limit 但是通常只有在 Goroutine 出现阻塞操作的情况下，才会遇到这种情况。这可能也预示着你的程序有问题。 若确切是需要那么多，还可以通过 debug.SetMaxThreads 方法进行设置。 ","date":"2022-03-16","objectID":"/gmp-model/:6:1","series":null,"tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/#m-的限制"},{"categories":["golang"],"content":" G 的限制Goroutine 的创建数量是否有限制？ 答案是：没有。但理论上会受内存的影响，假设一个 Goroutine 创建需要 4k 的连续的内存块： 4k * 80,000 = 320,000k ≈ 0.3G内存 4k * 1,000,000 = 4,000,000k ≈ 4G内存 以此就可以相对计算出来一台单机在通俗情况下，所能够创建 Goroutine 的大概数量级别。 ","date":"2022-03-16","objectID":"/gmp-model/:6:2","series":null,"tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/#g-的限制"},{"categories":["golang"],"content":" P 的限制P 的数量是否有限制，受什么影响？ 答案是：有限制。P 的数量受环境变量 GOMAXPROCS 的直接影响。 环境变量 GOMAXPROCS 又是什么？在 Go 语言中，通过设置 GOMAXPROCS，用户可以调整调度中 P（Processor）的数量。 另一个重点在于，与 P 相关联的的 M（系统线程），是需要绑定 P 才能进行具体的任务执行的，因此 P 的多少会影响到 Go 程序的运行表现。 P 的数量基本是受本机的核数影响，没必要太过度纠结他。 那 P 的数量是否会影响 Goroutine 的数量创建呢？ 答案是：不影响。且 Goroutine 多了少了，P 也该干嘛干嘛，不会带来灾难性问题。 ","date":"2022-03-16","objectID":"/gmp-model/:6:3","series":null,"tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/#p-的限制"},{"categories":["golang"],"content":" 小结 M：有限制，默认数量限制是 10000，可调整。 G：没限制，但受内存影响。 P：受本机的核数影响，可大可小，不影响 G 的数量创建。 所以Goroutine 数量怎么预算，才叫合理？ 在真实的应用场景中，如果你 Goroutine： 在频繁请求 HTTP，MySQL，打开文件等，那假设短时间内有几十万个协程在跑，那肯定就不大合理了（可能会导致 too many files open）。 常见的 Goroutine 泄露所导致的 CPU、Memory 上涨等，还是得看你的 Goroutine 里具体在跑什么东西。 跑的如果是 “资源怪兽”，只运行几个 Goroutine 都可以跑死。 ","date":"2022-03-16","objectID":"/gmp-model/:6:4","series":null,"tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/#小结"},{"categories":["golang"],"content":" 为什么要有 P// TODO ","date":"2022-03-16","objectID":"/gmp-model/:7:0","series":null,"tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/#为什么要有-p"},{"categories":["golang"],"content":" 参考 Go 为什么这么“快” 让你很快就能理解-go的协程调度原理 Goroutine 数量控制在多少合适，会影响 GC 和调度？ Golang goroutine与调度器 进程与线程的一个简单解释 [典藏版] Golang 调度器 GMP 原理与调度全分析 ","date":"2022-03-16","objectID":"/gmp-model/:8:0","series":null,"tags":["golang"],"title":"Go GMP 模型","uri":"/gmp-model/#参考"},{"categories":["golang"],"content":"go build 常用命令,golang,编译,go 编译","date":"2022-03-16","objectID":"/go-build-args/","series":null,"tags":["golang"],"title":"Go 常用命令","uri":"/go-build-args/"},{"categories":["golang"],"content":" 常用编译参数 参数 说明 -o 指定输出可执行文件名 -v 编译时显示包名，可以理解成输出详细编译信息 -race 开启竞态检测 *.go 编译当前目录下的所有go文件，也可以写成 f2.go f2.go … -a 强制重新构建 -w 去掉DWARF调试信息，得到的程序就不能用gdb调试了 -s 去掉符号表,panic时候的stack trace就没有任何文件名/行号信息了，这个等价于普通C/C++程序被strip的效果 -X 设置包中的变量值 -gcflags \"-N -l\" 编译目标程序的时候会嵌入运行时(runtime)的二进制，禁止优化和内联可以让运行时(runtime)中的函数变得更容易调试。gcflags 其实是给go编译器传入参数，也就是传给go tool compile的参数，因此可以用go tool compile --help查看所有可用的参数 -ldflags 给go链接器传入参数，实际是给go tool link的参数，可以用go tool link --help查看可用的参数。 -ldflags '-extldflags \"-static\"' 静态编译 ","date":"2022-03-16","objectID":"/go-build-args/:1:0","series":null,"tags":["golang"],"title":"Go 常用命令","uri":"/go-build-args/#常用编译参数"},{"categories":["golang"],"content":" 交叉编译 参数 说明 GOOS GOARCH linux 386 / amd64 / arm darwin 386 / amd64 feedbsd 386 / amd64 windows 386 / amd64 对于编译给ARM使用的Go程序，需要根据实际情况配置$GOARM，这是用来控制CPU的浮点协处理器的参数。 $GOARM默认是6，对于不支持VFP使用软件运算的老版本ARM平台要设置成5，支持VFPv1的设置成6，支持VFPv3的设置成7。 示例 GOARM=7 GOARCH=arm GOOS=linux go build -v -o fca ","date":"2022-03-16","objectID":"/go-build-args/:2:0","series":null,"tags":["golang"],"title":"Go 常用命令","uri":"/go-build-args/#交叉编译"},{"categories":["golang"],"content":" go mod// TODO ","date":"2022-03-16","objectID":"/go-build-args/:3:0","series":null,"tags":["golang"],"title":"Go 常用命令","uri":"/go-build-args/#go-mod"},{"categories":["golang"],"content":" 参考 golang编译时的参数传递（gcflags, ldflags） Golang交叉编译（跨平台编译）简述 交叉编译Go程序 ARM flags GOARM go mod使用 ","date":"2022-03-16","objectID":"/go-build-args/:4:0","series":null,"tags":["golang"],"title":"Go 常用命令","uri":"/go-build-args/#参考"},{"categories":["web"],"content":"node,nodejs, heap out of memory","date":"2022-03-16","objectID":"/node-oom/","series":null,"tags":["web","node"],"title":"JavaScript heap out of memory","uri":"/node-oom/"},{"categories":["web"],"content":"刚在打包项目时执行 yarn run build 时出现了 oom 的情况，具体报错信息如下： 我的环境是 win10 专业版 WSL。 解决办法，设置 export NODE_OPTIONS=--max_old_space_size=4096，设置完之后重新执行 yarn run build 即可。 ","date":"2022-03-16","objectID":"/node-oom/:0:0","series":null,"tags":["web","node"],"title":"JavaScript heap out of memory","uri":"/node-oom/#"},{"categories":["web"],"content":" 参考 Node.js heap out of memory ","date":"2022-03-16","objectID":"/node-oom/:1:0","series":null,"tags":["web","node"],"title":"JavaScript heap out of memory","uri":"/node-oom/#参考"},{"categories":["开发者手册"],"content":"Google,浏览器插件,插件下载","date":"2022-03-16","objectID":"/googe-plugin-download/","series":null,"tags":["chrome"],"title":"将google浏览器插件下载到本地","uri":"/googe-plugin-download/"},{"categories":["开发者手册"],"content":"国内的网络太复杂了，在不能访问 google 的情况下，甚至都不能打开网上应用商店，所以我们需要一个方便的方式来下载google浏览器插件并分享 给需要的小伙伴。 我们打开任意一个浏览器插件，如： URL 地址栏中有一串字符串，这是唯一的，通过这个字符串可以获取到插件的下载地址，如： 下载地址为： https://clients2.google.com/service/update2/crx?response=redirect\u0026os=win\u0026arch=x64\u0026os_arch=x86_64\u0026nacl_arch=x86-64\u0026prod=chromecrx\u0026prodchannel=\u0026prodversion=77.0.3865.90\u0026lang=zh-CN\u0026acceptformat=crx2,crx3\u0026x=id%3D{XXXX}%26installsource%3Dondemand%26uc 将以上的 {XXXX} 替换为插件的 ID，就可以下载到本地了。 以下这个地址是Mote：语音笔记和反馈插件的下载地址，成功下载的插件是 .crx 结尾的文件。直接拖到浏览器中就会自动安装。 https://clients2.google.com/service/update2/crx?response=redirect\u0026os=win\u0026arch=x64\u0026os_arch=x86_64\u0026nacl_arch=x86-64\u0026prod=chromecrx\u0026prodchannel=\u0026prodversion=77.0.3865.90\u0026lang=zh-CN\u0026acceptformat=crx2,crx3\u0026x=id%3Dajphlblkfpppdpkgokiejbjfohfohhmk%26installsource%3Dondemand%26uc ","date":"2022-03-16","objectID":"/googe-plugin-download/:0:0","series":null,"tags":["chrome"],"title":"将google浏览器插件下载到本地","uri":"/googe-plugin-download/#"},{"categories":null,"content":"xiaobinqt's personal website,xiaobinqt,程序员,程序猿,xiaobinqt@163.com","date":"2022-03-06","objectID":"/about/","series":null,"tags":null,"title":"About Me","uri":"/about/"},{"categories":null,"content":" 缘起很喜欢电影《五亿探长雷洛传》 ，电影一开头，由潮州迁居香港的青年雷洛，为了生计，投考香港警察，面试官问他，为什么要当警察，他回答，为了吃饭。 为了吃饭 大学毕业后，为了吃饭，便利用自己了解的一点编程知识开始码农之路。大学学的是通信工程，非计算机专业，编程之路困难重重，但好歹自己有自知之明，笨人多努力，相信通过自己的努力，可以让自己的编程技术更加完善。 好记性不如烂笔头，想着把自己工作期间遇到的问题，平时看到的好的文章，记录下来，方便查找和温习，也是记录自己的不足和成长，便有了这个网站。 建站的初衷不是为了炫耀所知，而是记录无知。 人知道得越多，就会发现无知的越多。有更广袤的世界可以探索，是莫大的快乐呀！ ","date":"2022-03-06","objectID":"/about/:1:0","series":null,"tags":null,"title":"About Me","uri":"/about/#缘起"},{"categories":null,"content":" 关于作者 👨‍💻 半路出家的程序猿，技术不精，但有一个成为技术大拿的梦想。 🤪 拖延症患者，持续性混吃等死，间接性踌躇满志。 💕 爱好读书，但总是边读边忘 😢。 ","date":"2022-03-06","objectID":"/about/:2:0","series":null,"tags":null,"title":"About Me","uri":"/about/#关于作者"},{"categories":null,"content":" 职业生涯2017/4 - 2019/3 在西安一家科技公司从事 PHP 开发。 2019/4 - 2021/3 在北京腾讯外包从事 PHP 和 Go 开发。 2021/4 - 至今 版权说明 本站图片和文字，除原创作品之外，部分来自互联网。对于转载的文章，本站都标识有【转载】。 此类资源的原版权所有者可在任何时候、以任何理由要求本站停止使用，其中包括被本站编辑（比如加注说明）过的资源， 联系方式见下文。 ","date":"2022-03-06","objectID":"/about/:3:0","series":null,"tags":null,"title":"About Me","uri":"/about/#职业生涯"},{"categories":null,"content":" 联系方式 邮箱：xiaobinqt@163.com Github：@xiaobinqt 微信： 个人微信 ","date":"2022-03-06","objectID":"/about/:4:0","series":null,"tags":null,"title":"About Me","uri":"/about/#联系方式"},{"categories":["golang"],"content":"golang grpc,golang,rpc,grpc,grpc 入门","date":"2022-03-05","objectID":"/grpc-demo/","series":null,"tags":["golang"],"title":"grpc 入门应用","uri":"/grpc-demo/"},{"categories":["golang"],"content":"RPC 是一种跨语言的协议，它可以让我们在不同的语言之间进行通信。 远程过程调用（英语：Remote Procedure Call，缩写为 RPC）是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一个 地址空间（通常为一个开放网络的一台计算机）的子程序，而程序员就像调用本地程序一样，无需额外地为这个交互作用编程（无需关注细节）。 RPC是一种服务器-客户端（Client/Server）模式，经典实现是一个通过发送请求-接受回应进行信息交互的系统。 ","date":"2022-03-05","objectID":"/grpc-demo/:0:0","series":null,"tags":["golang"],"title":"grpc 入门应用","uri":"/grpc-demo/#"},{"categories":["golang"],"content":" 安装 go install github.com/golang/protobuf/protoc-gen-go@v1.4.0 go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.1 不推荐使用 google.golang.org/protobuf/cmd/protoc-gen-go@v1.26 这个版本太高了，可能会遇到以下这个问题， --go_out: protoc-gen-go: plugins are not supported; use 'protoc --go-grpc_out=...' to generate gRPC See https://grpc.io/docs/languages/go/quickstart/#regenerate-grpc-code for more information. 参考解决方案记一次奇妙的go-protobuf包升级之旅 protoc 工具安装 下载地址，下载解压将 bin 目录添加到环境变量中。 ","date":"2022-03-05","objectID":"/grpc-demo/:1:0","series":null,"tags":["golang"],"title":"grpc 入门应用","uri":"/grpc-demo/#安装"},{"categories":["golang"],"content":" 定义 proto 文件 syntax = \"proto3\"; // 使用protobuf版本3 option go_package = \"./protobuf\"; // 这个影响生成的目录及go的package命名 // 定义一个计算服务, 输入为CalcRequest, 输出为CalcResponse service CalculatorService { rpc calc(CalcRequest) returns (CalcResponse) {}; } // 计算两个数某种运算(如加法)的参数 message CalcRequest { double a = 1; double b = 2; string op = 3; } // 计算结果 message CalcResponse { double r = 1; } ","date":"2022-03-05","objectID":"/grpc-demo/:2:0","series":null,"tags":["golang"],"title":"grpc 入门应用","uri":"/grpc-demo/#定义-proto-文件"},{"categories":["golang"],"content":" 生成 .pb.go 文件 protoc --go_out=plugins=grpc:. calculator.proto ","date":"2022-03-05","objectID":"/grpc-demo/:3:0","series":null,"tags":["golang"],"title":"grpc 入门应用","uri":"/grpc-demo/#生成-pbgo-文件"},{"categories":["golang"],"content":" rpc server package main import ( \"context\" \"fmt\" \"net\" \"go.src/grpc/calculator/protobuf\" \"google.golang.org/grpc\" ) // 实现: CalculatorServiceServer接口, 在calculator.pb.go中定义 type server struct{} func (s server) Calc(ctx context.Context, req *protobuf.CalcRequest) (resp *protobuf.CalcResponse, err error) { a := req.GetA() b := req.GetB() op := req.GetOp() resp = \u0026protobuf.CalcResponse{} switch op { case \"+\": resp.R = a + b case \"-\": resp.R = a - b case \"*\": resp.R = a * b case \"/\": if b == 0 { err = fmt.Errorf(\"divided by zero\") return } resp.R = a / b } return } // 启动rpc server func main() { listener, err := net.Listen(\"tcp\", \"localhost:3233\") if err != nil { panic(err) } s := grpc.NewServer() protobuf.RegisterCalculatorServiceServer(s, \u0026server{}) fmt.Println(\"server start\") err = s.Serve(listener) if err != nil { panic(err) } } ","date":"2022-03-05","objectID":"/grpc-demo/:4:0","series":null,"tags":["golang"],"title":"grpc 入门应用","uri":"/grpc-demo/#rpc-server"},{"categories":["golang"],"content":" rpc client package main import ( \"context\" \"fmt\" \"log\" \"go.src/grpc/calculator/protobuf\" \"google.golang.org/grpc\" \"google.golang.org/grpc/credentials/insecure\" ) func main() { // 连上grpc server //conn, err := grpc.Dial(\"localhost:3233\", grpc.WithInsecure()) conn, err := grpc.Dial(\"localhost:3233\", grpc.WithTransportCredentials(insecure.NewCredentials())) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer conn.Close() c := protobuf.NewCalculatorServiceClient(conn) // 调用远程方法 resp, err := c.Calc(context.Background(), \u0026protobuf.CalcRequest{ A: 1, B: 2, Op: \"+\", }) if err != nil { fmt.Println(\"calc err: \", err.Error()) return } fmt.Println(\"calc success,respR: \", resp.GetR()) // 3 } ","date":"2022-03-05","objectID":"/grpc-demo/:5:0","series":null,"tags":["golang"],"title":"grpc 入门应用","uri":"/grpc-demo/#rpc-client"},{"categories":["golang"],"content":" 运行测试 server client ","date":"2022-03-05","objectID":"/grpc-demo/:6:0","series":null,"tags":["golang"],"title":"grpc 入门应用","uri":"/grpc-demo/#运行测试"},{"categories":["golang"],"content":" 示例下载示例源码地址 ","date":"2022-03-05","objectID":"/grpc-demo/:7:0","series":null,"tags":["golang"],"title":"grpc 入门应用","uri":"/grpc-demo/#示例下载"},{"categories":["web"],"content":"Ajax 在请求时携带 cookie 信息,cookie 信息会被添加到请求头中,Cookie","date":"2022-03-01","objectID":"/ajax-req-add-cookie/","series":null,"tags":["web"],"title":"ajax 在请求时携带 cookie 信息","uri":"/ajax-req-add-cookie/"},{"categories":["web"],"content":"最近有个需求在使用 $.ajax 时需要把 cookie 信息也带着，google 下发现可以这么写： $.ajax({ url: \"/nodered/nodes\", headers: { Accept: \"text/html\", }, xhrFields: { withCredentials: true // 携带 cookie 信息 }, success: function (data) { console.log(data) $(\"#red-ui-palette-container\").html(data) }, error: function (jqXHR) { console.log(jqXHR) } }); ","date":"2022-03-01","objectID":"/ajax-req-add-cookie/:0:0","series":null,"tags":["web"],"title":"ajax 在请求时携带 cookie 信息","uri":"/ajax-req-add-cookie/#"},{"categories":["golang"],"content":"running gcc failed: exit status 1","date":"2022-02-10","objectID":"/build-running-gcc-failed/","series":null,"tags":["golang","build"],"title":"running gcc failed: exit status 1","uri":"/build-running-gcc-failed/"},{"categories":["golang"],"content":"今天在编译 go 项目时出现了如下错误： /usr/local/go/pkg/tool/linux_amd64/link: running gcc failed: exit status 1 /usr/bin/ld: cannot find -lpthread /usr/bin/ld: cannot find -lc collect2: error: ld returned 1 exit status 解决办法： yum install glibc-static.x86_64 -y ","date":"2022-02-10","objectID":"/build-running-gcc-failed/:0:0","series":null,"tags":["golang","build"],"title":"running gcc failed: exit status 1","uri":"/build-running-gcc-failed/#"},{"categories":["golang"],"content":"xiaobinqt,实现下载文件的断点续传,断点续传,go 实现下载文件的断点续传","date":"2022-01-21","objectID":"/go-breakpoint-resume/","series":null,"tags":["golang","断点续传"],"title":"Go 实现下载文件的断点续传","uri":"/go-breakpoint-resume/"},{"categories":["golang"],"content":" 断点续传断点继传就是下载的文件可以在你下载了一半的时候暂停，下一次下载的时候可以从你暂停的地方继续下载，不用从头开始下载。 ","date":"2022-01-21","objectID":"/go-breakpoint-resume/:1:0","series":null,"tags":["golang","断点续传"],"title":"Go 实现下载文件的断点续传","uri":"/go-breakpoint-resume/#断点续传"},{"categories":["golang"],"content":" 服务端","date":"2022-01-21","objectID":"/go-breakpoint-resume/:2:0","series":null,"tags":["golang","断点续传"],"title":"Go 实现下载文件的断点续传","uri":"/go-breakpoint-resume/#服务端"},{"categories":["golang"],"content":" martini 实现martini 框架实现👇 package main import ( \"bufio\" \"crypto/md5\" \"encoding/hex\" \"fmt\" \"io\" \"net/http\" \"os\" \"strconv\" \"strings\" \"github.com/go-martini/martini\" \"github.com/pkg/errors\" ) // 大文件 var path = \"/mnt/d/code-server-3.11.0-linux-amd64.tar.gz\" func download(w http.ResponseWriter, r *http.Request) { filename := \"download\" file, err := os.Open(path) if err != nil { err = errors.Wrapf(err, \"download openfile err\") w.WriteHeader(500) w.Write([]byte(err.Error())) return } defer file.Close() info, err := file.Stat() if err != nil { err = errors.Wrapf(err, \"download stat err\") w.WriteHeader(500) w.Write([]byte(err.Error())) return } md5sum, err := MD5sum(file) if err != nil { err = errors.Wrapf(err, \"download md5sum err\") w.WriteHeader(500) w.Write([]byte(err.Error())) return } fmt.Println(\"md5sum = \", md5sum) w.Header().Add(\"Accept-Ranges\", \"bytes\") w.Header().Add(\"Content-Disposition\", \"attachment; filename=\"+filename) w.Header().Add(\"Content-Md5\", md5sum) var start, end int64 if r := r.Header.Get(\"Range\"); r != \"\" { if strings.Contains(r, \"bytes=\") \u0026\u0026 strings.Contains(r, \"-\") { fmt.Sscanf(r, \"bytes=%d-%d\", \u0026start, \u0026end) if end == 0 { end = info.Size() - 1 } // start 从 0 开始,所以 end = info.Size() 也是有问题的，end 最大是 `info.Size() - 1` if start \u003e end || start \u003c 0 || end \u003c 0 || end \u003e= info.Size() { w.WriteHeader(http.StatusRequestedRangeNotSatisfiable) w.Write([]byte(\"参数错误....\")) return } w.Header().Add(\"Content-Length\", strconv.FormatInt(end-start+1, 10)) w.Header().Add(\"Content-Range\", fmt.Sprintf(\"bytes %d-%d/%d\", start, end, info.Size())) w.Header().Set(\"Content-Type\", \"application/octet-stream\") w.WriteHeader(http.StatusPartialContent) } else { w.WriteHeader(400) w.Write([]byte(\"header Range\")) return } } else { w.Header().Add(\"Content-Length\", strconv.FormatInt(info.Size(), 10)) w.Header().Set(\"Content-Type\", \"application/octet-stream\") start = 0 end = info.Size() - 1 } _, err = file.Seek(start, 0) if err != nil { err = errors.Wrapf(err, \"file seek err\") w.WriteHeader(500) w.Write([]byte(err.Error())) return } n := 2048 buf := make([]byte, n) for { if end-start+1 \u003c int64(n) { n = int(end - start + 1) } _, err = file.Read(buf[:n]) if err != nil { if err != io.EOF { err = errors.Wrapf(err, \"io.Eof err\") w.WriteHeader(500) w.Write([]byte(err.Error())) return } return } _, err = w.Write(buf[:n]) if err != nil { err = errors.Wrapf(err, \"Writer.Write err\") w.WriteHeader(500) w.Write([]byte(err.Error())) return } start += int64(n) if start \u003e= end+1 { return } } } func MD5sum(file *os.File) (string, error) { hash := md5.New() for buf, reader := make([]byte, 65536), bufio.NewReader(file); ; { n, err := reader.Read(buf) if err != nil { if err == io.EOF { break } return \"\", err } hash.Write(buf[:n]) } return hex.EncodeToString(hash.Sum(nil)), nil } func main() { route := martini.Classic() route.Get(\"/download\", download) route.RunOnAddr(\":8080\") } ","date":"2022-01-21","objectID":"/go-breakpoint-resume/:2:1","series":null,"tags":["golang","断点续传"],"title":"Go 实现下载文件的断点续传","uri":"/go-breakpoint-resume/#martini-实现"},{"categories":["golang"],"content":" 客户端下载 package main import ( \"bufio\" \"fmt\" \"io\" \"net/http\" \"net/url\" \"os\" \"os/exec\" \"strconv\" \"github.com/pkg/errors\" \"github.com/sirupsen/logrus\" ) func DownloadDownloadArtifact(downloadPath, surl string) (err error) { dfn := downloadPath var ( file *os.File size int64 headerMd5sum, downloadMd5sum string ) file, err = os.OpenFile(dfn, os.O_RDWR|os.O_CREATE, 0644) defer file.Close() if err != nil { err = errors.Wrapf(err, \"download openfile err\") return err } stat, _ := file.Stat() size = stat.Size() sk, err := file.Seek(size, 0) if err != nil { err = errors.Wrapf(err, \"seek err\") return err } if sk != size { err = fmt.Errorf(\"seek length not equal file size,seek=%d,size=%d\", sk, size) logrus.Error(err.Error()) return err } request := http.Request{} request.Method = http.MethodGet if size != 0 { header := http.Header{} header.Set(\"Range\", \"bytes=\"+strconv.FormatInt(size, 10)+\"-\") request.Header = header } parse, _ := url.Parse(surl) request.URL = parse resp, err := http.DefaultClient.Do(\u0026request) //resp, err := http.DefaultClient.Do(\u0026request) defer resp.Body.Close() if err != nil { err = errors.Wrapf(err, \"client do err\") logrus.Error(err.Error()) return err } headerMd5sum = resp.Header.Get(\"Content-Md5\") if headerMd5sum == \"\" { return fmt.Errorf(\"resp header md5sum empty\") } body := resp.Body writer := bufio.NewWriter(file) bs := make([]byte, 1024*1024) for { var read int read, err = body.Read(bs) if err != nil { if err != io.EOF { err = errors.Wrapf(err, \"body read not io eof\") logrus.Error(err.Error()) return err } if err == io.EOF \u0026\u0026 resp.StatusCode != http.StatusOK { err = nil return } if read != 0 { _, err = writer.Write(bs[:read]) if err != nil { err = errors.Wrapf(err, \"writer write err\") return err } } err = nil break } _, err = writer.Write(bs[:read]) if err != nil { err = errors.Wrapf(err, \"writer write err\") return err } } if err != nil { return err } err = writer.Flush() if err != nil { err = errors.Wrapf(err, \"writer.Flush err\") return err } // 比对 md5 是否一致 downloadMd5sum, err = md5sum(downloadPath) if err != nil { err = errors.Wrapf(err, \"get download md5dum err\") logrus.Error(err.Error()) // md5 不一致直接删除 os.Remove(downloadPath) return err } logrus.Debugf(\"downloadMd5sum: %s,headerMd5sum:%s \", downloadMd5sum, headerMd5sum) if downloadMd5sum == headerMd5sum { return nil } // 错误了删除 tar 包 os.Remove(downloadPath) return fmt.Errorf(\"download md5sum not equal header md5dum\") } func md5sum(downloadPath string) (string, error) { cmdStr := fmt.Sprintf(\"printf $(md5sum %s)\", downloadPath) cmdOutput, err := exec.Command(\"/bin/sh\", \"-c\", cmdStr).CombinedOutput() logrus.Debugf(\"md5sum: %s \", cmdStr) if err != nil { err = errors.Wrapf(err, \"md5sum [%s] exec.Command err\", cmdStr) logrus.Error(err.Error()) return \"\", err } return string(cmdOutput), nil } func main() { err := DownloadDownloadArtifact(\"/mnt/d/tmp/xxx.111.test\", \"http://127.0.0.1:8080/download\") if err != nil { fmt.Println(\"download err\", err.Error()) return } fmt.Println(\"success..........\") } ","date":"2022-01-21","objectID":"/go-breakpoint-resume/:3:0","series":null,"tags":["golang","断点续传"],"title":"Go 实现下载文件的断点续传","uri":"/go-breakpoint-resume/#客户端下载"},{"categories":["算法与数学"],"content":"一致性Hash,hash算法,数据倾斜","date":"2022-01-15","objectID":"/consistent-hash/","series":null,"tags":["算法"],"title":"一致性 hash","uri":"/consistent-hash/"},{"categories":["算法与数学"],"content":" 存在的意义一致性哈希算法解决了普通余数 Hash 算法伸缩性差的问题，可以保证在上线、下线服务器的情况下尽量有多的请求命中原来路由到的服务器。 ","date":"2022-01-15","objectID":"/consistent-hash/:1:0","series":null,"tags":["算法"],"title":"一致性 hash","uri":"/consistent-hash/#存在的意义"},{"categories":["算法与数学"],"content":" 优化一致性哈希算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜问题。可以通过通过增加虚拟节点来解决数据倾斜问题。 如果存在大量的虚拟节点，节点的查找性能就成为必须考虑的因数。可以使用红黑树 来加快查找速度， ","date":"2022-01-15","objectID":"/consistent-hash/:2:0","series":null,"tags":["算法"],"title":"一致性 hash","uri":"/consistent-hash/#优化"},{"categories":["算法与数学"],"content":" 参考 一致性Hash(Consistent Hashing)原理剖析及Java实现 图解一致性哈希算法 golang实现一致性hash环及优化方法 一致性哈希 ","date":"2022-01-15","objectID":"/consistent-hash/:3:0","series":null,"tags":["算法"],"title":"一致性 hash","uri":"/consistent-hash/#参考"},{"categories":["开发者手册"],"content":"xiaobinqt","date":"2021-12-28","objectID":"/reuse-jetbrains/","series":null,"tags":["jetbrains","ide"],"title":"Jetbrains 家族 ide 破解方法，支持最新版","uri":"/reuse-jetbrains/"},{"categories":["开发者手册"],"content":" Jetbrains 家族的 ide 对开发者友好，基本支持所有的开发语言，可以去下载页面下载对应的 ide https://www.jetbrains.com/products/。 下载界面 ","date":"2021-12-28","objectID":"/reuse-jetbrains/:0:0","series":null,"tags":["jetbrains","ide"],"title":"Jetbrains 家族 ide 破解方法，支持最新版","uri":"/reuse-jetbrains/#"},{"categories":["开发者手册"],"content":" ja-netfilter-allide 下载完成后，去 https://jetbra.in/s 页面下载 ja-netfilter-all 包， 下载 ja-netfilter-all 将下载的 ja-netfilter-all.zip 解压到不带中文的任意目录， ","date":"2021-12-28","objectID":"/reuse-jetbrains/:1:0","series":null,"tags":["jetbrains","ide"],"title":"Jetbrains 家族 ide 破解方法，支持最新版","uri":"/reuse-jetbrains/#ja-netfilter-all"},{"categories":["开发者手册"],"content":" 配置 vmoptions在 ide 的安装目录中找打 idea64.exe.vmoptions 文件，比如我的路径是 D:\\mySoft\\GoLand\\GoLand 2021.1\\bin： XXX.vmoptions 路径 使用文本编辑打开，在最后添加一行配置，指向上一步解压出来的 ja-netfilter.jar 文件： -javaagent:路径\\ja-netfilter.jar=jetbrains 编辑 vmoptions ","date":"2021-12-28","objectID":"/reuse-jetbrains/:2:0","series":null,"tags":["jetbrains","ide"],"title":"Jetbrains 家族 ide 破解方法，支持最新版","uri":"/reuse-jetbrains/#配置-vmoptions"},{"categories":["开发者手册"],"content":" 激活码以上配置完后，重启 ide，复制激活码到 ide 激活即可： 复制激活码 激活 ","date":"2021-12-28","objectID":"/reuse-jetbrains/:3:0","series":null,"tags":["jetbrains","ide"],"title":"Jetbrains 家族 ide 破解方法，支持最新版","uri":"/reuse-jetbrains/#激活码"},{"categories":["开发者手册"],"content":"xiaobinqt,golang 使用 swagger,go 原生 swagger 的使用","date":"2021-12-27","objectID":"/go-swagger/","series":null,"tags":["golang","swagger"],"title":"Go 使用原生 Swagger","uri":"/go-swagger/"},{"categories":["开发者手册"],"content":" Swagger 是一个规范且完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web 服务。 当通过 Swagger 进行正确定义，用户可以理解远程服务并使用最少实现逻辑与远程服务进行交互。 支持 API 自动生成同步的在线文档。使用 Swagger 后可以直接通过代码生成文档，不再需要自己手动编写接口文档了，对程序员来说非常友好，可以节约写文档的时间。 提供 Web 页面在线测试 API。光有文档还不够，Swagger 生成的文档还支持在线测试。参数和格式都定好了，直接在界面上输入参数对应的值即可在线测试接口。 ","date":"2021-12-27","objectID":"/go-swagger/:0:0","series":null,"tags":["golang","swagger"],"title":"Go 使用原生 Swagger","uri":"/go-swagger/#"},{"categories":["开发者手册"],"content":" swag cli 安装Swag 能将 Go 的注释转换为 Swagger 文档。 # 安装swag go get github.com/swaggo/swag/cmd/swag # 查看版本 swag -v swag cli ","date":"2021-12-27","objectID":"/go-swagger/:1:0","series":null,"tags":["golang","swagger"],"title":"Go 使用原生 Swagger","uri":"/go-swagger/#swag-cli-安装"},{"categories":["开发者手册"],"content":" swagger-ui 库从 swagger-ui 库下载 dist 文件夹到自己的项目中，并更名为 swagger（更名不是必须的）。 把 swagger 中的 swagger-initializer.js 文件中有个 url 参数，全局替换 swagger 文件夹中的这个 url 参数值为 ./swagger.json。 swagger-initializer.js url 替换所有 ","date":"2021-12-27","objectID":"/go-swagger/:2:0","series":null,"tags":["golang","swagger"],"title":"Go 使用原生 Swagger","uri":"/go-swagger/#swagger-ui-库"},{"categories":["开发者手册"],"content":" swagger 文档swagger 是以注释的方式描述的，然后使用 swag cli 生成的文档。 比如我有一个 main.go 文件，文件内容如下： package main import ( \"embed\" \"net/http\" \"github.com/go-martini/martini\" ) //go:embed swagger var embededFiles embed.FS // @title 测试 API // @version 4.0 // @description 测试 API V4.0 // @securityDefinitions.apiKey MyApiKey // @in header // @name Xiaobinqt-Api-Key // @BasePath / func main() { m := martini.Classic() m.Get(\"/swagger/**\", http.FileServer(http.FS(embededFiles)).ServeHTTP) m.Post(\"/api/login\", Login) m.Run() } type Req struct { Email string `json:\"email\"` // 邮箱 Password string `json:\"password\"` } // @Tags 用户管理 // @Summary 用户登录 // @Security MyApiKey // @accept application/json // @Produce application/json // @Param data body Req true \"email: 用户名，password: 密码\" // @Success 200 {object} EdgeInstanceList // @Router /api/login [POST] func Login(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"hello\")) } type EdgeInstanceList struct { A string B string } 使用 swag cli 生成 swagger 文档： 生成 swagger 文档 访问 swagger 路由： 访问 swagger 路由 ","date":"2021-12-27","objectID":"/go-swagger/:3:0","series":null,"tags":["golang","swagger"],"title":"Go 使用原生 Swagger","uri":"/go-swagger/#swagger-文档"},{"categories":["开发者手册"],"content":" 常见问题","date":"2021-12-27","objectID":"/go-swagger/:4:0","series":null,"tags":["golang","swagger"],"title":"Go 使用原生 Swagger","uri":"/go-swagger/#常见问题"},{"categories":["开发者手册"],"content":" apiKey有的 api 需要加上 header 头信息才能正确访问，这时可以添加注释信息： // @securityDefinitions.apiKey MyApiKey // @in header // @name Xiaobinqt-Api-Key @in header 设置，在请求 header 中，@name Xiaobinqt-Api-Key header 字段为Xiaobinqt-Api-Key，@securityDefinitions.apiKey 固定写法，MyApiKey在每个方法中的@Security注释信息值，如： // @Tags 用户管理 // @Summary 用户登录 // @Security MyApiKey // @accept application/json // @Produce application/json // @Param data body Req true \"email: 用户名，password: 密码\" // @Success 200 {object} EdgeInstanceList // @Router /api/login [POST] func Login(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"hello\")) } 如果在 swagger 页面 Available authorizations 的值不为空，那么每次请求都会带着Xiaobinqt-Api-Key这个 header 头，值就是Available authorizations 填入的值。 Available authorizations Execute ","date":"2021-12-27","objectID":"/go-swagger/:4:1","series":null,"tags":["golang","swagger"],"title":"Go 使用原生 Swagger","uri":"/go-swagger/#apikey"},{"categories":["开发者手册"],"content":" paramType具体可以参看：https://swagger.io/docs/specification/describing-parameters/ body type Req struct { Email string `json:\"email\"` // 邮箱 Password string `json:\"password\"` } // @Param data body Req true \"email: 用户名，password: 密码\" path // @Param user_id path string true \"用户ID\" query // @Param search query string false \"搜索内容\" 完整注释如下： // @Tags 用户管理 // @Summary 用户登录 // @Security MyApiKey // @accept application/json // @Produce application/json // @Param user_id path string true \"用户ID\" // @Param search query string false \"搜索内容\" // @Param data body Req true \"email: 用户名，password: 密码\" // @Success 200 {object} EdgeInstanceList // @Router /api/login/{user_id} [POST] func Login(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"hello\")) } 效果_01 效果_02 ","date":"2021-12-27","objectID":"/go-swagger/:4:2","series":null,"tags":["golang","swagger"],"title":"Go 使用原生 Swagger","uri":"/go-swagger/#paramtype"},{"categories":["开发者手册"],"content":" 参考 ☝️文章示例源码 swagger官方文档 https://github.com/swaggo/swag/blob/master/README_zh-CN.md https://www.jianshu.com/p/9313d0c5395d Go embed 简明教程 Go 1.16新特性-embed包及其使用 ","date":"2021-12-27","objectID":"/go-swagger/:5:0","series":null,"tags":["golang","swagger"],"title":"Go 使用原生 Swagger","uri":"/go-swagger/#参考"},{"categories":["web"],"content":"xiaobinqt,node-addon-api,js 调用 c 语言,jc call c function,How to call C function from nodeJS,node 调用C/C++ 方法","date":"2021-11-18","objectID":"/js-call-c/","series":null,"tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/"},{"categories":["web"],"content":"最近在 node 项目开发中，有个需求是 nodeJS 需要支持调用 C 语言的函数，node-addon-api 可以支持这个需求。 ","date":"2021-11-18","objectID":"/js-call-c/:0:0","series":null,"tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/#"},{"categories":["web"],"content":" 开发环境我用的开发环境 docker 起的 code-server 环境，code-server 版本为 code-server:version-v3.11.1 。可以把 code-server 理解成一个在线 vscode 环境，就像 github 的在线 web 编辑器一样。 docker pull linuxserver/code-server:version-v3.11.1 code-server github web 编辑器 ","date":"2021-11-18","objectID":"/js-call-c/:1:0","series":null,"tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/#开发环境"},{"categories":["web"],"content":" 加法器示例开发环境搭建成功后，可以实现一个小功能，以熟悉 node-addon-api 的使用。 现在实现一个加法器，JS 调用 C 语言的 add 方法，传入 2 个参数，C 语言累加后返回结果。 ","date":"2021-11-18","objectID":"/js-call-c/:2:0","series":null,"tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/#加法器示例"},{"categories":["web"],"content":" 项目初始化创建项目并进行 npm init 初始化： 创建项目并初始化 安装 node-addon-api： npm i node-addon-api 安装 npm 依赖包 ","date":"2021-11-18","objectID":"/js-call-c/:2:1","series":null,"tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/#项目初始化"},{"categories":["web"],"content":" c 代码新建一个 cal.cc 文件，内容如下： #include \u003cnapi.h\u003e // 定义一个 Add() 方法 Napi::Value Add(const Napi::CallbackInfo\u0026 info) { Napi::Env env = info.Env(); // 获取 js 上下文信息 if (info.Length() \u003c 2) { Napi::TypeError::New(env, \"Wrong number of arguments\") .ThrowAsJavaScriptException(); return env.Null(); } if (!info[0].IsNumber() || !info[1].IsNumber()) { Napi::TypeError::New(env, \"Wrong arguments\").ThrowAsJavaScriptException(); return env.Null(); } int arg0 = info[0].As\u003cNapi::Number\u003e().Int32Value(); int arg1 = info[1].As\u003cNapi::Number\u003e().Int32Value(); int arg2 = arg0 + arg1; Napi::Number num = Napi::Number::New(env, arg2); return num; } // 导出函数，可使用 exports.Set() 导出多个函数 Napi::Object Init(Napi::Env env, Napi::Object exports) { exports.Set(Napi::String::New(env, \"add\"), Napi::Function::New(env, Add)); return exports; } NODE_API_MODULE(addon, Init) ","date":"2021-11-18","objectID":"/js-call-c/:2:2","series":null,"tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/#c-代码"},{"categories":["web"],"content":" binding.gyp编译带第三方扩展库的 c/c++ 程序，通常需要在编译时指定额外的头文件包含路径和链接第三方库，这些都是在 binding.gyp 文件中指定的，这些指定在 nodeJS 自动编译的时候，会解析并应用在命令行的编译工具中。 新建一个 binding.gyp 文件，内容如下： { \"targets\": [ { \"target_name\": \"test\", \"sources\": [ \"cal.cc\" ], \"include_dirs\": [ \"\u003c!@(node -p \\\"require('node-addon-api').include\\\")\" ], \"libraries\": [ ], \"dependencies\": [ \"\u003c!(node -p \\\"require('node-addon-api').gyp\\\")\" ], \"cflags!\": [ \"-fno-exceptions\" ], \"cflags_cc!\": [ \"-fno-exceptions\" ], \"defines\": [ \"NAPI_CPP_EXCEPTIONS\" ], \"xcode_settings\": { \"GCC_ENABLE_CPP_EXCEPTIONS\": \"YES\" } } ] } target_name 指定了编译之后模块的名称。 sources 指明 c/c++ 的源文件，如果有多个文件，需要用逗号隔开，放到同一个数组中。 include_dirs 是编译时使用的头文件引入路径，这里使用 node -p 执行 node-addon-api 模块中的预置变量。 dependencies 是必须的，一般不要改变。 cflags!，cflags_cc!，defines 三行指定如果c++程序碰到意外错误的时候，由 NAPI 接口来处理，而不是通常的由 c/c++ 程序自己处理。这防止因为 c/c++ 部分程序碰到意外直接就退出了程序，而是由 nodeJS 程序来捕获处理，如果是在Linux中编译使用，有这三行就够了。 ","date":"2021-11-18","objectID":"/js-call-c/:2:3","series":null,"tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/#bindinggyp"},{"categories":["web"],"content":" 编译调用 编译 每次修改代码后都需要执行 npm i 重新编译 npm i 编译后，进入 nodeJS 中可以直接 require 调用。 调用 这里 require 的 test.node，.node 后缀是固定的，test 就是 binding.gyp 文件里 target_name 的值。 1+3=4 从调用结果来看，符合预期。 ","date":"2021-11-18","objectID":"/js-call-c/:2:4","series":null,"tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/#编译调用"},{"categories":["web"],"content":" bindings 包现在我们 require 编译后的 node 需要这样写： require('./build/Release/nodecamera.node'); 可以用 bindings 包简化 require 。 npm i bindings --save 通估👆命令安装 bindings 包。 bindings 包使用 所以以上示例简化后的 require 为： const addon = require('bindings')('test.node'); ","date":"2021-11-18","objectID":"/js-call-c/:2:5","series":null,"tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/#bindings-包"},{"categories":["web"],"content":" 常见数据类型转换JS 与 C 的数据类型有较大差别，比如 C 中没有字符串的概念，只有字节数组等。node-addon-api 可以很好的支持 JS 与 C 数据类型的转换。 ","date":"2021-11-18","objectID":"/js-call-c/:3:0","series":null,"tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/#常见数据类型转换"},{"categories":["web"],"content":" 字符串 std::string temp = info[0].As\u003cNapi::String\u003e().ToString(); 字符串转换示例 ","date":"2021-11-18","objectID":"/js-call-c/:3:1","series":null,"tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/#字符串"},{"categories":["web"],"content":" ArrayBuffer Napi::ArrayBuffer ABuffer(const Napi::CallbackInfo\u0026 info) { Napi::Env env = info.Env(); int8_t num[4] = {14,25,45,88}; Napi::ArrayBuffer x = Napi::ArrayBuffer::New(env,num,4); return x; } ArrayBuffer 示例 ","date":"2021-11-18","objectID":"/js-call-c/:3:2","series":null,"tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/#arraybuffer"},{"categories":["web"],"content":" 数组JS 将数组作为 C 函数参数。 Napi::Value ArrayArg(const Napi::CallbackInfo\u0026 info) { Napi::Env env = info.Env(); Napi::Array b = info[0].As\u003cNapi::Array\u003e(); for (int i = 0; i \u003c b.Length(); i++) { Napi::Value v = b[i]; if (v.IsString()){ std::string value = (std::string)v.As\u003cNapi::String\u003e(); return Napi::String::New(env,value); } } } 编译可能有 warning 编译时可能有 warning，但是不影响。 数组参数 ","date":"2021-11-18","objectID":"/js-call-c/:3:3","series":null,"tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/#数组"},{"categories":["web"],"content":" FAQ","date":"2021-11-18","objectID":"/js-call-c/:4:0","series":null,"tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/#faq"},{"categories":["web"],"content":" 持久化函数这个功能可以理解成在 C 的内存空间中有一个 JS 的函数对象且在生命周期内不会被 C 垃圾回收，可以直接在 C 中调用这个 JS 函数。 以下示例，C 提供了 debug 函数，但是参数是一个函数，这个函数会持久在 C 的内存中，在 C 的 Str 函数中用 Call 来调用这个函数并传入对应的参数。 js-call-c-demo.js const addon = require('bindings')('test.node'); // 调用 c 中的 debug 函数，将函数注入到 c 中 addon.debug(msg =\u003e { console.log(\"debug console, c 中传入的 msg 需要打印的参数值为：\", msg) }) // 调用 c 的 str 函数，在 str 函数中会调用 debug 函数中的 console.log() console.log(\"str 函数的返回值为: \", addon.str(\"xiaobinqt\")) cal.cc #include \u003cnapi.h\u003e Napi::FunctionReference Debug; napi_env DebugEnv; Napi::Value DebugFun(const Napi::CallbackInfo\u0026 info) { Napi::Env env = info.Env(); Debug = Napi::Persistent(info[0].As\u003cNapi::Function\u003e()); DebugEnv = env; return Napi::String::New(env,\"OK\"); } Napi::Value Str(const Napi::CallbackInfo\u0026 info) { Napi::Env env = info.Env(); std::string temp = info[0].As\u003cNapi::String\u003e().ToString(); Napi::String s = Napi::String::New(env, temp); // 调用 Debug 函数 Debug.Call({Napi::String::New(DebugEnv,\"我是一个测试 debug\")}); return s; } // 定义一个 Add() 方法 Napi::Value Add(const Napi::CallbackInfo\u0026 info) { Napi::Env env = info.Env(); // 获取 js 上下文信息 if (info.Length() \u003c 2) { Napi::TypeError::New(env, \"Wrong number of arguments\") .ThrowAsJavaScriptException(); return env.Null(); } if (!info[0].IsNumber() || !info[1].IsNumber()) { Napi::TypeError::New(env, \"Wrong arguments\").ThrowAsJavaScriptException(); return env.Null(); } int arg0 = info[0].As\u003cNapi::Number\u003e().Int32Value(); int arg1 = info[1].As\u003cNapi::Number\u003e().Int32Value(); int arg2 = arg0 + arg1; Napi::Number num = Napi::Number::New(env, arg2); return num; } Napi::ArrayBuffer ABuffer(const Napi::CallbackInfo\u0026 info) { Napi::Env env = info.Env(); int8_t num[4] = {14,25,45,88}; Napi::ArrayBuffer x = Napi::ArrayBuffer::New(env,num,4); return x; } Napi::Value ArrayArg(const Napi::CallbackInfo\u0026 info) { Napi::Env env = info.Env(); Napi::Array b = info[0].As\u003cNapi::Array\u003e(); for (int i = 0; i \u003c b.Length(); i++) { Napi::Value v = b[i]; if (v.IsString()){ std::string value = (std::string)v.As\u003cNapi::String\u003e(); return Napi::String::New(env,value); } } } // 导出函数，可使用 exports.Set() 导出多个函数 Napi::Object Init(Napi::Env env, Napi::Object exports) { exports.Set(Napi::String::New(env, \"add\"), Napi::Function::New(env, Add)); exports.Set(Napi::String::New(env, \"str\"), Napi::Function::New(env, Str)); exports.Set(Napi::String::New(env, \"ab\"), Napi::Function::New(env, ABuffer)); exports.Set(Napi::String::New(env, \"arr\"), Napi::Function::New(env, ArrayArg)); exports.Set(Napi::String::New(env, \"debug\"), Napi::Function::New(env, DebugFun)); return exports; } NODE_API_MODULE(addon, Init) 测试结果 ","date":"2021-11-18","objectID":"/js-call-c/:4:1","series":null,"tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/#持久化函数"},{"categories":["web"],"content":" 参考 简单上手nodejs调用c++(c++和js的混合编程) node-addon-api-doc https://github.com/nodejs/node-addon-api https://nodejs.github.io/node-addon-examples/special-topics/object-function-refs#persistent-reference ","date":"2021-11-18","objectID":"/js-call-c/:5:0","series":null,"tags":["js"],"title":"nodeJS 调用 C 语言","uri":"/js-call-c/#参考"},{"categories":["web","开发者手册"],"content":"xiaobinqt,","date":"2021-09-08","objectID":"/go-use-sse/","series":null,"tags":["sse","golang"],"title":"Go 使用 sse","uri":"/go-use-sse/"},{"categories":["web","开发者手册"],"content":" SSE 的本质严格地说，HTTP 协议无法做到服务器主动推送信息。但是，有一种变通方法，就是服务器向客户端声明，接下来要发送的是流信息（streaming）。 也就是说，发送的不是一次性的数据包，而是一个数据流，会连续不断地发送过来。这时，客户端不会关闭连接，会一直等着服务器发过来的新的数据流，视频播放就是这样的例子。本质上，这种通信就是以流信息的方式，完成一次用时很长的下载。 SSE 就是利用这种机制，使用流信息向浏览器推送信息。它基于 HTTP 协议，目前除了 IE/Edge，其他浏览器都支持。 ","date":"2021-09-08","objectID":"/go-use-sse/:1:0","series":null,"tags":["sse","golang"],"title":"Go 使用 sse","uri":"/go-use-sse/#sse-的本质"},{"categories":["web","开发者手册"],"content":" SSE 的特点SSE 与 WebSocket 作用相似，都是建立浏览器与服务器之间的通信渠道，然后服务器向浏览器推送信息。 总体来说，WebSocket 更强大和灵活。因为它是全双工通道，可以双向通信；SSE 是单向通道，只能服务器向浏览器发送，因为流信息本质上就是下载。如果浏览器向服务器发送信息，就变成了另一次 HTTP 请求。 ","date":"2021-09-08","objectID":"/go-use-sse/:2:0","series":null,"tags":["sse","golang"],"title":"Go 使用 sse","uri":"/go-use-sse/#sse-的特点"},{"categories":["web","开发者手册"],"content":" 优点 SSE 使用 HTTP 协议，现有的服务器软件都支持。WebSocket 是一个独立协议。 SSE 属于轻量级，使用简单；WebSocket 协议相对复杂。 SSE 默认支持断线重连，WebSocket 需要自己实现。 SSE 一般只用来传送文本，二进制数据需要编码后传送，WebSocket 默认支持传送二进制数据。 SSE 支持自定义发送的消息类型。 ","date":"2021-09-08","objectID":"/go-use-sse/:2:1","series":null,"tags":["sse","golang"],"title":"Go 使用 sse","uri":"/go-use-sse/#优点"},{"categories":["web","开发者手册"],"content":" go 实现 package main import ( \"fmt\" \"net/http\" \"time\" ) type SSE struct { } func (sse *SSE) ServeHTTP(rw http.ResponseWriter, req *http.Request) { flusher, ok := rw.(http.Flusher) if !ok { http.Error(rw, \"Streaming unsupported!\", http.StatusInternalServerError) return } rw.Header().Set(\"Content-Type\", \"text/event-stream\") rw.Header().Set(\"Cache-Control\", \"no-cache\") rw.Header().Set(\"Connection\", \"keep-alive\") rw.Header().Set(\"Access-Control-Allow-Origin\", \"*\") for { select { case \u003c-req.Context().Done(): fmt.Println(\"req done...\") return case \u003c-time.After(500 * time.Millisecond): fmt.Fprintf(rw, \"id: %d\\nevent: ping \\ndata: %d\\n\\n\", time.Now().Unix(), time.Now().Unix()) flusher.Flush() } } } func main() { //route := gin.New() //route.GET(\"sse\", gin.WrapH(\u0026SSE{})) //route.Run(\":8080\") http.Handle(\"/sse\", \u0026SSE{}) http.ListenAndServe(\":8080\", nil) } index.html \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003eTitle\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch2\u003eSSE\u003c/h2\u003e \u003c/body\u003e \u003cscript\u003e const source = new EventSource('http://127.0.0.1:8080/sse'); source.onopen = () =\u003e { console.log('链接成功'); } source.onmessage = (res) =\u003e { console.log('获得的数据是:' + res.data); } source.onerror = (err) =\u003e { console.log(err); } \u003c/script\u003e \u003c/html\u003e 直接访问 前端访问 ","date":"2021-09-08","objectID":"/go-use-sse/:3:0","series":null,"tags":["sse","golang"],"title":"Go 使用 sse","uri":"/go-use-sse/#go-实现"},{"categories":["web","开发者手册"],"content":" 参考 Example SSE server in Golang Server-Sent Events 教程 ","date":"2021-09-08","objectID":"/go-use-sse/:4:0","series":null,"tags":["sse","golang"],"title":"Go 使用 sse","uri":"/go-use-sse/#参考"},{"categories":["golang"],"content":"go构造函数,go初始化函数,go函数式选项模式,golang函数式编程","date":"2021-08-23","objectID":"/functional-options-pattern/","series":null,"tags":["golang"],"title":"go 函数式选项模式","uri":"/functional-options-pattern/"},{"categories":["golang"],"content":"Go 语言没有构造函数，一般通过定义 New 函数来充当构造函数。但是，如果结构有较多字段，要初始化这些字段，就有很多种方式，有一种方式被认为是最优雅的，就是函数式选项模式（Functional Options Pattern）。 函数式选项模式用于构造函数和其他公共 API 中的可选参数，你预计这些参数需要扩展，尤其是在这些函数上已经有三个或更多参数的情况下。 ","date":"2021-08-23","objectID":"/functional-options-pattern/:0:0","series":null,"tags":["golang"],"title":"go 函数式选项模式","uri":"/functional-options-pattern/#"},{"categories":["golang"],"content":" 常规方式我们有如下结构体： type Server struct { host string // 必填 port int // 必填 timeout time.Duration // 可选 maxConn int // 可选 } host 和 port 字段是必须的，timeout 和 maxConn 字段是可选的。 之前我的做法是这样处理的，定义一个 New 函数，初始化必填字段，对每个可选字段都定义了一个 SetXXX 函数，如下： package main import \"time\" type Server struct { host string // 必填 port int // 必填 timeout time.Duration // 可选 maxConn int // 可选 } func New(host string, port int) *Server { return \u0026Server{ host: host, port: port, timeout: 0, maxConn: 0, } } func (s *Server) SetTimeout(timeout time.Duration) { s.timeout = timeout } func (s *Server) SetMaxConn(maxConn int) { s.maxConn = maxConn } func main() { } 个人觉得这种方式其实已经很优雅了，一般情况下也是够用的。 ","date":"2021-08-23","objectID":"/functional-options-pattern/:1:0","series":null,"tags":["golang"],"title":"go 函数式选项模式","uri":"/functional-options-pattern/#常规方式"},{"categories":["golang"],"content":" Functional Option Pattern package main import ( \"log\" \"time\" ) type Server struct { host string // 必填 port int // 必填 timeout time.Duration // 可选 maxConn int // 可选 } type Option func(*Server) func New(options ...Option) *Server { svr := \u0026Server{} for _, f := range options { f(svr) } return svr } func WithHost(host string) Option { return func(s *Server) { s.host = host } } func WithPort(port int) Option { return func(s *Server) { s.port = port } } func WithTimeout(timeout time.Duration) Option { return func(s *Server) { s.timeout = timeout } } func WithMaxConn(maxConn int) Option { return func(s *Server) { s.maxConn = maxConn } } func (s *Server) Run() error { // ... return nil } func main() { svr := New( WithHost(\"localhost\"), WithPort(8080), WithTimeout(time.Minute), WithMaxConn(120), ) if err := svr.Run(); err != nil { log.Fatal(err) } } 在这个模式中，我们定义一个 Option 函数类型，它接收一个参数：*Server。然后，Server 的构造函数接收一个 Option 类型的不定参数。 func New(options ...Option) *Server { svr := \u0026Server{} for _, f := range options { f(svr) } return svr } 选项的定义需要定义一系列相关返回 Option 的函数，如： func WithPort(port int) Option { return func(s *Server) { s.port = port } } 如果增加选项，只需要增加对应的 WithXXX 函数即可。 ","date":"2021-08-23","objectID":"/functional-options-pattern/:2:0","series":null,"tags":["golang"],"title":"go 函数式选项模式","uri":"/functional-options-pattern/#functional-option-pattern"},{"categories":["golang"],"content":" 参考 Golang Functional Options Pattern ","date":"2021-08-23","objectID":"/functional-options-pattern/:3:0","series":null,"tags":["golang"],"title":"go 函数式选项模式","uri":"/functional-options-pattern/#参考"},{"categories":["开发者手册"],"content":"xiaobinqt,centos 7.9 安装 Redis 6.0.16,centos 安装 redis","date":"2021-07-13","objectID":"/centos-7.9-install-redis-6.0.16/","series":null,"tags":["redis","linux"],"title":"Centos 7.9 安装 Redis 6.0.16","uri":"/centos-7.9-install-redis-6.0.16/"},{"categories":["开发者手册"],"content":" 服务器版本 linux version ","date":"2021-07-13","objectID":"/centos-7.9-install-redis-6.0.16/:1:0","series":null,"tags":["redis","linux"],"title":"Centos 7.9 安装 Redis 6.0.16","uri":"/centos-7.9-install-redis-6.0.16/#服务器版本"},{"categories":["开发者手册"],"content":" 下载可以直接去官网下载需要的版本即可，这里已 6.0.16 版本为准。 redis download ","date":"2021-07-13","objectID":"/centos-7.9-install-redis-6.0.16/:2:0","series":null,"tags":["redis","linux"],"title":"Centos 7.9 安装 Redis 6.0.16","uri":"/centos-7.9-install-redis-6.0.16/#下载"},{"categories":["开发者手册"],"content":" 安装我把下载的 tar 包放在了 /root 目录下，这里可以自行修改。 tar -xzf redis-6.0.16.tar.gz cd redis-6.0.16 make \u0026\u0026 make install 安装成功启动服务： install success redis start ","date":"2021-07-13","objectID":"/centos-7.9-install-redis-6.0.16/:3:0","series":null,"tags":["redis","linux"],"title":"Centos 7.9 安装 Redis 6.0.16","uri":"/centos-7.9-install-redis-6.0.16/#安装"},{"categories":["开发者手册"],"content":" 常见问题","date":"2021-07-13","objectID":"/centos-7.9-install-redis-6.0.16/:4:0","series":null,"tags":["redis","linux"],"title":"Centos 7.9 安装 Redis 6.0.16","uri":"/centos-7.9-install-redis-6.0.16/#常见问题"},{"categories":["开发者手册"],"content":" cc: command not found需要安装 gcc👇 yum -y install gcc gcc-c++ libstdc++-devel ","date":"2021-07-13","objectID":"/centos-7.9-install-redis-6.0.16/:4:1","series":null,"tags":["redis","linux"],"title":"Centos 7.9 安装 Redis 6.0.16","uri":"/centos-7.9-install-redis-6.0.16/#cc-command-not-found"},{"categories":["开发者手册"],"content":" struct redisServer server_xxx需要升级 gcc 到 9 版本👇 yum -y install centos-release-scl yum -y install devtoolset-9-gcc devtoolset-9-gcc-c++ devtoolset-9-binutils scl enable devtoolset-9 bash 设置永久升级 echo \"source /opt/rh/devtoolset-9/enable\" \u003e\u003e/etc/profile ","date":"2021-07-13","objectID":"/centos-7.9-install-redis-6.0.16/:4:2","series":null,"tags":["redis","linux"],"title":"Centos 7.9 安装 Redis 6.0.16","uri":"/centos-7.9-install-redis-6.0.16/#struct-redisserver-server_xxx"},{"categories":["golang"],"content":"go make 和 new 的区别,golang make,golang new,defference with golang make and new","date":"2021-06-21","objectID":"/new-make-difference/","series":null,"tags":["golang"],"title":"golang make 和 new 的区别","uri":"/new-make-difference/"},{"categories":["golang"],"content":" make 的作用是初始化内置的数据结构，也就是 slice、map和 channel。 new 的作用是根据传入的类型分配一片内存空间并返回指向这片内存空间的指针。 ","date":"2021-06-21","objectID":"/new-make-difference/:0:0","series":null,"tags":["golang"],"title":"golang make 和 new 的区别","uri":"/new-make-difference/#"},{"categories":["golang"],"content":" make内置函数 make 仅支持 slice、map、channel 三种数据类型的内存创建，其返回值是所创建类型的本身，而不是新的指针引用。 func make(t Type, size ...IntegerType) Type func main() { v1 := make([]int, 1, 5) v2 := make(map[int]bool, 5) v3 := make(chan int, 1) fmt.Println(v1, v2, v3) } 在☝️代码中，我们分别对三种类型调用了 make 函数进行了初始化。会发现有的入参是有多个长度指定，有的没有。 这里的区别主要 是长度（len）和容量（cap）的指定，有的类型是没有容量这一说法。 输出结果： [0] map[] 0xc000044070 调用 make函数去初始化切片（slice）的类型时，会带有零值。 ","date":"2021-06-21","objectID":"/new-make-difference/:1:0","series":null,"tags":["golang"],"title":"golang make 和 new 的区别","uri":"/new-make-difference/#make"},{"categories":["golang"],"content":" new内置函数 new 可以对任意类型进行内存创建和初始化。其返回值是所创建类型的指针引用。 func new(Type) *Type new(T) 和 \u0026T{} 效果是一样的。 ","date":"2021-06-21","objectID":"/new-make-difference/:2:0","series":null,"tags":["golang"],"title":"golang make 和 new 的区别","uri":"/new-make-difference/#new"},{"categories":["golang"],"content":" 区别make 函数在初始化时，会初始化 slice、chan、map 类型的内部数据结构，new 函数并不会。 例如，在 map 类型中，合理的长度（len）和容量（cap）可以提高效率和减少开销。 make 函数： 能够分配并初始化类型所需的内存空间和结构，返回引用类型的本身。 具有使用范围的局限性，仅支持 channel、map、slice 三种类型。 具有独特的优势，make 函数会对三种类型的内部数据结构（长度、容量等）赋值。 new 函数： 能够分配类型所需的内存空间，返回指针引用（指向内存的指针），同时把分配的内存置为零，也就是类型的零值。 可被替代，其实不常用，我们通常都是采用短语句声明以及结构体的字面量达到我们的目的，比如： i := 0 u := user{} ","date":"2021-06-21","objectID":"/new-make-difference/:3:0","series":null,"tags":["golang"],"title":"golang make 和 new 的区别","uri":"/new-make-difference/#区别"},{"categories":["golang"],"content":" 零值 array、struct 每个元素或字段都是对应该类型的零值 slice、map 对于零值 nil ","date":"2021-06-21","objectID":"/new-make-difference/:4:0","series":null,"tags":["golang"],"title":"golang make 和 new 的区别","uri":"/new-make-difference/#零值"},{"categories":["golang"],"content":" 参考 make、new操作 Go make 和 new的区别 Go - var \u0026 make \u0026 new 在复杂类型上的使用区别 面试官：Golang 的 new 与make 区别是什么？ ","date":"2021-06-21","objectID":"/new-make-difference/:5:0","series":null,"tags":["golang"],"title":"golang make 和 new 的区别","uri":"/new-make-difference/#参考"},{"categories":["golang"],"content":"xiaobinqt,go,go build,go build 添加版本等信息,golang 构建添加额外的参数","date":"2021-05-24","objectID":"/go-build-add-ldflags/","series":null,"tags":["golang"],"title":"Go build 添加版本等信息","uri":"/go-build-add-ldflags/"},{"categories":["golang"],"content":" Go 在编译时可以添加一些额外的参数，这些参数可以用来添加版本信息。 比如有以下的 t.go 文件源码👇 package main import ( \"flag\" \"fmt\" \"runtime\" ) var ( version string buildTime string commitID string ) func init() { flag.StringVar(\u0026version, \"version\", \"\", \"版本信息\") } func main() { flag.Parse() if version == \"version\" || version == \"v\" { fmt.Printf(\"Git commit: %s\\nGo version: %s\\nBuilt: %s\\nOS/Arch: %s/%s\\n \", commitID, runtime.Version(), buildTime, runtime.GOOS, runtime.GOARCH) return } fmt.Println(\"hello world\") } 用以下命令编译👇 注意：-X 后面要写完整的包路径，示例中是 main 包。 go build -ldflags \"-X 'main.version=1.0.0' -X 'main.buildTime=$(date +\"%Y-%m-%d %H:%M:%S\")' -X 'main.commitID=1234567890'\" -o t t.go 示例结果 可以看到，编译后的文件中包含了版本信息，比如版本号、编译时间、提交ID等。 ","date":"2021-05-24","objectID":"/go-build-add-ldflags/:0:0","series":null,"tags":["golang"],"title":"Go build 添加版本等信息","uri":"/go-build-add-ldflags/#"},{"categories":["golang"],"content":"xiaobinqt,go web 框架 martini,martini,go-martini/martini","date":"2021-05-20","objectID":"/go-web-martini-glance/","series":null,"tags":["golang","martini"],"title":"Go Web 框架 martini 笔记","uri":"/go-web-martini-glance/"},{"categories":["golang"],"content":" 之前在CSDN写过一个关于 martini 的笔记golang martini 包的简单使用 ，最近读来感觉不是很清楚，而且也有一些错误，花了点时间重新整理了下那篇笔记。 ","date":"2021-05-20","objectID":"/go-web-martini-glance/:0:0","series":null,"tags":["golang","martini"],"title":"Go Web 框架 martini 笔记","uri":"/go-web-martini-glance/#"},{"categories":["golang"],"content":" 中间件的使用根据文档中间件的使用有 2 种方法，我总结为通过 Handlers 函数方法和非 Handlers 函数方法，其实主要区别就是 Handlers 函数方法优先级更高，它将会替换掉之前的任何设置过的中间件，但是group组中的中间件还是会执行。。 ","date":"2021-05-20","objectID":"/go-web-martini-glance/:1:0","series":null,"tags":["golang","martini"],"title":"Go Web 框架 martini 笔记","uri":"/go-web-martini-glance/#中间件的使用"},{"categories":["golang"],"content":" 非 Handlers 方式的中间件 package main import ( \"github.com/go-martini/martini\" \"fmt\" ) func main() { m := martini.Classic() m.Use(func(c martini.Context) { fmt.Println(\"第 1 个中间件 before a request\") }) m.Use(func() { fmt.Println(\"第 2 个中间件 111111111111\") }) m.Use(func(c martini.Context) { fmt.Println(\"第 3 个中间件 group middleware\") }) m.Group(\"/api\", func(r martini.Router) { r.Get(\"/test\", func() string { return \"...... return success .....\" }) }, func() { fmt.Println(\"第 4 个中间件 group middleware\") }) m.RunOnAddr(\":8081\") } ☝️是一个例子，从请求后的打印结果可以看出是按着代码顺序执行的。 执行结果 ","date":"2021-05-20","objectID":"/go-web-martini-glance/:1:1","series":null,"tags":["golang","martini"],"title":"Go Web 框架 martini 笔记","uri":"/go-web-martini-glance/#非-handlers-方式的中间件"},{"categories":["golang"],"content":" Handlers 方式的中间件可以把上面代码稍微改下，如下： package main import ( \"github.com/go-martini/martini\" \"fmt\" ) func main() { m := martini.Classic() m.Use(func(c martini.Context) { fmt.Println(\"第 1 个中间件 before a request\") }) m.Use(func() { fmt.Println(\"第 2 个中间件 111111111111\") }) m.Use(func(c martini.Context) { fmt.Println(\"第 3 个中间件 group middleware\") }) m.Handlers( func() { fmt.Println(\"其他中间件都失效，只有我执行了..\") }, ) m.Group(\"/api\", func(r martini.Router) { r.Get(\"/test\", func() string { return \"...... return success .....\" }) }, func() { fmt.Println(\"第 4 个中间件 group middleware\") }) m.RunOnAddr(\":8081\") } 执行后打印结果如下👇，说明只执行了 Handlers 函数中的中间件和 group 组中的中间件。 示例结果 ","date":"2021-05-20","objectID":"/go-web-martini-glance/:1:2","series":null,"tags":["golang","martini"],"title":"Go Web 框架 martini 笔记","uri":"/go-web-martini-glance/#handlers-方式的中间件"},{"categories":["golang"],"content":" Next 函数的使用Context.Next()是一个可选的函数用于中间件处理器暂时放弃执行直到其他的处理器都执行完毕. 这样就可以很好的处理在 http 请求完成后需要做的操作。 对于 Next 我觉得可以理解成遇到 Next() 后就入栈，先进后出。 以下我也是通过有没有 Handlers 函数中间件来举例。 ","date":"2021-05-20","objectID":"/go-web-martini-glance/:2:0","series":null,"tags":["golang","martini"],"title":"Go Web 框架 martini 笔记","uri":"/go-web-martini-glance/#next-函数的使用"},{"categories":["golang"],"content":" Next 在非 Handlers 中间件中使用 package main import ( \"github.com/go-martini/martini\" \"fmt\" ) func main() { m := martini.Classic() m.Use(func(c martini.Context) { fmt.Println(\"第 1 个中间件 before a request\") c.Next() fmt.Println(\"第 6 个中间件 after a request\") // 入栈等待处理 }) m.Use(func() { fmt.Println(\"第 2 个中间件 111111111111\") }) m.Use(func(c martini.Context) { fmt.Println(\"第 3 个中间件 group middleware\") c.Next() fmt.Println(\"第 5 个中间件 group middleware\") // 入栈等待处理 }) m.Group(\"/api\", func(r martini.Router) { r.Get(\"/test\", func() string { fmt.Println(\"......last 最后执行........\") return \"...... return success .....\" }) }, func() { fmt.Println(\"分组中的中间件 group middleware\") }) m.RunOnAddr(\":8080\") } 执行后的打印结果如下： 示例结果 请注意第 5 和第 6 个中间件是最后执行的，是在 http请求完成后 执行的。 ","date":"2021-05-20","objectID":"/go-web-martini-glance/:2:1","series":null,"tags":["golang","martini"],"title":"Go Web 框架 martini 笔记","uri":"/go-web-martini-glance/#next-在非-handlers-中间件中使用"},{"categories":["golang"],"content":" Next 在 Handlers 中间件中使用 package main import ( \"github.com/go-martini/martini\" \"fmt\" ) func main() { m := martini.Classic() m.Handlers( func() { fmt.Println(\"第 1 次执行....\") }, func(c martini.Context) { c.Next() fmt.Println(\"http 请求完成后执行....\") }, func() { fmt.Println(\"第 2 次执行....\") }, ) m.Group(\"/api\", func(r martini.Router) { r.Get(\"/test\", func() string { fmt.Println(\"......last........\") return \"...... return success .....\" }) }, func() { fmt.Println(\"组中的中间件 group middleware\") }) m.RunOnAddr(\":8081\") } 执行后的打印结果如下： 示例结果 ","date":"2021-05-20","objectID":"/go-web-martini-glance/:2:2","series":null,"tags":["golang","martini"],"title":"Go Web 框架 martini 笔记","uri":"/go-web-martini-glance/#next-在-handlers-中间件中使用"},{"categories":["golang"],"content":" 依赖注入和控制反转正常情况下，对函数或方法的调用是调用方的主动直接行为，调用方清楚地知道被调的函数名是什么，参数有哪些类型，直接主动地调用；包括对象的初始化也是显式地直接初始化。所谓的“控制反转”就是将这种主动行为变为间接的行为，主调方不是直接调用函数或对象，而是借助框架代码进行间接的调用和初始化，这种行为被称为“控制反转”，控制反转可以解耦调用方和被调方。 一般情况下，使用库的程序是程序主动地调用库的功能，但使用框架的程序常常由框架驱动整个程序，在框架下写的业务代码是被框架驱动的，这种模式就是“控制反转”。 “依赖注入”是实现“控制反转”的一种方法，是通过注入的参数或实例的方式实现控制反转。 控制反转的价值在哪里？一句话就是“解耦”，可以让控制反转的框架代码读取配置，动态地构建对象。 控制反转是解决复杂问题的一种方法，特别是在 web 框架中为路由和中间件的灵活注入提供了很好的方法。 ","date":"2021-05-20","objectID":"/go-web-martini-glance/:3:0","series":null,"tags":["golang","martini"],"title":"Go Web 框架 martini 笔记","uri":"/go-web-martini-glance/#依赖注入和控制反转"},{"categories":["golang"],"content":" 服务服务即是被注入到处理器中的参数，你可以映射一个服务到全局或者请求的级别。 关于服务最强悍的地方之一就是它能够映射服务到接口。例如说，假设你想要覆盖http.ResponseWriter成为一个对象，那么你可以封装它并包含你自己的额外操作。 ","date":"2021-05-20","objectID":"/go-web-martini-glance/:4:0","series":null,"tags":["golang","martini"],"title":"Go Web 框架 martini 笔记","uri":"/go-web-martini-glance/#服务"},{"categories":["golang"],"content":" 全局映射 package main import ( \"fmt\" \"net/http\" \"github.com/go-martini/martini\" ) func myHandle(w http.ResponseWriter, ttt int, s1, s2 string) { ret := fmt.Sprintf(\"Hello, World! %d,%s , %s\", ttt, s1, s2) w.Write([]byte(ret)) } func registerMyRouter(r martini.Router) { r.Group(\"/api\", func(r martini.Router) { r.Get(\"/test\", myHandle) }, func() { fmt.Println(\"组中的中间件 group middleware\") }) } func main() { m := martini.Classic() gv1 := \"1111111\" gv2 := \"2222222\" m.Map(gv1) m.Map(gv2) age := 23 m.Map(age) registerMyRouter(m) m.RunOnAddr(\":8083\") } 这是一个简单的全局映射的服务，gv1，gv2，age 这几个服务(参数) 将可以在所有的处理器中被使用到。我们的 myHandle() 处理器(方法) 就直接用了，且没有报错。执行后打印的结果如下： 示例结果 ⚠️这里有个点需要注意，同一个类型的服务，如果多次 map 映射，后面的值会把前面的覆盖，☝️上面的，gv2的值就把 gv1的值覆盖了。 ","date":"2021-05-20","objectID":"/go-web-martini-glance/:4:1","series":null,"tags":["golang","martini"],"title":"Go Web 框架 martini 笔记","uri":"/go-web-martini-glance/#全局映射"},{"categories":["golang"],"content":" 请求级别的映射 package main import ( \"github.com/go-martini/martini\" \"fmt\" \"net/http\" \"time\" ) type ReqContext struct { ExString string Req *http.Request } func do(reqCtx *ReqContext) string { return fmt.Sprintf(`handle return....%s, ex val= :%s `, reqCtx.Req.URL.Query().Encode(), reqCtx.ExString) } func myHandle(req *http.Request, c martini.Context) { reqC := \u0026ReqContext{ ExString: \"1112323 \" + time.Now().Format(\"2006-01-02 15:04:05\"), Req: req, } c.Map(reqC) } func myHandle2(reqCtx *ReqContext) string { return fmt.Sprintf(`handle return....%s, ex val= :%s `, reqCtx.Req.URL.Query().Encode(), reqCtx.ExString) } func myHandle3(req *http.Request) string { return fmt.Sprintf(`handle return....%s`, req.URL.Query().Encode()) } func registerMyRouter(r martini.Router) { r.Group(\"/api\", func(r martini.Router) { r.Get(\"/test\", myHandle, do) r.Get(\"/test2\", myHandle2) r.Get(\"/test3\", myHandle3) }, func() { fmt.Println(\"组中的中间件 group middleware\") }) } func main() { m := martini.Classic() registerMyRouter(m) m.RunOnAddr(\":8085\") } 执行后打印结果如下： 执行结果 ✔️这是一个简单的请求级别的映射，我们对 /api/test 做了映射，所以在 /api/test 请求中 ReqContext 参数是全局可以用，这也就是 do() 方法中的参数 do(reqCtx *ReqContext) 中的来历。 ✔️在 /api/test3 中我们没有用任何其他的参数，上下文用的是*http.Request，所以可以正确执行请求。 ❌在 /api/test2 中我们用到了 ReqContext 参数，这是 /api/test 请求映射的，只能在 /api/test 中使用，这就是报错是原因。 ","date":"2021-05-20","objectID":"/go-web-martini-glance/:4:2","series":null,"tags":["golang","martini"],"title":"Go Web 框架 martini 笔记","uri":"/go-web-martini-glance/#请求级别的映射"},{"categories":["golang"],"content":" 映射值到接口假设你想要覆盖http.ResponseWriter成为一个对象, 那么你可以封装它并包含你自己的额外操作。 我个人理解的就是一种类型覆盖。 package main import ( \"github.com/go-martini/martini\" \"fmt\" \"net/http\" \"time\" ) type ReqContext struct { Svr interface{} ExString string Req *http.Request } type ServiceGetXIImpl interface { GetXI() } type Tmp struct { } func (s *Tmp) GetXI() { fmt.Println(\"tmp GetXI()\") } type Service struct { Name string Age int } func (s *Service) printXX() { fmt.Println(\"我是 service printXX() 方法\") } func (s *Service) GetXI() { fmt.Println(\"我是 Service 实现了 ServiceGetXIImpl 接口中的 GetXI() 方法\") } func myHandle(reqCtx *ReqContext) string { fmt.Printf(\"最后 myHandle 中的 reqCtx.Svr 类型为 %T \\n \", reqCtx.Svr) return fmt.Sprintf(`handle return....%s, ex val= :%s `, reqCtx.Req.URL.Query().Encode(), reqCtx.ExString) } func registerMyRouter(r martini.Router) { r.Group(\"/api\", func(r martini.Router) { r.Get(\"/test\", myHandle) }, func() { fmt.Println(\"组中的中间件 group middleware\") }) } func ExecMapTo(ctx *ReqContext) *ReqContext { ctx.Svr = \u0026Service{} return ctx } func WrapMapTo(reqCtx *ReqContext, c martini.Context) { fmt.Printf(\"MapTo 前 Svr 类型为: %T \\n\", reqCtx.Svr) rw := ExecMapTo(reqCtx) c.MapTo(rw, (*ServiceGetXIImpl)(nil)) // 覆盖 reqCtx.Svr 的 Tmp 类型为 Service fmt.Printf(\"MapTo 后 Svr 类型为: %T \\n\", reqCtx.Svr) } func main() { m := martini.Classic() m.Use(func(req *http.Request, c martini.Context) { reqC := \u0026ReqContext{ Svr: Tmp{}, ExString: \"1112323 \" + time.Now().Format(\"2006-01-02 15:04:05\"), Req: req, } c.Map(reqC) }) m.Use(func(reqCtx *ReqContext, c martini.Context) { WrapMapTo(reqCtx, c) }) registerMyRouter(m) m.RunOnAddr(\":8086\") } 这个例子主要是想把 ReqContext 中的 Svr 从 Tmp 类型替换为 Service 类型。 执行后的打印结果如下： 执行结果 ","date":"2021-05-20","objectID":"/go-web-martini-glance/:4:3","series":null,"tags":["golang","martini"],"title":"Go Web 框架 martini 笔记","uri":"/go-web-martini-glance/#映射值到接口"},{"categories":["golang"],"content":" 参考 https://github.com/go-martini/martini/blob/master/translations/README_zh_cn.md inject库 ","date":"2021-05-20","objectID":"/go-web-martini-glance/:5:0","series":null,"tags":["golang","martini"],"title":"Go Web 框架 martini 笔记","uri":"/go-web-martini-glance/#参考"},{"categories":["golang"],"content":"xiaobinqt,golang 打印pdf,go 打印 pdf,chromedp 使用","date":"2021-05-11","objectID":"/go-print-pdf/","series":null,"tags":["golang","pdf"],"title":"Go 利用 chromedp 生成 pdf","uri":"/go-print-pdf/"},{"categories":["golang"],"content":" chromedpPackage chromedp is a faster, simpler way to drive browsers supporting the Chrome DevTools Protocol in Go without external dependencies. 可以查看官方的示例。 以下示例用的版本为 github.com/chromedp/chromedp v0.8.4。 ","date":"2021-05-11","objectID":"/go-print-pdf/:1:0","series":null,"tags":["golang","pdf"],"title":"Go 利用 chromedp 生成 pdf","uri":"/go-print-pdf/#chromedp"},{"categories":["golang"],"content":" 示例","date":"2021-05-11","objectID":"/go-print-pdf/:2:0","series":null,"tags":["golang","pdf"],"title":"Go 利用 chromedp 生成 pdf","uri":"/go-print-pdf/#示例"},{"categories":["golang"],"content":" 打印在线页面👇 有时需要打印一个在线页面成 pdf，比如把https://www.baidu.com/这个页面打印成 pdf，如下 package main import ( \"bufio\" \"bytes\" \"context\" \"fmt\" \"net/http\" \"github.com/chromedp/cdproto/page\" \"github.com/chromedp/chromedp\" \"github.com/gin-gonic/gin\" \"github.com/pkg/errors\" ) func dp(c *gin.Context) { var ( err error buf = make([]byte, 0) ) ctx, cancel := chromedp.NewContext(context.Background()) defer cancel() err = chromedp.Run(ctx, chromedp.Tasks{ chromedp.Navigate(\"https://www.baidu.com/\"), chromedp.ActionFunc(func(ctx context.Context) error { buf, _, err = page.PrintToPDF(). WithPrintBackground(true). Do(ctx) return err }), }) if err != nil { err = errors.Wrapf(err, \"chromedp Run failed\") c.JSON(http.StatusInternalServerError, gin.H{ \"msg\": err.Error(), }) return } buffer := \u0026bytes.Buffer{} buffer.WriteString(\"\\xEF\\xBB\\xBF\") // 防止中文乱码 writer := bufio.NewWriter(buffer) _, err = writer.Write(buf) if err != nil { err = errors.Wrapf(err, \"bufio Write err\") c.JSON(http.StatusInternalServerError, gin.H{ \"msg\": err.Error(), }) return } _ = writer.Flush() fileName := fmt.Sprintf(\"1111.pdf\") c.Header(\"Content-Type\", \"text/pdf\") c.Header(\"Content-Disposition\", \"attachment;filename=\"+fileName) _, _ = c.Writer.Write(buffer.Bytes()) return } func main() { route := gin.New() route.GET(\"/dp\", dp) route.Run(\":8080\") } ","date":"2021-05-11","objectID":"/go-print-pdf/:2:1","series":null,"tags":["golang","pdf"],"title":"Go 利用 chromedp 生成 pdf","uri":"/go-print-pdf/#打印在线页面"},{"categories":["golang"],"content":" 打印本地文件有时需要将一个本地的 html 文件渲染后，提供下载链接，下载成一个 pdf 格式的文件。 package main import ( \"bufio\" \"bytes\" \"context\" \"fmt\" \"html/template\" \"net/http\" \"net/http/httptest\" \"os\" \"github.com/chromedp/cdproto/page\" \"github.com/chromedp/chromedp\" \"github.com/gin-gonic/gin\" \"github.com/pkg/errors\" ) type Content struct { ChannelLogo template.URL ProductLogo template.URL Title string Content string UserName string OrgName string ChannelName string Number string Date string BackgroundImage template.URL } func writeHTML(content Content) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"text/html\") var ( wd string err error tmpl *template.Template ) wd, _ = os.Getwd() tmpl, err = template.ParseFiles(wd + \"/tmp.html\") if err != nil { err = errors.Wrapf(err, \"template.ParseFiles err\") _, _ = w.Write([]byte(err.Error())) return } buffer := \u0026bytes.Buffer{} err = tmpl.Execute(buffer, content) if err != nil { err = errors.Wrapf(err, \"tmpl.Execute err\") _, _ = w.Write([]byte(err.Error())) return } _, _ = w.Write(buffer.Bytes()) }) } func dp(c *gin.Context) { var ( err error buf = make([]byte, 0) ) ctx, cancel := chromedp.NewContext(context.Background()) defer cancel() mux := http.NewServeMux() mux.Handle(\"/pre\", writeHTML(Content{ ChannelLogo: \"\", ProductLogo: \"\", Title: \"我是title\", Content: \"我是内容\", UserName: \"1111\", OrgName: \"2222\", ChannelName: \"3333\", Number: \"4444\", Date: \"2006-01-02 15:04:05\", BackgroundImage: \"\", })) ts := httptest.NewServer(mux) defer ts.Close() url := fmt.Sprintf(\"%s/pre\", ts.URL) err = chromedp.Run(ctx, chromedp.Tasks{ chromedp.Navigate(url), chromedp.WaitReady(\"body\"), chromedp.ActionFunc(func(ctx context.Context) error { var err error buf, _, err = page.PrintToPDF(). WithPrintBackground(true). WithPageRanges(\"1\"). Do(ctx) return err }), }) if err != nil { err = errors.Wrapf(err, \"chromedp Run failed\") c.JSON(http.StatusInternalServerError, gin.H{ \"msg\": err.Error(), }) return } buffer := \u0026bytes.Buffer{} buffer.WriteString(\"\\xEF\\xBB\\xBF\") // 防止中文乱码 writer := bufio.NewWriter(buffer) _, err = writer.Write(buf) if err != nil { err = errors.Wrapf(err, \"bufio Write err\") c.JSON(http.StatusInternalServerError, gin.H{ \"msg\": err.Error(), }) return } _ = writer.Flush() fileName := fmt.Sprintf(\"1111.pdf\") c.Header(\"Content-Type\", \"text/pdf\") c.Header(\"Content-Disposition\", \"attachment;filename=\"+fileName) _, _ = c.Writer.Write(buffer.Bytes()) return } func main() { route := gin.New() route.GET(\"/dp\", dp) route.Run(\":8080\") } 本地文件 tmp.html： \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003cmeta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e \u003ctitle\u003eDocument\u003c/title\u003e \u003cstyle\u003e .root { width: 210mm; height: 276mm; padding: 0; margin: 0 auto; background-color: white; /*background-image: url();*/ /* background-size: 210mm 276mm; */ position: relative; overflow: hidden; } .background-box { position: absolute; top: 0; left: 0; bottom: 0; right: 0; z-index: 0; } .background-box img { width: 210mm; } .container { position: absolute; top: 0; left: 0; bottom: 0; right: 0; z-index: 1; /*border: 1px solid #000;*/ } .title { font-size: 38px; text-align: center; margin-top: 100px; } .content { margin-top: 50px; font-size: 22px; line-height: 2em; padding: 0 108px; } .content p { text-indent: 2em; white-space: normal; word-break: break-all; } p strong { font-weight: normal; text-indent: 0; padding: 2px 1em; border-bottom: 1px solid #000; } .logo-box { padding: 130px 100px 0 100px; display: flex; justify-content: space-between; } .logo-box img { max-width: 260px; } .signature { padding: 36px 100px 0 100px; font-size: 22px; display: flex; justify-content: space-between; text-align: center; } .signature p { line-height: 2em; width: 48%; } .footer { font-size: 14px; position: absolute; bottom: 36px; left: 100px; right: 100px; top: auto; line-height: 1.8em; } @media screen { .root { width: 210mm; height: 276mm; displa","date":"2021-05-11","objectID":"/go-print-pdf/:2:2","series":null,"tags":["golang","pdf"],"title":"Go 利用 chromedp 生成 pdf","uri":"/go-print-pdf/#打印本地文件"},{"categories":["开发者手册"],"content":"xiaobinqt,nginx","date":"2021-05-06","objectID":"/nginx-glance/","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/"},{"categories":["开发者手册"],"content":" Nginx 概述Nginx 是开源、高性能、高可靠的 Web 和反向代理服务器，而且支持热部署，几乎可以做到 7 * 24 小时不间断运行，即使运行几个月也不需要重新启动，还能在不间断服务的情况下对软件版本进行热更新。性能是 Nginx 最重要的考量，其占用内存少、并发能力强、能支持高达 5w 个并发连接数，最重要的是， Nginx 是免费的并可以商业化，配置使用也比较简单。 ","date":"2021-05-06","objectID":"/nginx-glance/:1:0","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#nginx-概述"},{"categories":["开发者手册"],"content":" Nginx 特点 高并发、高性能； 模块化架构使得它的扩展性非常好； 异步非阻塞的事件驱动模型这点和 Node.js 相似； 相对于其它服务器来说它可以连续几个月甚至更长而不需要重启服务器使得它具有高可靠性； 热部署、平滑升级； 完全开源，生态繁荣； ","date":"2021-05-06","objectID":"/nginx-glance/:2:0","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#nginx-特点"},{"categories":["开发者手册"],"content":" Nginx 作用Nginx 的最重要的几个使用场景： 静态资源服务，通过本地文件系统提供服务； 反向代理服务，延伸出包括缓存、负载均衡等； API 服务， OpenResty ； 对于前端来说 Node.js 并不陌生， Nginx 和 Node.js 的很多理念类似， HTTP 服务器、事件驱动、异步非阻塞等，且 Nginx 的大部分功能使用 Node.js 也可以实现，但 Nginx 和 Node.js 并不冲突，都有自己擅长的领域。 Nginx 擅长于底层服务器端资源的处理（静态资源处理转发、反向代理，负载均衡等）， Node.js 更擅长上层具体业务逻辑的处理，两者可以完美组合。 用一张图表示： ","date":"2021-05-06","objectID":"/nginx-glance/:3:0","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#nginx-作用"},{"categories":["开发者手册"],"content":" Nginx 安装本文演示的是 Linux centOS 7.x 的操作系统上安装 Nginx ，至于在其它操作系统上进行安装可以网上自行搜索，都非常简单的。 使用 yum 安装 Nginx ： yum install nginx -y 安装完成后，通过 rpm -ql nginx 命令查看 Nginx 的安装信息： # Nginx配置文件 /etc/nginx/nginx.conf # nginx 主配置文件 /etc/nginx/nginx.conf.default # 可执行程序文件 /usr/bin/nginx-upgrade /usr/sbin/nginx # nginx库文件 /usr/lib/systemd/system/nginx.service # 用于配置系统守护进程 /usr/lib64/nginx/modules # Nginx模块目录 # 帮助文档 /usr/share/doc/nginx-1.16.1 /usr/share/doc/nginx-1.16.1/CHANGES /usr/share/doc/nginx-1.16.1/README /usr/share/doc/nginx-1.16.1/README.dynamic /usr/share/doc/nginx-1.16.1/UPGRADE-NOTES-1.6-to-1.10 # 静态资源目录 /usr/share/nginx/html/404.html /usr/share/nginx/html/50x.html /usr/share/nginx/html/index.html # 存放Nginx日志文件 /var/log/nginx 主要关注的文件夹有两个： /etc/nginx/conf.d/ 是子配置项存放处， /etc/nginx/nginx.conf 主配置文件会默认把这个文件夹中所有子配置项都引入； /usr/share/nginx/html/ 静态文件都放在这个文件夹，也可以根据你自己的习惯放在其他地方； ","date":"2021-05-06","objectID":"/nginx-glance/:4:0","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#nginx-安装"},{"categories":["开发者手册"],"content":" Nginx 常用命令systemctl 系统命令： # 开机配置 systemctl enable nginx # 开机自动启动 systemctl disable nginx # 关闭开机自动启动 # 启动Nginx systemctl start nginx # 启动Nginx成功后，可以直接访问主机IP，此时会展示Nginx默认页面 # 停止Nginx systemctl stop nginx # 重启Nginx systemctl restart nginx # 重新加载Nginx systemctl reload nginx # 查看 Nginx 运行状态 systemctl status nginx # 查看Nginx进程 ps -ef | grep nginx # 杀死Nginx进程 kill -9 pid # 根据上面查看到的Nginx进程号，杀死Nginx进程，-9 表示强制结束进程 Nginx 应用程序命令： nginx -s reload # 向主进程发送信号，重新加载配置文件，热重启 nginx -s reopen # 重启 Nginx nginx -s stop # 快速关闭 nginx -s quit # 等待工作进程处理完成后关闭 nginx -T # 查看当前 Nginx 最终的配置 nginx -t # 检查配置是否有问题 ","date":"2021-05-06","objectID":"/nginx-glance/:5:0","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#nginx-常用命令"},{"categories":["开发者手册"],"content":" Nginx 核心配置","date":"2021-05-06","objectID":"/nginx-glance/:6:0","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#nginx-核心配置"},{"categories":["开发者手册"],"content":" 配置文件结构Nginx 的典型配置示例： # main段配置信息 user nginx; # 运行用户，默认即是nginx，可以不进行设置 worker_processes auto; # Nginx 进程数，一般设置为和 CPU 核数一样 error_log /var/log/nginx/error.log warn; # Nginx 的错误日志存放目录 pid /var/run/nginx.pid; # Nginx 服务启动时的 pid 存放位置 # events段配置信息 events { use epoll; # 使用epoll的I/O模型(如果你不知道Nginx该使用哪种轮询方法，会自动选择一个最适合你操作系统的) worker_connections 1024; # 每个进程允许最大并发数 } # http段配置信息 # 配置使用最频繁的部分，代理、缓存、日志定义等绝大多数功能和第三方模块的配置都在这里设置 http { # 设置日志模式 log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; # Nginx访问日志存放位置 sendfile on; # 开启高效传输模式 tcp_nopush on; # 减少网络报文段的数量 tcp_nodelay on; keepalive_timeout 65; # 保持连接的时间，也叫超时时间，单位秒 types_hash_max_size 2048; include /etc/nginx/mime.types; # 文件扩展名与类型映射表 default_type application/octet-stream; # 默认文件类型 include /etc/nginx/conf.d/*.conf; # 加载子配置项 # server段配置信息 server { listen 80; # 配置监听的端口 server_name localhost; # 配置的域名 # location段配置信息 location / { root /usr/share/nginx/html; # 网站根目录 index index.html index.htm; # 默认首页文件 deny 172.168.22.11; # 禁止访问的ip地址，可以为all allow 172.168.33.44；# 允许访问的ip地址，可以为all } error_page 500 502 503 504 /50x.html; # 默认50x对应的访问页面 error_page 400 404 error.html; # 同上 } } main 全局配置，对全局生效； events 配置影响 Nginx 服务器与用户的网络连接； http 配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置； server 配置虚拟主机的相关参数，一个 http 块中可以有多个 server 块； location 用于配置匹配的 uri ； upstream 配置后端服务器具体地址，负载均衡配置不可或缺的部分； 用一张图清晰的展示它的层级结构： ","date":"2021-05-06","objectID":"/nginx-glance/:6:1","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#配置文件结构"},{"categories":["开发者手册"],"content":" 配置文件 main 段核心参数 user指定运行 Nginx 的 woker 子进程的属主和属组，其中组可以不指定。 user USERNAME [GROUP] user nginx lion; # 用户是nginx;组是lion pid指定运行 Nginx master 主进程的 pid 文件存放路径。 pid /opt/nginx/logs/nginx.pid # master主进程的的pid存放在nginx.pid的文件 worker_rlimit_nofile_number指定 worker 子进程可以打开的最大文件句柄数。 worker_rlimit_nofile 20480; # 可以理解成每个worker子进程的最大连接数量。 worker_rlimit_core指定 worker 子进程异常终止后的 core 文件，用于记录分析问题。 worker_rlimit_core 50M; # 存放大小限制 working_directory /opt/nginx/tmp; # 存放目录 worker_processes_number指定 Nginx 启动的 worker 子进程数量。 worker_processes 4; # 指定具体子进程数量 worker_processes auto; # 与当前cpu物理核心数一致 worker_cpu_affinity将每个 worker 子进程与我们的 cpu 物理核心绑定。 worker_cpu_affinity 0001 0010 0100 1000; # 4个物理核心，4个worker子进程 将每个 worker 子进程与特定 CPU 物理核心绑定，优势在于，避免同一个 worker 子进程在不同的 CPU 核心上切换，缓存失效，降低性能。但其并不能真正的避免进程切换。 worker_priority指定 worker 子进程的 nice 值，以调整运行 Nginx 的优先级，通常设定为负值，以优先调用 Nginx 。 worker_priority -10; # 120-10=110，110就是最终的优先级 Linux 默认进程的优先级值是120，值越小越优先； nice 定范围为 -20 到 +19 。 [备注] 应用的默认优先级值是120加上 nice 值等于它最终的值，这个值越小，优先级越高。 worker_shutdown_timeout指定 worker 子进程优雅退出时的超时时间。 worker_shutdown_timeout 5s; timer_resolutionworker 子进程内部使用的计时器精度，调整时间间隔越大，系统调用越少，有利于性能提升；反之，系统调用越多，性能下降。 timer_resolution 100ms; 在 Linux 系统中，用户需要获取计时器时需要向操作系统内核发送请求，有请求就必然会有开销，因此这个间隔越大开销就越小。 daemon指定 Nginx 的运行方式，前台还是后台，前台用于调试，后台用于生产。 daemon off; # 默认是on，后台运行模式 ","date":"2021-05-06","objectID":"/nginx-glance/:6:2","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#配置文件-main-段核心参数"},{"categories":["开发者手册"],"content":" 配置文件 main 段核心参数 user指定运行 Nginx 的 woker 子进程的属主和属组，其中组可以不指定。 user USERNAME [GROUP] user nginx lion; # 用户是nginx;组是lion pid指定运行 Nginx master 主进程的 pid 文件存放路径。 pid /opt/nginx/logs/nginx.pid # master主进程的的pid存放在nginx.pid的文件 worker_rlimit_nofile_number指定 worker 子进程可以打开的最大文件句柄数。 worker_rlimit_nofile 20480; # 可以理解成每个worker子进程的最大连接数量。 worker_rlimit_core指定 worker 子进程异常终止后的 core 文件，用于记录分析问题。 worker_rlimit_core 50M; # 存放大小限制 working_directory /opt/nginx/tmp; # 存放目录 worker_processes_number指定 Nginx 启动的 worker 子进程数量。 worker_processes 4; # 指定具体子进程数量 worker_processes auto; # 与当前cpu物理核心数一致 worker_cpu_affinity将每个 worker 子进程与我们的 cpu 物理核心绑定。 worker_cpu_affinity 0001 0010 0100 1000; # 4个物理核心，4个worker子进程 将每个 worker 子进程与特定 CPU 物理核心绑定，优势在于，避免同一个 worker 子进程在不同的 CPU 核心上切换，缓存失效，降低性能。但其并不能真正的避免进程切换。 worker_priority指定 worker 子进程的 nice 值，以调整运行 Nginx 的优先级，通常设定为负值，以优先调用 Nginx 。 worker_priority -10; # 120-10=110，110就是最终的优先级 Linux 默认进程的优先级值是120，值越小越优先； nice 定范围为 -20 到 +19 。 [备注] 应用的默认优先级值是120加上 nice 值等于它最终的值，这个值越小，优先级越高。 worker_shutdown_timeout指定 worker 子进程优雅退出时的超时时间。 worker_shutdown_timeout 5s; timer_resolutionworker 子进程内部使用的计时器精度，调整时间间隔越大，系统调用越少，有利于性能提升；反之，系统调用越多，性能下降。 timer_resolution 100ms; 在 Linux 系统中，用户需要获取计时器时需要向操作系统内核发送请求，有请求就必然会有开销，因此这个间隔越大开销就越小。 daemon指定 Nginx 的运行方式，前台还是后台，前台用于调试，后台用于生产。 daemon off; # 默认是on，后台运行模式 ","date":"2021-05-06","objectID":"/nginx-glance/:6:2","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#user"},{"categories":["开发者手册"],"content":" 配置文件 main 段核心参数 user指定运行 Nginx 的 woker 子进程的属主和属组，其中组可以不指定。 user USERNAME [GROUP] user nginx lion; # 用户是nginx;组是lion pid指定运行 Nginx master 主进程的 pid 文件存放路径。 pid /opt/nginx/logs/nginx.pid # master主进程的的pid存放在nginx.pid的文件 worker_rlimit_nofile_number指定 worker 子进程可以打开的最大文件句柄数。 worker_rlimit_nofile 20480; # 可以理解成每个worker子进程的最大连接数量。 worker_rlimit_core指定 worker 子进程异常终止后的 core 文件，用于记录分析问题。 worker_rlimit_core 50M; # 存放大小限制 working_directory /opt/nginx/tmp; # 存放目录 worker_processes_number指定 Nginx 启动的 worker 子进程数量。 worker_processes 4; # 指定具体子进程数量 worker_processes auto; # 与当前cpu物理核心数一致 worker_cpu_affinity将每个 worker 子进程与我们的 cpu 物理核心绑定。 worker_cpu_affinity 0001 0010 0100 1000; # 4个物理核心，4个worker子进程 将每个 worker 子进程与特定 CPU 物理核心绑定，优势在于，避免同一个 worker 子进程在不同的 CPU 核心上切换，缓存失效，降低性能。但其并不能真正的避免进程切换。 worker_priority指定 worker 子进程的 nice 值，以调整运行 Nginx 的优先级，通常设定为负值，以优先调用 Nginx 。 worker_priority -10; # 120-10=110，110就是最终的优先级 Linux 默认进程的优先级值是120，值越小越优先； nice 定范围为 -20 到 +19 。 [备注] 应用的默认优先级值是120加上 nice 值等于它最终的值，这个值越小，优先级越高。 worker_shutdown_timeout指定 worker 子进程优雅退出时的超时时间。 worker_shutdown_timeout 5s; timer_resolutionworker 子进程内部使用的计时器精度，调整时间间隔越大，系统调用越少，有利于性能提升；反之，系统调用越多，性能下降。 timer_resolution 100ms; 在 Linux 系统中，用户需要获取计时器时需要向操作系统内核发送请求，有请求就必然会有开销，因此这个间隔越大开销就越小。 daemon指定 Nginx 的运行方式，前台还是后台，前台用于调试，后台用于生产。 daemon off; # 默认是on，后台运行模式 ","date":"2021-05-06","objectID":"/nginx-glance/:6:2","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#pid"},{"categories":["开发者手册"],"content":" 配置文件 main 段核心参数 user指定运行 Nginx 的 woker 子进程的属主和属组，其中组可以不指定。 user USERNAME [GROUP] user nginx lion; # 用户是nginx;组是lion pid指定运行 Nginx master 主进程的 pid 文件存放路径。 pid /opt/nginx/logs/nginx.pid # master主进程的的pid存放在nginx.pid的文件 worker_rlimit_nofile_number指定 worker 子进程可以打开的最大文件句柄数。 worker_rlimit_nofile 20480; # 可以理解成每个worker子进程的最大连接数量。 worker_rlimit_core指定 worker 子进程异常终止后的 core 文件，用于记录分析问题。 worker_rlimit_core 50M; # 存放大小限制 working_directory /opt/nginx/tmp; # 存放目录 worker_processes_number指定 Nginx 启动的 worker 子进程数量。 worker_processes 4; # 指定具体子进程数量 worker_processes auto; # 与当前cpu物理核心数一致 worker_cpu_affinity将每个 worker 子进程与我们的 cpu 物理核心绑定。 worker_cpu_affinity 0001 0010 0100 1000; # 4个物理核心，4个worker子进程 将每个 worker 子进程与特定 CPU 物理核心绑定，优势在于，避免同一个 worker 子进程在不同的 CPU 核心上切换，缓存失效，降低性能。但其并不能真正的避免进程切换。 worker_priority指定 worker 子进程的 nice 值，以调整运行 Nginx 的优先级，通常设定为负值，以优先调用 Nginx 。 worker_priority -10; # 120-10=110，110就是最终的优先级 Linux 默认进程的优先级值是120，值越小越优先； nice 定范围为 -20 到 +19 。 [备注] 应用的默认优先级值是120加上 nice 值等于它最终的值，这个值越小，优先级越高。 worker_shutdown_timeout指定 worker 子进程优雅退出时的超时时间。 worker_shutdown_timeout 5s; timer_resolutionworker 子进程内部使用的计时器精度，调整时间间隔越大，系统调用越少，有利于性能提升；反之，系统调用越多，性能下降。 timer_resolution 100ms; 在 Linux 系统中，用户需要获取计时器时需要向操作系统内核发送请求，有请求就必然会有开销，因此这个间隔越大开销就越小。 daemon指定 Nginx 的运行方式，前台还是后台，前台用于调试，后台用于生产。 daemon off; # 默认是on，后台运行模式 ","date":"2021-05-06","objectID":"/nginx-glance/:6:2","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#worker_rlimit_nofile_number"},{"categories":["开发者手册"],"content":" 配置文件 main 段核心参数 user指定运行 Nginx 的 woker 子进程的属主和属组，其中组可以不指定。 user USERNAME [GROUP] user nginx lion; # 用户是nginx;组是lion pid指定运行 Nginx master 主进程的 pid 文件存放路径。 pid /opt/nginx/logs/nginx.pid # master主进程的的pid存放在nginx.pid的文件 worker_rlimit_nofile_number指定 worker 子进程可以打开的最大文件句柄数。 worker_rlimit_nofile 20480; # 可以理解成每个worker子进程的最大连接数量。 worker_rlimit_core指定 worker 子进程异常终止后的 core 文件，用于记录分析问题。 worker_rlimit_core 50M; # 存放大小限制 working_directory /opt/nginx/tmp; # 存放目录 worker_processes_number指定 Nginx 启动的 worker 子进程数量。 worker_processes 4; # 指定具体子进程数量 worker_processes auto; # 与当前cpu物理核心数一致 worker_cpu_affinity将每个 worker 子进程与我们的 cpu 物理核心绑定。 worker_cpu_affinity 0001 0010 0100 1000; # 4个物理核心，4个worker子进程 将每个 worker 子进程与特定 CPU 物理核心绑定，优势在于，避免同一个 worker 子进程在不同的 CPU 核心上切换，缓存失效，降低性能。但其并不能真正的避免进程切换。 worker_priority指定 worker 子进程的 nice 值，以调整运行 Nginx 的优先级，通常设定为负值，以优先调用 Nginx 。 worker_priority -10; # 120-10=110，110就是最终的优先级 Linux 默认进程的优先级值是120，值越小越优先； nice 定范围为 -20 到 +19 。 [备注] 应用的默认优先级值是120加上 nice 值等于它最终的值，这个值越小，优先级越高。 worker_shutdown_timeout指定 worker 子进程优雅退出时的超时时间。 worker_shutdown_timeout 5s; timer_resolutionworker 子进程内部使用的计时器精度，调整时间间隔越大，系统调用越少，有利于性能提升；反之，系统调用越多，性能下降。 timer_resolution 100ms; 在 Linux 系统中，用户需要获取计时器时需要向操作系统内核发送请求，有请求就必然会有开销，因此这个间隔越大开销就越小。 daemon指定 Nginx 的运行方式，前台还是后台，前台用于调试，后台用于生产。 daemon off; # 默认是on，后台运行模式 ","date":"2021-05-06","objectID":"/nginx-glance/:6:2","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#worker_rlimit_core"},{"categories":["开发者手册"],"content":" 配置文件 main 段核心参数 user指定运行 Nginx 的 woker 子进程的属主和属组，其中组可以不指定。 user USERNAME [GROUP] user nginx lion; # 用户是nginx;组是lion pid指定运行 Nginx master 主进程的 pid 文件存放路径。 pid /opt/nginx/logs/nginx.pid # master主进程的的pid存放在nginx.pid的文件 worker_rlimit_nofile_number指定 worker 子进程可以打开的最大文件句柄数。 worker_rlimit_nofile 20480; # 可以理解成每个worker子进程的最大连接数量。 worker_rlimit_core指定 worker 子进程异常终止后的 core 文件，用于记录分析问题。 worker_rlimit_core 50M; # 存放大小限制 working_directory /opt/nginx/tmp; # 存放目录 worker_processes_number指定 Nginx 启动的 worker 子进程数量。 worker_processes 4; # 指定具体子进程数量 worker_processes auto; # 与当前cpu物理核心数一致 worker_cpu_affinity将每个 worker 子进程与我们的 cpu 物理核心绑定。 worker_cpu_affinity 0001 0010 0100 1000; # 4个物理核心，4个worker子进程 将每个 worker 子进程与特定 CPU 物理核心绑定，优势在于，避免同一个 worker 子进程在不同的 CPU 核心上切换，缓存失效，降低性能。但其并不能真正的避免进程切换。 worker_priority指定 worker 子进程的 nice 值，以调整运行 Nginx 的优先级，通常设定为负值，以优先调用 Nginx 。 worker_priority -10; # 120-10=110，110就是最终的优先级 Linux 默认进程的优先级值是120，值越小越优先； nice 定范围为 -20 到 +19 。 [备注] 应用的默认优先级值是120加上 nice 值等于它最终的值，这个值越小，优先级越高。 worker_shutdown_timeout指定 worker 子进程优雅退出时的超时时间。 worker_shutdown_timeout 5s; timer_resolutionworker 子进程内部使用的计时器精度，调整时间间隔越大，系统调用越少，有利于性能提升；反之，系统调用越多，性能下降。 timer_resolution 100ms; 在 Linux 系统中，用户需要获取计时器时需要向操作系统内核发送请求，有请求就必然会有开销，因此这个间隔越大开销就越小。 daemon指定 Nginx 的运行方式，前台还是后台，前台用于调试，后台用于生产。 daemon off; # 默认是on，后台运行模式 ","date":"2021-05-06","objectID":"/nginx-glance/:6:2","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#worker_processes_number"},{"categories":["开发者手册"],"content":" 配置文件 main 段核心参数 user指定运行 Nginx 的 woker 子进程的属主和属组，其中组可以不指定。 user USERNAME [GROUP] user nginx lion; # 用户是nginx;组是lion pid指定运行 Nginx master 主进程的 pid 文件存放路径。 pid /opt/nginx/logs/nginx.pid # master主进程的的pid存放在nginx.pid的文件 worker_rlimit_nofile_number指定 worker 子进程可以打开的最大文件句柄数。 worker_rlimit_nofile 20480; # 可以理解成每个worker子进程的最大连接数量。 worker_rlimit_core指定 worker 子进程异常终止后的 core 文件，用于记录分析问题。 worker_rlimit_core 50M; # 存放大小限制 working_directory /opt/nginx/tmp; # 存放目录 worker_processes_number指定 Nginx 启动的 worker 子进程数量。 worker_processes 4; # 指定具体子进程数量 worker_processes auto; # 与当前cpu物理核心数一致 worker_cpu_affinity将每个 worker 子进程与我们的 cpu 物理核心绑定。 worker_cpu_affinity 0001 0010 0100 1000; # 4个物理核心，4个worker子进程 将每个 worker 子进程与特定 CPU 物理核心绑定，优势在于，避免同一个 worker 子进程在不同的 CPU 核心上切换，缓存失效，降低性能。但其并不能真正的避免进程切换。 worker_priority指定 worker 子进程的 nice 值，以调整运行 Nginx 的优先级，通常设定为负值，以优先调用 Nginx 。 worker_priority -10; # 120-10=110，110就是最终的优先级 Linux 默认进程的优先级值是120，值越小越优先； nice 定范围为 -20 到 +19 。 [备注] 应用的默认优先级值是120加上 nice 值等于它最终的值，这个值越小，优先级越高。 worker_shutdown_timeout指定 worker 子进程优雅退出时的超时时间。 worker_shutdown_timeout 5s; timer_resolutionworker 子进程内部使用的计时器精度，调整时间间隔越大，系统调用越少，有利于性能提升；反之，系统调用越多，性能下降。 timer_resolution 100ms; 在 Linux 系统中，用户需要获取计时器时需要向操作系统内核发送请求，有请求就必然会有开销，因此这个间隔越大开销就越小。 daemon指定 Nginx 的运行方式，前台还是后台，前台用于调试，后台用于生产。 daemon off; # 默认是on，后台运行模式 ","date":"2021-05-06","objectID":"/nginx-glance/:6:2","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#worker_cpu_affinity"},{"categories":["开发者手册"],"content":" 配置文件 main 段核心参数 user指定运行 Nginx 的 woker 子进程的属主和属组，其中组可以不指定。 user USERNAME [GROUP] user nginx lion; # 用户是nginx;组是lion pid指定运行 Nginx master 主进程的 pid 文件存放路径。 pid /opt/nginx/logs/nginx.pid # master主进程的的pid存放在nginx.pid的文件 worker_rlimit_nofile_number指定 worker 子进程可以打开的最大文件句柄数。 worker_rlimit_nofile 20480; # 可以理解成每个worker子进程的最大连接数量。 worker_rlimit_core指定 worker 子进程异常终止后的 core 文件，用于记录分析问题。 worker_rlimit_core 50M; # 存放大小限制 working_directory /opt/nginx/tmp; # 存放目录 worker_processes_number指定 Nginx 启动的 worker 子进程数量。 worker_processes 4; # 指定具体子进程数量 worker_processes auto; # 与当前cpu物理核心数一致 worker_cpu_affinity将每个 worker 子进程与我们的 cpu 物理核心绑定。 worker_cpu_affinity 0001 0010 0100 1000; # 4个物理核心，4个worker子进程 将每个 worker 子进程与特定 CPU 物理核心绑定，优势在于，避免同一个 worker 子进程在不同的 CPU 核心上切换，缓存失效，降低性能。但其并不能真正的避免进程切换。 worker_priority指定 worker 子进程的 nice 值，以调整运行 Nginx 的优先级，通常设定为负值，以优先调用 Nginx 。 worker_priority -10; # 120-10=110，110就是最终的优先级 Linux 默认进程的优先级值是120，值越小越优先； nice 定范围为 -20 到 +19 。 [备注] 应用的默认优先级值是120加上 nice 值等于它最终的值，这个值越小，优先级越高。 worker_shutdown_timeout指定 worker 子进程优雅退出时的超时时间。 worker_shutdown_timeout 5s; timer_resolutionworker 子进程内部使用的计时器精度，调整时间间隔越大，系统调用越少，有利于性能提升；反之，系统调用越多，性能下降。 timer_resolution 100ms; 在 Linux 系统中，用户需要获取计时器时需要向操作系统内核发送请求，有请求就必然会有开销，因此这个间隔越大开销就越小。 daemon指定 Nginx 的运行方式，前台还是后台，前台用于调试，后台用于生产。 daemon off; # 默认是on，后台运行模式 ","date":"2021-05-06","objectID":"/nginx-glance/:6:2","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#worker_priority"},{"categories":["开发者手册"],"content":" 配置文件 main 段核心参数 user指定运行 Nginx 的 woker 子进程的属主和属组，其中组可以不指定。 user USERNAME [GROUP] user nginx lion; # 用户是nginx;组是lion pid指定运行 Nginx master 主进程的 pid 文件存放路径。 pid /opt/nginx/logs/nginx.pid # master主进程的的pid存放在nginx.pid的文件 worker_rlimit_nofile_number指定 worker 子进程可以打开的最大文件句柄数。 worker_rlimit_nofile 20480; # 可以理解成每个worker子进程的最大连接数量。 worker_rlimit_core指定 worker 子进程异常终止后的 core 文件，用于记录分析问题。 worker_rlimit_core 50M; # 存放大小限制 working_directory /opt/nginx/tmp; # 存放目录 worker_processes_number指定 Nginx 启动的 worker 子进程数量。 worker_processes 4; # 指定具体子进程数量 worker_processes auto; # 与当前cpu物理核心数一致 worker_cpu_affinity将每个 worker 子进程与我们的 cpu 物理核心绑定。 worker_cpu_affinity 0001 0010 0100 1000; # 4个物理核心，4个worker子进程 将每个 worker 子进程与特定 CPU 物理核心绑定，优势在于，避免同一个 worker 子进程在不同的 CPU 核心上切换，缓存失效，降低性能。但其并不能真正的避免进程切换。 worker_priority指定 worker 子进程的 nice 值，以调整运行 Nginx 的优先级，通常设定为负值，以优先调用 Nginx 。 worker_priority -10; # 120-10=110，110就是最终的优先级 Linux 默认进程的优先级值是120，值越小越优先； nice 定范围为 -20 到 +19 。 [备注] 应用的默认优先级值是120加上 nice 值等于它最终的值，这个值越小，优先级越高。 worker_shutdown_timeout指定 worker 子进程优雅退出时的超时时间。 worker_shutdown_timeout 5s; timer_resolutionworker 子进程内部使用的计时器精度，调整时间间隔越大，系统调用越少，有利于性能提升；反之，系统调用越多，性能下降。 timer_resolution 100ms; 在 Linux 系统中，用户需要获取计时器时需要向操作系统内核发送请求，有请求就必然会有开销，因此这个间隔越大开销就越小。 daemon指定 Nginx 的运行方式，前台还是后台，前台用于调试，后台用于生产。 daemon off; # 默认是on，后台运行模式 ","date":"2021-05-06","objectID":"/nginx-glance/:6:2","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#worker_shutdown_timeout"},{"categories":["开发者手册"],"content":" 配置文件 main 段核心参数 user指定运行 Nginx 的 woker 子进程的属主和属组，其中组可以不指定。 user USERNAME [GROUP] user nginx lion; # 用户是nginx;组是lion pid指定运行 Nginx master 主进程的 pid 文件存放路径。 pid /opt/nginx/logs/nginx.pid # master主进程的的pid存放在nginx.pid的文件 worker_rlimit_nofile_number指定 worker 子进程可以打开的最大文件句柄数。 worker_rlimit_nofile 20480; # 可以理解成每个worker子进程的最大连接数量。 worker_rlimit_core指定 worker 子进程异常终止后的 core 文件，用于记录分析问题。 worker_rlimit_core 50M; # 存放大小限制 working_directory /opt/nginx/tmp; # 存放目录 worker_processes_number指定 Nginx 启动的 worker 子进程数量。 worker_processes 4; # 指定具体子进程数量 worker_processes auto; # 与当前cpu物理核心数一致 worker_cpu_affinity将每个 worker 子进程与我们的 cpu 物理核心绑定。 worker_cpu_affinity 0001 0010 0100 1000; # 4个物理核心，4个worker子进程 将每个 worker 子进程与特定 CPU 物理核心绑定，优势在于，避免同一个 worker 子进程在不同的 CPU 核心上切换，缓存失效，降低性能。但其并不能真正的避免进程切换。 worker_priority指定 worker 子进程的 nice 值，以调整运行 Nginx 的优先级，通常设定为负值，以优先调用 Nginx 。 worker_priority -10; # 120-10=110，110就是最终的优先级 Linux 默认进程的优先级值是120，值越小越优先； nice 定范围为 -20 到 +19 。 [备注] 应用的默认优先级值是120加上 nice 值等于它最终的值，这个值越小，优先级越高。 worker_shutdown_timeout指定 worker 子进程优雅退出时的超时时间。 worker_shutdown_timeout 5s; timer_resolutionworker 子进程内部使用的计时器精度，调整时间间隔越大，系统调用越少，有利于性能提升；反之，系统调用越多，性能下降。 timer_resolution 100ms; 在 Linux 系统中，用户需要获取计时器时需要向操作系统内核发送请求，有请求就必然会有开销，因此这个间隔越大开销就越小。 daemon指定 Nginx 的运行方式，前台还是后台，前台用于调试，后台用于生产。 daemon off; # 默认是on，后台运行模式 ","date":"2021-05-06","objectID":"/nginx-glance/:6:2","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#timer_resolution"},{"categories":["开发者手册"],"content":" 配置文件 main 段核心参数 user指定运行 Nginx 的 woker 子进程的属主和属组，其中组可以不指定。 user USERNAME [GROUP] user nginx lion; # 用户是nginx;组是lion pid指定运行 Nginx master 主进程的 pid 文件存放路径。 pid /opt/nginx/logs/nginx.pid # master主进程的的pid存放在nginx.pid的文件 worker_rlimit_nofile_number指定 worker 子进程可以打开的最大文件句柄数。 worker_rlimit_nofile 20480; # 可以理解成每个worker子进程的最大连接数量。 worker_rlimit_core指定 worker 子进程异常终止后的 core 文件，用于记录分析问题。 worker_rlimit_core 50M; # 存放大小限制 working_directory /opt/nginx/tmp; # 存放目录 worker_processes_number指定 Nginx 启动的 worker 子进程数量。 worker_processes 4; # 指定具体子进程数量 worker_processes auto; # 与当前cpu物理核心数一致 worker_cpu_affinity将每个 worker 子进程与我们的 cpu 物理核心绑定。 worker_cpu_affinity 0001 0010 0100 1000; # 4个物理核心，4个worker子进程 将每个 worker 子进程与特定 CPU 物理核心绑定，优势在于，避免同一个 worker 子进程在不同的 CPU 核心上切换，缓存失效，降低性能。但其并不能真正的避免进程切换。 worker_priority指定 worker 子进程的 nice 值，以调整运行 Nginx 的优先级，通常设定为负值，以优先调用 Nginx 。 worker_priority -10; # 120-10=110，110就是最终的优先级 Linux 默认进程的优先级值是120，值越小越优先； nice 定范围为 -20 到 +19 。 [备注] 应用的默认优先级值是120加上 nice 值等于它最终的值，这个值越小，优先级越高。 worker_shutdown_timeout指定 worker 子进程优雅退出时的超时时间。 worker_shutdown_timeout 5s; timer_resolutionworker 子进程内部使用的计时器精度，调整时间间隔越大，系统调用越少，有利于性能提升；反之，系统调用越多，性能下降。 timer_resolution 100ms; 在 Linux 系统中，用户需要获取计时器时需要向操作系统内核发送请求，有请求就必然会有开销，因此这个间隔越大开销就越小。 daemon指定 Nginx 的运行方式，前台还是后台，前台用于调试，后台用于生产。 daemon off; # 默认是on，后台运行模式 ","date":"2021-05-06","objectID":"/nginx-glance/:6:2","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#daemon"},{"categories":["开发者手册"],"content":" 配置文件 events 段核心参数 useNginx 使用何种事件驱动模型。 use method; # 不推荐配置它，让nginx自己选择 method 可选值为：select、poll、kqueue、epoll、/dev/poll、eventport worker_connectionsworker 子进程能够处理的最大并发连接数。 worker_connections 1024 # 每个子进程的最大连接数为1024 accept_mutex是否打开负载均衡互斥锁。 accept_mutex on # 默认是off关闭的，这里推荐打开 ","date":"2021-05-06","objectID":"/nginx-glance/:6:3","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#配置文件-events-段核心参数"},{"categories":["开发者手册"],"content":" 配置文件 events 段核心参数 useNginx 使用何种事件驱动模型。 use method; # 不推荐配置它，让nginx自己选择 method 可选值为：select、poll、kqueue、epoll、/dev/poll、eventport worker_connectionsworker 子进程能够处理的最大并发连接数。 worker_connections 1024 # 每个子进程的最大连接数为1024 accept_mutex是否打开负载均衡互斥锁。 accept_mutex on # 默认是off关闭的，这里推荐打开 ","date":"2021-05-06","objectID":"/nginx-glance/:6:3","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#use"},{"categories":["开发者手册"],"content":" 配置文件 events 段核心参数 useNginx 使用何种事件驱动模型。 use method; # 不推荐配置它，让nginx自己选择 method 可选值为：select、poll、kqueue、epoll、/dev/poll、eventport worker_connectionsworker 子进程能够处理的最大并发连接数。 worker_connections 1024 # 每个子进程的最大连接数为1024 accept_mutex是否打开负载均衡互斥锁。 accept_mutex on # 默认是off关闭的，这里推荐打开 ","date":"2021-05-06","objectID":"/nginx-glance/:6:3","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#worker_connections"},{"categories":["开发者手册"],"content":" 配置文件 events 段核心参数 useNginx 使用何种事件驱动模型。 use method; # 不推荐配置它，让nginx自己选择 method 可选值为：select、poll、kqueue、epoll、/dev/poll、eventport worker_connectionsworker 子进程能够处理的最大并发连接数。 worker_connections 1024 # 每个子进程的最大连接数为1024 accept_mutex是否打开负载均衡互斥锁。 accept_mutex on # 默认是off关闭的，这里推荐打开 ","date":"2021-05-06","objectID":"/nginx-glance/:6:3","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#accept_mutex"},{"categories":["开发者手册"],"content":" server_name 指令指定虚拟主机域名。 server_name name1 name2 name3 # 示例： server_name www.nginx.com; 域名匹配的四种写法： 精确匹配： server_name www.nginx.com ; 左侧通配： server_name *.nginx.com ; 右侧统配： server_name www.nginx.* ; 正则匹配： server_name ~^www\\.nginx\\.*$ ; 匹配优先级：精确匹配 \u003e 左侧通配符匹配 \u003e 右侧通配符匹配 \u003e 正则表达式匹配 server_name 配置实例： 1、配置本地 DNS 解析 vim /etc/hosts （ macOS 系统） # 添加如下内容，其中 121.42.11.34 是阿里云服务器IP地址 121.42.11.34 www.nginx-test.com 121.42.11.34 mail.nginx-test.com 121.42.11.34 www.nginx-test.org 121.42.11.34 doc.nginx-test.com 121.42.11.34 www.nginx-test.cn 121.42.11.34 fe.nginx-test.club [注意] 这里使用的是虚拟域名进行测试，因此需要配置本地 DNS 解析，如果使用阿里云上购买的域名，则需要在阿里云上设置好域名解析。 2、配置阿里云 Nginx ，vim /etc/nginx/nginx.conf # 这里只列举了http端中的sever端配置 # 左匹配 server { listen80; server_name*.nginx-test.com; root/usr/share/nginx/html/nginx-test/left-match/; location / { index index.html; } } # 正则匹配 server { listen80; server_name~^.*\\.nginx-test\\..*$; root/usr/share/nginx/html/nginx-test/reg-match/; location / { index index.html; } } # 右匹配 server { listen80; server_namewww.nginx-test.*; root/usr/share/nginx/html/nginx-test/right-match/; location / { index index.html; } } # 完全匹配 server { listen80; server_namewww.nginx-test.com; root/usr/share/nginx/html/nginx-test/all-match/; location / { index index.html; } } 3、访问分析 当访问 www.nginx-test.com 时，都可以被匹配上，因此选择优先级最高的“完全匹配”； 当访问 mail.nginx-test.com 时，会进行“左匹配”； 当访问 www.nginx-test.org 时，会进行“右匹配”； 当访问 doc.nginx-test.com 时，会进行“左匹配”； 当访问 www.nginx-test.cn 时，会进行“右匹配”； 当访问 fe.nginx-test.club 时，会进行“正则匹配”； ","date":"2021-05-06","objectID":"/nginx-glance/:6:4","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#server_name-指令"},{"categories":["开发者手册"],"content":" root指定静态资源目录位置，它可以写在 http 、 server 、 location 等配置中。 root path 例如： location /image { root /opt/nginx/static; } 当用户访问 www.test.com/image/1.png 时，实际在服务器找的路径是 /opt/nginx/static/image/1.png [注意] root 会将定义路径与 URI 叠加， alias 则只取定义路径。 ","date":"2021-05-06","objectID":"/nginx-glance/:6:5","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#root"},{"categories":["开发者手册"],"content":" alias它也是指定静态资源目录位置，它只能写在 location 中。 location /image { alias /opt/nginx/static/image/; } 当用户访问 www.test.com/image/1.png 时，实际在服务器找的路径是 /opt/nginx/static/image/1.png [注意] 使用 alias 末尾一定要添加 / ，并且它只能位于 location 中。 ","date":"2021-05-06","objectID":"/nginx-glance/:6:6","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#alias"},{"categories":["开发者手册"],"content":" location配置路径。 location [ = | ~ | ~* | ^~ ] uri { ... } 匹配规则： = 精确匹配； ~ 正则匹配，区分大小写； ~* 正则匹配，不区分大小写； ^~ 匹配到即停止搜索； 匹配优先级： = \u003e ^~ \u003e ~ \u003e ~* \u003e 不带任何字符。 实例： server { listen80; server_namewww.nginx-test.com; # 只有当访问 www.nginx-test.com/match_all/ 时才会匹配到/usr/share/nginx/html/match_all/index.html location = /match_all/ { root/usr/share/nginx/html index index.html } # 当访问 www.nginx-test.com/1.jpg 等路径时会去 /usr/share/nginx/images/1.jpg 找对应的资源 location ~ \\.(jpeg|jpg|png|svg)$ { root /usr/share/nginx/images; } # 当访问 www.nginx-test.com/bbs/ 时会匹配上 /usr/share/nginx/html/bbs/index.html location ^~ /bbs/ { root /usr/share/nginx/html; index index.html index.htm; } } location 中的反斜线 location /test { ... } location /test/ { ... } 不带 / 当访问 www.nginx-test.com/test 时， Nginx 先找是否有 test 目录，如果有则找 test 目录下的 index.html ；如果没有 test 目录， nginx 则会找是否有 test 文件。 带 / 当访问 www.nginx-test.com/test 时， Nginx 先找是否有 test 目录，如果有则找 test 目录下的 index.html ，如果没有它也不会去找是否存在 test 文件。 return停止处理请求，直接返回响应码或重定向到其他 URL ；执行 return 指令后， location 中后续指令将不会被执行。 return code [text]; return code URL; return URL; 例如： location / { return 404; # 直接返回状态码 } location / { return 404 \"pages not found\"; # 返回状态码 + 一段文本 } location / { return 302 /bbs ; # 返回状态码 + 重定向地址 } location / { return https://www.baidu.com ; # 返回重定向地址 } ","date":"2021-05-06","objectID":"/nginx-glance/:6:7","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#location"},{"categories":["开发者手册"],"content":" location配置路径。 location [ = | ~ | ~* | ^~ ] uri { ... } 匹配规则： = 精确匹配； ~ 正则匹配，区分大小写； ~* 正则匹配，不区分大小写； ^~ 匹配到即停止搜索； 匹配优先级： = \u003e ^~ \u003e ~ \u003e ~* \u003e 不带任何字符。 实例： server { listen80; server_namewww.nginx-test.com; # 只有当访问 www.nginx-test.com/match_all/ 时才会匹配到/usr/share/nginx/html/match_all/index.html location = /match_all/ { root/usr/share/nginx/html index index.html } # 当访问 www.nginx-test.com/1.jpg 等路径时会去 /usr/share/nginx/images/1.jpg 找对应的资源 location ~ \\.(jpeg|jpg|png|svg)$ { root /usr/share/nginx/images; } # 当访问 www.nginx-test.com/bbs/ 时会匹配上 /usr/share/nginx/html/bbs/index.html location ^~ /bbs/ { root /usr/share/nginx/html; index index.html index.htm; } } location 中的反斜线 location /test { ... } location /test/ { ... } 不带 / 当访问 www.nginx-test.com/test 时， Nginx 先找是否有 test 目录，如果有则找 test 目录下的 index.html ；如果没有 test 目录， nginx 则会找是否有 test 文件。 带 / 当访问 www.nginx-test.com/test 时， Nginx 先找是否有 test 目录，如果有则找 test 目录下的 index.html ，如果没有它也不会去找是否存在 test 文件。 return停止处理请求，直接返回响应码或重定向到其他 URL ；执行 return 指令后， location 中后续指令将不会被执行。 return code [text]; return code URL; return URL; 例如： location / { return 404; # 直接返回状态码 } location / { return 404 \"pages not found\"; # 返回状态码 + 一段文本 } location / { return 302 /bbs ; # 返回状态码 + 重定向地址 } location / { return https://www.baidu.com ; # 返回重定向地址 } ","date":"2021-05-06","objectID":"/nginx-glance/:6:7","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#location-中的反斜线"},{"categories":["开发者手册"],"content":" location配置路径。 location [ = | ~ | ~* | ^~ ] uri { ... } 匹配规则： = 精确匹配； ~ 正则匹配，区分大小写； ~* 正则匹配，不区分大小写； ^~ 匹配到即停止搜索； 匹配优先级： = \u003e ^~ \u003e ~ \u003e ~* \u003e 不带任何字符。 实例： server { listen80; server_namewww.nginx-test.com; # 只有当访问 www.nginx-test.com/match_all/ 时才会匹配到/usr/share/nginx/html/match_all/index.html location = /match_all/ { root/usr/share/nginx/html index index.html } # 当访问 www.nginx-test.com/1.jpg 等路径时会去 /usr/share/nginx/images/1.jpg 找对应的资源 location ~ \\.(jpeg|jpg|png|svg)$ { root /usr/share/nginx/images; } # 当访问 www.nginx-test.com/bbs/ 时会匹配上 /usr/share/nginx/html/bbs/index.html location ^~ /bbs/ { root /usr/share/nginx/html; index index.html index.htm; } } location 中的反斜线 location /test { ... } location /test/ { ... } 不带 / 当访问 www.nginx-test.com/test 时， Nginx 先找是否有 test 目录，如果有则找 test 目录下的 index.html ；如果没有 test 目录， nginx 则会找是否有 test 文件。 带 / 当访问 www.nginx-test.com/test 时， Nginx 先找是否有 test 目录，如果有则找 test 目录下的 index.html ，如果没有它也不会去找是否存在 test 文件。 return停止处理请求，直接返回响应码或重定向到其他 URL ；执行 return 指令后， location 中后续指令将不会被执行。 return code [text]; return code URL; return URL; 例如： location / { return 404; # 直接返回状态码 } location / { return 404 \"pages not found\"; # 返回状态码 + 一段文本 } location / { return 302 /bbs ; # 返回状态码 + 重定向地址 } location / { return https://www.baidu.com ; # 返回重定向地址 } ","date":"2021-05-06","objectID":"/nginx-glance/:6:7","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#return"},{"categories":["开发者手册"],"content":" rewrite根据指定正则表达式匹配规则，重写 URL 。 语法：rewrite 正则表达式 要替换的内容 [flag]; 上下文：server、location、if 示例：rewirte /images/(.*\\.jpg)$ /pic/$1; # $1是前面括号(.*\\.jpg)的反向引用 flag 可选值的含义： last 重写后的 URL 发起新请求，再次进入 server 段，重试 location 的中的匹配； break 直接使用重写后的 URL ，不再匹配其它 location 中语句； redirect 返回302临时重定向； permanent 返回301永久重定向； server{ listen 80; server_name fe.lion.club; # 要在本地hosts文件进行配置 root html; location /search { rewrite ^/(.*) https://www.baidu.com redirect; } location /images { rewrite /images/(.*) /pics/$1; } location /pics { rewrite /pics/(.*) /photos/$1; } location /photos { } } 按照这个配置我们来分析： 当访问 fe.lion.club/search 时，会自动帮我们重定向到 https://www.baidu.com。 当访问 fe.lion.club/images/1.jpg 时，第一步重写 URL 为 fe.lion.club/pics/1.jpg ，找到 pics 的 location ，继续重写 URL 为 fe.lion.club/photos/1.jpg ，找到 /photos 的 location 后，去 html/photos 目录下寻找 1.jpg 静态资源。 ","date":"2021-05-06","objectID":"/nginx-glance/:6:8","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#rewrite"},{"categories":["开发者手册"],"content":" if 指令 语法：if (condition) {...} 上下文：server、location 示例： if($http_user_agent ~ Chrome){ rewrite /(.*)/browser/$1 break; } condition 判断条件： $variable 仅为变量时，值为空或以0开头字符串都会被当做 false 处理； = 或 != 相等或不等； ~ 正则匹配； ! ~ 非正则匹配； ~* 正则匹配，不区分大小写； -f 或 ! -f 检测文件存在或不存在； -d 或 ! -d 检测目录存在或不存在； -e 或 ! -e 检测文件、目录、符号链接等存在或不存在； -x 或 ! -x 检测文件可以执行或不可执行； 实例： server { listen 8080; server_name localhost; root html; location / { if ( $uri = \"/images/\" ){ rewrite (.*) /pics/ break; } } } 当访问 localhost:8080/images/ 时，会进入 if 判断里面执行 rewrite 命令。 ","date":"2021-05-06","objectID":"/nginx-glance/:6:9","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#if-指令"},{"categories":["开发者手册"],"content":" autoindex用户请求以 / 结尾时，列出目录结构，可以用于快速搭建静态资源下载网站。 autoindex.conf 配置信息： server { listen 80; server_name fe.lion-test.club; location /download/ { root /opt/source; autoindex on; # 打开 autoindex，，可选参数有 on | off autoindex_exact_size on; # 修改为off，以KB、MB、GB显示文件大小，默认为on，以bytes显示出⽂件的确切⼤⼩ autoindex_format html; # 以html的方式进行格式化，可选参数有 html | json | xml autoindex_localtime off; # 显示的⽂件时间为⽂件的服务器时间。默认为off，显示的⽂件时间为GMT时间 } } 当访问 fe.lion.com/download/ 时，会把服务器 /opt/source/download/ 路径下的文件展示出来，如下图所示： ","date":"2021-05-06","objectID":"/nginx-glance/:6:10","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#autoindex"},{"categories":["开发者手册"],"content":" 变量Nginx 提供给使用者的变量非常多，但是终究是一个完整的请求过程所产生数据， Nginx 将这些数据以变量的形式提供给使用者。 下面列举些项目中常用的变量： 变量名 含义 remote_addr 客户端 IP 地址 remote_port 客户端端口 server_addr 服务端 IP 地址 server_port 服务端端口 server_protocol 服务端协议 binary_remote_addr 二进制格式的客户端 IP 地址 connection TCP 连接的序号，递增 connection_request TCP 连接当前的请求数量 uri 请求的URL，不包含参数 request_uri 请求的URL，包含参数 scheme 协议名， http 或 https request_method 请求方法 request_length 全部请求的长度，包含请求行、请求头、请求体 args 全部参数字符串 arg_参数名 获取特定参数值 is_args URL 中是否有参数，有的话返回 ? ，否则返回空 query_string 与 args 相同 host 请求信息中的 Host ，如果请求中没有 Host 行，则在请求头中找，最后使用 nginx 中设置的 server_name 。 http_user_agent 用户浏览器 http_referer 从哪些链接过来的请求 http_via 每经过一层代理服务器，都会添加相应的信息 http_cookie 获取用户 cookie request_time 处理请求已消耗的时间 https 是否开启了 https ，是则返回 on ，否则返回空 request_filename 磁盘文件系统待访问文件的完整路径 document_root 由 URI 和 root/alias 规则生成的文件夹路径 limit_rate 返回响应时的速度上限值 实例演示 var.conf ： server{ listen 8081; server_name var.lion-test.club; root /usr/share/nginx/html; location / { return 200 \" remote_addr: $remote_addr remote_port: $remote_port server_addr: $server_addr server_port: $server_port server_protocol: $server_protocol binary_remote_addr: $binary_remote_addr connection: $connection uri: $uri request_uri: $request_uri scheme: $scheme request_method: $request_method request_length: $request_length args: $args arg_pid: $arg_pid is_args: $is_args query_string: $query_string host: $host http_user_agent: $http_user_agent http_referer: $http_referer http_via: $http_via request_time: $request_time https: $https request_filename: $request_filename document_root: $document_root \"; } } 当我们访问 http://var.lion-test.club:8081/test?pid=121414\u0026cid=sadasd 时，由于 Nginx 中写了 return 方法，因此 chrome 浏览器会默认为我们下载一个文件，下面展示的就是下载的文件内容： remote_addr: 27.16.220.84 remote_port: 56838 server_addr: 172.17.0.2 server_port: 8081 server_protocol: HTTP/1.1 binary_remote_addr: 茉 connection: 126 uri: /test/ request_uri: /test/?pid=121414\u0026cid=sadasd scheme: http request_method: GET request_length: 518 args: pid=121414\u0026cid=sadasd arg_pid: 121414 is_args: ? query_string: pid=121414\u0026cid=sadasd host: var.lion-test.club http_user_agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36 http_referer: http_via: request_time: 0.000 https: request_filename: /usr/share/nginx/html/test/ document_root: /usr/share/nginx/html Nginx 的配置还有非常多，以上只是罗列了一些常用的配置，在实际项目中还是要学会查阅文档。 ","date":"2021-05-06","objectID":"/nginx-glance/:6:11","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#变量"},{"categories":["开发者手册"],"content":" Nginx 应用核心概念代理是在服务器和客户端之间假设的一层服务器，代理将接收客户端的请求并将它转发给服务器，然后将服务端的响应转发给客户端。 不管是正向代理还是反向代理，实现的都是上面的功能。 ","date":"2021-05-06","objectID":"/nginx-glance/:7:0","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#nginx-应用核心概念"},{"categories":["开发者手册"],"content":" 正向代理 正向代理，意思是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。 正向代理是为我们服务的，即为客户端服务的，客户端可以根据正向代理访问到它本身无法访问到的服务器资源。 正向代理对我们是透明的，对服务端是非透明的，即服务端并不知道自己收到的是来自代理的访问还是来自真实客户端的访问。 ","date":"2021-05-06","objectID":"/nginx-glance/:7:1","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#正向代理"},{"categories":["开发者手册"],"content":" 反向代理 反向代理*（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。 反向代理是为服务端服务的，反向代理可以帮助服务器接收来自客户端的请求，帮助服务器做请求转发，负载均衡等。 反向代理对服务端是透明的，对我们是非透明的，即我们并不知道自己访问的是代理服务器，而服务器知道反向代理在为他服务。 反向代理的优势： 隐藏真实服务器； 负载均衡便于横向扩充后端动态服务； 动静分离，提升系统健壮性； 那么“动静分离”是什么？负载均衡又是什么？ ","date":"2021-05-06","objectID":"/nginx-glance/:7:2","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#反向代理"},{"categories":["开发者手册"],"content":" 动静分离动静分离是指在 web 服务器架构中，将静态页面与动态页面或者静态内容接口和动态内容接口分开不同系统访问的架构设计方法，进而提示整个服务的访问性和可维护性。 一般来说，都需要将动态资源和静态资源分开，由于 Nginx 的高并发和静态资源缓存等特性，经常将静态资源部署在 Nginx 上。如果请求的是静态资源，直接到静态资源目录获取资源，如果是动态资源的请求，则利用反向代理的原理，把请求转发给对应后台应用去处理，从而实现动静分离。 使用前后端分离后，可以很大程度提升静态资源的访问速度，即使动态服务不可用，静态资源的访问也不会受到影响。 ","date":"2021-05-06","objectID":"/nginx-glance/:7:3","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#动静分离"},{"categories":["开发者手册"],"content":" 负载均衡一般情况下，客户端发送多个请求到服务器，服务器处理请求，其中一部分可能要操作一些资源比如数据库、静态资源等，服务器处理完毕后，再将结果返回给客户端。 这种模式对于早期的系统来说，功能要求不复杂，且并发请求相对较少的情况下还能胜任，成本也低。随着信息数量不断增长，访问量和数据量飞速增长，以及系统业务复杂度持续增加，这种做法已无法满足要求，并发量特别大时，服务器容易崩。 很明显这是由于服务器性能的瓶颈造成的问题，除了堆机器之外，最重要的做法就是负载均衡。 请求爆发式增长的情况下，单个机器性能再强劲也无法满足要求了，这个时候集群的概念产生了，单个服务器解决不了的问题，可以使用多个服务器，然后将请求分发到各个服务器上，将负载分发到不同的服务器，这就是负载均衡，核心是「分摊压力」。 Nginx 实现负载均衡，一般来说指的是将请求转发给服务器集群。 举个具体的例子，晚高峰乘坐地铁的时候，入站口经常会有地铁工作人员大喇叭“请走 B 口， B 口人少车空….”，这个工作人员的作用就是负载均衡。 Nginx 实现负载均衡的策略： 轮询策略：默认情况下采用的策略，将所有客户端请求轮询分配给服务端。这种策略是可以正常工作的，但是如果其中某一台服务器压力太大，出现延迟，会影响所有分配在这台服务器下的用户。 最小连接数策略：将请求优先分配给压力较小的服务器，它可以平衡每个队列的长度，并避免向压力大的服务器添加更多的请求。 最快响应时间策略：优先分配给响应时间最短的服务器。 客户端 ip 绑定策略：来自同一个 ip 的请求永远只分配一台服务器，有效解决了动态网页存在的 session 共享问题。 ","date":"2021-05-06","objectID":"/nginx-glance/:7:4","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#负载均衡"},{"categories":["开发者手册"],"content":" Nginx 实战配置在配置反向代理和负载均衡等等功能之前，有两个核心模块是我们必须要掌握的，这两个模块应该说是 Nginx 应用配置中的核心，它们分别是： upstream 、proxy_pass 。 ","date":"2021-05-06","objectID":"/nginx-glance/:8:0","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#nginx-实战配置"},{"categories":["开发者手册"],"content":" upstream用于定义上游服务器（指的就是后台提供的应用服务器）的相关信息。 语法：upstream name { ... } 上下文：http 示例： upstream back_end_server{ server 192.168.100.33:8081 } 在 upstream 内可使用的指令： server 定义上游服务器地址； zone 定义共享内存，用于跨 worker 子进程； keepalive 对上游服务启用长连接； keepalive_requests 一个长连接最多请求 HTTP 的个数； keepalive_timeout 空闲情形下，一个长连接的超时时长； hash 哈希负载均衡算法； ip_hash 依据 IP 进行哈希计算的负载均衡算法； least_conn 最少连接数负载均衡算法； least_time 最短响应时间负载均衡算法； random 随机负载均衡算法； server定义上游服务器地址。 语法：server address [parameters] 上下文：upstream parameters 可选值： weight=number 权重值，默认为1； max_conns=number 上游服务器的最大并发连接数； fail_timeout=time 服务器不可用的判定时间； max_fails=numer 服务器不可用的检查次数； backup 备份服务器，仅当其他服务器都不可用时才会启用； down 标记服务器长期不可用，离线维护； keepalive限制每个 worker 子进程与上游服务器空闲长连接的最大数量。 keepalive connections; 上下文：upstream 示例：keepalive 16; keepalive_requests单个长连接可以处理的最多 HTTP 请求个数。 语法：keepalive_requests number; 默认值：keepalive_requests 100; 上下文：upstream keepalive_timeout空闲长连接的最长保持时间。 语法：keepalive_timeout time; 默认值：keepalive_timeout 60s; 上下文：upstream 配置实例 upstream back_end{ server 127.0.0.1:8081 weight=3 max_conns=1000 fail_timeout=10s max_fails=2; keepalive 32; keepalive_requests 50; keepalive_timeout 30s; } ","date":"2021-05-06","objectID":"/nginx-glance/:8:1","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#upstream"},{"categories":["开发者手册"],"content":" upstream用于定义上游服务器（指的就是后台提供的应用服务器）的相关信息。 语法：upstream name { ... } 上下文：http 示例： upstream back_end_server{ server 192.168.100.33:8081 } 在 upstream 内可使用的指令： server 定义上游服务器地址； zone 定义共享内存，用于跨 worker 子进程； keepalive 对上游服务启用长连接； keepalive_requests 一个长连接最多请求 HTTP 的个数； keepalive_timeout 空闲情形下，一个长连接的超时时长； hash 哈希负载均衡算法； ip_hash 依据 IP 进行哈希计算的负载均衡算法； least_conn 最少连接数负载均衡算法； least_time 最短响应时间负载均衡算法； random 随机负载均衡算法； server定义上游服务器地址。 语法：server address [parameters] 上下文：upstream parameters 可选值： weight=number 权重值，默认为1； max_conns=number 上游服务器的最大并发连接数； fail_timeout=time 服务器不可用的判定时间； max_fails=numer 服务器不可用的检查次数； backup 备份服务器，仅当其他服务器都不可用时才会启用； down 标记服务器长期不可用，离线维护； keepalive限制每个 worker 子进程与上游服务器空闲长连接的最大数量。 keepalive connections; 上下文：upstream 示例：keepalive 16; keepalive_requests单个长连接可以处理的最多 HTTP 请求个数。 语法：keepalive_requests number; 默认值：keepalive_requests 100; 上下文：upstream keepalive_timeout空闲长连接的最长保持时间。 语法：keepalive_timeout time; 默认值：keepalive_timeout 60s; 上下文：upstream 配置实例 upstream back_end{ server 127.0.0.1:8081 weight=3 max_conns=1000 fail_timeout=10s max_fails=2; keepalive 32; keepalive_requests 50; keepalive_timeout 30s; } ","date":"2021-05-06","objectID":"/nginx-glance/:8:1","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#server"},{"categories":["开发者手册"],"content":" upstream用于定义上游服务器（指的就是后台提供的应用服务器）的相关信息。 语法：upstream name { ... } 上下文：http 示例： upstream back_end_server{ server 192.168.100.33:8081 } 在 upstream 内可使用的指令： server 定义上游服务器地址； zone 定义共享内存，用于跨 worker 子进程； keepalive 对上游服务启用长连接； keepalive_requests 一个长连接最多请求 HTTP 的个数； keepalive_timeout 空闲情形下，一个长连接的超时时长； hash 哈希负载均衡算法； ip_hash 依据 IP 进行哈希计算的负载均衡算法； least_conn 最少连接数负载均衡算法； least_time 最短响应时间负载均衡算法； random 随机负载均衡算法； server定义上游服务器地址。 语法：server address [parameters] 上下文：upstream parameters 可选值： weight=number 权重值，默认为1； max_conns=number 上游服务器的最大并发连接数； fail_timeout=time 服务器不可用的判定时间； max_fails=numer 服务器不可用的检查次数； backup 备份服务器，仅当其他服务器都不可用时才会启用； down 标记服务器长期不可用，离线维护； keepalive限制每个 worker 子进程与上游服务器空闲长连接的最大数量。 keepalive connections; 上下文：upstream 示例：keepalive 16; keepalive_requests单个长连接可以处理的最多 HTTP 请求个数。 语法：keepalive_requests number; 默认值：keepalive_requests 100; 上下文：upstream keepalive_timeout空闲长连接的最长保持时间。 语法：keepalive_timeout time; 默认值：keepalive_timeout 60s; 上下文：upstream 配置实例 upstream back_end{ server 127.0.0.1:8081 weight=3 max_conns=1000 fail_timeout=10s max_fails=2; keepalive 32; keepalive_requests 50; keepalive_timeout 30s; } ","date":"2021-05-06","objectID":"/nginx-glance/:8:1","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#keepalive"},{"categories":["开发者手册"],"content":" upstream用于定义上游服务器（指的就是后台提供的应用服务器）的相关信息。 语法：upstream name { ... } 上下文：http 示例： upstream back_end_server{ server 192.168.100.33:8081 } 在 upstream 内可使用的指令： server 定义上游服务器地址； zone 定义共享内存，用于跨 worker 子进程； keepalive 对上游服务启用长连接； keepalive_requests 一个长连接最多请求 HTTP 的个数； keepalive_timeout 空闲情形下，一个长连接的超时时长； hash 哈希负载均衡算法； ip_hash 依据 IP 进行哈希计算的负载均衡算法； least_conn 最少连接数负载均衡算法； least_time 最短响应时间负载均衡算法； random 随机负载均衡算法； server定义上游服务器地址。 语法：server address [parameters] 上下文：upstream parameters 可选值： weight=number 权重值，默认为1； max_conns=number 上游服务器的最大并发连接数； fail_timeout=time 服务器不可用的判定时间； max_fails=numer 服务器不可用的检查次数； backup 备份服务器，仅当其他服务器都不可用时才会启用； down 标记服务器长期不可用，离线维护； keepalive限制每个 worker 子进程与上游服务器空闲长连接的最大数量。 keepalive connections; 上下文：upstream 示例：keepalive 16; keepalive_requests单个长连接可以处理的最多 HTTP 请求个数。 语法：keepalive_requests number; 默认值：keepalive_requests 100; 上下文：upstream keepalive_timeout空闲长连接的最长保持时间。 语法：keepalive_timeout time; 默认值：keepalive_timeout 60s; 上下文：upstream 配置实例 upstream back_end{ server 127.0.0.1:8081 weight=3 max_conns=1000 fail_timeout=10s max_fails=2; keepalive 32; keepalive_requests 50; keepalive_timeout 30s; } ","date":"2021-05-06","objectID":"/nginx-glance/:8:1","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#keepalive_requests"},{"categories":["开发者手册"],"content":" upstream用于定义上游服务器（指的就是后台提供的应用服务器）的相关信息。 语法：upstream name { ... } 上下文：http 示例： upstream back_end_server{ server 192.168.100.33:8081 } 在 upstream 内可使用的指令： server 定义上游服务器地址； zone 定义共享内存，用于跨 worker 子进程； keepalive 对上游服务启用长连接； keepalive_requests 一个长连接最多请求 HTTP 的个数； keepalive_timeout 空闲情形下，一个长连接的超时时长； hash 哈希负载均衡算法； ip_hash 依据 IP 进行哈希计算的负载均衡算法； least_conn 最少连接数负载均衡算法； least_time 最短响应时间负载均衡算法； random 随机负载均衡算法； server定义上游服务器地址。 语法：server address [parameters] 上下文：upstream parameters 可选值： weight=number 权重值，默认为1； max_conns=number 上游服务器的最大并发连接数； fail_timeout=time 服务器不可用的判定时间； max_fails=numer 服务器不可用的检查次数； backup 备份服务器，仅当其他服务器都不可用时才会启用； down 标记服务器长期不可用，离线维护； keepalive限制每个 worker 子进程与上游服务器空闲长连接的最大数量。 keepalive connections; 上下文：upstream 示例：keepalive 16; keepalive_requests单个长连接可以处理的最多 HTTP 请求个数。 语法：keepalive_requests number; 默认值：keepalive_requests 100; 上下文：upstream keepalive_timeout空闲长连接的最长保持时间。 语法：keepalive_timeout time; 默认值：keepalive_timeout 60s; 上下文：upstream 配置实例 upstream back_end{ server 127.0.0.1:8081 weight=3 max_conns=1000 fail_timeout=10s max_fails=2; keepalive 32; keepalive_requests 50; keepalive_timeout 30s; } ","date":"2021-05-06","objectID":"/nginx-glance/:8:1","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#keepalive_timeout"},{"categories":["开发者手册"],"content":" upstream用于定义上游服务器（指的就是后台提供的应用服务器）的相关信息。 语法：upstream name { ... } 上下文：http 示例： upstream back_end_server{ server 192.168.100.33:8081 } 在 upstream 内可使用的指令： server 定义上游服务器地址； zone 定义共享内存，用于跨 worker 子进程； keepalive 对上游服务启用长连接； keepalive_requests 一个长连接最多请求 HTTP 的个数； keepalive_timeout 空闲情形下，一个长连接的超时时长； hash 哈希负载均衡算法； ip_hash 依据 IP 进行哈希计算的负载均衡算法； least_conn 最少连接数负载均衡算法； least_time 最短响应时间负载均衡算法； random 随机负载均衡算法； server定义上游服务器地址。 语法：server address [parameters] 上下文：upstream parameters 可选值： weight=number 权重值，默认为1； max_conns=number 上游服务器的最大并发连接数； fail_timeout=time 服务器不可用的判定时间； max_fails=numer 服务器不可用的检查次数； backup 备份服务器，仅当其他服务器都不可用时才会启用； down 标记服务器长期不可用，离线维护； keepalive限制每个 worker 子进程与上游服务器空闲长连接的最大数量。 keepalive connections; 上下文：upstream 示例：keepalive 16; keepalive_requests单个长连接可以处理的最多 HTTP 请求个数。 语法：keepalive_requests number; 默认值：keepalive_requests 100; 上下文：upstream keepalive_timeout空闲长连接的最长保持时间。 语法：keepalive_timeout time; 默认值：keepalive_timeout 60s; 上下文：upstream 配置实例 upstream back_end{ server 127.0.0.1:8081 weight=3 max_conns=1000 fail_timeout=10s max_fails=2; keepalive 32; keepalive_requests 50; keepalive_timeout 30s; } ","date":"2021-05-06","objectID":"/nginx-glance/:8:1","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#配置实例"},{"categories":["开发者手册"],"content":" proxy_pass用于配置代理服务器。 语法：proxy_pass URL; 上下文：location、if、limit_except 示例： proxy_pass http://127.0.0.1:8081 proxy_pass http://127.0.0.1:8081/proxy URL 参数原则 URL 必须以 http 或 https 开头； URL 中可以携带变量； URL 中是否带 URI ，会直接影响发往上游请求的 URL ； 接下来让我们来看看两种常见的 URL 用法： proxy_pass http://192.168.100.33:8081 proxy_pass http://192.168.100.33:8081/ 这两种用法的区别就是带 / 和不带 / ，在配置代理时它们的区别可大了： 不带 / 意味着 Nginx 不会修改用户 URL ，而是直接透传给上游的应用服务器； 带 / 意味着 Nginx 会修改用户 URL ，修改方法是将 location 后的 URL 从用户 URL 中删除； 不带 / 的用法： location /bbs/{ proxy_pass http://127.0.0.1:8080; } 分析： 用户请求 URL ： /bbs/abc/test.html 请求到达 Nginx 的 URL ： /bbs/abc/test.html 请求到达上游应用服务器的 URL ： /bbs/abc/test.html 带 / 的用法： location /bbs/{ proxy_pass http://127.0.0.1:8080/; } 分析： 用户请求 URL ： /bbs/abc/test.html 请求到达 Nginx 的 URL ： /bbs/abc/test.html 请求到达上游应用服务器的 URL ： /abc/test.html 并没有拼接上 /bbs ，这点和 root 与 alias 之间的区别是保持一致的。 ","date":"2021-05-06","objectID":"/nginx-glance/:8:2","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#proxy_pass"},{"categories":["开发者手册"],"content":" 配置反向代理这里为了演示更加接近实际，作者准备了两台云服务器，它们的公网 IP 分别是： 121.42.11.34 与 121.5.180.193 。 我们把 121.42.11.34 服务器作为上游服务器，做如下配置： # /etc/nginx/conf.d/proxy.conf server{ listen 8080; server_name localhost; location /proxy/ { root /usr/share/nginx/html/proxy; index index.html; } } # /usr/share/nginx/html/proxy/index.html \u003ch1\u003e 121.42.11.34 proxy html \u003c/h1\u003e 配置完成后重启 Nginx 服务器 nginx -s reload 。 把 121.5.180.193 服务器作为代理服务器，做如下配置： # /etc/nginx/conf.d/proxy.conf upstream back_end { server 121.42.11.34:8080 weight=2 max_conns=1000 fail_timeout=10s max_fails=3; keepalive 32; keepalive_requests 80; keepalive_timeout 20s; } server { listen 80; server_name proxy.lion.club; location /proxy { proxy_pass http://back_end/proxy; } } 本地机器要访问 proxy.lion.club 域名，因此需要配置本地 hosts ，通过命令：vim /etc/hosts 进入配置文件，添加如下内容： 121.5.180.193 proxy.lion.club 分析： 当访问 proxy.lion.club/proxy 时通过 upstream 的配置找到 121.42.11.34:8080 ； 因此访问地址变为 http://121.42.11.34:8080/proxy ； 连接到 121.42.11.34 服务器，找到 8080 端口提供的 server ； 通过 server 找到 /usr/share/nginx/html/proxy/index.html 资源，最终展示出来。 ","date":"2021-05-06","objectID":"/nginx-glance/:8:3","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#配置反向代理"},{"categories":["开发者手册"],"content":" 配置负载均衡配置负载均衡主要是要使用 upstream 指令。 我们把 121.42.11.34 服务器作为上游服务器，做如下配置（ /etc/nginx/conf.d/balance.conf ）： server{ listen 8020; location / { return 200 'return 8020 \\n'; } } server{ listen 8030; location / { return 200 'return 8030 \\n'; } } server{ listen 8040; location / { return 200 'return 8040 \\n'; } } 配置完成后： nginx -t 检测配置是否正确； nginx -s reload 重启 Nginx 服务器； 执行 ss -nlt 命令查看端口是否被占用，从而判断 Nginx 服务是否正确启动。 把 121.5.180.193 服务器作为代理服务器，做如下配置（ /etc/nginx/conf.d/balance.conf ）： upstream demo_server { server 121.42.11.34:8020; server 121.42.11.34:8030; server 121.42.11.34:8040; } server { listen 80; server_name balance.lion.club; location /balance/ { proxy_pass http://demo_server; } } 配置完成后重启 Nginx 服务器。并且在需要访问的客户端配置好 ip 和域名的映射关系。 # /etc/hosts 121.5.180.193 balance.lion.club 在客户端机器执行 curl http://balance.lion.club/balance/ 命令： 不难看出，负载均衡的配置已经生效了，每次给我们分发的上游服务器都不一样。就是通过简单的轮询策略进行上游服务器分发。 接下来，我们再来了解下 Nginx 的其它分发策略。 ","date":"2021-05-06","objectID":"/nginx-glance/:8:4","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#配置负载均衡"},{"categories":["开发者手册"],"content":" hash 算法通过制定关键字作为 hash key ，基于 hash 算法映射到特定的上游服务器中。关键字可以包含有变量、字符串。 upstream demo_server { hash $request_uri; server 121.42.11.34:8020; server 121.42.11.34:8030; server 121.42.11.34:8040; } server { listen 80; server_name balance.lion.club; location /balance/ { proxy_pass http://demo_server; } } hash $request_uri 表示使用 request_uri 变量作为 hash 的 key 值，只要访问的 URI 保持不变，就会一直分发给同一台服务器。 ","date":"2021-05-06","objectID":"/nginx-glance/:8:5","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#hash-算法"},{"categories":["开发者手册"],"content":" ip_hash根据客户端的请求 ip 进行判断，只要 ip 地址不变就永远分配到同一台主机。它可以有效解决后台服务器 session 保持的问题。 upstream demo_server { ip_hash; server 121.42.11.34:8020; server 121.42.11.34:8030; server 121.42.11.34:8040; } server { listen 80; server_name balance.lion.club; location /balance/ { proxy_pass http://demo_server; } } ","date":"2021-05-06","objectID":"/nginx-glance/:8:6","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#ip_hash"},{"categories":["开发者手册"],"content":" 最少连接数算法各个 worker 子进程通过读取共享内存的数据，来获取后端服务器的信息。来挑选一台当前已建立连接数最少的服务器进行分配请求。 语法：least_conn; 上下文：upstream; 示例： upstream demo_server { zone test 10M; # zone可以设置共享内存空间的名字和大小 least_conn; server 121.42.11.34:8020; server 121.42.11.34:8030; server 121.42.11.34:8040; } server { listen 80; server_name balance.lion.club; location /balance/ { proxy_pass http://demo_server; } } 最后你会发现，负载均衡的配置其实一点都不复杂。 ","date":"2021-05-06","objectID":"/nginx-glance/:8:7","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#最少连接数算法"},{"categories":["开发者手册"],"content":" 配置缓存缓存可以非常有效的提升性能，因此不论是客户端（浏览器），还是代理服务器（ Nginx ），乃至上游服务器都多少会涉及到缓存。可见缓存在每个环节都是非常重要的。下面让我们来学习 Nginx 中如何设置缓存策略。 ","date":"2021-05-06","objectID":"/nginx-glance/:9:0","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#配置缓存"},{"categories":["开发者手册"],"content":" proxy_cache存储一些之前被访问过、而且可能将要被再次访问的资源，使用户可以直接从代理服务器获得，从而减少上游服务器的压力，加快整个访问速度。 语法：proxy_cache zone | off ; # zone 是共享内存的名称 默认值：proxy_cache off; 上下文：http、server、location ","date":"2021-05-06","objectID":"/nginx-glance/:9:1","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#proxy_cache"},{"categories":["开发者手册"],"content":" proxy_cache_path设置缓存文件的存放路径。 语法：proxy_cache_path path [level=levels] ...可选参数省略，下面会详细列举 默认值：proxy_cache_path off 上下文：http 参数含义： path 缓存文件的存放路径； level path 的目录层级； keys_zone 设置共享内存； inactive 在指定时间内没有被访问，缓存会被清理，默认10分钟； ","date":"2021-05-06","objectID":"/nginx-glance/:9:2","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#proxy_cache_path"},{"categories":["开发者手册"],"content":" proxy_cache_key设置缓存文件的 key 。 语法：proxy_cache_key 默认值：proxy_cache_key $scheme$proxy_host$request_uri; 上下文：http、server、location ","date":"2021-05-06","objectID":"/nginx-glance/:9:3","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#proxy_cache_key"},{"categories":["开发者手册"],"content":" proxy_cache_valid配置什么状态码可以被缓存，以及缓存时长。 语法：proxy_cache_valid [code...] time; 上下文：http、server、location 配置示例：proxy_cache_valid 200 304 2m;; # 说明对于状态为200和304的缓存文件的缓存时间是2分钟 ","date":"2021-05-06","objectID":"/nginx-glance/:9:4","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#proxy_cache_valid"},{"categories":["开发者手册"],"content":" proxy_no_cache定义相应保存到缓存的条件，如果字符串参数的至少一个值不为空且不等于“ 0”，则将不保存该响应到缓存。 语法：proxy_no_cache string; 上下文：http、server、location 示例：proxy_no_cache $http_pragma $http_authorization; ","date":"2021-05-06","objectID":"/nginx-glance/:9:5","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#proxy_no_cache"},{"categories":["开发者手册"],"content":" proxy_cache_bypass定义条件，在该条件下将不会从缓存中获取响应。 语法：proxy_cache_bypass string; 上下文：http、server、location 示例：proxy_cache_bypass $http_pragma $http_authorization; ","date":"2021-05-06","objectID":"/nginx-glance/:9:6","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#proxy_cache_bypass"},{"categories":["开发者手册"],"content":" upstream_cache_status 变量它存储了缓存是否命中的信息，会设置在响应头信息中，在调试中非常有用。 MISS: 未命中缓存 HIT： 命中缓存 EXPIRED: 缓存过期 STALE: 命中了陈旧缓存 REVALIDDATED: Nginx验证陈旧缓存依然有效 UPDATING: 内容陈旧，但正在更新 BYPASS: X响应从原始服务器获取 ","date":"2021-05-06","objectID":"/nginx-glance/:9:7","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#upstream_cache_status-变量"},{"categories":["开发者手册"],"content":" 配置实例我们把 121.42.11.34 服务器作为上游服务器，做如下配置（ /etc/nginx/conf.d/cache.conf ）： server { listen 1010; root /usr/share/nginx/html/1010; location / { index index.html; } } server { listen 1020; root /usr/share/nginx/html/1020; location / { index index.html; } } 把 121.5.180.193 服务器作为代理服务器，做如下配置（ /etc/nginx/conf.d/cache.conf ）： proxy_cache_path /etc/nginx/cache_temp levels=2:2 keys_zone=cache_zone:30m max_size=2g inactive=60m use_temp_path=off; upstream cache_server{ server 121.42.11.34:1010; server 121.42.11.34:1020; } server { listen 80; server_name cache.lion.club; location / { proxy_cache cache_zone; # 设置缓存内存，上面配置中已经定义好的 proxy_cache_valid 200 5m; # 缓存状态为200的请求，缓存时长为5分钟 proxy_cache_key $request_uri; # 缓存文件的key为请求的URI add_header Nginx-Cache-Status $upstream_cache_status # 把缓存状态设置为头部信息，响应给客户端 proxy_pass http://cache_server; # 代理转发 } } 缓存就是这样配置，我们可以在 /etc/nginx/cache_temp 路径下找到相应的缓存文件。 对于一些实时性要求非常高的页面或数据来说，就不应该去设置缓存，下面来看看如何配置不缓存的内容。 ... server { listen 80; server_name cache.lion.club; # URI 中后缀为 .txt 或 .text 的设置变量值为 \"no cache\" if ($request_uri ~ \\.(txt|text)$) { set $cache_name \"no cache\" } location / { proxy_no_cache $cache_name; # 判断该变量是否有值，如果有值则不进行缓存，如果没有值则进行缓存 proxy_cache cache_zone; # 设置缓存内存 proxy_cache_valid 200 5m; # 缓存状态为200的请求，缓存时长为5分钟 proxy_cache_key $request_uri; # 缓存文件的key为请求的URI add_header Nginx-Cache-Status $upstream_cache_status # 把缓存状态设置为头部信息，响应给客户端 proxy_pass http://cache_server; # 代理转发 } } ","date":"2021-05-06","objectID":"/nginx-glance/:9:8","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#配置实例-1"},{"categories":["开发者手册"],"content":" HTTPS在学习如何配置 HTTPS 之前，我们先来简单回顾下 HTTPS 的工作流程是怎么样的？它是如何进行加密保证安全的？ ","date":"2021-05-06","objectID":"/nginx-glance/:10:0","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#https"},{"categories":["开发者手册"],"content":" HTTPS 工作流程 客户端（浏览器）访问 https://www.baidu.com 百度网站； 百度服务器返回 HTTPS 使用的 CA 证书； 浏览器验证 CA 证书是否为合法证书； 验证通过，证书合法，生成一串随机数并使用公钥（证书中提供的）进行加密； 发送公钥加密后的随机数给百度服务器； 百度服务器拿到密文，通过私钥进行解密，获取到随机数（公钥加密，私钥解密，反之也可以）； 百度服务器把要发送给浏览器的内容，使用随机数进行加密后传输给浏览器； 此时浏览器可以使用随机数进行解密，获取到服务器的真实传输内容； 这就是 HTTPS 的基本运作原理，使用对称加密和非对称机密配合使用，保证传输内容的安全性。 关于HTTPS更多知识，可以查看作者的另外一篇文章《学习 HTTP 协议》 。 ","date":"2021-05-06","objectID":"/nginx-glance/:10:1","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#https-工作流程"},{"categories":["开发者手册"],"content":" 配置证书下载证书的压缩文件，里面有个 Nginx 文件夹，把 xxx.crt 和 xxx.key 文件拷贝到服务器目录，再进行如下配置： server { listen 443 ssl http2 default_server; # SSL 访问端口号为 443 server_name lion.club; # 填写绑定证书的域名(我这里是随便写的) ssl_certificate /etc/nginx/https/lion.club_bundle.crt; # 证书地址 ssl_certificate_key /etc/nginx/https/lion.club.key; # 私钥地址 ssl_session_timeout 10m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # 支持ssl协议版本，默认为后三个，主流版本是[TLSv1.2] location / { root /usr/share/nginx/html; index index.html index.htm; } } 如此配置后就能正常访问 HTTPS 版的网站了。 ","date":"2021-05-06","objectID":"/nginx-glance/:10:2","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#配置证书"},{"categories":["开发者手册"],"content":" 配置跨域 CORS先简单回顾下跨域究竟是怎么回事。 ","date":"2021-05-06","objectID":"/nginx-glance/:11:0","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#配置跨域-cors"},{"categories":["开发者手册"],"content":" 跨域的定义同源策略限制了从同一个源加载的文档或脚本如何与来自另一个源的资源进行交互。这是一个用于隔离潜在恶意文件的重要安全机制。通常不允许不同源间的读操作。 ","date":"2021-05-06","objectID":"/nginx-glance/:11:1","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#跨域的定义"},{"categories":["开发者手册"],"content":" 同源的定义如果两个页面的协议，端口（如果有指定）和域名都相同，则两个页面具有相同的源。 下面给出了与 URL http://store.company.com/dir/page.html 的源进行对比的示例: http://store.company.com/dir2/other.html 同源 https://store.company.com/secure.html 不同源，协议不同 http://store.company.com:81/dir/etc.html 不同源，端口不同 http://news.company.com/dir/other.html 不同源，主机不同 不同源会有如下限制： Web 数据层面，同源策略限制了不同源的站点读取当前站点的 Cookie 、 IndexDB 、 LocalStorage 等数据。 DOM 层面，同源策略限制了来自不同源的 JavaScript 脚本对当前 DOM 对象读和写的操作。 网络层面，同源策略限制了通过 XMLHttpRequest 等方式将站点的数据发送给不同源的站点。 ","date":"2021-05-06","objectID":"/nginx-glance/:11:2","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#同源的定义"},{"categories":["开发者手册"],"content":" Nginx 解决跨域的原理例如： 前端 server 的域名为： fe.server.com 后端服务的域名为： dev.server.com 现在我在 fe.server.com 对 dev.server.com 发起请求一定会出现跨域。 现在我们只需要启动一个 Nginx 服务器，将 server_name 设置为 fe.server.com 然后设置相应的 location 以拦截前端需要跨域的请求，最后将请求代理回 dev.server.com 。如下面的配置： server { listen 80; server_name fe.server.com; location / { proxy_pass dev.server.com; } } 这样可以完美绕过浏览器的同源策略： fe.server.com 访问 Nginx 的 fe.server.com 属于同源访问，而 Nginx 对服务端转发的请求不会触发浏览器的同源策略。 ","date":"2021-05-06","objectID":"/nginx-glance/:11:3","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#nginx-解决跨域的原理"},{"categories":["开发者手册"],"content":" 配置开启 gzip 压缩GZIP 是规定的三种标准 HTTP 压缩格式之一。目前绝大多数的网站都在使用 GZIP 传输 HTML 、CSS 、 JavaScript 等资源文件。 对于文本文件， GZiP 的效果非常明显，开启后传输所需流量大约会降至 1/4~1/3 。 并不是每个浏览器都支持 gzip 的，如何知道客户端是否支持 gzip 呢，请求头中的 Accept-Encoding 来标识对压缩的支持。 启用 gzip 同时需要客户端和服务端的支持，如果客户端支持 gzip 的解析，那么只要服务端能够返回 gzip 的文件就可以启用 gzip 了,我们可以通过 Nginx 的配置来让服务端支持 gzip 。下面的 respone 中 content-encoding:gzip ，指服务端开启了 gzip 的压缩方式。 在 /etc/nginx/conf.d/ 文件夹中新建配置文件 gzip.conf ： # # 默认off，是否开启gzip gzip on; # 要采用 gzip 压缩的 MIME 文件类型，其中 text/html 被系统强制启用； gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript; # ---- 以上两个参数开启就可以支持Gzip压缩了 ---- # # 默认 off，该模块启用后，Nginx 首先检查是否存在请求静态文件的 gz 结尾的文件，如果有则直接返回该 .gz 文件内容； gzip_static on; # 默认 off，nginx做为反向代理时启用，用于设置启用或禁用从代理服务器上收到相应内容 gzip 压缩； gzip_proxied any; # 用于在响应消息头中添加 Vary：Accept-Encoding，使代理服务器根据请求头中的 Accept-Encoding 识别是否启用 gzip 压缩； gzip_vary on; # gzip 压缩比，压缩级别是 1-9，1 压缩级别最低，9 最高，级别越高压缩率越大，压缩时间越长，建议 4-6； gzip_comp_level 6; # 获取多少内存用于缓存压缩结果，16 8k 表示以 8k*16 为单位获得； gzip_buffers 16 8k; # 允许压缩的页面最小字节数，页面字节数从header头中的 Content-Length 中进行获取。默认值是 0，不管页面多大都压缩。建议设置成大于 1k 的字节数，小于 1k 可能会越压越大； # gzip_min_length 1k; # 默认 1.1，启用 gzip 所需的 HTTP 最低版本； gzip_http_version 1.1; 其实也可以通过前端构建工具例如 webpack 、rollup 等在打生产包时就做好 Gzip 压缩，然后放到 Nginx 服务器中，这样可以减少服务器的开销，加快访问速度。 关于 Nginx 的实际应用就学习到这里，相信通过掌握了 Nginx 核心配置以及实战配置，之后再遇到什么需求，我们也能轻松应对。接下来，让我们再深入一点学习下 Nginx 的架构。 ","date":"2021-05-06","objectID":"/nginx-glance/:12:0","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#配置开启-gzip-压缩"},{"categories":["开发者手册"],"content":" Nginx 架构","date":"2021-05-06","objectID":"/nginx-glance/:13:0","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#nginx-架构"},{"categories":["开发者手册"],"content":" 进程结构多进程结构 Nginx 的进程模型图： 多进程中的 Nginx 进程架构如下图所示，会有一个父进程（ Master Process ），它会有很多子进程（ Child Processes ）。 Master Process 用来管理子进程的，其本身并不真正处理用户请求。 某个子进程 down 掉的话，它会向 Master 进程发送一条消息，表明自己不可用了，此时 Master 进程会去新起一个子进程。 某个配置文件被修改了 Master 进程会去通知 work 进程获取新的配置信息，这也就是我们所说的热部署。 子进程间是通过共享内存的方式进行通信的。 ","date":"2021-05-06","objectID":"/nginx-glance/:14:0","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#进程结构"},{"categories":["开发者手册"],"content":" 配置文件重载原理reload 重载配置文件的流程： 向 master 进程发送 HUP 信号（ reload 命令）； master 进程检查配置语法是否正确； master 进程打开监听端口； master 进程使用新的配置文件启动新的 worker 子进程； master 进程向老的 worker 子进程发送 QUIT 信号； 老的 worker 进程关闭监听句柄，处理完当前连接后关闭进程； 整个过程 Nginx 始终处于平稳运行中，实现了平滑升级，用户无感知； ","date":"2021-05-06","objectID":"/nginx-glance/:15:0","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#配置文件重载原理"},{"categories":["开发者手册"],"content":" Nginx 模块化管理机制Nginx 的内部结构是由核心部分和一系列的功能模块所组成。这样划分是为了使得每个模块的功能相对简单，便于开发，同时也便于对系统进行功能扩展。Nginx 的模块是互相独立的,低耦合高内聚。 ","date":"2021-05-06","objectID":"/nginx-glance/:16:0","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#nginx-模块化管理机制"},{"categories":["开发者手册"],"content":" 参考 万字总结，体系化带你全面认识 Nginx ！ ","date":"2021-05-06","objectID":"/nginx-glance/:17:0","series":null,"tags":["nginx"],"title":"Nginx 笔记，体系化带你全面认识 Nginx","uri":"/nginx-glance/#参考"},{"categories":["python"],"content":"xiaobinqt","date":"2021-05-06","objectID":"/py-glance/","series":null,"tags":["python"],"title":"python 学习笔记","uri":"/py-glance/"},{"categories":["python"],"content":" 安装 pip","date":"2021-05-06","objectID":"/py-glance/:1:0","series":null,"tags":["python"],"title":"python 学习笔记","uri":"/py-glance/#安装-pip"},{"categories":["python"],"content":" 安装 pip3环境为 armv7l apt-get install python3-pip 安装 pip3 安装成功 ","date":"2021-05-06","objectID":"/py-glance/:1:1","series":null,"tags":["python"],"title":"python 学习笔记","uri":"/py-glance/#安装-pip3"},{"categories":["python"],"content":" 安装 pip对于 python2 来说可以安装 pip： apt install python-pip ","date":"2021-05-06","objectID":"/py-glance/:1:2","series":null,"tags":["python"],"title":"python 学习笔记","uri":"/py-glance/#安装-pip-1"},{"categories":["python"],"content":" conda 和 pip 的区别 pip 仅仅是包管理工具，而 conda 不仅仅是包管理工具，conda 的功能比 pip 更多。 pip 仅限于 python 包的安装更新卸载，conda 包括且不限于 Python、C、R 等语言。 pip 能安装 pypi 里的一切 python 包。而 conda 可安装的 python 包数量相比 pip 要少很多。 pip 不支持创建 python 虚拟环境，得安装了 virtualenv 包才可以，而 conda 支持创建 python 虚拟环境。 pip install -r requirements.txt更加流畅，而 conda install -r时一旦未找到某个包，便会中断。 ","date":"2021-05-06","objectID":"/py-glance/:2:0","series":null,"tags":["python"],"title":"python 学习笔记","uri":"/py-glance/#conda-和-pip-的区别"},{"categories":["python"],"content":" 镜像","date":"2021-05-06","objectID":"/py-glance/:3:0","series":null,"tags":["python"],"title":"python 学习笔记","uri":"/py-glance/#镜像"},{"categories":["python"],"content":" 国内镜像源 https://pypi.douban.com/simple/ 豆瓣 https://mirrors.aliyun.com/pypi/simple/ 阿里 https://pypi.hustunique.com/simple/ 华中理工大学 https://pypi.sdutlinux.org/simple/ 山东理工大学 https://pypi.mirrors.ustc.edu.cn/simple/ 中国科学技术大学 https://pypi.tuna.tsinghua.edu.cn/simple 清华 ","date":"2021-05-06","objectID":"/py-glance/:3:1","series":null,"tags":["python"],"title":"python 学习笔记","uri":"/py-glance/#国内镜像源"},{"categories":["python"],"content":" 使用镜像安装可以参考这篇文章https://www.runoob.com/w3cnote/pip-cn-mirror.html ","date":"2021-05-06","objectID":"/py-glance/:3:2","series":null,"tags":["python"],"title":"python 学习笔记","uri":"/py-glance/#使用镜像安装"},{"categories":["python"],"content":" 更新 pip3 pip3 install --upgrade pip 或者 python3 -m pip install --upgrade pip ","date":"2021-05-06","objectID":"/py-glance/:4:0","series":null,"tags":["python"],"title":"python 学习笔记","uri":"/py-glance/#更新-pip3"},{"categories":["python"],"content":" 参考 python conda安装与使用教程 ","date":"2021-05-06","objectID":"/py-glance/:5:0","series":null,"tags":["python"],"title":"python 学习笔记","uri":"/py-glance/#参考"},{"categories":["开发者手册"],"content":"xiaobinqt,github PR,PR,pull request,如何使用 PR, Can’t automatically merge ","date":"2021-04-29","objectID":"/pull-request/","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/"},{"categories":["开发者手册"],"content":" 总览之前在 CSDN 上写过一篇关于 RP 的笔记 github fork PR 的简单使用 ，那篇文章写的比较随意且不是用命令行操作的，大部分操作都是基于 IDE，所以想着重新整理下那篇文章，同时也复习下 git 常用命令。 pull request ","date":"2021-04-29","objectID":"/pull-request/:1:0","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/#总览"},{"categories":["开发者手册"],"content":" 模拟场景公司有个项目为 xiao1996cc/git-dev ，有两个开发者，分别为 xiaobinqt 和 lovenarcissus 对这个项目进行开发，开发的新功能都会提交 PR 请求合并。 ","date":"2021-04-29","objectID":"/pull-request/:2:0","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/#模拟场景"},{"categories":["开发者手册"],"content":" fork两个开发者分别把项目 fork 项目到自己的账号下，以 xiaobinqt 为例： forking fork 成功后 xiaobinqt 现在已经 fork 了 xiao1996cc/git-dev 项目到自己的账号下 xiaobinqt/git-dev 。 ","date":"2021-04-29","objectID":"/pull-request/:3:0","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/#fork"},{"categories":["开发者手册"],"content":" xiaobinqt 开发新功能 每次开发新功能之前一定要先 fetch 下远程仓库。 ","date":"2021-04-29","objectID":"/pull-request/:4:0","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/#xiaobinqt-开发新功能"},{"categories":["开发者手册"],"content":" clonexiaobinqt fork 完成后，clone 项目到本地。 clone ","date":"2021-04-29","objectID":"/pull-request/:4:1","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/#clone"},{"categories":["开发者手册"],"content":" 添加远程仓库添加远程仓库 xiao1996cc/git-dev git remote add upstream git@github.com:xiao1996cc/git-dev.git 添加远程仓库 fetch 远程仓库 git fetch upstream fetch 远程仓库 ","date":"2021-04-29","objectID":"/pull-request/:4:2","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/#添加远程仓库"},{"categories":["开发者手册"],"content":" 新建分支并开发基于 upstream/main 创建新分支 dev_xiaobinqt git checkout -b dev_xiaobinqt upstream/main 创建 dev_xiaobinqt 分支 新建新文件 xiaobinqt.txt 并提交 新建文件并提交 push 成功后再 github 项目下可以看到成功提示 push 成功提示 ","date":"2021-04-29","objectID":"/pull-request/:4:3","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/#新建分支并开发"},{"categories":["开发者手册"],"content":" 新建 PR new pull request 选择要合并的分支并创建 pull request，这里我们将 xiabinqt/git-dev 下面的 dev_xiaobinqt 分支里的代码合并到 xiao1996cc/git-dev 下的 main 分支。 创建 pull request 创建 pull request 成功 ","date":"2021-04-29","objectID":"/pull-request/:4:4","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/#新建-pr"},{"categories":["开发者手册"],"content":" 合并 PRxiao1996cc/git-dev 项目下的 PR 只有管理这个项目的用户才有权限处理。管理员看到的界面是如下这样的： 管理者看到的 PR 界面 处理完的 PR 状态会变成 Merged Merge PR 至此 xiaobinqt 开发的一个功能已经成功提交到远程 upstream 仓库，也就是 xiao1996cc/git-dev 仓库，虽然只新建了一个文件，但是一个完整的 pull request 流程。 ","date":"2021-04-29","objectID":"/pull-request/:4:5","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/#合并-pr"},{"categories":["开发者手册"],"content":" lovenarcissus 开发新功能对于 lovenarcissus 开发者来说，也是先 fork 项目再 clone 项目到本地。 可以看到 lovenarcissus fork 后的项目是有 xiaobinqt.txt 这个文件的，同时也验证了 xiaobinqt 开发者成功向远程仓库提交了代码。这里默认已经 clone 到了本地仓库。 lovenarcissus fork ","date":"2021-04-29","objectID":"/pull-request/:5:0","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/#lovenarcissus-开发新功能"},{"categories":["开发者手册"],"content":" 添加远程仓库 添加远程仓库 fetch 远程仓库代码 git fetch upsteam ","date":"2021-04-29","objectID":"/pull-request/:5:1","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/#添加远程仓库-1"},{"categories":["开发者手册"],"content":" 新建分支并开发基于 upstream/main 创建新分支 dev_lovenarcissus 开发分支 git checkout -b dev_lovenarcissus upstream/main 新建开发分支 新建 lovenarcissus.txt 文件并修改 xiaobinqt.txt 文件 开发代码 提交代码 提交代码 ","date":"2021-04-29","objectID":"/pull-request/:5:2","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/#新建分支并开发-1"},{"categories":["开发者手册"],"content":" 新建 PR 新建 pull request ","date":"2021-04-29","objectID":"/pull-request/:5:3","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/#新建-pr-1"},{"categories":["开发者手册"],"content":" 合并 PR Merge pull request ","date":"2021-04-29","objectID":"/pull-request/:5:4","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/#合并-pr-1"},{"categories":["开发者手册"],"content":" 模拟并解决冲突","date":"2021-04-29","objectID":"/pull-request/:6:0","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/#模拟并解决冲突"},{"categories":["开发者手册"],"content":" 出现冲突现在开发者 xiaobinqt 和开发者 lovenarcissus 都提了代码到远程仓库 xiao1996cc/git-dev，仓库里文件如下： 远程代码仓库 现在开发者 xiaobinqt 需要开发一个新的功能，新增一个 xiaobinqt2.txt 文件，并且修改 xiaobinqt.txt 文件，但是这时 xiaobinqt 并没有 fetch 合并远程仓库的代码，xiaobinqt 本地仓库文件如下： 文件列表 直接新建 xiaobinqt2.txt 文件，并且修改 xiaobinqt.txt 文件 新建并修改文件 push 代码到远程仓库，并创建 pull request push 代码 可以看到在创建 pull request 时出现了冲突，提示 Can’t automatically merge ， PR 出现冲突 ","date":"2021-04-29","objectID":"/pull-request/:6:1","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/#出现冲突"},{"categories":["开发者手册"],"content":" 解决冲突出现冲突的原因是本地代码跟 origin 仓库的代码是一样的，但是 origin 仓库跟 upstream 仓库代码不一致。我们需要先 fetch 下 upstream 仓库的代码跟本地代码合并后再 push 到 origin 仓库，再从 origin 仓库提 pull request 到 upsteam 仓库。 由👇图可知，在 merge upstream/main 分支时出现了冲突 Automatic merge failed; fix conflicts and then commit the result. merge conflicts vim 打开冲突文件 xiaobinqt.txt 解决冲突 冲突文件内容 解决冲突后的文件 👇 解决冲突后的文件 以下👇是解决这次冲突的具体步骤： 解决冲突具体步骤 ","date":"2021-04-29","objectID":"/pull-request/:6:2","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/#解决冲突"},{"categories":["开发者手册"],"content":" 重提 PR这时对于 xiaobinqt 开发者来说，冲突已经解决，可以重新提 pull request 👇，可以看到现在的状态是 Able to merge.。 重提 PR 创建新的 pull request 成功 创建新的 PR 成功 ","date":"2021-04-29","objectID":"/pull-request/:6:3","series":null,"tags":["git"],"title":"github pull request","uri":"/pull-request/#重提-pr"},{"categories":["开发者手册"],"content":"mongodb , mongodb 概念,mongodb 查询语句,golang 操作 mongodb,globalsign/mgo 使用","date":"2021-04-14","objectID":"/mongodb-glance/","series":null,"tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/"},{"categories":["开发者手册"],"content":" win10 安装在 windows 下安装可以参考这篇文章mongodb-window-install。 ","date":"2021-04-14","objectID":"/mongodb-glance/:1:0","series":null,"tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/#win10-安装"},{"categories":["开发者手册"],"content":" 小坑我使用的是 windows 10 企业版，在安装时出现了个问题，如下： 问题截图 我是在网上找了大半天没有找到解决的办法，都是写文章作者可用，但是我一直不生效，我觉得的必须要用管理员权限安装导致的。后来我直接忽略了，用管理员权限运行。 ","date":"2021-04-14","objectID":"/mongodb-glance/:1:1","series":null,"tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/#小坑"},{"categories":["开发者手册"],"content":" 运行服务端用管理员 power shell 运行 具体命令可以参看文档 运行 mongodb 服务 01 .\\mongod.exe --dbpath D:\\mySoft\\mongoDB\\data\\db 运行 mongodb 服务 02 ","date":"2021-04-14","objectID":"/mongodb-glance/:1:2","series":null,"tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/#运行服务端"},{"categories":["开发者手册"],"content":" 运行客户端 .\\mongo.exe 客户端连接 ","date":"2021-04-14","objectID":"/mongodb-glance/:1:3","series":null,"tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/#运行客户端"},{"categories":["开发者手册"],"content":" Navicat Premiumnavicat premium 是一个数据库管理工具，可以支持 mysql，mongodb，oracle 等几乎所有的数据库。 navicat premium navicat premuim 使用 windows 安转教程可以参考navicat premium15破解教程 ","date":"2021-04-14","objectID":"/mongodb-glance/:2:0","series":null,"tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/#navicat-premium"},{"categories":["开发者手册"],"content":" 概念解析 SQL术语/概念 MongoDB术语/概念 说明 database database 数据库 table collection 数据库表/集合 row document 数据记录行/文档 column field 数据字段/域 index index 索引 table joins - 表连接，MongoDB 不支持 primary key primary key 主键，MongoDB 自动将 _id 字段设置为主键 RDBMS（关系型数据库） MongoDB 数据库 数据库 表格 集合 行 文档 列 字段 表联合 嵌入文档 主键 主键 (MongoDB 提供了 key 为 _id ) 对比图 ","date":"2021-04-14","objectID":"/mongodb-glance/:3:0","series":null,"tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/#概念解析"},{"categories":["开发者手册"],"content":" 常用命令 CMD 说明 db.help() 查看命令帮助 show dbs 查看所有数据库 use db_name 如果数据库不存在，则创建数据库，否则切换到指定数据库 show tables 查看该库下的所有表 db.getName() 查看当前所在的库 db 命令可以显示当前数据库对象或集合 db.version() 当前db版本 db.stats() 当前db状态 db.getMongo() 查看当前db的链接机器地址 db.dropDatabase() 删除当前使用的数据库 db.copyDatabase(\"mydb\", \"temp\", \"127.0.0.1\") 从指定的机器上复制指定数据库数据到某个数据库，本示例为：将本机的 mydb 的数据复制到 temp 数据库中 db.cloneDatabase(\"127.0.0.1\") 将指定机器上的数据库的数据克隆到当前数据库 db.repairDatabase() 修复当前数据库 show collections/show tables 查看已有集合 db.yourColl.help() 查看帮助 db.yourColl.count() 查询当前集合的数据条数 db.yourColl.dataSize() 查看数据空间大小 db.yourColl.getDB() 得到当前聚集集合所在的 db db.yourColl.stats() 得到当前集合的状态 db.yourColl.totalSize() 得到集合总大小 db.yourColl.storageSize() 聚集集合储存空间大小 db.yourColl.getShardVersion() Shard版本信息 db.userInfo.renameCollection(\"users\") 集合重命名。示例为：将 userInfo 重命名为 users db.yourColl.drop() 删除当前集合 yourColl 表示集合名 ","date":"2021-04-14","objectID":"/mongodb-glance/:4:0","series":null,"tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/#常用命令"},{"categories":["开发者手册"],"content":" 集合查询 📌 查询所有记录 db.mgotest.find() # 格式化输出 db.mgotest.find().pretty() 对比 SQL：select * from mgotest; 📌 查询去掉后的当前集合中的某列的重复数据 db.mgotest.distinct(\"interests\") 对比 SQL：select distinct interests from mgotest; distinct 📌 查询 age = 22 的记录 db.userInfo.find({\"age\": 22}); 对比 SQL：select * from userInfo where age = 22; 📌 查询 age \u003e 22 的记录 db.userInfo.find({age: {$gt: 22}}) 对比 SQL：select * from userInfo where age \u003e22; 📌 查询 age \u003c 22 的记录 db.userInfo.find({age: {$lt: 22}}); 对比 SQL：select * from userInfo where age \u003c 22; 📌 查询 age \u003e= 25 的记录 db.userInfo.find({age: {$gte: 25}}); 对比 SQL：select * from userInfo where age \u003e= 25; 📌 查询 age \u003c= 25 的记录 db.userInfo.find({age: {$lte: 25}}); 📌 查询 age \u003e= 23 并且 age \u003c= 26 db.userInfo.find({age: {$gte: 23, $lte: 26}}); 📌 查询 name 中包含 mongo 的数据 db.userInfo.find({name: /mongo/}); 对比 SQL：select * from userInfo where name like ‘%mongo%'; 📌 查询 name 中以 mongo 开头的 db.userInfo.find({name: /^mongo/}); 对比 SQL：select * from userInfo where name like ‘mongo%'; 📌 查询指定列 name、age 数据 db.userInfo.find({}, {name: 1, age: 1}); 对比 SQL：select name, age from userInfo; 📌 查询指定列 name、age 数据, age \u003e 25 db.userInfo.find({age: {$gt: 25}}, {name: 1, age: 1}); 对比 SQL：select name, age from userInfo where age \u003e25; 📌 按照年龄排序 # 升序 db.userInfo.find().sort({age: 1}); # 降序 db.userInfo.find().sort({age: -1}); 📌 查询 name = zhangsan, age = 22 的数据 db.userInfo.find({name: 'zhangsan', age: 22}); 对比 SQL：select * from userInfo where name = 'zhangsan' and age = '22'; 📌 查询前 5 条数据 db.userInfo.find().limit(5); 对比 SQL：select * from userInfo limit 5; 📌 查询 10 条以后的数据 db.userInfo.find().skip(10); 对比 SQL： select * from userInfo where id not in (select id from userInfo limit 10 ); 📌 查询在 5-10 之间的数据 db.userInfo.find().limit(10).skip(5); 可用于分页，limit 是 pageSize，skip 是 (第几页 * pageSize) 📌 or 查询 db.userInfo.find({$or: [{age: 22}, {age: 25}]}); 对比 SQL：select * from userInfo where age = 22 or age = 25; 📌 查询某个结果集的记录条数 db.userInfo.find({age: {$gte: 25}}).count(); 对比 SQL：select count(*) from userInfo where age \u003e= 20; 📌 按照某列进行排序 db.userInfo.find({sex: {$exists: true}}).count(); 对比 SQL：select count(sex) from userInfo; ","date":"2021-04-14","objectID":"/mongodb-glance/:5:0","series":null,"tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/#集合查询"},{"categories":["开发者手册"],"content":" 更新文档 db.collection.update( \u003cquery\u003e, \u003cupdate\u003e, { upsert: \u003cboolean\u003e, multi: \u003cboolean\u003e, writeConcern: \u003cdocument\u003e } ) # 示例 db.col.update({'title':'MongoDB 教程'},{$set:{'title':'MongoDB'}}) 参数说明： query : update 的查询条件，类似 sql update 查询内 where 后面的。 update : update 的对象和一些更新的操作符（如$,$inc…）等，也可以理解为 sql update 查询内 set 后面的 upsert : 可选，这个参数的意思是，如果不存在 update 的记录，是否插入 objNew, true 为插入，默认是 false ，不插入。 multi : 可选，mongodb 默认是 false,只更新找到的第一条记录，如果这个参数为 true, 就把按条件查出来多条记录全部更新。 writeConcern :可选，抛出异常的级别。 ","date":"2021-04-14","objectID":"/mongodb-glance/:6:0","series":null,"tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/#更新文档"},{"categories":["开发者手册"],"content":" $push$push 操作符添加指定的值到数组中，不去重。 ","date":"2021-04-14","objectID":"/mongodb-glance/:6:1","series":null,"tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/#push"},{"categories":["开发者手册"],"content":" $addToSet$addToSet 这个方法向数组中增加值，自动去重。 ","date":"2021-04-14","objectID":"/mongodb-glance/:6:2","series":null,"tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/#addtoset"},{"categories":["开发者手册"],"content":" globalsign/mgo 使用globalsign/mgo 是 Go 的 MongoDB 驱动，我现在维护的项目也在使用这个，在这里简单介绍一下。 package main import ( \"fmt\" \"time\" \"github.com/globalsign/mgo\" \"github.com/globalsign/mgo/bson\" ) type User struct { Id bson.ObjectId `bson:\"_id\" json:\"id\"` Username string `bson:\"name\" json:\"username\"` Pass string `bson:\"pass\" json:\"pass\"` Regtime int64 `bson:\"regtime\" json:\"regtime\"` Interests []string `bson:\"interests\" json:\"interests\"` } const URL string = \"127.0.0.1:27017\" var ( c *mgo.Collection session *mgo.Session ) func (user User) ToString() string { return fmt.Sprintf(\"%#v\", user) } func init() { session, _ = mgo.Dial(URL) // 切换到数据库 db := session.DB(\"blog\") // 切换到collection c = db.C(\"mgotest\") } // 新增数据 func add() { // defer session.Close() stu1 := new(User) stu1.Id = bson.NewObjectId() stu1.Username = \"stu_name\" + time.Now().String() stu1.Pass = \"stu1_pass\" stu1.Regtime = time.Now().Unix() stu1.Interests = []string{\"象棋\", \"游泳\", \"跑步\"} err := c.Insert(stu1) if err == nil { fmt.Println(\"insert success\") } else { fmt.Printf(\"insert error:%s \\n\", err.Error()) } } // 查询 func find() { // defer session.Close() var ( users []User err error ) // c.Find(nil).All(\u0026users) err = c.Find(bson.M{\"name\": \"stu_name\"}).All(\u0026users) if err != nil { fmt.Printf(\"find err:%s \\n\", err.Error()) return } for index, value := range users { fmt.Printf(\"index:%d,val:%s \\n\", index, value.ToString()) } // 根据ObjectId进行查询 // idStr := \"577fb2d1cde67307e819133d\" // objectId := bson.ObjectIdHex(idStr) // user := new(User) // err = c.Find(bson.M{\"_id\": objectId}).One(user) // if err != nil { // fmt.Printf(\"db find err:%s \\n\", err.Error()) // return // } // fmt.Println(\"查找成功..\", user) } // 根据id进行修改 func update() { interests := []string{\"象棋\", \"游泳\", \"跑步\"} err := c.Update(bson.M{\"_id\": bson.ObjectIdHex(\"6076c3954e947b3944d4a38b\")}, bson.M{\"$set\": bson.M{ \"name\": \"修改后的name\", \"pass\": \"修改后的pass\", \"regtime\": time.Now().Unix(), \"interests\": interests, }}) if err != nil { fmt.Println(\"修改失败\") } else { fmt.Println(\"修改成功\") } } // 删除 func del() { err := c.Remove(bson.M{\"_id\": bson.ObjectIdHex(\"6076c3954e947b3944d4a38b\")}) if err != nil { fmt.Println(\"删除失败\" + err.Error()) } else { fmt.Println(\"删除成功\") } } func main() { add() find() update() del() } ","date":"2021-04-14","objectID":"/mongodb-glance/:7:0","series":null,"tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/#globalsignmgo-使用"},{"categories":["开发者手册"],"content":" FAQ","date":"2021-04-14","objectID":"/mongodb-glance/:8:0","series":null,"tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/#faq"},{"categories":["开发者手册"],"content":" 空库不显示 show dbs 上图👆用 use test1 新建了一个数据库 test1，但是用 show dbs 却没有显示 test1，这是因为 test1 是空的，插入一条数据就可以显示。 显示新建的库 ","date":"2021-04-14","objectID":"/mongodb-glance/:8:1","series":null,"tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/#空库不显示"},{"categories":["开发者手册"],"content":" 定长表最近有个需求是，需要某个集合实现只保留固定数量的记录，自动淘汰老旧数据。 通过创建集合的命令 db.createCollection(name, options) 可知，有个可选的 options 参数： 参数说明： name: 要创建的集合名称 options: 可选参数, 指定有关内存大小及索引的选项 options 可以是如下参数： 字段 类型 描述 capped 布尔 （可选）如果为 true，则创建固定集合。固定集合是指有着固定大小的集合，当达到最大值时，它会自动覆盖最早的文档。当该值为 true 时，必须指定 size 参数。 autoIndexId 布尔 3.2 之后不再支持该参数。（可选）如为 true，自动在 _id 字段创建索引。默认为 false。 size 数值 （可选）为固定集合指定一个最大值，即字节数 。如果 capped 为 true，也需要指定该字段。 max 数值 （可选）指定固定集合中包含文档的最大数量。 如果是新建一个集合，这种方式肯定是可以的，但是如果要同步老数据呢？ 比如我有一个集合 daily_report 集合，里面有一些老数据： daily_report 集合 我的想法是，将 daily_report 重命名为 daily_report_bak，新建集合 daily_report，就 daily_report_bak 数据同步到 daily_report。 流程 重命名集合： db.daily_report.renameCollection(\"daily_report_bak\") 新建集合： db.createCollection(\"daily_report\", {capped:true,size:6142800,max:4}) 该集合最大值字节数为：6142800字节 = 6142800B ≈ 6000KB ≈ 5M 。 该集合中包含文档的最大数量为 4 条。 同步旧数据 db.daily_report_bak.find().forEach(function(doc){db.daily_report.insert(doc)}) ","date":"2021-04-14","objectID":"/mongodb-glance/:8:2","series":null,"tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/#定长表"},{"categories":["开发者手册"],"content":" 参考 MongoDB 教程 MongoDB高级查询 MongoDB常用操作命令大全 查询, 更新, 投射, 和集合算符 Field Update Operators ","date":"2021-04-14","objectID":"/mongodb-glance/:9:0","series":null,"tags":["mongodb"],"title":"Mongodb 学习笔记","uri":"/mongodb-glance/#参考"},{"categories":["golang"],"content":"golang upd 简单使用,UDP,HTTP","date":"2021-04-01","objectID":"/upd-demo/","series":null,"tags":["golang"],"title":"golang udp 简单使用","uri":"/upd-demo/"},{"categories":["golang"],"content":" server package main import ( \"fmt\" \"net\" \"time\" ) func main() { // 创建监听 socket, err := net.ListenUDP(\"udp4\", \u0026net.UDPAddr{ IP: []byte{127, 0, 0, 1}, Port: 8080, }) if err != nil { fmt.Println(\"监听失败!\", err) return } defer socket.Close() for { // 读取数据 data := make([]byte, 4096) read, remoteAddr, err := socket.ReadFromUDP(data) if err != nil { fmt.Println(\"读取数据失败!\", err) continue } fmt.Println(read, remoteAddr) fmt.Printf(\"接收到客户端数据，%s\\n\\n\", data) // 发送数据 senddata := []byte(\"server send data，hello client!\" + time.Now().Format(\"2006-01-02 15:04:05\")) _, err = socket.WriteToUDP(senddata, remoteAddr) if err != nil { fmt.Println(\"发送数据失败!\", err.Error()) return } } } ","date":"2021-04-01","objectID":"/upd-demo/:1:0","series":null,"tags":["golang"],"title":"golang udp 简单使用","uri":"/upd-demo/#server"},{"categories":["golang"],"content":" client package main import ( \"fmt\" \"net\" \"time\" ) func main() { // 创建连接 socket, err := net.DialUDP(\"udp4\", nil, \u0026net.UDPAddr{ IP: []byte{127, 0, 0, 1}, Port: 8080, }) if err != nil { fmt.Println(\"连接失败!\", err) return } defer socket.Close() // 发送数据 senddata := []byte(\"client send message，hello server!\" + time.Now().Format(\"2006-01-02 15:04:05\")) _, err = socket.Write(senddata) if err != nil { fmt.Println(\"发送数据失败!\", err) return } // 接收数据 data := make([]byte, 4096) read, remoteAddr, err := socket.ReadFromUDP(data) if err != nil { fmt.Println(\"读取数据失败!\", err) return } fmt.Println(read, remoteAddr) fmt.Printf(\"接收到服务器端数据，%s\\n\", data) } ","date":"2021-04-01","objectID":"/upd-demo/:2:0","series":null,"tags":["golang"],"title":"golang udp 简单使用","uri":"/upd-demo/#client"},{"categories":["golang"],"content":" 源码源码地址 ","date":"2021-04-01","objectID":"/upd-demo/:3:0","series":null,"tags":["golang"],"title":"golang udp 简单使用","uri":"/upd-demo/#源码"},{"categories":["开发者手册"],"content":"xiaobinqt,vmware,centos 7.9,静态 ip,vmware 安装,vmware 安装 CentOS 7.x,vmware 设置静态 ip,centos7,ifconfig command not found,修改网卡为 eth0","date":"2021-03-08","objectID":"/vmware-install-centos7.x/","series":null,"tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/"},{"categories":["开发者手册"],"content":" 下载镜像在 CentOS 的官网 https://wiki.centos.org/Download 可以下载 CentOS 各个版本的镜像文件。 CentOS Download 包括已经不在维护的各个版本： Archived Versions 也可以去阿里的镜像仓库去下载 mirrors.aliyun.com/centos。 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:1:0","series":null,"tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/#下载镜像"},{"categories":["开发者手册"],"content":" 安装 CentOS 7.9下载完 CentOS-7-x86_64-Minimal-2009 就可以安装了，这里用的是 Minimal 版本，安装完成后，系统中只有最基本的组件，方便学习。 新建虚拟机 典型配置 选择 ios 文件 命名虚拟机并选择系统文件保存路径： 命名虚拟机 如果物理机支持大与 4GB 以上的单文件，可以选择“将虚拟磁盘存储为单个文件” 指定磁盘容量 自定义硬件 由于服务器用不到声卡，USB 控制器，打印机这些设备，可以将这些设备移除： 移除硬件 移除后的硬件 完成 开启虚拟机 Install CentOS7 安装中 选择语言 点击 INSTALLATION DESTINATION INSTALLATION DESTINATION 默认值不用改，直接点 Done INSTALLATION DESTINATION Done Begin Installation 设置 ROOT 密码和创建用户 设置 ROOT 密码和创建用户 创建用户 等待安装任务 等待安装任务 安装任务完成重启 重启完成登录 root Login ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:2:0","series":null,"tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/#安装-centos-79"},{"categories":["开发者手册"],"content":" 网络配置一般新装的最小化的 CentOS 7.x 系统是没有网络配置的，而安装命令就是联网下载软件，所以网络配置是必须的。 ping CentOS 7.x 的默认网卡是 essxx，我们可以在配置文件 /etc/syconfig/network-scripts 中看到： network-scripts 可以先把 ifcfg-ensxx 这个文件备份下，然后修改 onboot 参数从 no 修改为 yes ，保存，重启机器。 update onboot 重启完后就可以 ping 通网络了： ping 2 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:3:0","series":null,"tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/#网络配置"},{"categories":["开发者手册"],"content":" ifconfig command not found安装 net-tools 工具 yum install -y net-tools net-tools install ifconfig ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:4:0","series":null,"tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/#ifconfig-command-not-found"},{"categories":["开发者手册"],"content":" 修改网卡为 eth0CentOS 7.x 的网卡不是 eth0 而是 ensxx，这是 CentOS 7.x 的一致性网络设备命名导致的，可以使用以下方式，将网卡名称回到 eth0 格式。 ensxx 修改配置 vim /etc/default/grub 中添加 biosdevname=0 net.ifnames=0 biosdevname=0 net.ifnames=0 执行命令 grub2-mkconfig -o /boot/grub2/grub.cfg 执行命令 reboot 重启机器 修改成功 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:5:0","series":null,"tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/#修改网卡为-eth0"},{"categories":["开发者手册"],"content":" 设置静态 IPNAT 模式是 VMware 虚拟机默认使用的模式，其最大的优势就是虚拟机接入网络非常简单，只要物理机可以访问网络，虚拟机就可以访问网络。网络结构如下图： NAT模式 所谓的静态 ip ，就是设置后固定不变的，因为在真实环境中，需要为所有的服务器配置静态 ip，从而确保通过一个 ip 地址只能找到一台服务器。 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:6:0","series":null,"tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/#设置静态-ip"},{"categories":["开发者手册"],"content":" 设置在修改配置文件之前，为了防止配置出错，建议提前备份配置文件 ifcfg-eth0。我把原来的 ifcfg-ens32 重命名为了 ifcfg-eth0。 cp /etc/sysconfig/network-scripts/ifcfg-eth0 /etc/sysconfig/network-scripts/ifcfg-eth0.bak 我们需要把 ifcfg-eth0 配置文件中的 BOOTPROTO 的值设置为 static，将 IPADDR(IP 地址)的值设置为其所在的子网中正确的，无冲突的 ip 地址即可。 在 NAT 模式中，我们需要找到 4 个地址才能确定我们的无冲突的 ip 到底是哪些。 子网 ip VMnet8 虚拟网卡 ip NAT 网关 ip DHCP 地址池 接下来找这几个参数： 图 1 图 2 点击【NAT设置】查看子网掩码和网关 IP。点击【DHCP 设置】查看起始 IP 地址和结束 IP 地址。 图 3 图 4 打开物理机的 cmd 输入 ipconfig 命令： 图 5 所以，除去这几个地址，192.168.48.3 ~ 192.168.48.127 范围内的 ip 都可以作为静态 ip 使用。 vi ifcfg-eth0 需要修改的地方为： 将 ONBOOT 改为 yes BOOTPROTO 由 dhcp 改为 static 增加 IPADDR(ip 地址) 增加 NETMASK(子网掩码) 增加 GATEWAY(网关) 增加 DNS1(首选域名服务器) 其中，网关不设置，虚拟机只能在局域网内访问，无法访问外部网络。DNS 不设置则无法解析域名。 DNS 可以设置成跟网关一样的地址。 图 6 设置完成后执行：systemctl restart network 命令使配置生效。 图 7 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:6:1","series":null,"tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/#设置"},{"categories":["开发者手册"],"content":" 访问测试","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:6:2","series":null,"tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/#访问测试"},{"categories":["开发者手册"],"content":" 物理机测试在物理机中 ping 虚拟机 ip 地址： 图 8 物理机共向 ip 地址 192.168.48.8 发送了 4 次 ping 请求，4 次都是成功的，发送的数据包为 32 字节，TTL(生存时间值)为 64，其中 TTL 在发送时的默认值为 64，每经过一个路由则减 1 ，此次显示最终结果为 64 说明中间没有经过路由。 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:6:3","series":null,"tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/#物理机测试"},{"categories":["开发者手册"],"content":" 虚拟机测试 图 9 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:6:4","series":null,"tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/#虚拟机测试"},{"categories":["开发者手册"],"content":" 说明因为我把默认网卡从 ensxx 改成了 eth0 所以在修改静态 ip 是把配置文件也改成了 ifcfg-eth0 图 10 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:6:5","series":null,"tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/#说明"},{"categories":["开发者手册"],"content":" FAQ ❓重启网卡问题 loaded (/etc/rc.d/init.d/network; bad; vendor preset: disabled) network err ☝️ 以上这个问题可以参考centos7重启网卡提示错误的解决方法这篇文章。 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:7:0","series":null,"tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/#faq"},{"categories":["开发者手册"],"content":" 参考 How To Configure Static IP Address in CentOS 7 / RHEL 7 ","date":"2021-03-08","objectID":"/vmware-install-centos7.x/:8:0","series":null,"tags":["CentOS 7.x","VMware"],"title":"VMware 安装 CentOS 7.x","uri":"/vmware-install-centos7.x/#参考"},{"categories":["开发者手册"],"content":"Redis, Redis 数据结构,redis 缓存击穿,redis 布隆过滤器,redis 淘汰机制,redis 持久化机制,redis 分布式缓存","date":"2021-02-20","objectID":"/redis-glance-faq/","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/"},{"categories":["开发者手册"],"content":"Redis 是一个使用 C 语言开发的数据库，与传统数据库不同的是 Redis 的数据是存在内存中的，我们把这种数据库叫做内存数据库。因为在内存中，所以读写速度非常快，因此 Redis 被广泛应用于缓存方向。 Redis 提供了多种数据类型来支持不同的业务场景，所以 Redis 除了做缓存之外，Redis 还经常用来做分布式锁，消息队列等。Redis 还支持事务、持久化、Lua 脚本、多集群方案。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:0:0","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#"},{"categories":["开发者手册"],"content":" 数据结构 Redis 五种数据结构 在 Redis 中，所有的 key 都是字符串。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:1:0","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#数据结构"},{"categories":["开发者手册"],"content":" 字符串字符串类型是 Redis 中最基本的数据类型，一个 key 对应一个 value。 字符串类型是二进制安全的，也就是说 Redis 中的字符串可以包含任何数据。如数字，字符串，jpg 图片或者序列化的对象。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:1:1","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#字符串"},{"categories":["开发者手册"],"content":" Hashhash 值本身又是一种键值对结构： hash 所有 hash 的命令都是 h 开头，如 hget、hset、hdel 等 ","date":"2021-02-20","objectID":"/redis-glance-faq/:1:2","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#hash"},{"categories":["开发者手册"],"content":" ListList 就是链表（redis 使用双端链表实现的 List），是有序的，value可以重复，可以通过下标取出对应的 value 值，左右两边都能进行插入和删除数据。 List 常用命令 使用技巧 lpush + lpop = Stack(栈) lpush + rpop = Queue（队列） lpush + ltrim = Capped Collection（有限集合） lpush + brpop = Message Queue（消息队列） ","date":"2021-02-20","objectID":"/redis-glance-faq/:1:3","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#list"},{"categories":["开发者手册"],"content":" ListList 就是链表（redis 使用双端链表实现的 List），是有序的，value可以重复，可以通过下标取出对应的 value 值，左右两边都能进行插入和删除数据。 List 常用命令 使用技巧 lpush + lpop = Stack(栈) lpush + rpop = Queue（队列） lpush + ltrim = Capped Collection（有限集合） lpush + brpop = Message Queue（消息队列） ","date":"2021-02-20","objectID":"/redis-glance-faq/:1:3","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#常用命令"},{"categories":["开发者手册"],"content":" ListList 就是链表（redis 使用双端链表实现的 List），是有序的，value可以重复，可以通过下标取出对应的 value 值，左右两边都能进行插入和删除数据。 List 常用命令 使用技巧 lpush + lpop = Stack(栈) lpush + rpop = Queue（队列） lpush + ltrim = Capped Collection（有限集合） lpush + brpop = Message Queue（消息队列） ","date":"2021-02-20","objectID":"/redis-glance-faq/:1:3","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#使用技巧"},{"categories":["开发者手册"],"content":" Set Set 集合类型也是用来保存多个字符串的元素，但和列表有几个不同的点： 👉 不允许有重复的元素 👉 集合中的元素是无序的，不能通过索引下标获取元素 👉 支持集合间的操作，可以取多个集合取交集、并集、差集 ","date":"2021-02-20","objectID":"/redis-glance-faq/:1:4","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#set"},{"categories":["开发者手册"],"content":" ZSet有序集合和集合有着必然的联系，保留了集合不能有重复成员的特性。 区别是，有序集合中的元素是可以排序的，它给每个元素设置一个分数，作为排序的依据。 有序集合中的元素不可以重复，但是 score 分数可以重复，就和一个班里的同学学号不能重复，但考试成绩可以相同。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:1:5","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#zset"},{"categories":["开发者手册"],"content":" FAQ","date":"2021-02-20","objectID":"/redis-glance-faq/:2:0","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#faq"},{"categories":["开发者手册"],"content":" 分布式缓存分布式缓存主要解决的是单机缓存的容量受服务器限制并且无法保存通用的信息，因为，本地缓存只在当前服务里有效。比如部署了两个相同的服务，他们两者之间的缓存数据是无法共同的。 分布式缓存的话，使用的较多的较多的解决方案是是 Memcached 和 Redis。不过，随着近些年 Redis 的发展，大家慢慢都转而使用更加强大的 Redis 而放弃 Memcached。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:2:1","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#分布式缓存"},{"categories":["开发者手册"],"content":" Redis 和 Memcached 的区别 共同点 都是基于内存的数据库。 都有过期策略。 两者的性能都非常高。 区别 Redis 支持更丰富的数据类型，也就是说可以支持更复杂的应用场景。Redis 不仅仅支持简单的 k/v 类型的数据，同时还提供 list，set，zset，hash 等数据结构的存储。而 Memcached 只支持最简单的 k/v 数据类型。 Redis 的容灾更好，支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而 Memecache 把数据全部存在内存中。 Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据，但是 Redis 目前是原生支持 cluster 模式的。 Redis 在服务器内存使用完之后，可以将不用的数据放到磁盘上。而 Memcached 在服务器内存使用完之后，就会直接报异常。 Memcached 是多线程，非阻塞 IO 复用的网络模型，Redis 使用单线程的多路 IO 复用模型（Redis 6.0 引入了多线程 IO ）。 Memcached过期数据的删除策略只用了惰性删除，而 Redis 同时使用了惰性删除与定期删除。 Redis 支持发布订阅模型、Lua 脚本、事务等功能，而 Memcached 不支持。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:2:2","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#redis-和-memcached-的区别"},{"categories":["开发者手册"],"content":" Redis 和 Memcached 的区别 共同点 都是基于内存的数据库。 都有过期策略。 两者的性能都非常高。 区别 Redis 支持更丰富的数据类型，也就是说可以支持更复杂的应用场景。Redis 不仅仅支持简单的 k/v 类型的数据，同时还提供 list，set，zset，hash 等数据结构的存储。而 Memcached 只支持最简单的 k/v 数据类型。 Redis 的容灾更好，支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而 Memecache 把数据全部存在内存中。 Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据，但是 Redis 目前是原生支持 cluster 模式的。 Redis 在服务器内存使用完之后，可以将不用的数据放到磁盘上。而 Memcached 在服务器内存使用完之后，就会直接报异常。 Memcached 是多线程，非阻塞 IO 复用的网络模型，Redis 使用单线程的多路 IO 复用模型（Redis 6.0 引入了多线程 IO ）。 Memcached过期数据的删除策略只用了惰性删除，而 Redis 同时使用了惰性删除与定期删除。 Redis 支持发布订阅模型、Lua 脚本、事务等功能，而 Memcached 不支持。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:2:2","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#共同点"},{"categories":["开发者手册"],"content":" Redis 和 Memcached 的区别 共同点 都是基于内存的数据库。 都有过期策略。 两者的性能都非常高。 区别 Redis 支持更丰富的数据类型，也就是说可以支持更复杂的应用场景。Redis 不仅仅支持简单的 k/v 类型的数据，同时还提供 list，set，zset，hash 等数据结构的存储。而 Memcached 只支持最简单的 k/v 数据类型。 Redis 的容灾更好，支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而 Memecache 把数据全部存在内存中。 Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据，但是 Redis 目前是原生支持 cluster 模式的。 Redis 在服务器内存使用完之后，可以将不用的数据放到磁盘上。而 Memcached 在服务器内存使用完之后，就会直接报异常。 Memcached 是多线程，非阻塞 IO 复用的网络模型，Redis 使用单线程的多路 IO 复用模型（Redis 6.0 引入了多线程 IO ）。 Memcached过期数据的删除策略只用了惰性删除，而 Redis 同时使用了惰性删除与定期删除。 Redis 支持发布订阅模型、Lua 脚本、事务等功能，而 Memcached 不支持。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:2:2","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#区别"},{"categories":["开发者手册"],"content":" Redis 如何判断数据是否过期Redis 通过一个叫做过期字典（可以看作是hash表）来保存数据过期的时间。过期字典的键指向 Redis 数据库中的某个key(键)，过期字典的值是一个long long 类型的整数，这个整数保存了 key 所指向的数据库键的过期时间（毫秒精度的UNIX时间戳）。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:2:3","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#redis-如何判断数据是否过期"},{"categories":["开发者手册"],"content":" 过期删除策略如果 Redis 中一批 key 只能存活 1 分钟，那么 1 分钟后，Redis 是怎么对这批 key 进行删除的呢？ 惰性删除：只会在取出 key 的时候才对数据进行过期检查。这样对 CPU 最友好，但是可能会造成太多过期 key 没有被删除。 定期删除：每隔一段时间抽取一批 key 执行删除过期 key 操作。Redis 底层会通过限制删除操作执行的时长和频率来减少删除操作对 CPU 时间的影响。 定期删除对内存更加友好，惰性删除对CPU更加友好。Redis 采用的是定期删除➕惰性删除。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:2:4","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#过期删除策略"},{"categories":["开发者手册"],"content":" 内存淘汰机制我们通过给 key 设置过期时间还是有问题的，因为还是可能存在定期删除和惰性删除漏掉了很多过期 key 的情况。这样就导致大量过期 key 堆积在内存里，然后就 OOM 了，Redis 的数据淘汰机制可以解决这个问题。 策略 说明 volatile-lru （least recently used），从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl 从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random 从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru （least recently used），当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的） allkeys-random 从数据集（server.db[i].dict）中任意选择数据淘汰 no-eviction 禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错😱。 volatile-lfu （least frequently used），从已设置过期时间的数据集(server.db[i].expires)中挑选最不经常使用的数据淘汰 allkeys-lfu （least frequently used），当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的 key ","date":"2021-02-20","objectID":"/redis-glance-faq/:2:5","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#内存淘汰机制"},{"categories":["开发者手册"],"content":" 持久化机制很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了备份数据，比如为了防止系统故障而将数据备份到一个另一台机器。 Redis 两种不同的持久化操作。一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file, AOF）。 RDB快照持久化是 Redis 默认采用的持久化方式，在 Redis.conf 配置文件中默认有此下配置： save 900 1 # 在 900 秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 300 10 # 在 300 秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 60 10000 # 在 60 秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 Redis 可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。 Redis 创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本，还可以将快照留在原地以便重启服务器的时候使用。 AOF与快照持久化 RDB 相比，AOF 持久化 的实时性更好，基本已成为主流的持久化方案。默认情况下 Redis 没有开启 AOF方式的持久化。 开启 AOF 持久化后每执行一条会更改 Redis 中的数据的命令，Redis 就会将该命令写入硬盘中的 AOF 文件。AOF 文件的保存位置和 RDB 文件的位置相同，都是通过 dir 参数设置的，默认的文件名是 appendonly.aof。 Redis 的配置文件中存在三种不同的 AOF 持久化方式，它们分别是： appendfsync always # 每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度 appendfsync everysec # 每秒钟同步一次，显示地将多个写命令同步到硬盘 appendfsync no # 让操作系统决定何时进行同步 为了兼顾数据和写入性能，可以考虑 appendfsync everysec 选项 ，让 Redis 每秒同步一次 AOF 文件，Redis 性能几乎没受到任何影响。 这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:2:6","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#持久化机制"},{"categories":["开发者手册"],"content":" 持久化机制很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了备份数据，比如为了防止系统故障而将数据备份到一个另一台机器。 Redis 两种不同的持久化操作。一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file, AOF）。 RDB快照持久化是 Redis 默认采用的持久化方式，在 Redis.conf 配置文件中默认有此下配置： save 900 1 # 在 900 秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 300 10 # 在 300 秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 60 10000 # 在 60 秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 Redis 可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。 Redis 创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本，还可以将快照留在原地以便重启服务器的时候使用。 AOF与快照持久化 RDB 相比，AOF 持久化 的实时性更好，基本已成为主流的持久化方案。默认情况下 Redis 没有开启 AOF方式的持久化。 开启 AOF 持久化后每执行一条会更改 Redis 中的数据的命令，Redis 就会将该命令写入硬盘中的 AOF 文件。AOF 文件的保存位置和 RDB 文件的位置相同，都是通过 dir 参数设置的，默认的文件名是 appendonly.aof。 Redis 的配置文件中存在三种不同的 AOF 持久化方式，它们分别是： appendfsync always # 每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度 appendfsync everysec # 每秒钟同步一次，显示地将多个写命令同步到硬盘 appendfsync no # 让操作系统决定何时进行同步 为了兼顾数据和写入性能，可以考虑 appendfsync everysec 选项 ，让 Redis 每秒同步一次 AOF 文件，Redis 性能几乎没受到任何影响。 这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:2:6","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#rdb"},{"categories":["开发者手册"],"content":" 持久化机制很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了备份数据，比如为了防止系统故障而将数据备份到一个另一台机器。 Redis 两种不同的持久化操作。一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file, AOF）。 RDB快照持久化是 Redis 默认采用的持久化方式，在 Redis.conf 配置文件中默认有此下配置： save 900 1 # 在 900 秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 300 10 # 在 300 秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 60 10000 # 在 60 秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 Redis 可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。 Redis 创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本，还可以将快照留在原地以便重启服务器的时候使用。 AOF与快照持久化 RDB 相比，AOF 持久化 的实时性更好，基本已成为主流的持久化方案。默认情况下 Redis 没有开启 AOF方式的持久化。 开启 AOF 持久化后每执行一条会更改 Redis 中的数据的命令，Redis 就会将该命令写入硬盘中的 AOF 文件。AOF 文件的保存位置和 RDB 文件的位置相同，都是通过 dir 参数设置的，默认的文件名是 appendonly.aof。 Redis 的配置文件中存在三种不同的 AOF 持久化方式，它们分别是： appendfsync always # 每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度 appendfsync everysec # 每秒钟同步一次，显示地将多个写命令同步到硬盘 appendfsync no # 让操作系统决定何时进行同步 为了兼顾数据和写入性能，可以考虑 appendfsync everysec 选项 ，让 Redis 每秒同步一次 AOF 文件，Redis 性能几乎没受到任何影响。 这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。 ","date":"2021-02-20","objectID":"/redis-glance-faq/:2:6","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#aof"},{"categories":["开发者手册"],"content":" 参考 Redis 命令参考 Redis(一)、Redis五种数据结构 Redis ","date":"2021-02-20","objectID":"/redis-glance-faq/:3:0","series":null,"tags":["redis"],"title":"Redis 学习笔记","uri":"/redis-glance-faq/#参考"},{"categories":["mysql"],"content":"xiaobinqt,Mysql 服务端如何处理客户端请求","date":"2020-12-20","objectID":"/mysql-server-exec-client-raw-sql/","series":null,"tags":["mysql"],"title":"Mysql 服务端如何处理客户端请求","uri":"/mysql-server-exec-client-raw-sql/"},{"categories":["mysql"],"content":" 不论客户端进程和服务器进程是采用哪种方式进行通信，最后实现的效果都是：客户端进程向服务器进程发送一段文本（MySQL语句），服务器进程处理后再向客户端进程发送一段文本（处理结果）。 那服务器进程对客户端进程发送的请求做了什么处理，才能产生最后的处理结果呢？客户端可以向服务器发送增删改查各类请求，这里以查询请求为例，图示一下大致的过程👇 client-server ","date":"2020-12-20","objectID":"/mysql-server-exec-client-raw-sql/:0:0","series":null,"tags":["mysql"],"title":"Mysql 服务端如何处理客户端请求","uri":"/mysql-server-exec-client-raw-sql/#"},{"categories":["mysql"],"content":" 连接管理每当有一个客户端进程连接到服务器进程时，服务器进程都会创建一个线程来专门处理与这个客户端的交互，当该客户端退出时会与服务器断开连接，服务器并不会立即把与该客户端交互的线程销毁掉，而是把它缓存起来，在另一个新的客户端再进行连接时，把这个缓存的线程分配给该新客户端。这样就起到了不频繁创建和销毁线程的效果，从而节省开销。 MySQL服务器会为每一个连接进来的客户端分配一个线程，但是线程分配的太多了会严重影响系统性能，所以也需要限制一下可以同时连接到服务器的客户端数量。max_connections可以设置允许同时连入的客户端数量。 在客户端程序发起连接的时候，需要携带主机信息、用户名、密码，服务器程序会对客户端程序提供的这些信息进行认证，如果认证失败，服务器程序会拒绝连接。 当连接建立后，与该客户端关联的服务器线程会一直等待客户端发送过来的请求，MySQL服务器接收到的请求只是一个文本消息，该文本消息还要经过各种处理。 ","date":"2020-12-20","objectID":"/mysql-server-exec-client-raw-sql/:1:0","series":null,"tags":["mysql"],"title":"Mysql 服务端如何处理客户端请求","uri":"/mysql-server-exec-client-raw-sql/#连接管理"},{"categories":["mysql"],"content":" 解析与优化","date":"2020-12-20","objectID":"/mysql-server-exec-client-raw-sql/:2:0","series":null,"tags":["mysql"],"title":"Mysql 服务端如何处理客户端请求","uri":"/mysql-server-exec-client-raw-sql/#解析与优化"},{"categories":["mysql"],"content":" 查询缓存MySQL服务器程序处理查询请求时，会把刚刚处理过的查询请求和结果缓存起来，如果下一次有一模一样的请求过来，直接从缓存中查找结果就好了，就不用再去底层的表中查找了。这个查询缓存可以在不同客户端之间共享，也就是说如果客户端A刚刚查询了一个语句，而客户端B之后发送了同样的查询请求，那么客户端B的这次查询就可以直接使用查询缓存中的数据。 当然，MySQL服务器并没有人聪明，如果两个查询请求在任何字符上的不同（例如：空格、注释、大小写），都会导致缓存不会命中。另外，如果查询请求中包含某些系统函数、用户自定义变量和函数、一些系统表，如 mysql 、information_schema、 performance_schema 数据库中的表，那这个请求就不会被缓存。以某些系统函数举例，可能同样的函数的两次调用会产生不一样的结果，比如函数NOW ，每次调用都会产生最新的当前时间，如果在一个查询请求中调用了这个函数，那即使查询请求的文本信息都一样，那不同时间的两次查询也应该得到不同的结果，如果在第一次查询时就缓存了，那第二次查询的时候直接使用第一次查询的结果就是错误的！ 既然是缓存，就有缓存失效的时候。MySQL的缓存系统会监测涉及到的每张表，只要该表的结构或者数据被修改，如对该表使用了INSERT、 UPDATE、DELETE、TRUNCATE TABLE、ALTER TABLE 、DROP TABLE或 DROP DATABASE语句，那使用该表的所有高速缓存查询都将变为无效并从高速缓存中删除！ 虽然查询缓存有时可以提升系统性能，但也不得不因维护这块缓存而造成一些开销，比如每次都要去查询缓存中检索，查询请求处理完需要更新查询缓存，维护该查询缓存对应的内存区域。从MySQL 5.7.20开始，不推荐使用查询缓存，并在MySQL 8.0中删除。 ","date":"2020-12-20","objectID":"/mysql-server-exec-client-raw-sql/:2:1","series":null,"tags":["mysql"],"title":"Mysql 服务端如何处理客户端请求","uri":"/mysql-server-exec-client-raw-sql/#查询缓存"},{"categories":["mysql"],"content":" 语法解析如果查询缓存没有命中，接下来就需要进入正式的查询阶段了。因为客户端程序发送过来的请求只是一段文本，所以MySQL服务器程序首先要对这段文本做分析，判断请求的语法是否正确，然后从文本中将要查询的表、各种查询条件都提取出来放到MySQL服务器内部使用的一些数据结构上来。 ","date":"2020-12-20","objectID":"/mysql-server-exec-client-raw-sql/:2:2","series":null,"tags":["mysql"],"title":"Mysql 服务端如何处理客户端请求","uri":"/mysql-server-exec-client-raw-sql/#语法解析"},{"categories":["mysql"],"content":" 查询优化语法解析之后，服务器程序获得到了需要的信息，比如要查询的列是哪些，表是哪个，搜索条件是什么等等，但光有这些是不够的，因为我们写的MySQL语句执行起来效率可能并不是很高，MySQL的优化程序会对我们的语句做一些优化，如外连接转换为内连接、表达式简化等。 优化的结果就是生成一个执行计划，这个执行计划表明了应该使用哪些索引进行查询，表之间的连接顺序是什么样的。可以使用EXPLAIN语句来查看某个语句的执行计划。 ","date":"2020-12-20","objectID":"/mysql-server-exec-client-raw-sql/:2:3","series":null,"tags":["mysql"],"title":"Mysql 服务端如何处理客户端请求","uri":"/mysql-server-exec-client-raw-sql/#查询优化"},{"categories":["mysql"],"content":" 存储引擎当服务器程序完成了查询优化，这时还没有真正的去访问真实的数据表，MySQL服务器把数据的存储和提取操作都封装到了一个叫存储引擎的模块里。 我们知道表是由一行一行的记录组成的，但这只是一个逻辑上的概念，物理上如何表示记录，怎么从表中读取数据，怎么把数据写入具体的物理存储器上，这都是存储引擎负责的事情。为了实现不同的功能，MySQL提供了各式各样的存储引擎，不同存储引擎管理的表具体的存储结构可能不同，采用的存取算法也可能不同。 为了管理方便，把连接管理、查询缓存、语法解析、查询优化这些并不涉及真实数据存储的功能划分为MySQL server的功能，把真实存取数据的功能划分为存储引擎的功能。各种不同的存储引擎向上边的MySQL server 层提供统一的调用接口（也就是存储引擎API），包含了几十个底层函数，像\"读取索引第一条内容\"、“读取索引下一条内容”、“插入记录\"等等。 所以在MySQL server完成了查询优化后，只需按照生成的执行计划调用底层存储引擎提供的API，获取到数据后返回给客户端就好了。 ","date":"2020-12-20","objectID":"/mysql-server-exec-client-raw-sql/:3:0","series":null,"tags":["mysql"],"title":"Mysql 服务端如何处理客户端请求","uri":"/mysql-server-exec-client-raw-sql/#存储引擎"},{"categories":["开发者手册"],"content":"xiaobinqt","date":"2020-11-19","objectID":"/linux-common-concept/","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/"},{"categories":["开发者手册"],"content":" linux 和 unix 的区别Linux和Unix之间的区别是什么？ ","date":"2020-11-19","objectID":"/linux-common-concept/:1:0","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#linux-和-unix-的区别"},{"categories":["开发者手册"],"content":" musl 和 glibc 的区别musl 和 glibc 都是 Linux 的标准库，区别是 musl 是一个 mini 版本，或是叫做基于 glibc 的库，而 glibc 是一个完整版本。 ","date":"2020-11-19","objectID":"/linux-common-concept/:2:0","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#musl-和-glibc-的区别"},{"categories":["开发者手册"],"content":" ubuntu","date":"2020-11-19","objectID":"/linux-common-concept/:3:0","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#ubuntu"},{"categories":["开发者手册"],"content":" 获取系统代号 lsb_release -cs 获取系统代号 ","date":"2020-11-19","objectID":"/linux-common-concept/:3:1","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#获取系统代号"},{"categories":["开发者手册"],"content":" 内核版本信息 uname -a # 或者 cat /proc/version 内核版本信息 ","date":"2020-11-19","objectID":"/linux-common-concept/:3:2","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#内核版本信息"},{"categories":["开发者手册"],"content":" lsb_releaseLSBLinux Standard Base 是 Linux 标准库的缩写， lsb_release 命令 用来与具体 Linux 发行版相关的 Linux 标准库信息。 CentOS 最小化安装时默认没有这个命令，需要安装 lsb_release 使用命令，👇以下是常见系统的安装 lsb_release 命令。 ","date":"2020-11-19","objectID":"/linux-common-concept/:4:0","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#lsb_release"},{"categories":["开发者手册"],"content":" Ubuntu, Debian sudo apt-get update \u0026\u0026 sudo apt-get install lsb-core ","date":"2020-11-19","objectID":"/linux-common-concept/:4:1","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#ubuntu-debian"},{"categories":["开发者手册"],"content":" CentOS sudo yum update \u0026\u0026 sudo yum install redhat-lsb-core ","date":"2020-11-19","objectID":"/linux-common-concept/:4:2","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#centos"},{"categories":["开发者手册"],"content":" Fedora sudo dnf update \u0026\u0026 sudo dnf install redhat-lsb-core ","date":"2020-11-19","objectID":"/linux-common-concept/:4:3","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#fedora"},{"categories":["开发者手册"],"content":" 参数 lsb_release 参数 -v：显示与你 Linux 发行版相对应的 Linux 版本库描述信息。Linux 版本库模块描述使用冒号 : 分分隔 -i：显示该 Linux 系统的发行商 -d：显示 Linux 发行版描述信息 -r：显示当前 Linux 发行版版本号 -c：显示当前 Linux 发行版代号codename -a：显示全部信息，包括 LSB、版本号、代号、版本描述信息 ","date":"2020-11-19","objectID":"/linux-common-concept/:4:4","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#参数"},{"categories":["开发者手册"],"content":" linux 常用命令以下命令都是笔者在工作中用到过的，因为不是专业的 shell 工程师，所以遍通过笔记记录下来防止忘记。 ","date":"2020-11-19","objectID":"/linux-common-concept/:5:0","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#linux-常用命令"},{"categories":["开发者手册"],"content":" cutcut 命令从文件的每一行剪切字节、字符和字段并将这些字节、字符和字段写至标准输出。 -d ：自定义分隔符，默认为制表符。 -f ：与-d一起使用，指定显示哪个区域。 cut -d -f ","date":"2020-11-19","objectID":"/linux-common-concept/:5:1","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#cut"},{"categories":["开发者手册"],"content":" grep grep -v name # 表示查看除了含有name之外的行内容 grep -v ","date":"2020-11-19","objectID":"/linux-common-concept/:5:2","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#grep"},{"categories":["开发者手册"],"content":" curl curl -o /dev/null -s -w %{http_code} https://www.baidu.com -o ：输出文件，默认为标准输出。 -s ：屏蔽掉输出，不显示任何内容。 -w ：输出http状态码。 ","date":"2020-11-19","objectID":"/linux-common-concept/:5:3","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#curl"},{"categories":["开发者手册"],"content":" 数组操作 #!/bin/bash let a=0 for i in $(seq 1 10);do array[a]=$i let a++ done echo \"一个添加多个元素：e f g\" array+=(e f g) echo \"数组所有元素 ${array[*]}\" # shellcheck disable=SC2145 echo \"数组所有下标: ${!array[@]}\" echo \"数组长度为：${#array[*]}\" echo \"数组第一个元素为：${array[0]}\" echo \"删除数组第二个元素\" # shellcheck disable=SC2184 unset array[1] echo \"再次数组所有元素 ${array[*]}\" 数组操作 ","date":"2020-11-19","objectID":"/linux-common-concept/:5:4","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#数组操作"},{"categories":["开发者手册"],"content":" sed -e多重编辑 tail /etc/services | sed -e '1,2d' -e 's/blp5/test/' 1,2d：删除第一行和第二行 s/blp5/test/：将blp5替换为test 可以用;分隔多个命令👇效果一样： tail /etc/services | sed '1,2d;s/blp5/test/' sed多行编辑 读取下一行n 读取下一行到模式空间 seq 6 | sed -n 'n;p' sed 先读取第一行 1，执行 n 命令，获取下一行 2，此时模式空间是 2，执行 p 命令，打印模式空间。现在模式空间是 2， sed 再读取 3，执行 n 命令，获取下一行 4，此时模式空间为 4，执行 p 命令，以此类推。 打印偶数 ","date":"2020-11-19","objectID":"/linux-common-concept/:5:5","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#sed"},{"categories":["开发者手册"],"content":" sed -e多重编辑 tail /etc/services | sed -e '1,2d' -e 's/blp5/test/' 1,2d：删除第一行和第二行 s/blp5/test/：将blp5替换为test 可以用;分隔多个命令👇效果一样： tail /etc/services | sed '1,2d;s/blp5/test/' sed多行编辑 读取下一行n 读取下一行到模式空间 seq 6 | sed -n 'n;p' sed 先读取第一行 1，执行 n 命令，获取下一行 2，此时模式空间是 2，执行 p 命令，打印模式空间。现在模式空间是 2， sed 再读取 3，执行 n 命令，获取下一行 4，此时模式空间为 4，执行 p 命令，以此类推。 打印偶数 ","date":"2020-11-19","objectID":"/linux-common-concept/:5:5","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#-e多重编辑"},{"categories":["开发者手册"],"content":" sed -e多重编辑 tail /etc/services | sed -e '1,2d' -e 's/blp5/test/' 1,2d：删除第一行和第二行 s/blp5/test/：将blp5替换为test 可以用;分隔多个命令👇效果一样： tail /etc/services | sed '1,2d;s/blp5/test/' sed多行编辑 读取下一行n 读取下一行到模式空间 seq 6 | sed -n 'n;p' sed 先读取第一行 1，执行 n 命令，获取下一行 2，此时模式空间是 2，执行 p 命令，打印模式空间。现在模式空间是 2， sed 再读取 3，执行 n 命令，获取下一行 4，此时模式空间为 4，执行 p 命令，以此类推。 打印偶数 ","date":"2020-11-19","objectID":"/linux-common-concept/:5:5","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#读取下一行"},{"categories":["开发者手册"],"content":" 参考 curl -w,–write-out参数详解 ","date":"2020-11-19","objectID":"/linux-common-concept/:6:0","series":null,"tags":["linux"],"title":"Linux 常用命令备忘","uri":"/linux-common-concept/#参考"},{"categories":["开发者手册"],"content":"xiaobinqt,docker IPv4 forwarding is disabled. Networking will not work","date":"2020-11-08","objectID":"/ipv4-forwarding-is-disabled-networking-will-not-work/","series":null,"tags":["docker"],"title":"IPv4 forwarding is disabled. Networking will not work","uri":"/ipv4-forwarding-is-disabled-networking-will-not-work/"},{"categories":["开发者手册"],"content":" 问题今天在操作 docker 时遇到了一个问题IPv4 forwarding is disabled. Networking will not work👇 报错信息 我的系统是 CentOS7.9 系统信息 ","date":"2020-11-08","objectID":"/ipv4-forwarding-is-disabled-networking-will-not-work/:1:0","series":null,"tags":["docker"],"title":"IPv4 forwarding is disabled. Networking will not work","uri":"/ipv4-forwarding-is-disabled-networking-will-not-work/#问题"},{"categories":["开发者手册"],"content":" 解决方案在宿主机执行 echo \"net.ipv4.ip_forward=1\" \u003e\u003e/usr/lib/sysctl.d/00-system.conf 然后重启网络和 docker systemctl restart network systemctl restart docker 问题解决 ","date":"2020-11-08","objectID":"/ipv4-forwarding-is-disabled-networking-will-not-work/:2:0","series":null,"tags":["docker"],"title":"IPv4 forwarding is disabled. Networking will not work","uri":"/ipv4-forwarding-is-disabled-networking-will-not-work/#解决方案"},{"categories":["理解计算机"],"content":"https,加密算法,什么是 https/ssl/tls,什么是安全,https 为什么安全,对称加密和非对称加密,CA 证书","date":"2020-10-27","objectID":"/what-is-https/","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/"},{"categories":["理解计算机"],"content":"为什么有 HTTPS？因为 HTTP 不安全！ 现在的互联网已经不再是 “田园时代”，“黑暗森林” 已经到来。上网的记录会被轻易截获，网站是否真实也无法验证，黑客可以伪装成银行网站，盗取真实姓名、密码、银行卡等敏感信息，威胁人身安全和财产安全。 上网的时候必须步步为营、处处小心，否则就会被不知道埋伏在哪里的黑客所“猎杀”。 HTTPS 如何实现安全通信？如何构建出固若金汤的网络城堡？主要涉及的知识点如下： 什么是 HTTPS 什么样的才是安全的通信 对称加密与非对称加密、摘要算法、数字签名、完整性校验是什么 迁移 HTTPS 的必要性 ","date":"2020-10-27","objectID":"/what-is-https/:0:0","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/#"},{"categories":["理解计算机"],"content":" 什么是安全在通信过程中，具备以下特性则认为安全：机密性、完整性、不可否认、身份认证。 机密性：数据必须保密，只能有信任的人读取，其他人是不可见的秘密。就是不能让不相关的人看到不该看的东西。 完整性：也叫作一致性，也就是数据在传输过程中没有被非法篡改，内容不能多也不能少，一五一十的保持原状。 不可否认：不可抵赖，不能否认已经发生过的事情。 身份验证：确认对方的真实身份，“证明你是真的是你”，保证消息发送到可信的人，而不是非法之徒。 所以同时具备了机密性、完整性、身份认证、不可否认四个特性，通信双方的安全才有保证，才是真正的安全。 ","date":"2020-10-27","objectID":"/what-is-https/:1:0","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/#什么是安全"},{"categories":["理解计算机"],"content":" 什么是 HTTPSHTTPS 其实是一个“非常简单”的协议，规定了新的协议名“https”，默认端口号 443，至于其他的什么请求 - 应答模式、报文结构、请求方法、URI、头字段、连接管理等等都完全沿用 HTTP，没有任何新的东西。唯一的差别就是端口号不同、去掉明文传输。 那 HTTPS 凭啥就变得安全了呢？ 就是因为他在 TCP/IP 与 HTTP 之间加上了 SSL/TLS ，从原来的 HTTP over TCP/IP 变成了 HTTP over SSL/TLS，让 HTTP 运行在 安全的 SSL/TLS 协议上。 http 与 https ","date":"2020-10-27","objectID":"/what-is-https/:2:0","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/#什么是-https"},{"categories":["理解计算机"],"content":" SSL/TLSSSL 即安全套接层（Secure Sockets Layer），在 OSI 模型中处于第 5 层（会话层），由网景公司于 1994 年发明，有 v2 和 v3 两个版本，而 v1 因为有严重的缺陷从未公开过。 SSL 发展到 v3 时已经证明了它自身是一个非常好的安全通信协议，于是互联网工程组 IETF 在 1999 年把它改名为 TLS（传输层安全，Transport Layer Security），正式标准化，版本号从 1.0 重新算起，所以 TLS1.0 实际上就是 SSLv3.1。 TLS 由记录协议、握手协议、警告协议、变更密码规范协议、扩展协议等几个子协议组成，综合使用了对称加密、非对称加密、身份认证等许多密码学前沿技术。 浏览器与服务器在使用 TLS 建立连接的时候实际上就是选了一组加密算法实现安全通信，这些算法组合叫做 “密码套件（cipher suite）”。 套件命名很有规律，比如“ECDHE-RSA-AES256-GCM-SHA384”。按照 密钥交换算法 + 签名算法 + 对称加密算法 + 摘要算法”组成的. 所以这个套件的意思就是：使用 ECDHE 算法进行密钥交换，使用 RSA 签名和身份验证，握手后使用 AES 对称加密，密钥长度 256 位，分组模式 GCM，消息认证和随机数生成使用摘要算法 SHA384。 ","date":"2020-10-27","objectID":"/what-is-https/:3:0","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/#ssltls"},{"categories":["理解计算机"],"content":" 对称加密和非对称加密前面提到四个实现安全的必要条件，先说机密性，也就是消息只能给想给的人看到并且看得懂。 实现机密性的手段就是加密（encrypt），也就是将原本明文消息使用加密算法转换成别人看不懂的密文，只有掌握特有的密钥的人才能解密出原始内容。 钥匙也就是密钥（key），未加密的消息叫做明文 （plain text/clear text），加密后的内容叫做密文（cipher text），通过密钥解密出原文的过程叫做解密（decrypt） ，而加解密的整个过程就是加密算法。 由于 HTTPS、TLS 都运行在计算机上，所以“密钥”就是一长串的数字，但约定俗成的度量单位是“位”（bit），而不是“字节”（byte）。比如，说密钥长度是 128（位），就是 16 字节的二进制串，密钥长度 1024（位），就是 128 字节的二进制串。 加密算法通常有两大类：对称加密和非对称加密。 对称加密加密和解密使用的密钥都是同一个，是 “对称的”。双方只要保证不会有泄露其他人知道这个密钥，通信就具有机密性。 对称加密 对称加密算法常见的有 RC4、DES、3DES、AES、ChaCha20 等，但前三种算法都被认为是不安全的，通常都禁止使用，目前常用的只有 AES 和 ChaCha20。 AES 的意思是“高级加密标准”（Advanced Encryption Standard），密钥长度可以是 128、192 或 256。它是 DES 算法的替代者，安全强度很高，性能也很好，而且有的硬件还会做特殊优化，所以非常流行，是** 应用最广泛的对称加密算法**。 加密分组模式对称算法还有一个 “分组模式”的概念，目的是通过算法用固定长度的密钥加密任意长度的明文。 最新的分组模式被称为 AEAD（Authenticated Encryption with Associated Data），在加密的同时增加了认证的功能，常用的是 GCM、CCM 和 Poly1305。 非对称加密有对称加密，为何还搞出一个非对称加密呢？ 对称加密确实解决了机密性，只有相关的人才能读取出信息。但是最大的问题是：如何安全的把密钥传递对方，专业术语 “密钥交换”。 所以为了解决秘钥交换，非对称加密诞生了。 非对称加密由两个密钥组成，分别是公钥（public key）和“私钥（private key）”，两个密钥是不一样的，这也就是不对称的由来，公钥可以任何人使用，私钥则自己保密。 这里需要注意的是：公钥和私钥都可以用来加密解密，公钥加密的密文只能用私钥解密，反之亦然。 服务端保存私钥，在互联网上分发公钥，当访问服务器网站的时候使用授予的公钥加密明文即可，服务端则使用对应的私钥来解密。 非对称加密 TLS 中常见的加密算法有 DH、RSA、ECC、DSA 等。其中的 RSA 最常用，它的安全性基于“整数分解”的数学难题，使用两个超大素数的乘积作为生成密钥的材料，想要从公钥推算出私钥是非常困难的。 ECC（Elliptic Curve Cryptography）是非对称加密里的“后起之秀”，它基于“椭圆曲线离散对数”的数学难题，使用特定的曲线方程和基点生成公钥和私钥，子算法 ECDHE 用于密钥交换，ECDSA 用于数字签名。 比起 RSA，ECC 在安全强度和性能上都有明显的优势。160 位的 ECC 相当于 1024 位的 RSA，而 224 位的 ECC 则相当于 2048 位的 RSA。因为密钥短，所以相应的计算量、消耗的内存和带宽也就少，加密解密的性能就上去了，对于现在的移动互联网非常有吸引力。 现在我们为了机密性从对称加密到非对称加密，而非对称加密还解决了密钥交换不安全的问题。那么是否可以直接使用非对称加密来实现机密性呢？ 答案是否定的！ 因为非对称加密运算速度比较慢。所以需要两者结合，混合模式实现机密性问题，同时又有很好的性能。 ","date":"2020-10-27","objectID":"/what-is-https/:3:1","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/#对称加密和非对称加密"},{"categories":["理解计算机"],"content":" 对称加密和非对称加密前面提到四个实现安全的必要条件，先说机密性，也就是消息只能给想给的人看到并且看得懂。 实现机密性的手段就是加密（encrypt），也就是将原本明文消息使用加密算法转换成别人看不懂的密文，只有掌握特有的密钥的人才能解密出原始内容。 钥匙也就是密钥（key），未加密的消息叫做明文 （plain text/clear text），加密后的内容叫做密文（cipher text），通过密钥解密出原文的过程叫做解密（decrypt） ，而加解密的整个过程就是加密算法。 由于 HTTPS、TLS 都运行在计算机上，所以“密钥”就是一长串的数字，但约定俗成的度量单位是“位”（bit），而不是“字节”（byte）。比如，说密钥长度是 128（位），就是 16 字节的二进制串，密钥长度 1024（位），就是 128 字节的二进制串。 加密算法通常有两大类：对称加密和非对称加密。 对称加密加密和解密使用的密钥都是同一个，是 “对称的”。双方只要保证不会有泄露其他人知道这个密钥，通信就具有机密性。 对称加密 对称加密算法常见的有 RC4、DES、3DES、AES、ChaCha20 等，但前三种算法都被认为是不安全的，通常都禁止使用，目前常用的只有 AES 和 ChaCha20。 AES 的意思是“高级加密标准”（Advanced Encryption Standard），密钥长度可以是 128、192 或 256。它是 DES 算法的替代者，安全强度很高，性能也很好，而且有的硬件还会做特殊优化，所以非常流行，是** 应用最广泛的对称加密算法**。 加密分组模式对称算法还有一个 “分组模式”的概念，目的是通过算法用固定长度的密钥加密任意长度的明文。 最新的分组模式被称为 AEAD（Authenticated Encryption with Associated Data），在加密的同时增加了认证的功能，常用的是 GCM、CCM 和 Poly1305。 非对称加密有对称加密，为何还搞出一个非对称加密呢？ 对称加密确实解决了机密性，只有相关的人才能读取出信息。但是最大的问题是：如何安全的把密钥传递对方，专业术语 “密钥交换”。 所以为了解决秘钥交换，非对称加密诞生了。 非对称加密由两个密钥组成，分别是公钥（public key）和“私钥（private key）”，两个密钥是不一样的，这也就是不对称的由来，公钥可以任何人使用，私钥则自己保密。 这里需要注意的是：公钥和私钥都可以用来加密解密，公钥加密的密文只能用私钥解密，反之亦然。 服务端保存私钥，在互联网上分发公钥，当访问服务器网站的时候使用授予的公钥加密明文即可，服务端则使用对应的私钥来解密。 非对称加密 TLS 中常见的加密算法有 DH、RSA、ECC、DSA 等。其中的 RSA 最常用，它的安全性基于“整数分解”的数学难题，使用两个超大素数的乘积作为生成密钥的材料，想要从公钥推算出私钥是非常困难的。 ECC（Elliptic Curve Cryptography）是非对称加密里的“后起之秀”，它基于“椭圆曲线离散对数”的数学难题，使用特定的曲线方程和基点生成公钥和私钥，子算法 ECDHE 用于密钥交换，ECDSA 用于数字签名。 比起 RSA，ECC 在安全强度和性能上都有明显的优势。160 位的 ECC 相当于 1024 位的 RSA，而 224 位的 ECC 则相当于 2048 位的 RSA。因为密钥短，所以相应的计算量、消耗的内存和带宽也就少，加密解密的性能就上去了，对于现在的移动互联网非常有吸引力。 现在我们为了机密性从对称加密到非对称加密，而非对称加密还解决了密钥交换不安全的问题。那么是否可以直接使用非对称加密来实现机密性呢？ 答案是否定的！ 因为非对称加密运算速度比较慢。所以需要两者结合，混合模式实现机密性问题，同时又有很好的性能。 ","date":"2020-10-27","objectID":"/what-is-https/:3:1","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/#对称加密"},{"categories":["理解计算机"],"content":" 对称加密和非对称加密前面提到四个实现安全的必要条件，先说机密性，也就是消息只能给想给的人看到并且看得懂。 实现机密性的手段就是加密（encrypt），也就是将原本明文消息使用加密算法转换成别人看不懂的密文，只有掌握特有的密钥的人才能解密出原始内容。 钥匙也就是密钥（key），未加密的消息叫做明文 （plain text/clear text），加密后的内容叫做密文（cipher text），通过密钥解密出原文的过程叫做解密（decrypt） ，而加解密的整个过程就是加密算法。 由于 HTTPS、TLS 都运行在计算机上，所以“密钥”就是一长串的数字，但约定俗成的度量单位是“位”（bit），而不是“字节”（byte）。比如，说密钥长度是 128（位），就是 16 字节的二进制串，密钥长度 1024（位），就是 128 字节的二进制串。 加密算法通常有两大类：对称加密和非对称加密。 对称加密加密和解密使用的密钥都是同一个，是 “对称的”。双方只要保证不会有泄露其他人知道这个密钥，通信就具有机密性。 对称加密 对称加密算法常见的有 RC4、DES、3DES、AES、ChaCha20 等，但前三种算法都被认为是不安全的，通常都禁止使用，目前常用的只有 AES 和 ChaCha20。 AES 的意思是“高级加密标准”（Advanced Encryption Standard），密钥长度可以是 128、192 或 256。它是 DES 算法的替代者，安全强度很高，性能也很好，而且有的硬件还会做特殊优化，所以非常流行，是** 应用最广泛的对称加密算法**。 加密分组模式对称算法还有一个 “分组模式”的概念，目的是通过算法用固定长度的密钥加密任意长度的明文。 最新的分组模式被称为 AEAD（Authenticated Encryption with Associated Data），在加密的同时增加了认证的功能，常用的是 GCM、CCM 和 Poly1305。 非对称加密有对称加密，为何还搞出一个非对称加密呢？ 对称加密确实解决了机密性，只有相关的人才能读取出信息。但是最大的问题是：如何安全的把密钥传递对方，专业术语 “密钥交换”。 所以为了解决秘钥交换，非对称加密诞生了。 非对称加密由两个密钥组成，分别是公钥（public key）和“私钥（private key）”，两个密钥是不一样的，这也就是不对称的由来，公钥可以任何人使用，私钥则自己保密。 这里需要注意的是：公钥和私钥都可以用来加密解密，公钥加密的密文只能用私钥解密，反之亦然。 服务端保存私钥，在互联网上分发公钥，当访问服务器网站的时候使用授予的公钥加密明文即可，服务端则使用对应的私钥来解密。 非对称加密 TLS 中常见的加密算法有 DH、RSA、ECC、DSA 等。其中的 RSA 最常用，它的安全性基于“整数分解”的数学难题，使用两个超大素数的乘积作为生成密钥的材料，想要从公钥推算出私钥是非常困难的。 ECC（Elliptic Curve Cryptography）是非对称加密里的“后起之秀”，它基于“椭圆曲线离散对数”的数学难题，使用特定的曲线方程和基点生成公钥和私钥，子算法 ECDHE 用于密钥交换，ECDSA 用于数字签名。 比起 RSA，ECC 在安全强度和性能上都有明显的优势。160 位的 ECC 相当于 1024 位的 RSA，而 224 位的 ECC 则相当于 2048 位的 RSA。因为密钥短，所以相应的计算量、消耗的内存和带宽也就少，加密解密的性能就上去了，对于现在的移动互联网非常有吸引力。 现在我们为了机密性从对称加密到非对称加密，而非对称加密还解决了密钥交换不安全的问题。那么是否可以直接使用非对称加密来实现机密性呢？ 答案是否定的！ 因为非对称加密运算速度比较慢。所以需要两者结合，混合模式实现机密性问题，同时又有很好的性能。 ","date":"2020-10-27","objectID":"/what-is-https/:3:1","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/#加密分组模式"},{"categories":["理解计算机"],"content":" 对称加密和非对称加密前面提到四个实现安全的必要条件，先说机密性，也就是消息只能给想给的人看到并且看得懂。 实现机密性的手段就是加密（encrypt），也就是将原本明文消息使用加密算法转换成别人看不懂的密文，只有掌握特有的密钥的人才能解密出原始内容。 钥匙也就是密钥（key），未加密的消息叫做明文 （plain text/clear text），加密后的内容叫做密文（cipher text），通过密钥解密出原文的过程叫做解密（decrypt） ，而加解密的整个过程就是加密算法。 由于 HTTPS、TLS 都运行在计算机上，所以“密钥”就是一长串的数字，但约定俗成的度量单位是“位”（bit），而不是“字节”（byte）。比如，说密钥长度是 128（位），就是 16 字节的二进制串，密钥长度 1024（位），就是 128 字节的二进制串。 加密算法通常有两大类：对称加密和非对称加密。 对称加密加密和解密使用的密钥都是同一个，是 “对称的”。双方只要保证不会有泄露其他人知道这个密钥，通信就具有机密性。 对称加密 对称加密算法常见的有 RC4、DES、3DES、AES、ChaCha20 等，但前三种算法都被认为是不安全的，通常都禁止使用，目前常用的只有 AES 和 ChaCha20。 AES 的意思是“高级加密标准”（Advanced Encryption Standard），密钥长度可以是 128、192 或 256。它是 DES 算法的替代者，安全强度很高，性能也很好，而且有的硬件还会做特殊优化，所以非常流行，是** 应用最广泛的对称加密算法**。 加密分组模式对称算法还有一个 “分组模式”的概念，目的是通过算法用固定长度的密钥加密任意长度的明文。 最新的分组模式被称为 AEAD（Authenticated Encryption with Associated Data），在加密的同时增加了认证的功能，常用的是 GCM、CCM 和 Poly1305。 非对称加密有对称加密，为何还搞出一个非对称加密呢？ 对称加密确实解决了机密性，只有相关的人才能读取出信息。但是最大的问题是：如何安全的把密钥传递对方，专业术语 “密钥交换”。 所以为了解决秘钥交换，非对称加密诞生了。 非对称加密由两个密钥组成，分别是公钥（public key）和“私钥（private key）”，两个密钥是不一样的，这也就是不对称的由来，公钥可以任何人使用，私钥则自己保密。 这里需要注意的是：公钥和私钥都可以用来加密解密，公钥加密的密文只能用私钥解密，反之亦然。 服务端保存私钥，在互联网上分发公钥，当访问服务器网站的时候使用授予的公钥加密明文即可，服务端则使用对应的私钥来解密。 非对称加密 TLS 中常见的加密算法有 DH、RSA、ECC、DSA 等。其中的 RSA 最常用，它的安全性基于“整数分解”的数学难题，使用两个超大素数的乘积作为生成密钥的材料，想要从公钥推算出私钥是非常困难的。 ECC（Elliptic Curve Cryptography）是非对称加密里的“后起之秀”，它基于“椭圆曲线离散对数”的数学难题，使用特定的曲线方程和基点生成公钥和私钥，子算法 ECDHE 用于密钥交换，ECDSA 用于数字签名。 比起 RSA，ECC 在安全强度和性能上都有明显的优势。160 位的 ECC 相当于 1024 位的 RSA，而 224 位的 ECC 则相当于 2048 位的 RSA。因为密钥短，所以相应的计算量、消耗的内存和带宽也就少，加密解密的性能就上去了，对于现在的移动互联网非常有吸引力。 现在我们为了机密性从对称加密到非对称加密，而非对称加密还解决了密钥交换不安全的问题。那么是否可以直接使用非对称加密来实现机密性呢？ 答案是否定的！ 因为非对称加密运算速度比较慢。所以需要两者结合，混合模式实现机密性问题，同时又有很好的性能。 ","date":"2020-10-27","objectID":"/what-is-https/:3:1","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/#非对称加密"},{"categories":["理解计算机"],"content":" 混合加密流程 先创建一个随机数的对称加密密钥，会话密钥（session key）； 使用会话密钥加密需要传输的明文消息，因为对称加密性能较好，接着再使用非对称加密的公钥对会话密钥加密，因为会话密钥很短，通常只有 16 字节或 32 字节，所以加密也不会太慢 。 这里主要就是解决了非对称加密的性能问题，同时实现了会话密钥的机密交换。 另一方接收到密文后使用非对称加密的私钥解密出上一步加密的会话密钥，接着使用会话密钥解密出加密的消息明文。 混合加密 总结一下就是使用非对称加密算法来加密会话密钥，使用对称加密算法来加密消息明文，接收方则使用非对称加密算法的私钥解密出会话密钥，再利用会话密钥解密消息密文。 这样混合加密就解决了对称加密算法的密钥交换问题，而且安全和性能兼顾，完美地实现了机密性。 后面还有完整性、身份认证、不可否认等特性没有实现，所以现在的通信还不是绝对安全。 ","date":"2020-10-27","objectID":"/what-is-https/:3:2","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/#混合加密流程"},{"categories":["理解计算机"],"content":" 摘要算法与完整性摘要算法的主要目的就是实现完整性，通过常见的散列函数、哈希函数实现。 我们可以简单理解成这是一种特殊的压缩算法，将任意长度的明文数据处理成固定长度、又是独一无二的“摘要”字符串，就是该数据的指纹。 同时摘要算法是单向加密算法，没有密钥，加密后的数据也无法解密，也就是不能从“摘要”推导出明文。 比如我们听过或者用过的 MD5（Message-Digest 5）、SHA-1（Secure Hash Algorithm 1），它们就是最常用的两个摘要算法，能够生成 16 字节和 20 字节长度的数字摘要。 完整性实现有了摘要算法生成的数字摘要，那么我们只需要在明文数据附上对应的摘要，就能保证数据的完整性。 但是由于摘要算法不具有机密性，不能明文传输，否则黑客可以修改消息后把摘要也一起改了，网站还是鉴别不出完整性。 所以完整性还是要建立在机密性上，我们结合之前提到的混合加密使用 ”会话密钥“ 加密明文消息 + 摘要，这样的话黑客也就无法得到明文，无法做修改了。这里有个专业术语叫“哈希消息认证码（HMAC）”。 哈希消息认证码（HMAC） 比如诸葛亮使用上面提到的混合加密过程给关二爷发消息：“明天攻城” + “SHA-2 摘要”，关二爷收到后使用密钥将解密出来的会话密钥解密出明文消息，同时对明文消息使用解密出来的摘要算法进行摘要计算，接着比对两份“摘要”字符串是否一致，如果一致就说明消息完整可信，没有被敌军修改过。 消息被修改是很危险的，要以史为鉴，比如赵高与李斯伪造遗诏，直接把扶苏给送西天了，这太可怕了。 总结下就是通过摘要比对防止篡改，同时利用混合加密实现密文与摘要的安全传输。 ","date":"2020-10-27","objectID":"/what-is-https/:3:3","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/#摘要算法与完整性"},{"categories":["理解计算机"],"content":" 摘要算法与完整性摘要算法的主要目的就是实现完整性，通过常见的散列函数、哈希函数实现。 我们可以简单理解成这是一种特殊的压缩算法，将任意长度的明文数据处理成固定长度、又是独一无二的“摘要”字符串，就是该数据的指纹。 同时摘要算法是单向加密算法，没有密钥，加密后的数据也无法解密，也就是不能从“摘要”推导出明文。 比如我们听过或者用过的 MD5（Message-Digest 5）、SHA-1（Secure Hash Algorithm 1），它们就是最常用的两个摘要算法，能够生成 16 字节和 20 字节长度的数字摘要。 完整性实现有了摘要算法生成的数字摘要，那么我们只需要在明文数据附上对应的摘要，就能保证数据的完整性。 但是由于摘要算法不具有机密性，不能明文传输，否则黑客可以修改消息后把摘要也一起改了，网站还是鉴别不出完整性。 所以完整性还是要建立在机密性上，我们结合之前提到的混合加密使用 ”会话密钥“ 加密明文消息 + 摘要，这样的话黑客也就无法得到明文，无法做修改了。这里有个专业术语叫“哈希消息认证码（HMAC）”。 哈希消息认证码（HMAC） 比如诸葛亮使用上面提到的混合加密过程给关二爷发消息：“明天攻城” + “SHA-2 摘要”，关二爷收到后使用密钥将解密出来的会话密钥解密出明文消息，同时对明文消息使用解密出来的摘要算法进行摘要计算，接着比对两份“摘要”字符串是否一致，如果一致就说明消息完整可信，没有被敌军修改过。 消息被修改是很危险的，要以史为鉴，比如赵高与李斯伪造遗诏，直接把扶苏给送西天了，这太可怕了。 总结下就是通过摘要比对防止篡改，同时利用混合加密实现密文与摘要的安全传输。 ","date":"2020-10-27","objectID":"/what-is-https/:3:3","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/#完整性实现"},{"categories":["理解计算机"],"content":" 数字签名和 CA到这里已经很安全了，但是还是有漏洞，就是通信的两头。黑客可以伪装成网站来窃取信息。而反过来，他也可以伪装成你，向网站发送支付、转账等消息，网站没有办法确认你的身份，钱可能就这么被偷走了。 现在如何实现身份认证呢？ 现实生活中，解决身份认证的手段是签名和印章，只要在纸上写下签名或者盖个章，就能够证明这份文件确实是由本人而不是其他人发出的。 非对称加密依然可以解决此问题，只不过跟之前反过来用，使用私钥再加上摘要算法，就能够实现“数字签名”，同时实现“身份认证”和“不可否认”。 就是把公钥私钥的用法反过来，之前是公钥加密、私钥解密，现在是私钥加密、公钥解密。但又因为非对称加密效率太低，所以私钥只加密原文的摘要，这样运算量就小的多，而且得到的数字签名也很小，方便保管和传输。 重点就是使用非对称加密的“私钥”加密原文的摘要，对方则使用非对称加密的公钥解密出摘要，再比对解密出的原文通过摘要算法计算摘要与解密出的摘要比对是否一致。 这样就能像签署文件一样证明消息确实是你发送的。 签名验签 只要你和网站互相交换公钥，就可以用“签名”和“验签”来确认消息的真实性，因为私钥保密，黑客不能伪造签名，就能够保证通信双方的身份。 CA到这里似乎已经大功告成，可惜还不是。 综合使用对称加密、非对称加密和摘要算法，我们已经实现了安全的四大特性，是不是已经完美了呢？ 不是的，这里还有一个“公钥的信任”问题。因为谁都可以发布公钥，我们还缺少防止黑客伪造公钥的手段，也就是说，怎么来判断这个公钥就是你的公钥呢？ 我们常说的CA（Certificate Authority，证书认证机构），它就像网络世界里的公安局、教育部、公证中心，具有极高的可信度，由它来给各个公钥签名，用自身的信誉来保证公钥无法伪造，是可信的。 CA 对公钥的签名认证也是有格式的，不是简单地把公钥绑定在持有者身份上就完事了，还要包含序列号、用途、颁发者、有效时间等等，把这些打成一个包再签名，完整地证明公钥关联的各种信息，形成“数字证书”（Certificate）。 ","date":"2020-10-27","objectID":"/what-is-https/:3:4","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/#数字签名和-ca"},{"categories":["理解计算机"],"content":" 数字签名和 CA到这里已经很安全了，但是还是有漏洞，就是通信的两头。黑客可以伪装成网站来窃取信息。而反过来，他也可以伪装成你，向网站发送支付、转账等消息，网站没有办法确认你的身份，钱可能就这么被偷走了。 现在如何实现身份认证呢？ 现实生活中，解决身份认证的手段是签名和印章，只要在纸上写下签名或者盖个章，就能够证明这份文件确实是由本人而不是其他人发出的。 非对称加密依然可以解决此问题，只不过跟之前反过来用，使用私钥再加上摘要算法，就能够实现“数字签名”，同时实现“身份认证”和“不可否认”。 就是把公钥私钥的用法反过来，之前是公钥加密、私钥解密，现在是私钥加密、公钥解密。但又因为非对称加密效率太低，所以私钥只加密原文的摘要，这样运算量就小的多，而且得到的数字签名也很小，方便保管和传输。 重点就是使用非对称加密的“私钥”加密原文的摘要，对方则使用非对称加密的公钥解密出摘要，再比对解密出的原文通过摘要算法计算摘要与解密出的摘要比对是否一致。 这样就能像签署文件一样证明消息确实是你发送的。 签名验签 只要你和网站互相交换公钥，就可以用“签名”和“验签”来确认消息的真实性，因为私钥保密，黑客不能伪造签名，就能够保证通信双方的身份。 CA到这里似乎已经大功告成，可惜还不是。 综合使用对称加密、非对称加密和摘要算法，我们已经实现了安全的四大特性，是不是已经完美了呢？ 不是的，这里还有一个“公钥的信任”问题。因为谁都可以发布公钥，我们还缺少防止黑客伪造公钥的手段，也就是说，怎么来判断这个公钥就是你的公钥呢？ 我们常说的CA（Certificate Authority，证书认证机构），它就像网络世界里的公安局、教育部、公证中心，具有极高的可信度，由它来给各个公钥签名，用自身的信誉来保证公钥无法伪造，是可信的。 CA 对公钥的签名认证也是有格式的，不是简单地把公钥绑定在持有者身份上就完事了，还要包含序列号、用途、颁发者、有效时间等等，把这些打成一个包再签名，完整地证明公钥关联的各种信息，形成“数字证书”（Certificate）。 ","date":"2020-10-27","objectID":"/what-is-https/:3:4","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/#ca"},{"categories":["理解计算机"],"content":" OpenSSL它是一个著名的开源密码学程序库和工具包，几乎支持所有公开的加密算法和协议，已经成为了事实上的标准，许多应用软件都会使用它作为底层库来实现 TLS 功能，包括常用的 Web 服务器 Apache、Nginx 等。 由于 OpenSSL 是开源的，所以它还有一些代码分支，比如 Google 的 BoringSSL、OpenBSD 的 LibreSSL，这些分支在 OpenSSL 的基础上删除了一些老旧代码，也增加了一些新特性，虽然背后有“大金主”，但离取代 OpenSSL 还差得很远。 总结下就是：OpenSSL 是著名的开源密码学工具包，是 SSL/TLS 的具体实现。 ","date":"2020-10-27","objectID":"/what-is-https/:3:5","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/#openssl"},{"categories":["理解计算机"],"content":" 迁移 HTTPS 必要性如果你做移动应用开发的话，那么就一定知道，Apple、Android、某信等开发平台在 2017 年就相继发出通知，要求所有的应用必须使用 HTTPS 连接，禁止不安全的 HTTP。 在台式机上，主流的浏览器 Chrome、Firefox 等也早就开始“强推”HTTPS，把 HTTP 站点打上“不安全”的标签，给用户以“心理压力”。 Google 等搜索巨头还利用自身的“话语权”优势，降低 HTTP 站点的排名，而给 HTTPS 更大的权重，力图让网民只访问到 HTTPS 网站。 这些手段都逐渐“挤压”了纯明文 HTTP 的生存空间，“迁移到 HTTPS”已经不是“要不要做”的问题，而是“要怎么做”的问题了。HTTPS 的大潮无法阻挡，如果还是死守着 HTTP，那么无疑会被冲刷到互联网的角落里。 ","date":"2020-10-27","objectID":"/what-is-https/:4:0","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/#迁移-https-必要性"},{"categories":["理解计算机"],"content":" 顾虑阻碍 HTTPS 实施的因素还有一些这样、那样的顾虑，三个比较流行的观点：“慢、贵、难”。 而“慢”则是惯性思维，拿以前的数据来评估 HTTPS 的性能，认为 HTTPS 会增加服务器的成本，增加客户端的时延，影响用户体验。 其实现在服务器和客户端的运算能力都已经有了很大的提升，性能方面完全没有担心的必要，而且还可以应用很多的优化解决方案 所谓“贵”，主要是指证书申请和维护的成本太高，网站难以承担。 这也属于惯性思维，在早几年的确是个问题，向 CA 申请证书的过程不仅麻烦，而且价格昂贵，每年要交几千甚至几万元。 但现在就不一样了，为了推广 HTTPS，很多云服务厂商都提供了一键申请、价格低廉的证书，而且还出现了专门颁发免费证书的 CA，其中最著名的就是“Let’s Encrypt”。 所谓的“难”，是指 HTTPS 涉及的知识点太多、太复杂，有一定的技术门槛，不能很快上手。 ","date":"2020-10-27","objectID":"/what-is-https/:5:0","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/#顾虑"},{"categories":["理解计算机"],"content":" 总结HTTPS 主要就是通过 SSL/TLS 实现安全，而安全又有对称加密与非对称加密，非对称加密性能较弱，所以我们使用非对称加密来加密对称加密的“会话密钥”，利用会话密钥加密明文解决了性能问题。 通过混合加密实现了机密性，利用摘要算法实现了完整性，通过数字签名使用非对称加密的“私钥”加密原文的摘要，对方则使用非对称加密的公钥解密出摘要，再比对解密出的原文通过摘要算法计算摘要与解密出的摘要比对是否一致实现了身份认证与不可否认。 ","date":"2020-10-27","objectID":"/what-is-https/:6:0","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/#总结"},{"categories":["理解计算机"],"content":" 参考 透视HTTPS建造固若金汤的城堡 ","date":"2020-10-27","objectID":"/what-is-https/:7:0","series":null,"tags":["http/https","SSL","TLS"],"title":"透视 HTTPS","uri":"/what-is-https/#参考"},{"categories":["mysql"],"content":"xiaobinqt,mysql 存储程序,mysql 存储函数,存储过程,触发器,事件,mysql 游标,什么是游标,mysql 局部变量和自定义变量","date":"2020-10-08","objectID":"/mysql-stored-routine/","series":null,"tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/"},{"categories":["mysql"],"content":" 总览 存储程序 存储程序可以封装一些语句，然后给用户提供一种简单的方式来调用这个存储程序，从而间接地执行某些语句。根据调用方式的不同，可以把存储程序分为存储例程、触发器和事件，存储例程又分为存储函数和存储过程，如☝️ 上图。 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:1:0","series":null,"tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/#总览"},{"categories":["mysql"],"content":" 存储函数存储函数只有一个返回值，可以从 mysql 内置的函数理解，所有 mysql 内置的函数都是只有一个返回值，比如： mysql 内置函数 存储函数很好理解，就是一个函数，跟普通函数一样有函数名，函数体，参数列表和返回值。创建存储函数语句如下： CREATE FUNCTION 存储函数名称([参数列表]) RETURNS 返回值类型 BEGIN 函数体内容 END CMD 说明 SHOW FUNCTION STATUS [LIKE 需要匹配的函数名] 查看所有存储函数 SHOW CREATE FUNCTION 函数名 查看某个存储函数 DROP FUNCTION 函数名 删除某个存储函数 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:2:0","series":null,"tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/#存储函数"},{"categories":["mysql"],"content":" 示例现在写一个存储函数，输入用户名 name，返回用户手机号 phone： CREATE FUNCTION get_phone(qname VARCHAR (45)) RETURNS VARCHAR(11) BEGIN RETURN (SELECT phone from t where name = qname); END EOF 存储函数的调用跟普通函数的调用也是一样的👇 效果 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:2:1","series":null,"tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/#示例"},{"categories":["mysql"],"content":" 局部变量和自定义变量在存储函数中可以使用局部变量和自定义变量，二者的区别是，局部变量用 DECLARE 申明，不用加 @符，局部变量随着函数调用结束，变量销毁且只能在存储函数中使用。自定义变量需要加 @ 符，且可以在函数外调用。 CREATE FUNCTION get_phone(qname VARCHAR (45)) RETURNS VARCHAR(11) BEGIN DECLARE ph varchar(11) default \"\"; # 局部变量 SET @ii = 100; # 自定义变量 SET ph = (select phone from t where name = qname); # 给局部变量赋值 RETURN ph ; END EOF 局部变量和自定义变量 ☝️ 可知，在存储函数get_phone中有一个局部变量 ph和自定义变量 @ii，函数调用结束后 @ii 被赋值为100 且可以在函数执行完后访问，但是 @ph 是空的。 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:3:0","series":null,"tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/#局部变量和自定义变量"},{"categories":["mysql"],"content":" 存储过程存储函数侧重于执行某些语句并返回一个值，而存储过程更侧重于单纯的去执行这些语句。存储过程的定义不需要声明返回值类型。 CREATE PROCEDURE 存储过程名称([参数列表]) BEGIN 需要执行的语句 END 调用存储过程使用 CALL 关键字。 CMD 说明 SHOW PROCEDURE STATUS [LIKE 需要匹配的存储过程名称] 查看所有存储过程 SHOW CREATE PROCEDURE 存储过程名称 查看某个存储过程 DROP PROCEDURE 存储过程名称 删除某个存储过程 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:4:0","series":null,"tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/#存储过程"},{"categories":["mysql"],"content":" 示例以下示例定义了一个 my_operate 的存储过程： CREATE PROCEDURE my_operate(pname varchar (45)) BEGIN SELECT * FROM t; INSERT INTO t(phone, name) VALUES(\"15214254125\", \"卢俊义\"); SELECT * FROM t; SELECT * from t where name = pname; END EOF 存储过程 ☝️ my_operate 定义并执行了 4 条 sql，完美诠释了存储过程更侧重于单纯的去执行这些语句。 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:4:1","series":null,"tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/#示例-1"},{"categories":["mysql"],"content":" 存储过程的参数前缀存储过程在定义参数的时候可以选择添加一些前缀👇，如果不写，默认的前缀是IN： 参数类型 [IN | OUT | INOUT] 参数名 数据类型 前缀 实际参数是否必须是变量 描述 IN 否 用于调用者向存储过程传递数据，如果IN参数在过程中被修改，调用者不可见。 OUT 是 用于把存储过程运行过程中产生的数据赋值给OUT参数，存储过程执行结束后，调用者可以访问到OUT参数。 INOUT 是 综合IN和OUT的特点，既可以用于调用者向存储过程传递数据，也可以用于存放存储过程中产生的数据以供调用者使用。 👇以下的示例，综合了 in，out，inout 参数： CREATE PROCEDURE my_arg( in pname varchar (45), out ophone char(11), inout io_name varchar(45) ) BEGIN SELECT * FROM t; INSERT INTO t(phone, name) VALUES(\"15225632145\", \"公孙胜\"); SELECT * FROM t; SELECT phone from t where name = pname into ophone; SET pname = \"公孙胜\"; SET io_name = \"公孙胜\"; END EOF 综合示例 由☝️可以看出，虽然在存储过程中修改了 pname 的值为 公孙胜，但是并没有生效，值依然是最初的宋江。IN参数只能被用于读取，对它赋值是不会被调用者看到的。 out 参数 ophone 最初是空的，通过存储过程赋值成功为公孙胜。 inout 参数 io_name 最初是空的，通过存储过程赋值成功为公孙胜，这里如果 io_name 不为空，也会被修改为公孙胜。 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:5:0","series":null,"tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/#存储过程的参数前缀"},{"categories":["mysql"],"content":" 存储过程和存储函数的区别 存储函数在定义时需要显式用RETURNS语句标明返回的数据类型，而且在函数体中必须使用RETURN语句来显式指定返回的值，而存储过程不需要。 存储函数只支持IN参数，而存储过程支持IN参数、OUT参数、和INOUT参数。 存储函数只能返回一个值，而存储过程可以通过设置多个OUT参数或者INOUT参数来返回多个结果。 存储函数执行过程中产生的结果集并不会被显示到客户端，而存储过程执行过程中产生的结果集会被显示到客户端。 存储函数直接在表达式中调用，而存储过程只能通过CALL语句来显式调用。 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:6:0","series":null,"tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/#存储过程和存储函数的区别"},{"categories":["mysql"],"content":" 游标游标是为了方便访问结果集中的某一条记录，可以理解成循环。如果某个结果集中有 10 条记录，使用游标后，会一条一条的去访问这 10 条记录。 游标可以在存储函数和存储过程中使用。 cursor 使用游标分为 4 步： 创建游标：DECLARE 游标名称 CURSOR FOR 查询语句; 打开游标：OPEN 游标名称; 通过游标访问记录 关闭游标：CLOSE 游标名称; 不显式的使用CLOSE语句关闭游标的话，在该存储函数或存储过程执行完之后会自动关闭游标。 可以使用👇来获取结果集中的记录： FETCH 游标名 INTO 变量1, 变量2, ... 变量n ","date":"2020-10-08","objectID":"/mysql-stored-routine/:7:0","series":null,"tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/#游标"},{"categories":["mysql"],"content":" 示例🤑以下创建一个存储过程，在存储过程中使用游标。 创建游标t_cursor，游标执行语句为 SELECT phone, name FROM t。 DECLARE CONTINUE HANDLER FOR NOT FOUND 处理语句; 的作用是结果集遍历结束后会自动执行这句，这里也可以使用 WHILE 循环遍历，但是 while 有个弊端是需要提前知道结束条件，比如结果集的总数是多少。这样写的好处是直接遍历，遍历结束自动处理，将 done 变量设置为 1，也就是说只要 done = 1 就说明遍历结束了，利用 LEAVE 关键字跳出循环。 CREATE PROCEDURE my_cursor() BEGIN DECLARE v_phone char(11); DECLARE v_name varchar(45); DECLARE done INT DEFAULT 0; DECLARE t_cursor CURSOR FOR SELECT `phone`, `name` FROM t; DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = 1; OPEN t_cursor; flag: LOOP FETCH t_cursor INTO v_phone, v_name; IF done = 1 THEN LEAVE flag; END IF; SELECT v_phone, v_name, done; END LOOP flag; CLOSE t_cursor; END EOF 执行结果 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:7:1","series":null,"tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/#示例-2"},{"categories":["mysql"],"content":" 触发器和事件存储例程是需要手动调用的，而触发器和事件是 MySQL 服务器在特定情况下自动调用的。 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:8:0","series":null,"tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/#触发器和事件"},{"categories":["mysql"],"content":" 触发器创建触发器 CREATE TRIGGER 触发器名 {BEFORE|AFTER} {INSERT|DELETE|UPDATE} ON 表名 FOR EACH ROW BEGIN 触发器内容 END MySQL 目前只支持对INSERT、DELETE、UPDATE这三种类型的语句设置触发器。 FOR EACH ROW BEGIN ... END表示对具体语句影响的每一条记录都执行触发器内容。 对于INSERT语句来说，FOR EACH ROW影响的记录就是准备插入的那些新记录。 对于DELETE语句和UPDATE语句来说，FOR EACH ROW影响的记录就是符合条件的那些记录。 针对每一条受影响的记录，需要一种访问该记录中的内容的方式，MySQL提供了NEW和OLD两个单词来分别代表新记录和旧记录，它们在不同语句中的含义不同： 对于INSERT语句设置的触发器来说，NEW代表准备插入的记录，OLD无效。 对于DELETE语句设置的触发器来说，OLD代表删除前的记录，NEW无效。 对于UPDATE语句设置的触发器来说，NEW代表修改后的记录，OLD代表修改前的记录。 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:9:0","series":null,"tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/#触发器"},{"categories":["mysql"],"content":" 示例🤦‍♂️以下示例，对表 t 创建一个 my_trigger触发器，表 t 有三个字段，name，phone，my_join，对于每条 insert 的语句，在执行 insert 之前判断如果 name = admin 那么将即将插入的 name 值改为 valid，如果 name 值为空，将即将插入的 name 值改为无名氏，除此之外将 name 和 phone 拼接后赋给 my_join 字段。 CREATE TRIGGER my_trigger BEFORE INSERT ON t FOR EACH ROW BEGIN IF NEW.name = 'admin' THEN SET NEW.name = 'valid'; ELSEIF NEW.name = '' THEN SET NEW.name = '无名氏'; ELSE SET NEW.my_join = CONCAT(NEW.name, \"--\", NEW.phone); END IF; END EOF 示例演示 CMD 说明 SHOW TRIGGERS; 查看所有触发器 SHOW CREATE TRIGGER 触发器名; 查看某个触发器 DROP TRIGGER 触发器名; 删除某个触发器 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:9:1","series":null,"tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/#示例-3"},{"categories":["mysql"],"content":" 注意事项 触发器内容中不能有输出结果集的语句。 触发器内容中NEW代表记录的列的值可以被更改，OLD代表记录的列的值无法更改。 在BEFORE触发器中，我们可以使用SET NEW.列名 = 某个值的形式来更改待插入记录或者待更新记录的某个列的值，但是这种操作不能在AFTER触发器中使用，因为在执行AFTER 触发器的内容时记录已经被插入完成或者更新完成了。 如果我们的BEFORE触发器内容执行过程中遇到了错误，那这个触发器对应的具体语句将无法执行；如果具体的操作语句执行过程中遇到了错误，那与它对应的AFTER触发器的内容将无法执行。 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:9:2","series":null,"tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/#注意事项"},{"categories":["mysql"],"content":" 事件事件可以让 MySQL 服务器在某个时间点或者每隔一段时间自动地执行一些语句。 默认情况下，MySQL服务器并不会帮助我们执行事件，需要手动开启该功能： SET GLOBAL event_scheduler = ON; 开启事件功能 CREATE EVENT 事件名 ON SCHEDULE { AT 某个确定的时间点| EVERY 期望的时间间隔 [STARTS datetime][END datetime] } DO BEGIN 具体的语句 END ","date":"2020-10-08","objectID":"/mysql-stored-routine/:10:0","series":null,"tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/#事件"},{"categories":["mysql"],"content":" 某个时间点执行 CREATE EVENT insert_t1_event ON SCHEDULE AT '2022-01-03 11:20:11' # 或者 AT DATE_ADD(NOW(), INTERVAL 2 DAY) DO BEGIN INSERT INTO t(phone, name) VALUES('15210214254', '宋江'); END ","date":"2020-10-08","objectID":"/mysql-stored-routine/:10:1","series":null,"tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/#某个时间点执行"},{"categories":["mysql"],"content":" 每隔一段时间执行 CREATE EVENT insert_t1 ON SCHEDULE EVERY 1 HOUR STARTS '2019-09-04 15:48:54' ENDS '2019-09-16 15:48:54' DO BEGIN INSERT INTO t(phone, name) VALUES('15210214254', '宋江'); END 在创建好事件之后，到了指定时间，MySQL 服务器会自动执行。 CMD 说明 SHOW EVENTS; 查看所有事件 SHOW CREATE EVENT 事件名; 查看某个事件 DROP EVENT 事件名; 删除某个事件 ","date":"2020-10-08","objectID":"/mysql-stored-routine/:10:2","series":null,"tags":["mysql"],"title":"mysql 存储程序","uri":"/mysql-stored-routine/#每隔一段时间执行"},{"categories":["golang"],"content":"Linux 环境下安装 Go,Linux Platform Install Go,如何在 Linux 环境下安装 Go,go,golang","date":"2020-08-12","objectID":"/linux-platform-install-go/","series":null,"tags":["golang"],"title":"Linux 环境下安装 Go","uri":"/linux-platform-install-go/"},{"categories":["golang"],"content":" 安装在官网 https://go.dev/dl/，根据自己的环境下载对应的安装包： 官网安装包列表 可以直接用 wget 下载 下载安装包 执行 tar 解压到 /usr/loacl目录下（官方推荐），得到 go 文件夹等。 tar -C /usr/local -zxvf go1.17.7.linux-amd64.tar.gz go1.17.7.linux-amd64.tar.gz 换成你自己的 go 版本。 添加 /usr/loacl/go/bin 目录到 PATH 变量中。添加到 /etc/profile 或 $HOME/.profile 都可以。 vim /etc/profile # 在最后一行添加 export GOROOT=/usr/local/go export PATH=$PATH:$GOROOT/bin 添加环境变量 保存退出后source一下 source /etc/profile go env ","date":"2020-08-12","objectID":"/linux-platform-install-go/:1:0","series":null,"tags":["golang"],"title":"Linux 环境下安装 Go","uri":"/linux-platform-install-go/#安装"},{"categories":["golang"],"content":" Go环境变量 $GOROOT 表示 Go 在你的电脑上的安装位置，值一般都是 $HOME/go，当然，也可以安装在别的地方。 $GOARCH 表示目标机器的处理器架构，它的值可以是 386、amd64 或 arm。 $GOOS 表示目标机器的操作系统，它的值可以是 darwin、freebsd、linux 或 windows。 $GOBIN 表示编译器和链接器的安装位置，默认是 $GOROOT/bin，如果使用的是 Go 1.0.3 及以后的版本，一般情况下你可以将它的值设置为空，Go 将会使用默认值。 $GOPATH 默认采用和 $GOROOT 一样的值，但从 Go 1.1 版本开始，你必须修改为其它路径。它可以包含多个包含 Go 语言源码文件、包文件和可执行文件的路径，而这些路径下又必须分别包含三个规定的目录：src、pkg 和 bin，这三个目录分别用于存放源码文件、包文件和可执行文件。 $GOARM 专门针对基于 arm 架构的处理器，它的值可以是 5~7，默认为 6。 $GOMAXPROCS 用于设置应用程序可使用的处理器个数与核数。 ","date":"2020-08-12","objectID":"/linux-platform-install-go/:2:0","series":null,"tags":["golang"],"title":"Linux 环境下安装 Go","uri":"/linux-platform-install-go/#go环境变量"},{"categories":["开发者手册"],"content":"xiaobinqt,WARNING: bridge-nf-call-iptables is disabled","date":"2020-08-06","objectID":"/bridge-nf-call-iptables-is-disabled/","series":null,"tags":["docker"],"title":"WARNING: bridge-nf-call-iptables is disabled","uri":"/bridge-nf-call-iptables-is-disabled/"},{"categories":["开发者手册"],"content":" 问题今天在使用 docker 时出现如下问题： WARNING: bridge-nf-call-iptables is disabled warning 我的系统版本是CentOS 7.9， 系统版本 ","date":"2020-08-06","objectID":"/bridge-nf-call-iptables-is-disabled/:1:0","series":null,"tags":["docker"],"title":"WARNING: bridge-nf-call-iptables is disabled","uri":"/bridge-nf-call-iptables-is-disabled/#问题"},{"categories":["开发者手册"],"content":" 解决办法 vim /etc/sysctl.conf 在 /etc/sysctl.conf 中添加如下内容： net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 执行 sysctl -p 即可。 ","date":"2020-08-06","objectID":"/bridge-nf-call-iptables-is-disabled/:2:0","series":null,"tags":["docker"],"title":"WARNING: bridge-nf-call-iptables is disabled","uri":"/bridge-nf-call-iptables-is-disabled/#解决办法"},{"categories":["开发者手册"],"content":"xiaobinqt,","date":"2020-07-12","objectID":"/git-common-question/","series":null,"tags":["git"],"title":"Git 常见问题","uri":"/git-common-question/"},{"categories":["开发者手册"],"content":" unable to auto-detect email address这个问题网上固定的解决方案是全局设置用户名和邮箱： git config --global user.email \"you@example.com\" git config --global user.name \"Your Name\" 其实这个问题也可以在提交时单独设置： git -c \"user.name=Your Name\" -c \"user.email=Your email\" commit \"Your commit message\" ","date":"2020-07-12","objectID":"/git-common-question/:1:0","series":null,"tags":["git"],"title":"Git 常见问题","uri":"/git-common-question/#unable-to-auto-detect-email-address"},{"categories":["开发者手册"],"content":" 指定私钥文件 GIT_SSH_COMMAND='ssh -i \"/data/flexcloud/vscode/.ssh/id_rsa\"' git push origin master ","date":"2020-07-12","objectID":"/git-common-question/:2:0","series":null,"tags":["git"],"title":"Git 常见问题","uri":"/git-common-question/#指定私钥文件"},{"categories":["开发者手册"],"content":" are you sure you want to continue connecting 如何想要忽略这个提示，可以在终端中输入👇 GIT_SSH_COMMAND='ssh -o \"UserKnownHostsFile=/dev/null\" -o \"StrictHostKeyChecking=no\"' git push origin master ","date":"2020-07-12","objectID":"/git-common-question/:3:0","series":null,"tags":["git"],"title":"Git 常见问题","uri":"/git-common-question/#are-you-sure-you-want-to-continue-connecting"},{"categories":["开发者手册"],"content":" 参考 Override configured user for a single git commit ","date":"2020-07-12","objectID":"/git-common-question/:4:0","series":null,"tags":["git"],"title":"Git 常见问题","uri":"/git-common-question/#参考"},{"categories":["golang"],"content":"go interface,golang 使用 interface,接口和实例的相互转换,接口的实现,接口是否实现了某个接口","date":"2020-06-18","objectID":"/go-interface/","series":null,"tags":["golang"],"title":"Go interface","uri":"/go-interface/"},{"categories":["golang"],"content":" 定义 interface 可以表示任意一种类型 interface 是接口的方法集合，只要实现了接口中的所有方法，那么就认为实现了这个接口 ","date":"2020-06-18","objectID":"/go-interface/:1:0","series":null,"tags":["golang"],"title":"Go interface","uri":"/go-interface/#定义"},{"categories":["golang"],"content":" 用途","date":"2020-06-18","objectID":"/go-interface/:2:0","series":null,"tags":["golang"],"title":"Go interface","uri":"/go-interface/#用途"},{"categories":["golang"],"content":" 实现多态 package main import ( \"fmt\" ) type animal interface { Say() string Color() string } type Cat struct{} func (c Cat) Say() string { return \"i am a cat\" } func (c Cat) Color() string { return \"i am black\" } type Dog struct{} func (d Dog) Say() string { return \"i am a dog\" } func (d Dog) Color() string { return \"i am white\" } type Car struct{} func introduceSelf(input animal) { fmt.Println(input.Say() + \" and \" + input.Color()) } func main() { c := Cat{} d := Dog{} introduceSelf(c) introduceSelf(d) // car 没有实现 animal 接口 //car := Car{} //introduceSelf(car) } ","date":"2020-06-18","objectID":"/go-interface/:2:1","series":null,"tags":["golang"],"title":"Go interface","uri":"/go-interface/#实现多态"},{"categories":["golang"],"content":" 隐藏具体实现以 go 中的 context 包为例，context.Context() 是一个接口： // A Context carries a deadline, a cancellation signal, and other values across // API boundaries. // // Context's methods may be called by multiple goroutines simultaneously. type Context interface { // Deadline returns the time when work done on behalf of this context // should be canceled. Deadline returns ok==false when no deadline is // set. Successive calls to Deadline return the same results. Deadline() (deadline time.Time, ok bool) // Done returns a channel that's closed when work done on behalf of this // context should be canceled. Done may return nil if this context can // never be canceled. Successive calls to Done return the same value. // The close of the Done channel may happen asynchronously, // after the cancel function returns. // // WithCancel arranges for Done to be closed when cancel is called; // WithDeadline arranges for Done to be closed when the deadline // expires; WithTimeout arranges for Done to be closed when the timeout // elapses. // // Done is provided for use in select statements: // // // Stream generates values with DoSomething and sends them to out // // until DoSomething returns an error or ctx.Done is closed. // func Stream(ctx context.Context, out chan\u003c- Value) error { // for { // v, err := DoSomething(ctx) // if err != nil { // return err // } // select { // case \u003c-ctx.Done(): // return ctx.Err() // case out \u003c- v: // } // } // } // // See https://blog.golang.org/pipelines for more examples of how to use // a Done channel for cancellation. Done() \u003c-chan struct{} // If Done is not yet closed, Err returns nil. // If Done is closed, Err returns a non-nil error explaining why: // Canceled if the context was canceled // or DeadlineExceeded if the context's deadline passed. // After Err returns a non-nil error, successive calls to Err return the same error. Err() error // Value returns the value associated with this context for key, or nil // if no value is associated with key. Successive calls to Value with // the same key returns the same result. // // Use context values only for request-scoped data that transits // processes and API boundaries, not for passing optional parameters to // functions. // // A key identifies a specific value in a Context. Functions that wish // to store values in Context typically allocate a key in a global // variable then use that key as the argument to context.WithValue and // Context.Value. A key can be any type that supports equality; // packages should define keys as an unexported type to avoid // collisions. // // Packages that define a Context key should provide type-safe accessors // for the values stored using that key: // // // Package user defines a User type that's stored in Contexts. // package user // // import \"context\" // // // User is the type of value stored in the Contexts. // type User struct {...} // // // key is an unexported type for keys defined in this package. // // This prevents collisions with keys defined in other packages. // type key int // // // userKey is the key for user.User values in Contexts. It is // // unexported; clients use user.NewContext and user.FromContext // // instead of using this key directly. // var userKey key // // // NewContext returns a new Context that carries value u. // func NewContext(ctx context.Context, u *User) context.Context { // return context.WithValue(ctx, userKey, u) // } // // // FromContext returns the User value stored in ctx, if any. // func FromContext(ctx context.Context) (*User, bool) { // u, ok := ctx.Value(userKey).(*User) // return u, ok // } Value(key interface{}) interface{} } WithCancel 和 WithValue 返回的第一个参数都是 context，但是各自返回的 Context 结构体又不是一样的： WithCancel 返回结构体为 cancelCtx WithValue 返回的结构体为 valueCtx 这样的话尽管返回的都是 context，但是具体实现却不一样，实现了功能的多样化。 ","date":"2020-06-18","objectID":"/go-interface/:2:2","series":null,"tags":["golang"],"title":"Go interface","uri":"/go-interface/#隐藏具体实现"},{"categories":["golang"],"content":" 解耦依赖下面的示例👇，如果缓存从 Redis 换成了 MemoryCache, 我们只需要修改 MemoryCache 的实现和初始化的地方，而不需要修改 Redis 的实现，这样就解耦了依赖，更加灵活。 package main type Cache interface { GetValue(key string) string } // 假设这是redis客户端 type Redis struct { } func (r Redis) GetValue(key string) string { panic(\"not implement\") } // 假设这是自定义的一个缓存器 type MemoryCache struct { } func (m MemoryCache) GetValue(key string) string { panic(\"not implement\") } // 通过接口实现：检查用户是否有权限的功能 func AuthExpire(token string, cache Cache) bool { res := cache.GetValue(token) if res == \"\" { return false } else { // 正常处理 return true } } func main() { token := \"test\" cache := Redis{} // cache := MemoryCache{},修改这一句即可 AuthExpire(token, cache) } ","date":"2020-06-18","objectID":"/go-interface/:2:3","series":null,"tags":["golang"],"title":"Go interface","uri":"/go-interface/#解耦依赖"},{"categories":["golang"],"content":" FAQ","date":"2020-06-18","objectID":"/go-interface/:3:0","series":null,"tags":["golang"],"title":"Go interface","uri":"/go-interface/#faq"},{"categories":["golang"],"content":" var _ Interface = (*Type)(nil) var _ Person = (*Student)(nil) 以上☝️的语句：将空值 nil 转换为 *Student 类型，再转换为 Person 接口，如果转换失败，说明 Student 并没有实现 Person 接口的所有方法。 这是确保接口被实现常用的方式。即利用强制类型转换，确保 struct Student 实现了接口 Person。这样 IDE 和编译期间就可以检查，而不是等到使用的时候。 ","date":"2020-06-18","objectID":"/go-interface/:3:1","series":null,"tags":["golang"],"title":"Go interface","uri":"/go-interface/#var-_-interface--typenil"},{"categories":["golang"],"content":" 实例、接口相互转换实例可以强制类型转换为接口，接口也可以强制类型转换为实例。 package main import \"fmt\" type Student struct { name string age int } type Person interface { getName() string } func (stu *Student) getName() string { return stu.name } func (stu *Student) getAge() int { return stu.age } func main() { var s *Student = \u0026Student{ name: \"narcissus\", age: 20, } var p Person = \u0026Student{ name: \"Tom\", age: 18, } stu := p.(*Student) // 接口转为实例 fmt.Println(stu.getName(), \"---\", stu.getAge()) var pp Person = (*Student)(s) // 实例转为接口 // var _ Person = s // 实例转为接口 fmt.Println(pp.getName()) // 这里不能调用 pp.getAge() 因为 Person 接口中没有 getAge() 方法 } 通过运行后的打印结果👇可知，结果复合预期。 运行打印 ","date":"2020-06-18","objectID":"/go-interface/:3:2","series":null,"tags":["golang"],"title":"Go interface","uri":"/go-interface/#实例接口相互转换"},{"categories":["golang"],"content":" 接口是否实现了某个接口http/server.go 中有这样的判断语句： rw, err := l.Accept() if err != nil { select { case \u003c-srv.getDoneChan(): return ErrServerClosed default: } if ne, ok := err.(net.Error); ok \u0026\u0026 ne.Temporary() { if tempDelay == 0 { tempDelay = 5 * time.Millisecond } else { tempDelay *= 2 } if max := 1 * time.Second; tempDelay \u003e max { tempDelay = max } srv.logf(\"http: Accept error: %v; retrying in %v\", err, tempDelay) time.Sleep(tempDelay) continue } return err } ne, ok := err.(net.Error) 就是判断 err 变量是不是 err.Error 接口类型： // An Error represents a network error. type Error interface { error Timeout() bool // Is the error a timeout? Temporary() bool // Is the error temporary? } 以下示例自定了一个 MyError 接口验证以上推断： package main import \"fmt\" type MyError interface { Msg() string } type myErrorStruct struct { errMsg string } func (e *myErrorStruct) Msg() string { return e.errMsg } func myNew(msg string) MyError { return \u0026myErrorStruct{errMsg: msg} } func main() { err1 := fmt.Errorf(\"error\") e1, ok1 := err1.(MyError) fmt.Println(\"e1 = \", e1, \"ok1 = \", ok1) err2 := myNew(\"我是一个错误\") e2, ok2 := err2.(MyError) fmt.Printf(\"e2 = %#v ,,ok2: %t\", e2, ok2) } 从 👇 运行打印结果来看，符合预期。 ","date":"2020-06-18","objectID":"/go-interface/:3:3","series":null,"tags":["golang"],"title":"Go interface","uri":"/go-interface/#接口是否实现了某个接口"},{"categories":["golang"],"content":" 参考 Consider adding “var _ Interface = (*Type)(nil)” to the list of recommendations 类似var _ PeerPicker = (*HTTPPool)(nil)这种设计目的是什么 ","date":"2020-06-18","objectID":"/go-interface/:4:0","series":null,"tags":["golang"],"title":"Go interface","uri":"/go-interface/#参考"},{"categories":["开发者手册"],"content":"xiaobinqt,docker swarm","date":"2020-06-14","objectID":"/docker-swarm/","series":null,"tags":["docker"],"title":"Docker Swarm 笔记","uri":"/docker-swarm/"},{"categories":["开发者手册"],"content":" 作为容器集群管理器，Swarm 最大的优势之一就是原生支持 Docker API。各种基于标准 API 的工具比如 Compose、Docker SDK、各种管理软件，甚至 Docker 本身等都可以很容易的与 Swarm 进行集成，这大大方便了用户将原先基于单节点的系统移植到 Swarm 上。同时 Swarm 内置了对 Docker 网络插件的支持，这样就可以很容易地部署跨主机的容器集群服务。 主从结构 Swarm 采用了典型的主从结构，通过 Raft 协议来在多个管理节点Manager中实现共识。工作节点Worker 上运行 agent 接受管理节点的统一管理和任务分配。用户提交服务请求只需要发给管理节点即可，管理节点会按照调度策略在集群中分配节点来运行服务相关的任务。 ","date":"2020-06-14","objectID":"/docker-swarm/:0:0","series":null,"tags":["docker"],"title":"Docker Swarm 笔记","uri":"/docker-swarm/#"},{"categories":["开发者手册"],"content":" 基本概念","date":"2020-06-14","objectID":"/docker-swarm/:1:0","series":null,"tags":["docker"],"title":"Docker Swarm 笔记","uri":"/docker-swarm/#基本概念"},{"categories":["开发者手册"],"content":" Swarm集群Swarm集群Cluster为一组被统一管理起来的 Docker 主机。集群是 Swarm 所管理的对象。这些主机通过 Docker 引擎的 Swarm 模式相互沟通，其中部分主机可能作为管理节点manager响应外部的管理请求，其他主机作为工作节点worker 来实际运行 Docker 容器。同一个主机也可以既作为管理节点，同时作为工作节点。 当使用 Swarm 集群时，首先定义一个服务（指定状态、复制个数、网络、存储、暴露端口等），然后通过管理节点发出启动服务的指令，管理节点随后会按照指定的服务规则进行调度，在集群中启动起来整个服务，并确保它正常运行。 ","date":"2020-06-14","objectID":"/docker-swarm/:1:1","series":null,"tags":["docker"],"title":"Docker Swarm 笔记","uri":"/docker-swarm/#swarm集群"},{"categories":["开发者手册"],"content":" 节点节点Node是 Swarm 集群的最小资源单位。每个节点实际上都是一台 Docker 主机。Swarm 集群中节点分为两种： 管理节点manager node：负责响应外部对集群的操作请求，并维持集群中资源，分发任务给工作节点。同时，多个管理节点之间通过Raft协议构成共识。一般推荐每个集群设置5个或7个管理节点。 工作节点worker node：负责执行管理节点安排的具体任务。默认情况下，管理节点自身也同时是工作节点。每个工作节点上运行代理（agent）来汇报任务完成情况。 可以通过docker node promote命令来提升一个工作节点为管理节点；或者通过docker node demote命令来将一个管理节点降级为工作节点。 ","date":"2020-06-14","objectID":"/docker-swarm/:1:2","series":null,"tags":["docker"],"title":"Docker Swarm 笔记","uri":"/docker-swarm/#节点"},{"categories":["开发者手册"],"content":" 服务一个服务Service可以由若干个任务组成，每个任务为某个具体的应用。服务还包括对应的存储、网络、端口映射、副本个数、访问配置、升级配置等附加参数。 一般来说，服务需要面向特定的场景，例如一个典型的 Web 服务可能包括前端应用、后端应用，以及数据库等。这些应用都属于该服务的管理范畴。 Swarm 集群中服务类型也分为两种（可以通过-mode指定）： 复制服务replicated services模式：默认模式，每个任务在集群中会存在若干副本，这些副本会被管理节点按照调度策略分发到集群中的工作节点上。此模式下可以使用-replicas参数设置副本数量。 全局服务global services模式：调度器将在每个可用节点都执行一个相同的任务。该模式适合运行节点的检查，如监控应用等 ","date":"2020-06-14","objectID":"/docker-swarm/:1:3","series":null,"tags":["docker"],"title":"Docker Swarm 笔记","uri":"/docker-swarm/#服务"},{"categories":["开发者手册"],"content":" 任务任务是 Swarm 集群中最小的调度单位，即一个指定的应用容器。例如仅仅运行前端业务的前端容器。任务从生命周期上将可能处于创建NEW、等待PENDING 、分配ASSIGNED、接受ACCEPTED、准备PREPARING、开始STARTING、运行RUNNING、完成COMPLETE、失败FAILED 、关闭SHUTDOWN、拒绝REJECTED、孤立ORPHANED等不同状态。 Swarm 集群中的管理节点会按照调度要求将任务分配到工作节点上。例如指定副本为 2 时，可能会被分配到两个不同的工作节点上。一旦当某个任务被分配到一个工作节点，将无法被转移到另外的工作节点，即 Swarm 中的任务不支持迁移。 ","date":"2020-06-14","objectID":"/docker-swarm/:1:4","series":null,"tags":["docker"],"title":"Docker Swarm 笔记","uri":"/docker-swarm/#任务"},{"categories":["开发者手册"],"content":" 外部访问Swarm 集群中的服务要被集群外部访问，必须要能允许任务的响应端口映射出来。Swarm 中支持入口负载均衡ingress load balancing的映射模式。该模式下，每个服务都会被分配一个[公开端口]^( PublishedPort)，该端口在集群中任意节点上都可以访问到，并被保留给该服务。 当有请求发送到任意节点的公开端口时，该节点若并没有实际执行服务相关的容器，则会通过路由机制将请求转发给实际执行了服务容器的工作节点。 ","date":"2020-06-14","objectID":"/docker-swarm/:1:5","series":null,"tags":["docker"],"title":"Docker Swarm 笔记","uri":"/docker-swarm/#外部访问"},{"categories":["开发者手册"],"content":" 搭建集群我用 VMWare 搭建了 2 台主机，IP 分别为 192.168.48.125 和 192.168.48.8，现在将192.168.48.128作为管理节点，将192.168.48.8作为工作节点。 在集群中，时间同步是很重要，可以使用ntp先同步时间。 yum -y install ntp systemctl enable ntpd systemctl start ntpd ","date":"2020-06-14","objectID":"/docker-swarm/:2:0","series":null,"tags":["docker"],"title":"Docker Swarm 笔记","uri":"/docker-swarm/#搭建集群"},{"categories":["开发者手册"],"content":" 创建集群 docker swarm init --advertise-addr 192.168.48.125 swarm init 注意返回的 token 串，这是集群的唯一 id，加入集群的各个节点将需要这个信息。 默认的管理服务端口为 2377，需要能被工作节点访问到；另外，为了支持集群的成员发现和外部服务映射，还需要再所有节点上开启 7946 TCP/UDP 端口和 4789 UDP 端口。 开发端口 关于 Centos7.x 开放端口可以参考https://blog.csdn.net/qq_39007083/article/details/106875997 。 ","date":"2020-06-14","objectID":"/docker-swarm/:2:1","series":null,"tags":["docker"],"title":"Docker Swarm 笔记","uri":"/docker-swarm/#创建集群"},{"categories":["开发者手册"],"content":" 加入集群在所有要加入集群的普通节点上面执行swarm join命令，表示把这台机器加入指定集群当中。例如，在192.168.48.8工作节点上，将其加入刚创建的集群，则可以通过： docker swarm join --token SWMTKN-1-15692f3ho3t3oi68ljnv0fi5nxerox2jsuplmhv0qzerzqpfh1-er9ufvvh4bym5o3iifummtvnf 192.168.48.125:2377 加入集群 此时在管理节点可以看到刚加进来的192.168.48.8工作节点： 工作节点加入成功 ","date":"2020-06-14","objectID":"/docker-swarm/:2:2","series":null,"tags":["docker"],"title":"Docker Swarm 笔记","uri":"/docker-swarm/#加入集群"},{"categories":["开发者手册"],"content":" 使用集群服务搭建成功的集群，可以使用使用docker service命令使用 Swarm 提供的服务。 可以在管理节点上执行如下命令来快速创建一个应用服务，并制定服务的复制份数为 2。如下命令所示，默认会自动检查确认服务状态都正常： docker service create --replicas 2 --name ping_app debian:jessie ping docker.com docker service create 在管理节点上使用 docker service ls 可以查看集群中服务情况： service ls 可以看到，管理节点和工作节点上都运行了一个容器，镜像为debian:jessie： service ps ","date":"2020-06-14","objectID":"/docker-swarm/:2:3","series":null,"tags":["docker"],"title":"Docker Swarm 笔记","uri":"/docker-swarm/#使用集群服务"},{"categories":["开发者手册"],"content":" 扩展服务可以通过 docker service scale \u003cSERVICE-ID\u003e=\u003cNUMBER-OF-TASKS\u003e 命令来对服务进行伸缩，例如将服务复制个数从 2 改为 1。 扩展服务 ","date":"2020-06-14","objectID":"/docker-swarm/:2:4","series":null,"tags":["docker"],"title":"Docker Swarm 笔记","uri":"/docker-swarm/#扩展服务"},{"categories":["开发者手册"],"content":" 离开集群节点可以在任何时候通过swarm leave命令离开一个集群。命令格式为dockerswarm leave [OPTIONS]，支持-f, --force意味着强制离开集群。 ","date":"2020-06-14","objectID":"/docker-swarm/:2:5","series":null,"tags":["docker"],"title":"Docker Swarm 笔记","uri":"/docker-swarm/#离开集群"},{"categories":["开发者手册"],"content":" 常用命令Docker 通过 service 命令来管理应用服务，主要包括create、inspect、logs、ls、ps、rm、rollback、scale、update等若干子命令： 常用命令 ","date":"2020-06-14","objectID":"/docker-swarm/:2:6","series":null,"tags":["docker"],"title":"Docker Swarm 笔记","uri":"/docker-swarm/#常用命令"},{"categories":["开发者手册"],"content":" 参考 Docker Swarm 深入浅出 docker-swarm 节点增加、删除、权限提升、降低、服务部署、配置可视化界面、stack等一系列操作 https://docs.docker.com/engine/swarm/ ","date":"2020-06-14","objectID":"/docker-swarm/:3:0","series":null,"tags":["docker"],"title":"Docker Swarm 笔记","uri":"/docker-swarm/#参考"},{"categories":["golang"],"content":"go,continue,break,goto label的区别,Golang","date":"2020-05-16","objectID":"/break-continue-goto-label/","series":null,"tags":["golang"],"title":"Go break，continue，goto label 的区别","uri":"/break-continue-goto-label/"},{"categories":["golang"],"content":" 在其他语言，比如 php 中可以直接在 break 和 continue 后加 num ，比如 break 2或 continue 2。 break num 是结束外层第 num 层整个循环体，continue num 是结束外层第 num 层单次循环。 go 中不能直接在关键字后加 num ，但是可以用 label 关键代替 num。支持 goto label，也可以用 break label 和 continue label。 goto 可以跳到代码的任何地方，比如跳到 A 处，再从 A 继续往下执行。break label 如果跳到了循环外，不会再执行循环。continue label 如果跳出了循环，会再执行循环。 ","date":"2020-05-16","objectID":"/break-continue-goto-label/:0:0","series":null,"tags":["golang"],"title":"Go break，continue，goto label 的区别","uri":"/break-continue-goto-label/#"},{"categories":["golang"],"content":" continue label package main import ( \"fmt\" \"math\" ) func main() { // 找出 int 切片的最小值 var matrix = []int{10, 2, 4, 0} var min = math.MinInt64 next: for _, v := range matrix { for _, v1 := range matrix { if v \u003e v1 { continue next // 终止当前循环，跳到 label 继续下一次循环 } } min = v } fmt.Println(\"最小值为: \", min) } ","date":"2020-05-16","objectID":"/break-continue-goto-label/:1:0","series":null,"tags":["golang"],"title":"Go break，continue，goto label 的区别","uri":"/break-continue-goto-label/#continue-label"},{"categories":["golang"],"content":" break label👇 以下例子，虽然 break 跳出循环到 label 处, label 在 for 循环上，但是不会再执行 for 循环，直接执行fmt.Println()。 package main import ( \"fmt\" \"math\" ) func main() { // 获取 index 2 的值，这里使用 2 层循环主要是为了说明问题 var matrix = []int{10, 2, 4, 0} var index2Val = math.MinInt64 next: for _, v := range matrix { fmt.Println(v) for index, v1 := range matrix { index2Val = v1 if index == 2 { break next } } } fmt.Println(\"index 3 值为: \", index2Val) } ","date":"2020-05-16","objectID":"/break-continue-goto-label/:2:0","series":null,"tags":["golang"],"title":"Go break，continue，goto label 的区别","uri":"/break-continue-goto-label/#break-label"},{"categories":["golang"],"content":" goto label非必要不使用，可以跳到任何地方。 package main import ( \"fmt\" \"math\" ) func main() { var matrix = []int{10, 2, 4, 0} var index2Val = math.MinInt64 for _, v := range matrix { fmt.Println(v) for index, v1 := range matrix { index2Val = v1 if index == 2 { goto next } } } fmt.Println(\"index 3 值为: \", index2Val) next: fmt.Println(\"goto this....\") } ","date":"2020-05-16","objectID":"/break-continue-goto-label/:3:0","series":null,"tags":["golang"],"title":"Go break，continue，goto label 的区别","uri":"/break-continue-goto-label/#goto-label"},{"categories":["开发者手册"],"content":"xiaobinqt,docker root用户执行","date":"2020-05-06","objectID":"/docker-summary-of-common-usage/","series":null,"tags":["docker","备忘"],"title":"Docker 常用命令备忘","uri":"/docker-summary-of-common-usage/"},{"categories":["开发者手册"],"content":" root 用户执行有时进入容器后，用户就是变成非 root 用户，这种时候又没有密码，在执行一些操作的时候就会非常不方便，这是可以用 -u root 来指定用户。 非root用户 执行简单命令可以这样👇 图01 如果需要进入容器，可以这样👇 docker exec -u root -it 容器名 bash #或者 docker exec -u root -it 容器名 sh 图02 ","date":"2020-05-06","objectID":"/docker-summary-of-common-usage/:1:0","series":null,"tags":["docker","备忘"],"title":"Docker 常用命令备忘","uri":"/docker-summary-of-common-usage/#root-用户执行"},{"categories":["golang"],"content":"xiaobinqt,go select 用法,go select 阻塞,如何使用 go select,如何跳出 go select, nil 通道如何处理","date":"2020-04-15","objectID":"/go-select/","series":null,"tags":["golang","go-select"],"title":"Go select 用法简述","uri":"/go-select/"},{"categories":["golang"],"content":" select 功能在多个通道上进行读或写操作，让函数可以处理多个事情，但 1 次只处理 1 个。select 有以下特征： 每次执行 select ，都会只执行其中 1 个 case 或者执行 default 语句。 当没有 case 或者 default 可以执行时，select 则阻塞，等待直到有 1 个 case 可以执行。 当有多个 case 可以执行时，则随机选择 1 个 case 执行。 case后面跟的必须是读或者写通道的操作，否则编译出错。 由select和case组成，default不是必须的。 package main import \"fmt\" func main() { readCh := make(chan int, 1) writeCh := make(chan int, 1) y := 1 select { case x := \u003c-readCh: fmt.Printf(\"Read %d\\n\", x) case writeCh \u003c- y: fmt.Printf(\"Write %d\\n\", y) default: fmt.Println(\"Do what you want\") } } 我们创建了readCh和writeCh2个通道： readCh中没有数据，所以case x := \u003c-readCh读不到数据，所以这个case不能执行。 writeCh是带缓冲区的通道，它里面是空的，可以写入1个数据，所以case writeCh \u003c- y可以执行。 有case可以执行，所以default不会执行。 这个测试的结果是 λ go run t.go Write 1 ","date":"2020-04-15","objectID":"/go-select/:1:0","series":null,"tags":["golang","go-select"],"title":"Go select 用法简述","uri":"/go-select/#select-功能"},{"categories":["golang"],"content":" 用打豆豆实践 select有句话说，“吃饭睡觉打豆豆”，这一句话里包含了3件事： 妈妈喊你吃饭，你去吃饭。 时间到了，要睡觉。 没事做，打豆豆。 在Golang里，select 就是干这个事的：到吃饭了去吃饭，该睡觉了就睡觉，没事干就打豆豆。 我们看看select怎么实现打豆豆：eat()函数会启动1个协程，该协程先睡几秒，事件不定，然后喊你吃饭，main()函数中的sleep是个定时器，每3秒喊你吃1次饭，select则处理3种情况： 从eatCh中读到数据，代表有人喊我吃饭，我要吃饭了。 从sleep.C中读到数据，代表闹钟时间到了，我要睡觉。 default是，没人喊我吃饭，也不到时间睡觉，我就打豆豆。 package main import ( \"fmt\" \"math/rand\" \"time\" ) func eat() chan string { out := make(chan string) go func() { rand.Seed(time.Now().UnixNano()) time.Sleep(time.Duration(rand.Intn(5)) * time.Second) out \u003c- \"Mom call you eating\" close(out) }() return out } func main() { eatCh := eat() sleep := time.NewTimer(time.Second * 3) select { case s := \u003c-eatCh: fmt.Println(s) case \u003c-sleep.C: fmt.Println(\"Time to sleep\") default: fmt.Println(\"Beat DouDou\") } } 由于前2个case都要等待一会，所以都不能执行，所以执行default，运行结果一直是打豆豆： λ go run t.go Beat DouDou 现在不打豆豆了，把default的逻辑删掉，多运行几次，有时候会吃饭，有时候会睡觉，比如这样： λ go run x.go Mom call you eating λ go run x.go Time to sleep λ go run x.go Time to sleep ","date":"2020-04-15","objectID":"/go-select/:2:0","series":null,"tags":["golang","go-select"],"title":"Go select 用法简述","uri":"/go-select/#用打豆豆实践-select"},{"categories":["golang"],"content":" nil通道永远阻塞当case上读一个通道时，如果这个通道是nil，则该case永远阻塞。 这个功能有1个妙用，select通常处理的是多个通道，当某个读通道关闭了，但不想select再继续关注此case，继续处理其他case，把该通道设置为nil即可。 下面是一个合并程序等待两个输入通道都关闭后才退出的例子，就使用了这个特性。 package main import ( \"fmt\" \"time\" ) func main() { ch1 := gen(0, 1) ch2 := gen(5, 25) out := combine(ch1, ch2) for x := range out { fmt.Println(x) } time.Sleep(20 * time.Second) } func gen(min, max int) chan int { ch := make(chan int) go func() { defer close(ch) for i := min; i \u003c= max; i++ { x := i ch \u003c- x } }() return ch } // inCh1,inCh2 只读 func combine(inCh1, inCh2 \u003c-chan int) \u003c-chan int { // 输出通道 out := make(chan int) // 启动协程合并数据 go func() { defer close(out) for { select { case x, open := \u003c-inCh1: fmt.Printf(\"inCh1: %v, %v\\n\", x, open) if !open { fmt.Println(\"inCh1 closed break\") inCh1 = nil break // 这里 break 不会跳出 for 循环，只会跳出 select,下次再次进入 select 将会从 inCh2 中读取数据 } out \u003c- x case x, open := \u003c-inCh2: if !open { fmt.Println(\"inCh2 closed break\") inCh2 = nil break } out \u003c- x } fmt.Println(\"hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\") // 当ch1和ch2都关闭时才退出 if inCh1 == nil \u0026\u0026 inCh2 == nil { fmt.Printf(\"222222222 inCh1:%v, inCh2:%v, inCh1 is nil: %t ,inCh2 is nil: %t \\n\", inCh1, inCh2, inCh1 == nil, inCh2 == nil) break } } }() return out } ","date":"2020-04-15","objectID":"/go-select/:3:0","series":null,"tags":["golang","go-select"],"title":"Go select 用法简述","uri":"/go-select/#nil通道永远阻塞"},{"categories":["golang"],"content":" 如何跳出for-selectbreak在select内的并不能跳出for-select循环。 👇下面的例子，consume函数从通道inCh不停读数据，期待在inCh关闭后退出for-select循环，但结果是永远没有退出。 package main import ( \"fmt\" \"time\" ) func main() { ch := make(chan int) go func(ch chan int) { defer close(ch) for i := 0; i \u003c 5; i++ { ch \u003c- i time.Sleep(1 * time.Second) } }(ch) consume(ch) time.Sleep(1 * time.Hour) } func consume(inCh \u003c-chan int) { i := 0 for { fmt.Printf(\"for: %d\\n\", i) select { case x, open := \u003c-inCh: if !open { fmt.Println(\"closed................\") time.Sleep(3 * time.Second) break } fmt.Printf(\"read: %d\\n\", x) } i++ } fmt.Println(\"consume-routine exit\") } 运行结果： λ go run t.go for: 0 read: 0 for: 1 read: 1 for: 2 read: 2 for: 3 read: 3 for: 4 read: 4 for: 5 closed................ for: 6 closed................ ... // never stop 既然break不能跳出for-select，那怎么办呢😢？以下是三种方式： 在满足条件的case内，使用return，如果有结尾工作，尝试交给defer。 在select外for内使用break挑出循环，如combine函数。 使用goto。 ","date":"2020-04-15","objectID":"/go-select/:4:0","series":null,"tags":["golang","go-select"],"title":"Go select 用法简述","uri":"/go-select/#如何跳出for-select"},{"categories":["golang"],"content":" select{}永远阻塞 package main import ( \"fmt\" \"time\" ) func main() { ch := make(chan int) go func(ch chan int) { defer close(ch) var i = 0 for { fmt.Printf(\"i: %d\\n\", i) ch \u003c- time.Now().Second() time.Sleep(1 * time.Second) i++ } }(ch) go func(ch \u003c-chan int) { for x := range ch { fmt.Printf(\"read: %d\\n\", x) } }(ch) select {} } select{}的效果等价于创建了1个通道，直接从通道读数据👇 ch := make(chan int) \u003c-ch 但是，这个写起来多麻烦，没select{}简洁。 永远阻塞能有什么用呢！？ 当你开发一个并发程序的时候，main函数千万不能在子协程干完活前退出啊，不然所有的协程都被迫退出了，还怎么提供服务呢？ 比如，写了个Web服务程序，端口监听、后端处理等等都在子协程跑起来了，main函数这时候能退出吗？ ","date":"2020-04-15","objectID":"/go-select/:5:0","series":null,"tags":["golang","go-select"],"title":"Go select 用法简述","uri":"/go-select/#select永远阻塞"},{"categories":["golang"],"content":" 参考 https://github.com/Shitaibin/golang_step_by_step/tree/master/golang_select ","date":"2020-04-15","objectID":"/go-select/:6:0","series":null,"tags":["golang","go-select"],"title":"Go select 用法简述","uri":"/go-select/#参考"},{"categories":["算法与数学"],"content":"常见缓存淘汰策略，淘汰策略的实现，FIFO，LRU，LFU","date":"2020-03-05","objectID":"/common-cache-strategies/","series":null,"tags":["算法"],"title":"常见缓存淘汰策略","uri":"/common-cache-strategies/"},{"categories":["算法与数学"],"content":" 常见缓存淘汰策略","date":"2020-03-05","objectID":"/common-cache-strategies/:0:0","series":null,"tags":["算法"],"title":"常见缓存淘汰策略","uri":"/common-cache-strategies/#常见缓存淘汰策略"},{"categories":["算法与数学"],"content":" FIFOFirst In First Out(FIFO)，先进先出，也就是淘汰缓存中最老(最早添加)的记录。FIFO 认为，最早添加的记录，其不再被使用的可能性比刚添加的可能性大。这种算法的实现也非常简单，创建一个队列，新增记录添加到队尾， 每次内存不够时，淘汰队首。但是很多场景下，部分记录虽然是最早添加但也最常被访问，而不得不因为呆的时间太长而被淘汰。这类数据会被频繁地添加进缓存，又被淘汰出去，导致缓存命中率降低。 ","date":"2020-03-05","objectID":"/common-cache-strategies/:1:0","series":null,"tags":["算法"],"title":"常见缓存淘汰策略","uri":"/common-cache-strategies/#fifo"},{"categories":["算法与数学"],"content":" LFULeast Frequently Used(LFU)，最少使用，也就是淘汰缓存中访问频率最低的记录。LFU 认为，如果数据过去被访问多次， 那么将来被访问的频率也更高。LFU 的实现需要维护一个按照访问次数排序的队列，每次访问，访问次数加1，队列重新排序， 淘汰时选择访问次数最少的即可。LFU 算法的命中率是比较高的，但缺点也非常明显，维护每个记录的访问次数，对内存的消耗是很高的； 另外，如果数据的访问模式发生变化，LFU 需要较长的时间去适应，也就是说 LFU 算法受历史数据的影响比较大。例如某个数据历史上访问次数奇高，但在某个时间点之后几乎不再被访问，但因为历史访问次数过高，而迟迟不能被淘汰。 ","date":"2020-03-05","objectID":"/common-cache-strategies/:2:0","series":null,"tags":["算法"],"title":"常见缓存淘汰策略","uri":"/common-cache-strategies/#lfu"},{"categories":["算法与数学"],"content":" LRULeast Recently Used(LRU)，最近最少使用，相对于仅考虑时间因素的 FIFO 和仅考虑访问频率的 LFU，LRU 算法可以认为是相对平衡的 一种淘汰算法。LRU 认为，如果数据最近被访问过，那么将来被访问的概率也会更高。LRU 算法的实现非常简单，维护一个队列，如果某条记录被访问了， 则移动到队尾，那么队首则是最近最少访问的数据，淘汰该条记录即可。 ","date":"2020-03-05","objectID":"/common-cache-strategies/:3:0","series":null,"tags":["算法"],"title":"常见缓存淘汰策略","uri":"/common-cache-strategies/#lru"},{"categories":["开发者手册"],"content":"缩写,SGTM,PR,LGTM,WIP,QPS,FD,OOM,TTL","date":"2020-02-18","objectID":"/funny-abbreviations/","series":null,"tags":["chore"],"title":"那些迷之缩写","uri":"/funny-abbreviations/"},{"categories":["开发者手册"],"content":"互联网是个造词的行业，娴熟的司机们都会使用缩写来达到提高逼格的效果。 某些缩写在我们第一次看到时会有一脸懵逼的感觉，这里整理一下作者在工作和生活中遇到的一些缩写及其含义，以后我们也可以欢快地装逼了。 缩写 说明 PR Pull Request，给其他项目提交代码 LGTM Looks Good To Me. 朕知道了 代码已经过 review，可以合并 SGTM Sounds Good To Me. 和上面那句意思差不多，也是已经通过了 review 的意思 WIP Work In Progress. 传说中提 PR 的最佳实践是，如果你有个改动很大的 PR，可以在写了一部分的情况下先提交，但是在标题里写上 WIP，以告诉项目维护者这个功能还未完成，方便维护者提前 review 部分提交的代码。 PTAL Please Take A Look. 你来瞅瞅？用来提示别人来看一下 TBR To Be Reviewed，提示维护者进行 review TL;DR Too Long; Didn’t Read. 太长懒得看。也有很多文档在做简略描述之前会写这么一句 TBD To Be Done (or Defined/Discussed/Decided/Determined). 根据语境不同意义有所区别，但一般都是还没搞定的意思 QPS Query Per Second，服务器每秒可以执行的查询次数 fd 文件描述符File descriptor，是一个用于表述指向文件的引用的抽象化概念 OOM Out of memory，内存用完了 TTL the remaining time to live ，剩余存活时间 NFS Network File System，使用者访问网络上别处的文件就像在使用自己的计算机一样 FAQ frequently asked questions，常见问题 CS/cs cs 是计算机科学的缩写，全称叫 Client Server ","date":"2020-02-18","objectID":"/funny-abbreviations/:0:0","series":null,"tags":["chore"],"title":"那些迷之缩写","uri":"/funny-abbreviations/#"},{"categories":["mysql"],"content":"xiaobinqt, mysql 常见问题,主键和 unique 的区别,什么是 mysql 结束符,什么是 mysql 外键,什么是 ZEROFILL显示宽度,limit offset 的区别,mysql 判断语句,mysql 循环语句,mysql DUPLICATE KEY UPDATE 用法,mysql 变量","date":"2019-10-03","objectID":"/mysql-simple-faq/","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/"},{"categories":["mysql"],"content":" 主键和 UNIQUE 的区别主键和UNIQUE约束都能保证某个列或者列组合的唯一性，但是： 一张表中只能定义一个主键，却可以定义多个UNIQUE约束！ 主键列不允许存放NULL，而声明了UNIQUE属性的列可以存放NULL，而且NULL可以重复地出现在多条记录中。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:1:0","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#主键和-unique-的区别"},{"categories":["mysql"],"content":" 结束符 delimiter EOF # 将结束符改为 EOF 修改结束符 由☝️图可知，将默认的结束符从 ; 改为 EOF。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:2:0","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#结束符"},{"categories":["mysql"],"content":" 什么是外键如果A表中的某个列或者某些列依赖与B表中的某个列或者某些列，那么就称A表为子表，B表为父表。子表和父表可以使用外键来关联起来。 父表中被子表依赖的列或者列组合必须建立索引，如果该列或者列组合已经是主键或者有UNIQUE属性，那么也就被默认建立了索引。 定义外键的语法： CONSTRAINT [外键名称] FOREIGN KEY(列1, 列2, ...) REFERENCES 父表名(父列1, 父列2, ...); ","date":"2019-10-03","objectID":"/mysql-simple-faq/:3:0","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#什么是外键"},{"categories":["mysql"],"content":" 示例 CREATE TABLE student_score ( number INT, -- 学号 subject VARCHAR(30), score TINYINT, PRIMARY KEY (number, subject), CONSTRAINT FOREIGN KEY (number) REFERENCES student_info (number) ); ☝️ 如上，在对student_score表插入数据的时候，MySQL都会检查插入的学号是否能在student_info 表中找到，如果找不到则会报错，因为student_score表中的number 列依赖于student_info表的number列，也就是，如果没有这个学生，何来成绩？ ","date":"2019-10-03","objectID":"/mysql-simple-faq/:3:1","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#示例"},{"categories":["mysql"],"content":" ZEROFILL对于无符号整数类型的列，可以在查询数据的时候让数字左边补 0，如果想实现这个效果需要给该列加一个ZEROFILL属性： CREATE TABLE zerofill_table ( i1 INT(10) UNSIGNED ZEROFILL, i2 INT UNSIGNED ); INT后边的(5)，这个 5 就是显示宽度，默认是10，也就是 INT 也 INT(10) 效果是一样的。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:4:0","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#zerofill"},{"categories":["mysql"],"content":" 注意 该列必须是整数类型 该列必须有 UNSIGNED ZEROFILL的属性 该列的实际值的位数必须小于显示宽度 在创建表的时候，如果声明了ZEROFILL属性的列没有声明UNSIGNED属性，MySQL会为该列自动生成UNSIGNED属性 显示宽度并不会影响实际类型的实际存储空间 对于没有声明ZEROFILL属性的列，显示宽度没有任何作用，只有在查询声明了ZEROFILL属性的列时，显示宽度才会起作用，否则可以忽略显示宽度这个东西的存在。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:4:1","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#注意"},{"categories":["mysql"],"content":" limit、offset 区别 从 0 开始计数，第1条记录在 MYSQL 中是第 0 条。 limit 和 offset 都可以用来限制查询条数，一般用做分页。 当 limit 后面跟一个参数的时候，该参数表示要取的数据的数量 select* from user limit 3 表示直接取前三条数据。 当 limit 后面跟两个参数的时候，第一个数表示开始行，后一位表示要取的数量，例如 select * from user limit 1,3; 从 0 行开始计算，取第 1 - 3 条数据，也就是取 1,2,3 三条数据。 当 limit 和 offset 组合使用的时候，limit 后面只能有一个参数，表示要取的的数量，offset 表示开始行。 select * from user limit 3 offset 1; 从 0 行开始计算，取第 1 - 3 条数据，也就是取 1,2,3 三条数据。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:5:0","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#limitoffset-区别"},{"categories":["mysql"],"content":" 常用函数","date":"2019-10-03","objectID":"/mysql-simple-faq/:6:0","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#常用函数"},{"categories":["mysql"],"content":" 文本处理函数 名称 调用示例 示例结果 描述 LEFT LEFT('abc123', 3) abc 给定字符串从左边取指定长度的子串 RIGHT RIGHT('abc123', 3) 123 给定字符串从右边取指定长度的子串 LENGTH LENGTH('abc') 3 给定字符串的长度 LOWER LOWER('ABC') abc 给定字符串的小写格式 UPPER UPPER('abc') ABC 给定字符串的大写格式 LTRIM LTRIM(' abc') abc 给定字符串左边空格去除后的格式 RTRIM RTRIM('abc ') abc 给定字符串右边空格去除后的格式 SUBSTRING SUBSTRING('abc123', 2, 3) bc1 给定字符串从指定位置截取指定长度的子串 CONCAT CONCAT('abc', '123', 'xyz') abc123xyz 将给定的各个字符串拼接成一个新字符串 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:6:1","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#文本处理函数"},{"categories":["mysql"],"content":" 时间处理函数 名称 调用示例 示例结果 描述 NOW NOW() 2019-08-16 17:10:43 返回当前日期和时间 CURDATE CURDATE() 2019-08-16 返回当前日期 CURTIME CURTIME() 17:10:43 返回当前时间 DATE DATE('2019-08-16 17:10:43') 2019-08-16 将给定日期和时间值的日期提取出来 DATE_ADD DATE_ADD('2019-08-16 17:10:43', INTERVAL 2 DAY) 2019-08-18 17:10:43 将给定的日期和时间值添加指定的时间间隔 DATE_SUB DATE_SUB('2019-08-16 17:10:43', INTERVAL 2 DAY) 2019-08-14 17:10:43 将给定的日期和时间值减去指定的时间间隔 DATEDIFF DATEDIFF('2019-08-16', '2019-08-17') -1 返回两个日期之间的天数（负数代表前一个参数代表的日期比较小） DATE_FORMAT DATE_FORMAT(NOW(),'%m-%d-%Y') 08-16-2019 用给定的格式显示日期和时间 常见时间单位 时间单位 描述 MICROSECOND 毫秒 SECOND 秒 MINUTE 分钟 HOUR 小时 DAY 天 WEEK 星期 MONTH 月 QUARTER 季度 YEAR 年 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:6:2","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#时间处理函数"},{"categories":["mysql"],"content":" 数值处理函数 名称 调用示例 示例结果 描述 ABS ABS(-1) 1 取绝对值 Pi PI() 3.141593 返回圆周率 COS COS(PI()) -1 返回一个角度的余弦 EXP EXP(1) 2.718281828459045 返回e的指定次方 MOD MOD(5,2) 1 返回除法的余数 RAND RAND() 0.7537623539136372 返回一个随机数 SIN SIN(PI()/2) 1 返回一个角度的正弦 SQRT SQRT(9) 3 返回一个数的平方根 TAN TAN(0) 0 返回一个角度的正切 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:6:3","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#数值处理函数"},{"categories":["mysql"],"content":" COUNT 函数COUNT函数使用来统计行数的，有下边两种使用方式： COUNT(*)：对表中行的数目进行计数，不管列的值是不是NULL。 COUNT(列名)：对特定的列进行计数，会忽略掉该列为NULL的行。 两者的区别是会不会忽略统计列的值为NULL的行。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:7:0","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#count-函数"},{"categories":["mysql"],"content":" 查询where 竟然可以这么写😇 select * from edge where (ip, mode) = ('192.168.50.101', 2); in 竟然可以这么写😂 select * from edge where (ip, mode) in (select '192.168.50.101', 2); ","date":"2019-10-03","objectID":"/mysql-simple-faq/:8:0","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#查询"},{"categories":["mysql"],"content":" 判断语句","date":"2019-10-03","objectID":"/mysql-simple-faq/:9:0","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#判断语句"},{"categories":["mysql"],"content":" if then IF 表达式 THEN 处理语句列表 [ELSEIF 表达式 THEN 处理语句列表] ... # 这里可以有多个ELSEIF语句 [ELSE 处理语句列表] END IF; ","date":"2019-10-03","objectID":"/mysql-simple-faq/:9:1","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#if-then"},{"categories":["mysql"],"content":" case when CASE WHEN 表达式 THEN 处理语句 else 表达式 end ## 或者 CASE when 表达式 then 处理语句 when 表达式 then 处理语句 ... 可以与多个 when 表达式 then 处理语句 END 示例： select *, CASE WHEN name='大彬' THEN '角色1' else '角色2' end as processed_name , case when status = 1 then '已处理' when status = 0 then '未处理' when status = 2 then '待处理' end as processed_status from user; ","date":"2019-10-03","objectID":"/mysql-simple-faq/:9:2","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#case-when"},{"categories":["mysql"],"content":" 循环语句","date":"2019-10-03","objectID":"/mysql-simple-faq/:10:0","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#循环语句"},{"categories":["mysql"],"content":" WHILE WHILE 表达式 DO 处理语句列表 END WHILE; ","date":"2019-10-03","objectID":"/mysql-simple-faq/:10:1","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#while"},{"categories":["mysql"],"content":" REPEAT REPEAT 处理语句列表 UNTIL 表达式 END REPEAT; ","date":"2019-10-03","objectID":"/mysql-simple-faq/:10:2","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#repeat"},{"categories":["mysql"],"content":" LOOP LOOP 处理语句列表 END LOOP; 在使用 LOOP 时可以使用RETURN语句直接让函数结束就可以达到停止循环的效果，也可以使用LEAVE语句，不过使用LEAVE 时，需要先在LOOP语句前边放置一个所谓的标记。 CREATE FUNCTION sum_all(n INT UNSIGNED) RETURNS INT BEGIN DECLARE result INT DEFAULT 0; DECLARE i INT DEFAULT 1; flag:LOOP IF i \u003e n THEN LEAVE flag; END IF; SET result = result + i; SET i = i + 1; END LOOP flag; RETURN result; END ☝️示例中，在LOOP语句前加了一个flag:，相当于为这个循环打了一个名叫flag的标记，然后在对应的END LOOP 语句后边也把这个标记名flag 给写上了。在存储函数的函数体中使用LEAVE flag语句来结束flag这个标记所代表的循环。 标记主要是为了可以跳到指定的语句中 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:10:3","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#loop"},{"categories":["mysql"],"content":" DUPLICATE KEY UPDATE对于主键或者有唯一性约束 的列或列组合来说，新插入的记录如果和表中已存在的记录重复的话，我们可以选择的策略不仅仅是忽略（INSERT IGNORE ）该条记录的插入，也可以选择更新这条重复的旧记录。 CREATE TABLE `t` ( `idt` int(11) NOT NULL AUTO_INCREMENT, `phone` char(11) DEFAULT NULL, `name` varchar(45) DEFAULT NULL, PRIMARY KEY (`idt`), UNIQUE KEY `idt_UNIQUE` (`idt`), UNIQUE KEY `phone_UNIQUE` (`phone`) ) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8 如上表，idt 是唯一主键，phone 是 UNIQUE 唯一约束。 图1 表里有条记录 phone = 15212124125，name = '吴彦祖'，现在再添加一条记录，phone 跟 name = '吴彦祖' 是一样的，但是 name='宋江' INSERT INTO t (phone, name) VALUES ('15212124125', '宋江') ON DUPLICATE KEY UPDATE name = '宋江'; -- 对于批量插入可以这么写，`VALUES(列名)`的形式来引用待插入记录中对应列的值 INSERT INTO t (phone, name) VALUES ('15212124125', '宋江'), ('15212124126', '李逵') ON DUPLICATE KEY UPDATE name = VALUES (`name`); 结果： 图2 由结果可知，phone 电话的值没有改变，但是 name 被修改成了宋江。 也就是说，如果 t 表中已经存在 phone 的列值为 15212124125 的记录（因为 phone列具有UNIQUE 约束），那么就把该记录的 name列更新为'宋江'。 对于那些是主键或者具有UNIQUE约束的列或者列组合来说，如果表中已存在的记录中有与待插入记录在这些列或者列组合上重复的值，我们可以使用VALUES( 列名)的形式来引用待插入记录中对应列的值 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:11:0","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#duplicate-key-update"},{"categories":["mysql"],"content":" 自定义变量","date":"2019-10-03","objectID":"/mysql-simple-faq/:12:0","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#自定义变量"},{"categories":["mysql"],"content":" 单个变量设置单个变量可以使用 SET 关键字。 设置单个变量 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:12:1","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#单个变量"},{"categories":["mysql"],"content":" 多个变量设置多个变量可以使用 INTO 关键字。 设置多个变量 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:12:2","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#多个变量"},{"categories":["mysql"],"content":" innodb 和 myisam 的区别 innodb 支持事务，而 myisam 不支持事务。 innodb 支持外键，而 myisam 不支持外键。 innodb 默认表锁，使用索引检索条件时是行锁，而myisam是表锁（每次更新增加删除都会锁住表）。 innodb 和 myisam 的索引都是基于b+树，但他们具体实现不一样，innodb 的 b+ 树的叶子节点是存放数据的，myisam 的 b+ 树的叶子节点是存放指针的。 innodb 是聚簇索引，必须要有主键，一定会基于主键查询，但是辅助索引就会查询两次，myisam是非聚簇索引，索引和数据是分离的，索引里保存的是数据地址的指针，主键索引和辅助索引是分开的。 innodb 不存储表的行数，所以select count(*)的时候会全表查询，而 myisam 会存放表的行数，select count(*）的时候会查的很快。 总结：mysql 默认使用 innodb，如果要用事务和外键就使用 innodb，如果这张表只用来查询，可以用 myisam。如果更新删除增加频繁就使用 innodb。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:13:0","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#innodb-和-myisam-的区别"},{"categories":["mysql"],"content":" 索引","date":"2019-10-03","objectID":"/mysql-simple-faq/:14:0","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#索引"},{"categories":["mysql"],"content":" 功能逻辑 唯一索引指索引中的索引节点值不允许重复，一般配合唯一约束使用。 主键索引主键索引是一种特殊的唯一索引，和普通唯一索引的区别在于不允许有空值。 普通索引通过KEY、INDEX关键字创建的索引就是这个类型，没什么限制，就是单纯的可以让查询快一点。 全文索引在 5.7 版本之前，只有 MyISAM 引擎支持。 全文索引只能创建在CHAR、VARCHAR、TEXT等这些文本类型字段上，而且使用全文索引查询时，条件字符数量必须大于 3 才生效。如果想要创建出的全文索引支持中文，需要在最后指定解析器：with parser ngram。 具体的使用可以参看 全文索引的创建与使用 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:14:1","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#功能逻辑"},{"categories":["mysql"],"content":" 功能逻辑 唯一索引指索引中的索引节点值不允许重复，一般配合唯一约束使用。 主键索引主键索引是一种特殊的唯一索引，和普通唯一索引的区别在于不允许有空值。 普通索引通过KEY、INDEX关键字创建的索引就是这个类型，没什么限制，就是单纯的可以让查询快一点。 全文索引在 5.7 版本之前，只有 MyISAM 引擎支持。 全文索引只能创建在CHAR、VARCHAR、TEXT等这些文本类型字段上，而且使用全文索引查询时，条件字符数量必须大于 3 才生效。如果想要创建出的全文索引支持中文，需要在最后指定解析器：with parser ngram。 具体的使用可以参看 全文索引的创建与使用 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:14:1","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#唯一索引"},{"categories":["mysql"],"content":" 功能逻辑 唯一索引指索引中的索引节点值不允许重复，一般配合唯一约束使用。 主键索引主键索引是一种特殊的唯一索引，和普通唯一索引的区别在于不允许有空值。 普通索引通过KEY、INDEX关键字创建的索引就是这个类型，没什么限制，就是单纯的可以让查询快一点。 全文索引在 5.7 版本之前，只有 MyISAM 引擎支持。 全文索引只能创建在CHAR、VARCHAR、TEXT等这些文本类型字段上，而且使用全文索引查询时，条件字符数量必须大于 3 才生效。如果想要创建出的全文索引支持中文，需要在最后指定解析器：with parser ngram。 具体的使用可以参看 全文索引的创建与使用 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:14:1","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#主键索引"},{"categories":["mysql"],"content":" 功能逻辑 唯一索引指索引中的索引节点值不允许重复，一般配合唯一约束使用。 主键索引主键索引是一种特殊的唯一索引，和普通唯一索引的区别在于不允许有空值。 普通索引通过KEY、INDEX关键字创建的索引就是这个类型，没什么限制，就是单纯的可以让查询快一点。 全文索引在 5.7 版本之前，只有 MyISAM 引擎支持。 全文索引只能创建在CHAR、VARCHAR、TEXT等这些文本类型字段上，而且使用全文索引查询时，条件字符数量必须大于 3 才生效。如果想要创建出的全文索引支持中文，需要在最后指定解析器：with parser ngram。 具体的使用可以参看 全文索引的创建与使用 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:14:1","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#普通索引"},{"categories":["mysql"],"content":" 功能逻辑 唯一索引指索引中的索引节点值不允许重复，一般配合唯一约束使用。 主键索引主键索引是一种特殊的唯一索引，和普通唯一索引的区别在于不允许有空值。 普通索引通过KEY、INDEX关键字创建的索引就是这个类型，没什么限制，就是单纯的可以让查询快一点。 全文索引在 5.7 版本之前，只有 MyISAM 引擎支持。 全文索引只能创建在CHAR、VARCHAR、TEXT等这些文本类型字段上，而且使用全文索引查询时，条件字符数量必须大于 3 才生效。如果想要创建出的全文索引支持中文，需要在最后指定解析器：with parser ngram。 具体的使用可以参看 全文索引的创建与使用 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:14:1","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#全文索引"},{"categories":["mysql"],"content":" 存储方式 聚簇索引在聚簇索引中，索引数据和表数据在磁盘中的位置是一起的。 一张表中只能存在一个聚簇索引，一般都会选用主键作为聚簇索引。 一般聚簇索引要求索引必须是非空唯一索引才行。 非聚簇索引在非聚簇索引中，索引节点和表数据之间用物理地址的方式维护两者的联系。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:14:2","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#存储方式"},{"categories":["mysql"],"content":" 存储方式 聚簇索引在聚簇索引中，索引数据和表数据在磁盘中的位置是一起的。 一张表中只能存在一个聚簇索引，一般都会选用主键作为聚簇索引。 一般聚簇索引要求索引必须是非空唯一索引才行。 非聚簇索引在非聚簇索引中，索引节点和表数据之间用物理地址的方式维护两者的联系。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:14:2","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#聚簇索引"},{"categories":["mysql"],"content":" 存储方式 聚簇索引在聚簇索引中，索引数据和表数据在磁盘中的位置是一起的。 一张表中只能存在一个聚簇索引，一般都会选用主键作为聚簇索引。 一般聚簇索引要求索引必须是非空唯一索引才行。 非聚簇索引在非聚簇索引中，索引节点和表数据之间用物理地址的方式维护两者的联系。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:14:2","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#非聚簇索引"},{"categories":["mysql"],"content":" 回表参看 索引查询时的回表问题 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:14:3","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#回表"},{"categories":["mysql"],"content":" 建立原则 经常频繁用作查询条件的字段应酌情考虑为其创建索引。 表的主外键或连表字段，必须建立索引，因为能很大程度提升连表查询的性能。 建立索引的字段，一般值的区分性要足够高，这样才能提高索引的检索效率。 建立索引的字段，值不应该过长，如果较长的字段要建立索引，可以选择前缀索引。 建立联合索引，应当遵循最左前缀原则，将多个字段之间按优先级顺序组合。 经常根据范围取值、排序、分组的字段应建立索引，因为索引有序，能加快排序时间。 对于唯一索引，如果确认不会利用该字段排序，那可以将结构改为Hash结构。 尽量使用联合索引代替单值索引，联合索引比多个单值索引查询效率要高。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:14:4","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#建立原则"},{"categories":["mysql"],"content":" 注意点 值经常会增删改的字段，不合适建立索引，因为每次改变后需维护索引结构。 一个字段存在大量的重复值时，不适合建立索引，比如之前举例的性别字段。 索引不能参与计算，因此经常带函数查询的字段，并不适合建立索引。 一张表中的索引数量并不是越多越好，一般控制在3，最多不能超过5。 建立联合索引时，一定要考虑优先级，查询频率最高的字段应当放首位。 当表的数据较少，不应当建立索引，因为数据量不大时，维护索引反而开销更大。 索引的字段值无序时，不推荐建立索引，因为会造成页分裂，尤其是主键索引。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:14:5","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#注意点"},{"categories":["mysql"],"content":" 正确使用 查询 SQL 中尽量不要使用OR关键字，可以使用多 SQL 或子查询代替。 模糊查询尽量不要以%开头，如果实在要实现这个功能可以建立全文索引。 ⚠️编写 SQL 时一定要注意字段的数据类型，否则 MySQL 的隐式转换会导致索引失效。 一定不要在编写 SQL 时让索引字段执行计算工作，尽量将计算工作放在客户端中完成。 对于索引字段尽量不要使用计算类函数，一定要使用时请记得将函数计算放在=后面。 多条件的查询 SQL 一定要使用联合索引中的第一个字段，否则会打破最左匹配原则。 对于需要对比多个字段的查询业务时，可以拆分为连表查询，使用临时表代替。 在 SQL 中不要使用反范围性的查询条件，大部分反范围性、不等性查询都会让索引失效。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:14:6","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#正确使用"},{"categories":["mysql"],"content":" 索引失效哪些情况下会导致索引失效，参看 索引失效的一些场景 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:14:7","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#索引失效"},{"categories":["mysql"],"content":" 索引覆盖，索引下推，MRR要查询的列，在使用的索引中已经包含，被所使用的索引覆盖，这种情况称之为索引覆盖。 参看 索引覆盖，索引下推，MRR ","date":"2019-10-03","objectID":"/mysql-simple-faq/:14:8","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#索引覆盖索引下推mrr"},{"categories":["mysql"],"content":" 索引为何选择 B+Tree为什么 mysql 选择 B+Tree 作为索引的数据结构。参看索引为何要选择B+Tree？ ","date":"2019-10-03","objectID":"/mysql-simple-faq/:14:9","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#索引为何选择-btree"},{"categories":["mysql"],"content":" Explain参看 执行分析工具 - ExPlain ","date":"2019-10-03","objectID":"/mysql-simple-faq/:15:0","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#explain"},{"categories":["mysql"],"content":" 事务","date":"2019-10-03","objectID":"/mysql-simple-faq/:16:0","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#事务"},{"categories":["mysql"],"content":" 事务的 ACID 原则 A/Atomicity：原子性 C/Consistency：一致性 I/Isolation：独立性/隔离性 D/Durability：持久性 原子性要求事务中所有操作要么全部成功，要么全部失败，这点是基于undo-log来实现的，因为在该日志中会生成相应的反 SQL，执行失败时会利用该日志来回滚所有写入操作。 持久性要求的是所有 SQL 写入的数据都必须能落入磁盘存储，确保数据不会丢失，这点则是基于redo-log实现的。 隔离性的要求是一个事务不会受到另一个事务的影响，对于这点则是通过锁机制和 MVCC 机制实现的，只不过 MySQL 屏蔽了加锁和 MVCC 的细节。 一致性要求数据库的整体数据变化，只能从一个一致性状态变为另一个一致性状态，其实前面的原子性、持久性、隔离性都是为了确保这点而存在的。 undo-log：主要记录 SQL 的撤销日志，比如目前是insert语句，就记录一条delete日志。 redo-log：记录当前 SQL 归属事务的状态，以及记录修改内容和修改页的位置。 bin-log：记录每条 SQL 操作日志，只要是用于数据的主从复制与数据恢复/备份。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:16:1","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#事务的-acid-原则"},{"categories":["mysql"],"content":" 两阶段提交redo log 保证的是数据库的 crash-safe 能力。采用的策略就是常说的“两阶段提交”。 一条 update 的 SQL 语句是按照这样的流程来执行的： 将数据页加载到内存 → 修改数据 → 更新数据 → 写redo log（状态为prepare） → 写binlog → 提交事务（数据写入成功后将redo log状态改为commit） 只有当两个日志都提交成功（刷入磁盘），事务才算真正的完成。 一旦发生系统故障（不管是宕机、断电、重启等等），都可以配套使用 redo log 与 binlog 做数据修复。 binlog状态 redo log 状态 对策 有记录 commit 事务已经正常完成 有记录 prepare 在binlog写完、提交事务之前发生故障。此时数据完整。恢复策略：提交事务 无记录 prepare 在binglog写完之前发生故障。恢复策略：回滚 无记录 无记录 在写redo log之前发生故障。恢复策略：回滚 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:16:2","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#两阶段提交"},{"categories":["mysql"],"content":" 脏读、幻读、不可重复读参考 脏读、幻读、不可重复读问题 脏读：指一个事务读到了其他事务还未提交的数据，也就是当前事务读到的数据，由于还未提交，因此有可能会回滚。 幻读：另外一个事务在第一个事务要处理的目标数据范围之内新增了数据，然后先于第一个事务提交造成的问题。 幻读是对自己来说的，比如，事务 A 在对表中多行数据进行修改，将性别「男、女」改为「0、1」，此时事务 B 又插入了一条性别为男的数据， 当事务 A 提交后，再次查询表时，会发现表中依旧存在一条性别为男的数据。 不可重复读：指在一个事务中，多次读取同一数据，先后读取到的数据不一致。 事务 A 执行下单业务时，因为添加物流信息的时候出错了，导致整个事务回滚，事务回滚完成后，事务 A 就结束了。但事务 B 却并未结束，在事务 B 中，在事务 A 执行时读取了一次剩余库存，然后在事务 A 回滚后又读取了一次剩余库存，仔细想想：B 事务第一次读到的剩余库存是扣减之后的，第二次读到 的剩余库存则是扣减之前的（因为 A 事务回滚又加回去了），导致两次读取的数据不一致，这就是不可重复读。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:16:3","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#脏读幻读不可重复读"},{"categories":["mysql"],"content":" 脏写问题参考 3.1、RU（Read Uncommitted）读未提交级别的实现 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:16:4","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#脏写问题"},{"categories":["mysql"],"content":" 事务的隔离级别 Read uncommitted/RU：读未提交，处于该隔离级别的数据库，脏读、不可重复读、幻读问题都有可能发生。 Read committed/RC：读已提交，处于该隔离级别的数据库，解决了脏读问题，不可重复读、幻读问题依旧存在。 Repeatable read/RR：可重复读，处于该隔离级别的数据库，解决了脏读、不可重复读问题，幻读问题依旧存在。 Serializable：序列化/串行化，处于该隔离级别的数据库，解决了脏读、不可重复读、幻读问题都不存在。 上述四个级别，越靠后并发控制度越高，也就是在多线程并发操作的情况下，出现问题的几率越小，但对应的也性能越差，MySQL 的事务隔离级别， 默认为第三级别：Repeatable read可重复读。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:16:5","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#事务的隔离级别"},{"categories":["mysql"],"content":" 锁","date":"2019-10-03","objectID":"/mysql-simple-faq/:17:0","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#锁"},{"categories":["mysql"],"content":" 共享锁共享锁又被称之为 S 锁，它是Shared Lock的简称，不同事务之间不会排斥，可以同时获取锁并执行，这里所谓的不会排斥，仅仅只是指不会排斥其他事务来读数据，但其他事务尝试写数据时，就会出现排斥性。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:17:1","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#共享锁"},{"categories":["mysql"],"content":" 排他锁排他锁也被称之为独占锁，当一个线程获取到独占锁后，会排斥其他线程，如若其他线程也想对共享资源/同一数据进行操作，必须等到当前线程释放锁并竞争到锁资源才行。 排他锁并不是只能用于写操作，对于一个读操作，也可以手动的指定为获取排他锁，当一个事务在读数据时，获取了排他锁，那当其他事务来读、写同一数据时，都会被排斥。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:17:2","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#排他锁"},{"categories":["mysql"],"content":" 间隙锁与临键锁参看 间隙锁（Gap Lock） ","date":"2019-10-03","objectID":"/mysql-simple-faq/:17:3","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#间隙锁与临键锁"},{"categories":["mysql"],"content":" 死锁关于死锁问题，可以参考 死锁检测算法 - wait-for graph ","date":"2019-10-03","objectID":"/mysql-simple-faq/:17:4","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#死锁"},{"categories":["mysql"],"content":" 如何避免死锁的产生 合理的设计索引结构，使业务 SQL 在执行时能通过索引定位到具体的几行数据，减小锁的粒度。 业务允许的情况下，也可以将隔离级别调低，因为级别越低，锁的限制会越小。 调整业务 SQL 的逻辑顺序，较大、耗时较长的事务尽量放在特定时间去执行（如凌晨对账等）。 尽可能的拆分业务的粒度，一个业务组成的大事务，尽量拆成多个小事务，缩短一个事务持有锁的时间。 如果没有强制性要求，就尽量不要手动在事务中获取排他锁，否则会造成一些不必要的锁出现，增大产生死锁的几率。 简单来说，就是在业务允许的情况下，尽量缩短一个事务持有锁的时间、减小锁的粒度以及锁的数量。 数据库事务的隔离级别，由低到高依次为 Read uncommitted 、Read committed、Repeatable read 、Serializable。 ","date":"2019-10-03","objectID":"/mysql-simple-faq/:17:5","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#如何避免死锁的产生"},{"categories":["mysql"],"content":" 参考 建立索引的正确姿势与使用索引的最佳指南 MySQL的redo log、undo log、binlog ","date":"2019-10-03","objectID":"/mysql-simple-faq/:18:0","series":null,"tags":["mysql"],"title":"mysql 常见问题","uri":"/mysql-simple-faq/#参考"},{"categories":["开发者手册"],"content":"xiaobinqt,软件alpha、beta、rc、stable各个版本有什么区别","date":"2019-08-06","objectID":"/alpha-beta-rc-stable-diff/","series":null,"tags":["chore"],"title":"版本 alpha、beta、rc、stable 的区别","uri":"/alpha-beta-rc-stable-diff/"},{"categories":["开发者手册"],"content":" 版本管理 ","date":"2019-08-06","objectID":"/alpha-beta-rc-stable-diff/:0:0","series":null,"tags":["chore"],"title":"版本 alpha、beta、rc、stable 的区别","uri":"/alpha-beta-rc-stable-diff/#"},{"categories":["开发者手册"],"content":" alpha内部测试版。α是希腊字母的第一个，表示最早的版本，一般用户不要下载这个版本，这个版本包含很多 bug，功能也不全，主要是给开发人员和测试人员测试和找 bug 用的。 ","date":"2019-08-06","objectID":"/alpha-beta-rc-stable-diff/:1:0","series":null,"tags":["chore"],"title":"版本 alpha、beta、rc、stable 的区别","uri":"/alpha-beta-rc-stable-diff/#alpha"},{"categories":["开发者手册"],"content":" beta公开测试版。β是希腊字母的第二个，顾名思义，这个版本比 alpha 版发布得晚一些，主要是给“部落”用户和忠实用户测试用的，该版本任然存在很多 bug，但是相对 alpha 版要稳定一些。这个阶段版本的软件还会不断增加新功能。如果你是发烧友，可以下载这个版本。 ","date":"2019-08-06","objectID":"/alpha-beta-rc-stable-diff/:2:0","series":null,"tags":["chore"],"title":"版本 alpha、beta、rc、stable 的区别","uri":"/alpha-beta-rc-stable-diff/#beta"},{"categories":["开发者手册"],"content":" rcRelease Candidate候选版本，该版本又较 beta 版更进一步了，该版本功能不再增加，和最终发布版功能一样。这个版本有点像最终发行版之前的一个类似预览版，这个的发布就标明离最终发行版不远了。作为普通用户，如果你很急着用这个软件的话，也可以下载这个版本。 ","date":"2019-08-06","objectID":"/alpha-beta-rc-stable-diff/:3:0","series":null,"tags":["chore"],"title":"版本 alpha、beta、rc、stable 的区别","uri":"/alpha-beta-rc-stable-diff/#rc"},{"categories":["开发者手册"],"content":" stable稳定版。在开源软件中，都有 stable 版，这个就是开源软件的最终发行版，用户可以放心大胆的用了。 ","date":"2019-08-06","objectID":"/alpha-beta-rc-stable-diff/:4:0","series":null,"tags":["chore"],"title":"版本 alpha、beta、rc、stable 的区别","uri":"/alpha-beta-rc-stable-diff/#stable"},{"categories":["开发者手册"],"content":" GAGAGeneral Availability：正式发布的版本；在国外都是用 GA 来说明 release 版本的。 ","date":"2019-08-06","objectID":"/alpha-beta-rc-stable-diff/:5:0","series":null,"tags":["chore"],"title":"版本 alpha、beta、rc、stable 的区别","uri":"/alpha-beta-rc-stable-diff/#ga"},{"categories":["开发者手册"],"content":" RTMRTMRelease to Manufacturing：给生产商的 release 版本；RTM 版本并不一定意味着创作者解决了软件所有问题，仍有可能向公众发布前更新版本。 ","date":"2019-08-06","objectID":"/alpha-beta-rc-stable-diff/:6:0","series":null,"tags":["chore"],"title":"版本 alpha、beta、rc、stable 的区别","uri":"/alpha-beta-rc-stable-diff/#rtm"},{"categories":["mysql"],"content":"xiaobinqt,mysql 数据类型","date":"2019-06-15","objectID":"/mysql-data-type/","series":null,"tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/"},{"categories":["mysql"],"content":" MySQL 是以字节为单位存储数据的，一个字节拥有8个比特位。如果存储的不足 1 个字节，MySQL 会自动填充成 1 个字节。 字符（Character）是各种文字和符号的总称，包括各国家文字、标点符号、图形符号、数字等。 字符集（Character set）是一个系统支持的所有抽象字符的集合。 字符编码（Character encoding）是把字符集中的字符编码为特定的二进制数，以便在计算机中存储。每个字符集中的字符都对应一个唯一的二进制编码。 ","date":"2019-06-15","objectID":"/mysql-data-type/:0:0","series":null,"tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/#"},{"categories":["mysql"],"content":" 字符编码字符是面向人的概念，字节是面向计算机的概念。如果想在计算机中表示字符，那就需要将该字符与一个特定的字节序列对应起来，这个映射过程称之为编码。但是，这种映射关系并不是唯一的，不同的人制作了不同的编码方案。 根据表示一个字符使用的字节数量是不是固定的，编码方案可以分为以下两种： ","date":"2019-06-15","objectID":"/mysql-data-type/:1:0","series":null,"tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/#字符编码"},{"categories":["mysql"],"content":" 定长编码表示不同的字符所需要的字节数量是相同的。比如 ASCII 编码方案采用 1 个字节来编码一个字符，ucs2 采用 2 个字节来编码一个字符。 ","date":"2019-06-15","objectID":"/mysql-data-type/:1:1","series":null,"tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/#定长编码"},{"categories":["mysql"],"content":" 变长编码表示不同的字符所需要的字节数量是不同的。比方说 utf8 编码方案采用 1~3 个字节来编码一个字符，gb2312 采用 1~2 个字节来编码一个字符。 对于不同的字符编码方案来说，同一个字符可能被编码成不同的字节序列。比如同样一个字符：我，在 utf8 和 gb2312 这两种编码方案下被映射成如下的字节序列： utf8 编码方案 字符我被编码成 111001101000100010010001，共占用 3 个字节，用十六进制表示就是：0xE68891。 gb2312 编码方案 字符我被编码成 1100111011010010，共占用 2 个字节，用十六进制表示就是：0xCED2。 MySQL 对编码方案和字符集这两个概念并没做什么区分，也就是说 utf8 字符集指的就是 utf8 编码方案，gb2312 字符集指的也就是 gb2312 编码方案。 正宗的 utf8 字符集是使用 1~4 个字节来编码一个字符的，不过MySQL中对utf8字符集做了阉割，编码一个字符最多使用3个字节。 如果有存储使用4个字节来编码的字符的情景，可以使用一种称之为utf8mb4的字符集，它才是正宗的utf8字符集。 ","date":"2019-06-15","objectID":"/mysql-data-type/:1:2","series":null,"tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/#变长编码"},{"categories":["mysql"],"content":" 整数 类型 占用空间 无符号数取值范围 有符号数取值范围 含义 TINYINT 1 0 ~ 2⁸-1 -2⁷ ~ 2⁷-1 非常小的整数 SMALLINT 2 0 ~ 2¹⁶-1 -2¹⁵ ~ 2¹⁵-1 小的整数 MEDIUMINT 3 0 ~ 2²⁴-1 -2²³ ~ 2²³-1 中等大小的整数 INT（别名：INTEGER） 4 0 ~ 2³²-1 -2³¹ ~ 2³¹-1 标准的整数 BIGINT 8 0 ~ 2⁶⁴-1 -2⁶³ ~ 2⁶³-1 大整数 ","date":"2019-06-15","objectID":"/mysql-data-type/:2:0","series":null,"tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/#整数"},{"categories":["mysql"],"content":" 浮点数 类型 占用空间 绝对值最小非0值 绝对值最大非0值 含义 FLOAT 4 ±1.175494351E-38 ±3.402823466E+38 单精度浮点数 DOUBLE 8 ±2.2250738585072014E-308 ±1.7976931348623157E+308 双精度浮点数 有的十进制小数，比如 1.875 可以被很容易的转换成二进制数 1.111 ，但是更多的小数是无法直接转换成二进制的，比如说 0.3 ，它转换成的二进制小数就是一个无限小数，但是现在只能用 4 个字节或者 8 个字节来表示这个小数，所以只能进行一些舍入来近似的表示，所以说计算机的浮点数表示有时是不精确的。 ","date":"2019-06-15","objectID":"/mysql-data-type/:3:0","series":null,"tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/#浮点数"},{"categories":["mysql"],"content":" 设置最大最小位数可以使用 FLOAT(M, D)或者 DOUBLE(M, D) 来限制可以存储到本列中的小数范围。其中： M表示该小数最多需要的十进制有效数字个数。 D表示该小数的小数点后的十进制数字个数。 M的取值范围是1~255，D的取值范围是0~30，而且D的值必须不大于M。 M和D都是可选的，如果省略，它们的值按照机器支持的最大值来存储。 计算十进制有效数字个数时，不计入正负号，不计入最左边的 0，也不计入小数点。所以 -2.3 来说有效数字个数就是 2，0.9 有效数字个数就是 1。 ","date":"2019-06-15","objectID":"/mysql-data-type/:3:1","series":null,"tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/#设置最大最小位数"},{"categories":["mysql"],"content":" 定点数因为用浮点数表示小数可能会有不精确的情况，在一些情况下我们必须保证小数是精确的，可以使用定点数的数据类型。 与浮点数相比，定点数需要更多的空间来存储数据，所以如果不是在某些需要存储精确小数的场景下，一般的小数用浮点数表示就足够了。 类型 占用空间 取值范围 DECIMAL(M, D) 取决于M和D 取决于M和D DECIMAL 如果不指定精度，默认的 M 的值是 10 ，默认的 D 的值是 0，也就是说下列等式是成立的： DECIMAL = DECIMAL(10) = DECIMAL(10, 0) DECIMAL(n) = DECIMAL(n, 0) M 的范围是 1~65，D的范围是0~30，且D的值不能超过M。 ","date":"2019-06-15","objectID":"/mysql-data-type/:4:0","series":null,"tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/#定点数"},{"categories":["mysql"],"content":" 无符号数值对于数值类型，包括整数、浮点数和定点数，有些情况下只需要用到无符号数（就是非负数）。 MySQL 提供了一个表示无符号数值类型的方式，就是在原数值类型后加一个单词UNSIGNED： 数值类型 UNSIGNED 可以把它当成一种新类型对待，比如INT UNSIGNED就表示无符号整数，FLOAT UNSIGNED表示无符号浮点数，DECIMAL UNSIGNED表示无符号定点数。 在使用的存储空间大小相同的情况下，无符号整数可以表示的正整数范围比有符号整数能表示的正整数范围大一倍。 不过受浮点数和定点数具体的存储格式影响，无符号浮点数和定点数并不能提升正数的表示范围。 ","date":"2019-06-15","objectID":"/mysql-data-type/:5:0","series":null,"tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/#无符号数值"},{"categories":["mysql"],"content":" 时间和日期 类型 存储空间 取值范围 含义 YEAR 1 1901~2155 年份值 DATE 3 1000-01-01~9999-12-31 日期值 TIME 3 -838:59:59~838:59:59 时间值 DATETIME 8 1000-01-01 00:00:00~9999-12-31 23:59:59 日期加时间值 TIMESTAMP 4 1970-01-01 00:00:01~2038-01-19 03:14:07 时间戳 TIME表示时间，格式是hh:mm:ss[.uuuuuu]或者hhh:mm:ss[.uuuuuu] 用时间戳存储时间的好处就是，它展示的值可以随着时区的变化而变化。 DATETIME 中的时间部分表示的是一天内的时间(00:00:00 ~ 23:59:59)，而 TIME 表示的是一段时间，而且可以表示负值。 ","date":"2019-06-15","objectID":"/mysql-data-type/:6:0","series":null,"tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/#时间和日期"},{"categories":["mysql"],"content":" 字符串 类型 最大长度 存储空间要求 含义 CHAR(M) M个字符 M×W个字节 固定长度的字符串 VARCHAR(M) M个字符 L+1 或 L+2 个字节 可变长度的字符串 TINYTEXT 2⁸-1 个字节 L+1个字节 非常小型的字符串 TEXT 2¹⁶-1 个字节 L+2 个字节 小型的字符串 MEDIUMTEXT 2²⁴-1 个字节 L+3个字节 中等大小的字符串 LONGTEXT 2³²-1 个字节 L+4个字节 大型的字符串 M 代表该数据类型最多能存储的字符数量，L 代表我们实际向该类型的属性中存储的字符串在特定字符集下所占的字节数，W代表在该特定字符集下，编码一个字符最多需要的字节数。 VARCHAR(M) 中的M也是代表该类型最多可以存储的字符数量，理论上的取值范围是 1~65535。但是MySQL中还有一个规定，表中某一行包含的所有列中存储的数据大小总共不得超过 65535 个字节，也就是说 VARCHAR(M) 类型实际能够容纳的字符数量是小于 65535 的。 ","date":"2019-06-15","objectID":"/mysql-data-type/:7:0","series":null,"tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/#字符串"},{"categories":["mysql"],"content":" 参考 在线进制转换 MySQL数据类型 ","date":"2019-06-15","objectID":"/mysql-data-type/:8:0","series":null,"tags":["mysql"],"title":"mysql 常见数据类型","uri":"/mysql-data-type/#参考"},{"categories":["开发者手册"],"content":"阿里云,SSL,免费证书使用,aliyun,ali,证书,TLS","date":"2019-06-07","objectID":"/ali-ssl/","series":null,"tags":["SSL","aliyun"],"title":"阿里云 SSL 免费证书使用","uri":"/ali-ssl/"},{"categories":["开发者手册"],"content":" 申请证书申请地址 申请完成页面 将主机记录解析 将主机记录和记录值填写 解析成功后下载证书 我用的是 Apache ,所以下载的是 Apache ","date":"2019-06-07","objectID":"/ali-ssl/:1:0","series":null,"tags":["SSL","aliyun"],"title":"阿里云 SSL 免费证书使用","uri":"/ali-ssl/#申请"},{"categories":["开发者手册"],"content":" 上传证书由于本人使用的是 apache ,以下配置是 apache 的通用配置,具体可参看官方 文档 在 apache 的路径下新建一个 cert 目录,其实该目录建在哪里都可以,但是放在 apache 下方便管理。 在 cert 目录下可以建不同的文件夹放在不同域名或子域名的 ssl 文件。 把我们刚才下载的证书上传到服务器上 ","date":"2019-06-07","objectID":"/ali-ssl/:2:0","series":null,"tags":["SSL","aliyun"],"title":"阿里云 SSL 免费证书使用","uri":"/ali-ssl/#上传证书"},{"categories":["开发者手册"],"content":" 配置这是基本的配置语句 # 添加 SSL 协议支持协议，去掉不安全的协议 SSLProtocol all -SSLv2 -SSLv3 # 修改加密套件如下 SSLCipherSuite HIGH:!RC4:!MD5:!aNULL:!eNULL:!NULL:!DH:!EDH:!EXP:+MEDIUM SSLHonorCipherOrder on # 证书公钥配置 SSLCertificateFile cert/a_public.crt # 证书私钥配置 SSLCertificateKeyFile cert/a.key # 证书链配置，如果该属性开头有 '#'字符，请删除掉 SSLCertificateChainFile cert/a_chain.crt 我们将默认的配置 copy 一份出来,取一个跟域名有关的文件名 cp /etc/apache2/sites-available/000-default.conf /etc/apache2/sites-available/www.xiaobinqt.cn.conf 具体配置可参考 \u003cVirtualHost *:80\u003e ServerName www.xiaobinqt.cn Redirect permanent / https://www.xiaobinqt.cn/ \u003c/VirtualHost\u003e \u003cVirtualHost *:443\u003e SSLEngine On # 添加 SSL 协议支持协议，去掉不安全的协议 SSLProtocol all -SSLv2 -SSLv3 # 修改加密套件如下 SSLCipherSuite HIGH:!RC4:!MD5:!aNULL:!eNULL:!NULL:!DH:!EDH:!EXP:+MEDIUM SSLHonorCipherOrder on # 证书公钥配置 SSLCertificateFile cert/xiaobinqt.cn/2324042_www.xiaobinqt.cn_public.crt # 证书私钥配置 SSLCertificateKeyFile cert/xiaobinqt.cn/2324042_www.xiaobinqt.cn.key # 证书链配置，如果该属性开头有 '#'字符，请删除掉 SSLCertificateChainFile cert/xiaobinqt.cn/2324042_www.xiaobinqt.cn_chain.crt # etc ServerName www.xiaobinqt.cn ProxyPreserveHost On ProxyRequests Off ProxyPass / http://localhost:30007/ ProxyPassReverse / http://localhost:30007/ \u003c/VirtualHost\u003e 我用的是 docker 服务,如果你的只是项目文件夹可以参考这样配置 \u003cVirtualHost *:80\u003e # The ServerName directive sets the request scheme, hostname and port that # the server uses to identify itself. This is used when creating # redirection URLs. In the context of virtual hosts, the ServerName # specifies what hostname must appear in the request's Host: header to # match this virtual host. For the default virtual host (this file) this # value is not decisive as it is used as a last resort host regardless. # However, you must set it for any further virtual host explicitly. #ServerName www.example.com #ServerAdmin webmaster@localhost ServerName www.xiaobinqt.cn DocumentRoot /var/www/html Redirect permanent / https://www.xiaobinqt.cn/ # Available loglevels: trace8, ..., trace1, debug, info, notice, warn, # error, crit, alert, emerg. # It is also possible to configure the loglevel for particular # modules, e.g. #LogLevel info ssl:warn ErrorLog ${APACHE_LOG_DIR}/error.log CustomLog ${APACHE_LOG_DIR}/access.log combined # For most configuration files from conf-available/, which are # enabled or disabled at a global level, it is possible to # include a line for only one particular virtual host. For example the # following line enables the CGI configuration for this host only # after it has been globally disabled with \"a2disconf\". #Include conf-available/serve-cgi-bin.conf \u003c/VirtualHost\u003e \u003cVirtualHost *:443\u003e SSLEngine On # 添加 SSL 协议支持协议，去掉不安全的协议 SSLProtocol all -SSLv2 -SSLv3 # 修改加密套件如下 SSLCipherSuite HIGH:!RC4:!MD5:!aNULL:!eNULL:!NULL:!DH:!EDH:!EXP:+MEDIUM SSLHonorCipherOrder on # 证书公钥配置 SSLCertificateFile cert/xiaobinqt.cn/public.pem # 证书私钥配置 SSLCertificateKeyFile cert/xiaobinqt.cn/214792197160511.key # 证书链配置，如果该属性开头有 '#'字符，请删除掉 SSLCertificateChainFile cert/xiaobinqt.cn/chain.pem # etc ServerName www.xiaobinqt.cn \u003c/VirtualHost\u003e 以上配置全部基于 apache ,如果你用的不是 apache ,以上配置可能不适合你. 关于 apache 服务的一些其他知识可以参考这篇文章,该文章可能需要翻~墙访问. 配置完成后重启服务,可以利用 curl 命令查看是否配置成功. curl -I localhost:xxx 对于 ssl 是否配置成功可以通过浏览器查看. 可以看到这是我们最新申请的一年的 ssl 证书. ","date":"2019-06-07","objectID":"/ali-ssl/:3:0","series":null,"tags":["SSL","aliyun"],"title":"阿里云 SSL 免费证书使用","uri":"/ali-ssl/#配置"},{"categories":["开发者手册"],"content":"xiaobinqt,docker compose 使用方法,docker compose 如何映射端口,docker compose 常用命令,dokcer compose 指定镜像,文件挂载,使用数据卷","date":"2019-05-21","objectID":"/docker-compose/","series":null,"tags":["docker"],"title":"Docker Compose 笔记","uri":"/docker-compose/"},{"categories":["开发者手册"],"content":" 在日常工作中，经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个 Web 项目，除了 Web 服务容器本身，往往还需要再加上后端的数据库服务容器，甚至还包括前端的负载均衡容器等。Compose 恰好满足了这样的需求。Compose 定位是定义和运行多个Docker容器的应用。 Compose 允许用户通过一个单独的 docker-compose.yml 模板文件（YAML格式）来定义一组相关联的应用容器为一个服务栈stack。 Compose中有几个重要的概念： 任务task：一个容器被称为一个任务。任务拥有独一无二的ID，在同一个服务中的多个任务序号依次递增。 服务service：某个相同应用镜像的容器副本集合，一个服务可以横向扩展为多个容器实例。 服务栈stack：由多个服务组成，相互配合完成特定业务，如Web应用服务、数据库服务共同构成Web服务栈，一般由一个docker-compose.yml文件定义。 Compose 的默认管理对象是服务栈，通过子命令对栈中的多个服务进行便捷的生命周期管理。 ","date":"2019-05-21","objectID":"/docker-compose/:0:0","series":null,"tags":["docker"],"title":"Docker Compose 笔记","uri":"/docker-compose/#"},{"categories":["开发者手册"],"content":" 常用命令 CMD 说明 docker-compose up 根据 docker-compose.yml 中配置的内容，创建所有的容器、网络、数据卷等等内容，并将它们启动。 docker-compose down 停止所有的容器，并将它们删除，同时消除网络等配置内容。 docker-compose logs 服务名 查看服务日志 常用 compose 命令 具体可以参考 https://yeasy.gitbook.io/docker_practice/compose/commands https://weread.qq.com/web/reader/57f327107162732157facd6kbd432fb02a1bd4c9ab736c3 docker-compose 命令默认会识别当前控制台所在目录内的 docker-compose.yml 文件，会以这个目录的名字作为组装的应用项目的名称。如果需要改变它们，可以通过选项 -f 来修改识别的 Docker Compose 配置文件，通过 -p 选项来定义项目名👇。 docker-compose -f ./compose/docker-compose.yml -p myapp up -d ","date":"2019-05-21","objectID":"/docker-compose/:1:0","series":null,"tags":["docker"],"title":"Docker Compose 笔记","uri":"/docker-compose/#常用命令"},{"categories":["开发者手册"],"content":" 配置项 常见配置项 具体可以参看 https://yeasy.gitbook.io/docker_practice/compose/compose_file https://weread.qq.com/web/reader/57f327107162732157facd6kb73329202a0b73ce398cadd ","date":"2019-05-21","objectID":"/docker-compose/:2:0","series":null,"tags":["docker"],"title":"Docker Compose 笔记","uri":"/docker-compose/#配置项"},{"categories":["开发者手册"],"content":" 示例说明 version: \"3\" services: redis: image: redis:3.2 networks: - backend volumes: - ./redis/redis.conf:/etc/redis.conf:ro ports: - \"6379:6379\" command: [ \"redis-server\", \"/etc/redis.conf\" ] database: image: mysql:5.7 networks: - backend volumes: - ./mysql/my.cnf:/etc/mysql/my.cnf:ro - mysql-data:/var/lib/mysql environment: - MYSQL_ROOT_PASSWORD=my-secret-pw ports: - \"3306:3306\" webapp: build: ./webapp networks: - frontend - backend volumes: - ./webapp:/webapp depends_on: - redis - database nginx: image: nginx:1.12 networks: - frontend volumes: - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro - ./nginx/conf.d:/etc/nginx/conf.d:ro - ./webapp/html:/webapp/html depends_on: - webapp ports: - \"80:80\" - \"443:443\" networks: frontend: backend: volumes: mysql-data: ☝️如上： version，这个配置是可选的，代表定义的 docker-compose.yml 文件内容所采用的版本，目前 Docker Compose 的配置文件已经迭代至了第三版。 services ，是整个 docker-compose.yml 的核心，services定义了容器的各项细节。 在 Docker Compose 里不直接体现容器这个概念，而是把 service 作为配置的最小单元。虽然看上去每个 service 里的配置内容就像是在配置容器，但其实 service 代表的是一个应用集群的配置。 ","date":"2019-05-21","objectID":"/docker-compose/:3:0","series":null,"tags":["docker"],"title":"Docker Compose 笔记","uri":"/docker-compose/#示例说明"},{"categories":["开发者手册"],"content":" 定义服务在使用 docker compose 时，可以为为每个服务定义一个名称，用以区别不同的服务。在这个例子里，redis、database、webapp、nginx就是服务的名称。 ","date":"2019-05-21","objectID":"/docker-compose/:3:1","series":null,"tags":["docker"],"title":"Docker Compose 笔记","uri":"/docker-compose/#定义服务"},{"categories":["开发者手册"],"content":" 指定镜像容器最基础的就是镜像，而每个服务必须指定镜像。在 Docker Compose 里，可以通过两种方式为服务指定所采用的镜像。 一种是通过 image 这个配置，给出能在镜像仓库中找到镜像的名称即可。 另外一种指定镜像的方式就是直接采用 Dockerfile 来构建镜像，通过 build 这个配置能够定义构建的环境目录，可以通过这种方式指定镜像，Docker Compose 先会帮助我们执行镜像的构建，之后再通过这个镜像启动容器。 在docker build里还能通过选项定义许多内容，这些在 Docker Compose 里依然可以👇，我们能够指定更多的镜像构建参数，例如 Dockerfile 的文件名，构建参数等等。 webapp: build: context: ./webapp dockerfile: webapp-dockerfile args: - JAVA_VERSION=1.6 ","date":"2019-05-21","objectID":"/docker-compose/:3:2","series":null,"tags":["docker"],"title":"Docker Compose 笔记","uri":"/docker-compose/#指定镜像"},{"categories":["开发者手册"],"content":" 依赖声明如果服务间有非常强的依赖关系，就必须告知 Docker Compose 容器的先后启动顺序。只有当被依赖的容器完全启动后，Docker Compose 才会创建和启动这个容器。 定义依赖的方式很简单，在上面的例子里可以看到，就是 depends_on 这个配置项，只需要通过它列出这个服务所有依赖的其他服务即可。在 Docker Compose 为我们启动项目的时候，会检查所有依赖，形成正确的启动顺序并按这个顺序来依次启动容器。 ","date":"2019-05-21","objectID":"/docker-compose/:3:3","series":null,"tags":["docker"],"title":"Docker Compose 笔记","uri":"/docker-compose/#依赖声明"},{"categories":["开发者手册"],"content":" 文件挂载Docker Compose 里定义文件挂载的方式与 Docker Engine 里也并没有太多的区别，使用 volumes 配置可以像 docker CLI 里的 -v 选项一样来指定外部挂载和数据卷挂载。 在上面的例子里，可以看到几种常用挂载的方式。我们能够直接挂载宿主机文件系统中的目录，也可以通过数据卷的形式挂载内容。 可以直接指定相对目录进行挂载，这里的相对目录是指相对于 docker-compose.yml 文件的目录。 ","date":"2019-05-21","objectID":"/docker-compose/:3:4","series":null,"tags":["docker"],"title":"Docker Compose 笔记","uri":"/docker-compose/#文件挂载"},{"categories":["开发者手册"],"content":" 使用数据卷独立于 services 的 volumes 配置就是用来声明数据卷的。定义数据卷最简单的方式仅需要提供数据卷的名称。 如果想把属于 Docker Compose 项目以外的数据卷引入进来直接使用，可以将数据卷定义为外部引入，通过 external 这个配置就能完成这个定义。 volumes: mysql-data: external: true 在加入 external 定义后，Docker Compose 在创建项目时不会直接创建数据卷，而是优先从 Docker Engine 中已有的数据卷里寻找并直接采用。 ","date":"2019-05-21","objectID":"/docker-compose/:3:5","series":null,"tags":["docker"],"title":"Docker Compose 笔记","uri":"/docker-compose/#使用数据卷"},{"categories":["开发者手册"],"content":" 端口映射ports 这个配置项，它是用来定义端口映射的。可以利用它进行宿主机与容器端口的映射，这个配置与 docker CLI 中 -p 选项的使用方法是近似的。 由于 YAML 格式对 xx:yy 这种格式的解析有特殊性，在设置小于 60 的值时，会被当成时间而不是字符串来处理，所以最好使用引号将端口映射的定义包裹起来，避免歧义。 ","date":"2019-05-21","objectID":"/docker-compose/:3:6","series":null,"tags":["docker"],"title":"Docker Compose 笔记","uri":"/docker-compose/#端口映射"},{"categories":["开发者手册"],"content":" 参考 https://docs.docker.com/compose/ Docker-入门到实践 ","date":"2019-05-21","objectID":"/docker-compose/:4:0","series":null,"tags":["docker"],"title":"Docker Compose 笔记","uri":"/docker-compose/#参考"},{"categories":["开发者手册"],"content":"git使用，git，git基本操作，git clone,git push,git remote,.gitignore,git pull,git status,git add,git commit,git log,git diff,git rebase,git merge,git stash,git rebase,git rebase --continue,git rebase --skip,git rebase --abort,git","date":"2019-03-23","objectID":"/git-glance/","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/"},{"categories":["开发者手册"],"content":" 基本概念 基本概念 git work ","date":"2019-03-23","objectID":"/git-glance/:1:0","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#基本概念"},{"categories":["开发者手册"],"content":" .gitignore文件 # 此为注释 – 将被 Git 忽略 *.a # 忽略所有 .a 结尾的文件 !lib.a # 但 lib.a 除外 /TODO # 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO build/ # 忽略 build/ 目录下的所有文件 doc/*.txt # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt ","date":"2019-03-23","objectID":"/git-glance/:2:0","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#gitignore文件"},{"categories":["开发者手册"],"content":" git 常用命令","date":"2019-03-23","objectID":"/git-glance/:3:0","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#git-常用命令"},{"categories":["开发者手册"],"content":" git reset当已经把代码从暂存区提交到版本库了，git rest命令可以恢复到暂存区的状态。 git rest --hard HEAD $commit_id，如果只会退上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100。 如果知道 commit id 的话，可以直接用 commit id，commit id 没必要写全，前6位基本就可以用，git会自动去找。 commit id 可以通过git log命令查看，格式化log可以使用git log --pretty=oneline git log --pretty=oneline --hard 参数的作用当文件被修改过，并且 add 到了暂存区，git reset 命令会把文件状态恢复到最初的状态，也就是从暂存区撤销掉，此时跟git reset HEAD 命令一样。 如果文件从暂存区 commit 了，说明已经生成了最新的版本号了，此时回退，则需要回退到之前的一个版本，如果知道前一个版本的版本号，git reset 版本号这样就可以了，但是一般我们不会去记版本号， 可以执行git log 命令去查看，也可以使用 git reset HEAD^ 命令用于回退到上一个版本，会重新回到工作区，也就是 add 之前的状态。 如果使用了 --hard 参数会连工作区的状态内容也修改了。 以下是同样的 commit 之后，不加 --hard 参数和使用 --hard 参数的区别： 不使用 --hard 参数 使用 --hard 参数 ","date":"2019-03-23","objectID":"/git-glance/:3:1","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#git-reset"},{"categories":["开发者手册"],"content":" git reset当已经把代码从暂存区提交到版本库了，git rest命令可以恢复到暂存区的状态。 git rest --hard HEAD $commit_id，如果只会退上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100。 如果知道 commit id 的话，可以直接用 commit id，commit id 没必要写全，前6位基本就可以用，git会自动去找。 commit id 可以通过git log命令查看，格式化log可以使用git log --pretty=oneline git log --pretty=oneline --hard 参数的作用当文件被修改过，并且 add 到了暂存区，git reset 命令会把文件状态恢复到最初的状态，也就是从暂存区撤销掉，此时跟git reset HEAD 命令一样。 如果文件从暂存区 commit 了，说明已经生成了最新的版本号了，此时回退，则需要回退到之前的一个版本，如果知道前一个版本的版本号，git reset 版本号这样就可以了，但是一般我们不会去记版本号， 可以执行git log 命令去查看，也可以使用 git reset HEAD^ 命令用于回退到上一个版本，会重新回到工作区，也就是 add 之前的状态。 如果使用了 --hard 参数会连工作区的状态内容也修改了。 以下是同样的 commit 之后，不加 --hard 参数和使用 --hard 参数的区别： 不使用 --hard 参数 使用 --hard 参数 ","date":"2019-03-23","objectID":"/git-glance/:3:1","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#--hard-参数的作用"},{"categories":["开发者手册"],"content":" git diff cmd 说明 git diff [file] 显示暂存区和工作区的差异 git diff --cached [file]/git diff --staged [file] 显示暂存区和上一次提交(commit)的差异 ","date":"2019-03-23","objectID":"/git-glance/:3:2","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#git-diff"},{"categories":["开发者手册"],"content":" git refloggit reflog 用来记录你的每一次命令，可以查看你最近执行过的命令，可以用来回退到某一个时刻。所以为了好查记录，commit -m 的提交说明文案尽量写清楚。 git reflog cmd 说明 git reflog –date=local –all | grep dev 查看 dev 分支是基于哪个分支创建的 Tips markdown 表格中使用 | 可以使用\u0026#124; ","date":"2019-03-23","objectID":"/git-glance/:3:3","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#git-reflog"},{"categories":["开发者手册"],"content":" git log git log不传入任何参数的默认情况下，git log 会按时间先后顺序列出所有的提交，最近的更新排在最上面，当记录太多时会出现分页，可以按空格键翻页，按 q 键退出。 git log git log --pretty=oneline将每个提交放在一行显示，在浏览大量的提交时非常有用。 git log --pretty=oneline git log --graph --pretty=oneline --abbrev-commit 仅显示 SHA-1 校验和所有 40 个字符中的前几个字符。--oneline 是 --pretty=oneline --abbrev-commit 合用的简写。 所以 git log --graph --pretty=oneline 可以也可以写为 git log --graph --pretty=oneline --abbrev-commit。 --graph 在日志旁以 ASCII 图形显示分支与合并历史。 git graph ","date":"2019-03-23","objectID":"/git-glance/:3:4","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#git-log"},{"categories":["开发者手册"],"content":" git log git log不传入任何参数的默认情况下，git log 会按时间先后顺序列出所有的提交，最近的更新排在最上面，当记录太多时会出现分页，可以按空格键翻页，按 q 键退出。 git log git log --pretty=oneline将每个提交放在一行显示，在浏览大量的提交时非常有用。 git log --pretty=oneline git log --graph --pretty=oneline --abbrev-commit 仅显示 SHA-1 校验和所有 40 个字符中的前几个字符。--oneline 是 --pretty=oneline --abbrev-commit 合用的简写。 所以 git log --graph --pretty=oneline 可以也可以写为 git log --graph --pretty=oneline --abbrev-commit。 --graph 在日志旁以 ASCII 图形显示分支与合并历史。 git graph ","date":"2019-03-23","objectID":"/git-glance/:3:4","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#git-log-1"},{"categories":["开发者手册"],"content":" git log git log不传入任何参数的默认情况下，git log 会按时间先后顺序列出所有的提交，最近的更新排在最上面，当记录太多时会出现分页，可以按空格键翻页，按 q 键退出。 git log git log --pretty=oneline将每个提交放在一行显示，在浏览大量的提交时非常有用。 git log --pretty=oneline git log --graph --pretty=oneline --abbrev-commit 仅显示 SHA-1 校验和所有 40 个字符中的前几个字符。--oneline 是 --pretty=oneline --abbrev-commit 合用的简写。 所以 git log --graph --pretty=oneline 可以也可以写为 git log --graph --pretty=oneline --abbrev-commit。 --graph 在日志旁以 ASCII 图形显示分支与合并历史。 git graph ","date":"2019-03-23","objectID":"/git-glance/:3:4","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#git-log---prettyoneline"},{"categories":["开发者手册"],"content":" git log git log不传入任何参数的默认情况下，git log 会按时间先后顺序列出所有的提交，最近的更新排在最上面，当记录太多时会出现分页，可以按空格键翻页，按 q 键退出。 git log git log --pretty=oneline将每个提交放在一行显示，在浏览大量的提交时非常有用。 git log --pretty=oneline git log --graph --pretty=oneline --abbrev-commit 仅显示 SHA-1 校验和所有 40 个字符中的前几个字符。--oneline 是 --pretty=oneline --abbrev-commit 合用的简写。 所以 git log --graph --pretty=oneline 可以也可以写为 git log --graph --pretty=oneline --abbrev-commit。 --graph 在日志旁以 ASCII 图形显示分支与合并历史。 git graph ","date":"2019-03-23","objectID":"/git-glance/:3:4","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#git-log---graph---prettyoneline-"},{"categories":["开发者手册"],"content":" git checkout cmd 说明 git checkout dev 切换到dev分支 git checkout -- file 可以丢弃工作区的修改， git checkout -- readme.txt 意思是，把readme.txt文件在工作区的修改全部撤销。 这里有两种情况： 1. readme.txt自修改后还没有被放到暂存区，撤销修改就回到和版本库一模一样的状态。 2. readme.txt已经添加到暂存区后，又作了修改，撤销修改就回到添加到暂存区后的状态 git checkout -b yourbranchname origin/oldbranchname 在本地创建和远程分支对应的分支 ","date":"2019-03-23","objectID":"/git-glance/:3:5","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#git-checkout"},{"categories":["开发者手册"],"content":" git rmgit rm 有 2 个常用命令： git rm \u003cfile\u003e：同时从工作区和索引中删除文件。即本地的文件也被删除了，并把此次删除操作提交到了暂存区。 git rm file git rm --cached ：从索引中删除文件。但是本地文件还存在，只是不希望这个文件被版本控制。 git rm --cached 如果是文件夹需要加上 -r 参数，比如： git rm -r --cached 文件/文件夹名字 Tips 先手动删除文件，然后使用git rm \u003cfile\u003e和git add\u003cfile\u003e效果是一样的。 ","date":"2019-03-23","objectID":"/git-glance/:3:6","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#git-rm"},{"categories":["开发者手册"],"content":" git remote cmd 说明 git remote add 名字 地址 关联一个远程库时必须给远程库指定一个名字，如：git remote add origin git@server-name:path/repo-name.git git remote -v 查看远程库信息 git remote rm \u003cname\u003e 解除了本地和远程的绑定关系，如：git remote rm origin ","date":"2019-03-23","objectID":"/git-glance/:3:7","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#git-remote"},{"categories":["开发者手册"],"content":" git push把本地库的内容推送到远程，用git push命令，比如： git push -u origin master 实际上是把当前分支推送到远程的 master 分支上。 加上了-u参数，git不但会把本地的分支内容推送的远程的master分支，还会把本地的分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令，直接使用 git push。 ","date":"2019-03-23","objectID":"/git-glance/:3:8","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#git-push"},{"categories":["开发者手册"],"content":" git branch cmd 说明 git brahcn -b 分支名 新建并切换到新分支，如新建并切换到dev分支：git branch -b dev git branch 列出所有分支，当前分支前面会标一个*号 git branch -d 分支名 删除某个分支 git branch -a 查看远程分支，远程分支会用红色表示出来（如果开了颜色支持的话） git branch -D \u003cname\u003e 强行删除一个没有被合并过的分支 git branch --set-upstream branch-name origin/branch-name 建立本地分支和远程分支的关联 ","date":"2019-03-23","objectID":"/git-glance/:3:9","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#git-branch"},{"categories":["开发者手册"],"content":" git mergegit merge命令用于合并指定分支到当前分支。如：git merge dev 合并 dev 分支到当前分支。 ","date":"2019-03-23","objectID":"/git-glance/:3:10","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#git-merge"},{"categories":["开发者手册"],"content":" git switchgit 2.23+ 版本支持了 switch 命令用来切换分支，实际上，切换分支这个动作，用switch更好理解。 之前切换分支使用git checkout \u003cbranch\u003e，而撤销修改则是git checkout -- \u003cfile\u003e，同一个命令，有两种作用，确实有点令人迷惑。 操作 version 2.23- version 2.23+ 切换分支 git branch dev git switch dev 新建并切换分支 git branch -b dev git switch -c dev ","date":"2019-03-23","objectID":"/git-glance/:3:11","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#git-switch"},{"categories":["开发者手册"],"content":" git cherry-pick在合并代码的时候，有两种情况： 需要另一个分支的所有代码变动，那么就采用合并git merge。 只需要部分代码变动（某几个提交），这时可以采用 cherry pick。 git cherry-pick \u003ccommid_1\u003e \u003ccommit_2\u003e ","date":"2019-03-23","objectID":"/git-glance/:3:12","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#git-cherry-pick"},{"categories":["开发者手册"],"content":" git stash新增的文件，直接执行 git stash 是不会被存储的，需要先执行 git add 把文件加到版本控制里。 文件在版本控制里，并不等于就被stash起来了，git add 和 git stash 没有必然的关系，但是执行 git stash 能正确存储的前提是文件必须在 git 版本控制中才行。 可以多次stash，恢复的时候，先用git stash list查看，然后恢复指定的stash。 cmd 说明 git stash save \"save message\" 执行存储时，添加备注，方便查找，只有git stash 也要可以的，但查找时不方便识别 git stash list 查看stash了哪些存储 git stash show 显示做了哪些改动，默认show第一个存储,如果要显示其他存贮，后面加 stash@{$num}，比如第二个 git stash show stash@{1} git stash show -p 显示第一个存储的改动，如果想显示其他存存储，命令：git stash show stash@{$num} -p ，比如第二个：git stash show stash@{1} -p git stash apply 应用某个存储，但不会把存储从存储列表中删除，默认使用第一个存储，即stash@{0}，如果要使用其他个，git stash apply stash@{$num} ， 比如第二个：git stash apply stash@{1} git stash pop 命令恢复之前缓存的工作目录，将缓存堆栈中的对应stash删除，并将对应修改应用到当前的工作目录下，默认为第一个stash，即stash@{0}，如果要应用并删除其他stash，命令：git stash pop stash@{$num} ，比如应用并删除第二个：git stash pop stash@{1} git stash drop stash@{$num} 丢弃stash@{$num}存储，从列表中删除这个存储 git stash clear 删除所有缓存的stash ","date":"2019-03-23","objectID":"/git-glance/:3:13","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#git-stash"},{"categories":["开发者手册"],"content":" 分支管理HEAD严格来说不是指向提交，而是指向某个分支，如master分支，master才是指向提交的，所以，HEAD指向的就是当前分支。 HEAD 在合并分支时如果出现冲突，Git用\u003c\u003c\u003c\u003c\u003c\u003c\u003c，=======，\u003e\u003e\u003e\u003e\u003e\u003e\u003e标记出不同分支的内容。 合并分支有冲突 ","date":"2019-03-23","objectID":"/git-glance/:4:0","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#分支管理"},{"categories":["开发者手册"],"content":" Fast forward通常，合并分支时，如果可能，git 会用 Fast forward 模式，但这种模式下，删除分支后，会丢掉分支信息。 可以使用--no-ff强制禁用 Fast forward 模式，git就会在merge时生成一个新的 commit，这样，从分支历史上就可以看出分支信息。 git merge --no-ff 因为本次合并要创建一个新的 commit，所以加上 -m 参数，把 commit 描述写进去。 分支历史 ","date":"2019-03-23","objectID":"/git-glance/:4:1","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#fast-forward"},{"categories":["开发者手册"],"content":" 标签Tagtag 是基于某个分支下的某次 commit。如只执行 git tag v1.0，那么标签是打在该分支最新提交的 commit 上的。 创建的标签都只存储在本地，不会自动推送到远程，打错的标签可以在本地安全删除。如果标签已经推送到远程，得先删除本地标签，再删除远程标签。 cmd 说明 git tag -a v0.1 -m \"version 0.1 released\" 1094adb 基于某次 commit 打 tag，-a指定标签名，-m指定说明文字 git show \u003ctagname\u003e 显示 tag 的说明文字 git tag 可以查看所有标签 git tag -d v0.1 删除标签 git push origin \u003ctagname\u003e 推送某个标签到远程，如：git push origin v1.0 git push origin --tags 一次性推送全部尚未推送到远程的本地标签 git push origin :refs/tags/\u003ctagname\u003e 删除一个远程标签 ","date":"2019-03-23","objectID":"/git-glance/:5:0","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#标签tag"},{"categories":["开发者手册"],"content":" FAQ","date":"2019-03-23","objectID":"/git-glance/:6:0","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#faq"},{"categories":["开发者手册"],"content":" pull 和 fetch 区别git fetch 是将远程主机的最新内容拉到本地，用户检查后决定是否合并到分支中。 git pull 是将远程主机的最新内容拉下来直接合并，可以理解成，git pull = git fetch + git merge。在 merge 合并时如果产生冲突需要手动解决。 尽量不要用 git pull，用 git fetch 和 git merge 代替 git pull。 将 fetch 和 merge 放到一个命令里的另外一个弊端是，本地工作目录在未经确认的情况下就会被远程分支更新。 ","date":"2019-03-23","objectID":"/git-glance/:6:1","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#pull-和-fetch-区别"},{"categories":["开发者手册"],"content":" 将提交回到暂存区 git reset --soft $commitID 将 commit 后的代码回退到暂存区 或者 git reset HEAR 文件/文件夹 ☝️可以将某个文件/文件夹回滚到上一次操作。 ","date":"2019-03-23","objectID":"/git-glance/:6:2","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#将提交回到暂存区"},{"categories":["开发者手册"],"content":" 参考 git-fast-version-control Git教程 Git Cheat Sheet git cherry-pick 教程 git stash 用法总结和注意点 详解git pull和git fetch的区别 Git飞行规则(Flight Rules) ","date":"2019-03-23","objectID":"/git-glance/:7:0","series":null,"tags":["git"],"title":"git 使用笔记","uri":"/git-glance/#参考"},{"categories":["web"],"content":"xiaobinqt,什么是跨域,如何解决跨域问题,什么是JSONP,什么是 CORS,简单请求,复杂请求,跨域预检机制","date":"2019-03-18","objectID":"/cross-domain/","series":null,"tags":["web"],"title":"跨域问题","uri":"/cross-domain/"},{"categories":["web"],"content":" 同源策略同源策略是浏览器的一个安全行为，是指浏览器对不同源的脚本或文本的访问方式进行限制。比如，ajax 在进行请求时，浏览器要求当前网页和请求地址必须同源，也就是协议，域名和端口必须相同。 同源指的就是相同的协议，域名，端口号。 浏览器是公共资源，假如没有同源策略，A 网站的接口可以被任意来源的 ajax 请求访问，这样就会出现问题，这是浏览器出于对用户数据的保护。 比如在使用淘宝的过程中，淘宝返回了一个 cookie，下次请求你会带上cookie，这样子服务器就知道你登录过了。 假设，你买东西过程中，又点来了一个链接，由于没有同源策略，他就在后台操作向淘宝发起请求，那么就相当于不法网站利用你的账号为所欲为了。 同源策略限制的不同源之间的交互，主要是针对 JS 中的 XMLHttpRequest 请求。有一些情况是不受影响的如：html 的一些标签的请求，链接 a 标签，图片 img 标签等，这些标签的请求可以为不同源地址。 同源策略限制了 Cookie、LocalStorage 和 IndexDB 无法读取，DOM 和 JS 对象无法获取，Ajax 请求无法发送。 所以，协议，域名，端口号只要有一个不同就存在跨域。 解决跨域问题常用的有 JSONP 和 CORS 这两种方案，以下对他们分别进行介绍。 ","date":"2019-03-18","objectID":"/cross-domain/:1:0","series":null,"tags":["web"],"title":"跨域问题","uri":"/cross-domain/#同源策略"},{"categories":["web"],"content":" jsonpjsonp 虽然能解决跨域问题，它只支持GET请求😢。 jsonp 是利用\u003cscript\u003e标签没有跨域限制的“漏洞”来达到与第三方通讯的目的。当需要通讯时，本站脚本创建一个\u003cscript\u003e元素，地址指向第三方的API网址，形如： \u003cscript src=\"http://www.example.net/api?param1=1\u0026param2=2\"\u003e\u003c/script\u003e 并提供一个回调函数来接收数据（函数名可约定，或通过地址参数传递）。 第三方产生的响应为 json 数据的包装，所以称之为 jsonpjson padding，形如： callback({ \"name\": \"吴彦祖\", \"age\": \"28\" }) 这样浏览器会调用 callback 函数，并传递解析后json对象作为参数。本站脚本可在callback函数里处理所传入的数据。 接口返回的函数名一定要跟定义的函数一致 有一个接口http://127.0.0.1:8080/cb返回 jsonp 数据👇 jsonp 接口 html 文件： \u003c!doctype html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0\"\u003e \u003cmeta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"\u003e \u003ctitle\u003eDocument\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cscript\u003e let script = document.createElement('script'); script.type = 'text/javascript'; //请求接口地址和参数 script.src = 'http://127.0.0.1:8080/cb'; document.body.appendChild(script); //请求后的回调函数 function callback(res) { console.log(res) } \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e 效果👇 执行效果 ","date":"2019-03-18","objectID":"/cross-domain/:2:0","series":null,"tags":["web"],"title":"跨域问题","uri":"/cross-domain/#jsonp"},{"categories":["web"],"content":" CORS整个CORSCross-origin resource sharing 通信过程，都是浏览器自动完成，不需要用户参与。对于开发者来说，CORS通信与同源的AJAX通信没有差别，代码完全一样。 因此，实现CORS通信的关键是服务器。只要服务器实现了CORS接口，就可以跨源通信。它允许浏览器向跨源服务器，发出XMLHttpRequest 请求，浏览器一旦发现AJAX请求跨源，就会自动添加一些附加的头信息，有时还会多出一次附加的请求，从而克服了 AJAX 只能同源使用的限制。 ","date":"2019-03-18","objectID":"/cross-domain/:3:0","series":null,"tags":["web"],"title":"跨域问题","uri":"/cross-domain/#cors"},{"categories":["web"],"content":" 两种请求浏览器将CORS请求分成两类：简单请求simple request和非简单请求not-so-simple request。只要同时满足👇两大条件，就属于简单请求。 请求方法是以下三种方法之一： HEAD GET POST HTTP 头信息不超出以下几种字段： Accept Accept-Language Content-Language Last-Event-ID Content-Type：只限于三个值application/x-www-form-urlencoded、multipart/form-data、text/plain 除了由用户代理自动设置的头（如，Connection、 User-Agent） 这是为了兼容表单form，历史上表单一直可以发出跨域请求。AJAX 的跨域设计就是，只要表单可以发，AJAX 就可以直接发。 凡是不同时满足上面两个条件，就属于非简单请求。 浏览器对这两种请求的处理，是不一样的。 ","date":"2019-03-18","objectID":"/cross-domain/:3:1","series":null,"tags":["web"],"title":"跨域问题","uri":"/cross-domain/#两种请求"},{"categories":["web"],"content":" 简单请求对于简单请求，浏览器直接发出CORS请求。在头信息之中，增加一个Origin字段。 下面的例子，浏览器发现这次跨源 AJAX 请求是简单请求，就自动在头信息之中，添加一个Origin字段。 GET /cors HTTP/1.1 Origin: http://api.bob.com Host: api.alice.com Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... 上面的头信息中，Origin字段用来说明，本次请求来自哪个源（协议 + 域名 + 端口）。服务器根据这个值，决定是否同意这次请求。 如果Origin 指定的源，不在许可范围内，服务器会返回一个正常的 HTTP回应。浏览器发现，这个回应的头信息没有包含Access-Control-Allow-Origin字段（详见下文），就知道出错了，从而抛出一个错误，被XMLHttpRequest的onerror 回调函数捕获。这种错误无法通过状态码识别，因为HTTP回应的状态码有可能是200。 如果Origin指定的域名在许可范围内，服务器返回的响应，会多出几个头信息字段。 Access-Control-Allow-Origin: http://api.bob.com Access-Control-Allow-Credentials: true Access-Control-Expose-Headers: FooBar Content-Type: text/html; charset=utf-8 上面的头信息之中，有三个与CORS请求相关的字段，都以Access-Control-开头。 Access-Control-Allow-Origin 该字段是必须的。它的值要么是请求时Origin字段的值，要么是一个*，表示接受任意域名的请求。 Access-Control-Allow-Credentials 该字段可选。它的值是一个布尔值，表示是否允许发送Cookie。默认情况下，Cookie不包括在CORS请求之中。设为true，即表示服务器明确许可，Cookie可以包含在请求中，一起发给服务器。这个值也只能设为true，如果服务器不要浏览器发送Cookie，删除该字段即可。 Access-Control-Expose-Headers 该字段可选。CORS请求时，XMLHttpRequest对象的getResponseHeader() 方法只能拿到6个基本字段：Cache-Control、Content-Language、Content-Type、Expires、Last-Modified、Pragma 。如果想拿到其他字段，就必须在Access-Control-Expose-Headers里面指定。上面的例子指定，getResponseHeader('FooBar')可以返回FooBar字段的值。 withCredentials 属性CORS请求默认不发送Cookie和HTTP认证信息。如果要把Cookie发到服务器，一方面要服务器同意，指定Access-Control-Allow-Credentials字段。 Access-Control-Allow-Credentials: true 另一方面，开发者必须在AJAX请求中打开withCredentials属性。 var xhr = new XMLHttpRequest(); xhr.withCredentials = true; 否则，即使服务器同意发送Cookie，浏览器也不会发送。或者，服务器要求设置Cookie，浏览器也不会处理。 但是，如果省略withCredentials设置，有的浏览器还是会一起发送Cookie。这时，可以显式关闭withCredentials。 xhr.withCredentials = false; 需要注意的是，如果要发送Cookie，Access-Control-Allow-Origin就不能设为星号* ，必须指定明确的、与请求网页一致的域名。同时，Cookie依然遵循同源政策，只有用服务器域名设置的Cookie才会上传，其他域名的Cookie并不会上传，且（跨源）原网页代码中的document.cookie 也无法读取服务器域名下的Cookie。 ","date":"2019-03-18","objectID":"/cross-domain/:3:2","series":null,"tags":["web"],"title":"跨域问题","uri":"/cross-domain/#简单请求"},{"categories":["web"],"content":" 简单请求对于简单请求，浏览器直接发出CORS请求。在头信息之中，增加一个Origin字段。 下面的例子，浏览器发现这次跨源 AJAX 请求是简单请求，就自动在头信息之中，添加一个Origin字段。 GET /cors HTTP/1.1 Origin: http://api.bob.com Host: api.alice.com Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... 上面的头信息中，Origin字段用来说明，本次请求来自哪个源（协议 + 域名 + 端口）。服务器根据这个值，决定是否同意这次请求。 如果Origin 指定的源，不在许可范围内，服务器会返回一个正常的 HTTP回应。浏览器发现，这个回应的头信息没有包含Access-Control-Allow-Origin字段（详见下文），就知道出错了，从而抛出一个错误，被XMLHttpRequest的onerror 回调函数捕获。这种错误无法通过状态码识别，因为HTTP回应的状态码有可能是200。 如果Origin指定的域名在许可范围内，服务器返回的响应，会多出几个头信息字段。 Access-Control-Allow-Origin: http://api.bob.com Access-Control-Allow-Credentials: true Access-Control-Expose-Headers: FooBar Content-Type: text/html; charset=utf-8 上面的头信息之中，有三个与CORS请求相关的字段，都以Access-Control-开头。 Access-Control-Allow-Origin 该字段是必须的。它的值要么是请求时Origin字段的值，要么是一个*，表示接受任意域名的请求。 Access-Control-Allow-Credentials 该字段可选。它的值是一个布尔值，表示是否允许发送Cookie。默认情况下，Cookie不包括在CORS请求之中。设为true，即表示服务器明确许可，Cookie可以包含在请求中，一起发给服务器。这个值也只能设为true，如果服务器不要浏览器发送Cookie，删除该字段即可。 Access-Control-Expose-Headers 该字段可选。CORS请求时，XMLHttpRequest对象的getResponseHeader() 方法只能拿到6个基本字段：Cache-Control、Content-Language、Content-Type、Expires、Last-Modified、Pragma 。如果想拿到其他字段，就必须在Access-Control-Expose-Headers里面指定。上面的例子指定，getResponseHeader('FooBar')可以返回FooBar字段的值。 withCredentials 属性CORS请求默认不发送Cookie和HTTP认证信息。如果要把Cookie发到服务器，一方面要服务器同意，指定Access-Control-Allow-Credentials字段。 Access-Control-Allow-Credentials: true 另一方面，开发者必须在AJAX请求中打开withCredentials属性。 var xhr = new XMLHttpRequest(); xhr.withCredentials = true; 否则，即使服务器同意发送Cookie，浏览器也不会发送。或者，服务器要求设置Cookie，浏览器也不会处理。 但是，如果省略withCredentials设置，有的浏览器还是会一起发送Cookie。这时，可以显式关闭withCredentials。 xhr.withCredentials = false; 需要注意的是，如果要发送Cookie，Access-Control-Allow-Origin就不能设为星号* ，必须指定明确的、与请求网页一致的域名。同时，Cookie依然遵循同源政策，只有用服务器域名设置的Cookie才会上传，其他域名的Cookie并不会上传，且（跨源）原网页代码中的document.cookie 也无法读取服务器域名下的Cookie。 ","date":"2019-03-18","objectID":"/cross-domain/:3:2","series":null,"tags":["web"],"title":"跨域问题","uri":"/cross-domain/#withcredentials-属性"},{"categories":["web"],"content":" 非简单请求 预检请求非简单请求是那种对服务器有特殊要求的请求，比如请求方法是PUT或DELETE，或者Content-Type字段的类型是application/json。 非简单请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，称为“预检\"请求preflight request。 浏览器先询问服务器，当前网页所在的域名是否在服务器的许可名单之中，以及可以使用哪些HTTP动词和头信息字段。只有得到肯定答复，浏览器才会发出正式的XMLHttpRequest请求，否则就报错。 下面是一段浏览器的JavaScript脚本。 var url = 'http://api.alice.com/cors'; var xhr = new XMLHttpRequest(); xhr.open('PUT', url, true); xhr.setRequestHeader('X-Custom-Header', 'value'); xhr.send(); 上面代码中，HTTP请求的方法是PUT，并且发送一个自定义头信息X-Custom-Header。 浏览器发现，这是一个非简单请求，就自动发出一个\"预检\"请求，要求服务器确认可以这样请求。下面是这个\"预检\"请求的HTTP头信息。 OPTIONS /cors HTTP/1.1 Origin: http://api.bob.com Access-Control-Request-Method: PUT Access-Control-Request-Headers: X-Custom-Header Host: api.alice.com Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... “预检\"请求用的请求方法是OPTIONS，表示这个请求是用来询问的。头信息里面，关键字段是Origin，表示请求来自哪个源。 除了Origin字段，“预检\"请求的头信息包括两个特殊字段。 Access-Control-Request-Method 该字段是必须的，用来列出浏览器的CORS请求会用到哪些HTTP方法，上例是PUT。 Access-Control-Request-Headers 该字段是一个逗号分隔的字符串，指定浏览器CORS请求会额外发送的头信息字段，上例是X-Custom-Header。 预检请求的回应服务器收到\"预检\"请求以后，检查了Origin、Access-Control-Request-Method和Access-Control-Request-Headers字段以后，确认允许跨源请求，就可以做出回应。 HTTP/1.1 200 OK Date: Mon, 01 Dec 2008 01:15:39 GMT Server: Apache/2.0.61 (Unix) Access-Control-Allow-Origin: http://api.bob.com Access-Control-Allow-Methods: GET, POST, PUT Access-Control-Allow-Headers: X-Custom-Header Content-Type: text/html; charset=utf-8 Content-Encoding: gzip Content-Length: 0 Keep-Alive: timeout=2, max=100 Connection: Keep-Alive Content-Type: text/plain 上面的HTTP回应中，关键的是Access-Control-Allow-Origin字段，表示http://api.bob.com可以请求数据。该字段也可以设为星号，表示同意任意跨源请求。 Access-Control-Allow-Origin: * 如果服务器否定了\"预检\"请求，会返回一个正常的HTTP回应，但是没有任何CORS相关的头信息字段。这时，浏览器就会认定，服务器不同意预检请求，因此触发一个错误，被XMLHttpRequest对象的onerror 回调函数捕获。控制台会打印出如下的报错信息。 XMLHttpRequest cannot load http://api.alice.com. Origin http://api.bob.com is not allowed by Access-Control-Allow-Origin. 服务器回应的其他CORS相关字段如下。 Access-Control-Allow-Methods: GET, POST, PUT Access-Control-Allow-Headers: X-Custom-Header Access-Control-Allow-Credentials: true Access-Control-Max-Age: 1728000 Access-Control-Allow-Methods 该字段必需，它的值是逗号分隔的一个字符串，表明服务器支持的所有跨域请求的方法。注意，返回的是所有支持的方法，而不单是浏览器请求的那个方法。这是为了避免多次\"预检\"请求。 Access-Control-Allow-Headers 如果浏览器请求包括Access-Control-Request-Headers字段，则Access-Control-Allow-Headers字段是必需的。它也是一个逗号分隔的字符串，表明服务器支持的所有头信息字段，不限于浏览器在” 预检\"中请求的字段。 Access-Control-Allow-Credentials 该字段与简单请求时的含义相同。 Access-Control-Max-Age 该字段可选，用来指定本次预检请求的有效期，单位为秒。上面结果中，有效期是20天（1728000秒），即允许缓存该条回应1728000秒（即20天），在此期间，不用发出另一条预检请求。 浏览器的正常请求和回应一旦服务器通过了\"预检\"请求，以后每次浏览器正常的CORS请求，就都跟简单请求一样，会有一个Origin头信息字段。服务器的回应，也都会有一个Access-Control-Allow-Origin头信息字段。 下面是\"预检\"请求之后，浏览器的正常CORS请求。 PUT /cors HTTP/1.1 Origin: http://api.bob.com Host: api.alice.com X-Custom-Header: value Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... 上面头信息的Origin字段是浏览器自动添加的。 下面是服务器正常的回应。 Access-Control-Allow-Origin: http://api.bob.com Content-Type: text/html; charset=utf-8 上面头信息中，Access-Control-Allow-Origin字段是每次回应都必定包含的。 ","date":"2019-03-18","objectID":"/cross-domain/:3:3","series":null,"tags":["web"],"title":"跨域问题","uri":"/cross-domain/#非简单请求"},{"categories":["web"],"content":" 非简单请求 预检请求非简单请求是那种对服务器有特殊要求的请求，比如请求方法是PUT或DELETE，或者Content-Type字段的类型是application/json。 非简单请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，称为“预检\"请求preflight request。 浏览器先询问服务器，当前网页所在的域名是否在服务器的许可名单之中，以及可以使用哪些HTTP动词和头信息字段。只有得到肯定答复，浏览器才会发出正式的XMLHttpRequest请求，否则就报错。 下面是一段浏览器的JavaScript脚本。 var url = 'http://api.alice.com/cors'; var xhr = new XMLHttpRequest(); xhr.open('PUT', url, true); xhr.setRequestHeader('X-Custom-Header', 'value'); xhr.send(); 上面代码中，HTTP请求的方法是PUT，并且发送一个自定义头信息X-Custom-Header。 浏览器发现，这是一个非简单请求，就自动发出一个\"预检\"请求，要求服务器确认可以这样请求。下面是这个\"预检\"请求的HTTP头信息。 OPTIONS /cors HTTP/1.1 Origin: http://api.bob.com Access-Control-Request-Method: PUT Access-Control-Request-Headers: X-Custom-Header Host: api.alice.com Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... “预检\"请求用的请求方法是OPTIONS，表示这个请求是用来询问的。头信息里面，关键字段是Origin，表示请求来自哪个源。 除了Origin字段，“预检\"请求的头信息包括两个特殊字段。 Access-Control-Request-Method 该字段是必须的，用来列出浏览器的CORS请求会用到哪些HTTP方法，上例是PUT。 Access-Control-Request-Headers 该字段是一个逗号分隔的字符串，指定浏览器CORS请求会额外发送的头信息字段，上例是X-Custom-Header。 预检请求的回应服务器收到\"预检\"请求以后，检查了Origin、Access-Control-Request-Method和Access-Control-Request-Headers字段以后，确认允许跨源请求，就可以做出回应。 HTTP/1.1 200 OK Date: Mon, 01 Dec 2008 01:15:39 GMT Server: Apache/2.0.61 (Unix) Access-Control-Allow-Origin: http://api.bob.com Access-Control-Allow-Methods: GET, POST, PUT Access-Control-Allow-Headers: X-Custom-Header Content-Type: text/html; charset=utf-8 Content-Encoding: gzip Content-Length: 0 Keep-Alive: timeout=2, max=100 Connection: Keep-Alive Content-Type: text/plain 上面的HTTP回应中，关键的是Access-Control-Allow-Origin字段，表示http://api.bob.com可以请求数据。该字段也可以设为星号，表示同意任意跨源请求。 Access-Control-Allow-Origin: * 如果服务器否定了\"预检\"请求，会返回一个正常的HTTP回应，但是没有任何CORS相关的头信息字段。这时，浏览器就会认定，服务器不同意预检请求，因此触发一个错误，被XMLHttpRequest对象的onerror 回调函数捕获。控制台会打印出如下的报错信息。 XMLHttpRequest cannot load http://api.alice.com. Origin http://api.bob.com is not allowed by Access-Control-Allow-Origin. 服务器回应的其他CORS相关字段如下。 Access-Control-Allow-Methods: GET, POST, PUT Access-Control-Allow-Headers: X-Custom-Header Access-Control-Allow-Credentials: true Access-Control-Max-Age: 1728000 Access-Control-Allow-Methods 该字段必需，它的值是逗号分隔的一个字符串，表明服务器支持的所有跨域请求的方法。注意，返回的是所有支持的方法，而不单是浏览器请求的那个方法。这是为了避免多次\"预检\"请求。 Access-Control-Allow-Headers 如果浏览器请求包括Access-Control-Request-Headers字段，则Access-Control-Allow-Headers字段是必需的。它也是一个逗号分隔的字符串，表明服务器支持的所有头信息字段，不限于浏览器在” 预检\"中请求的字段。 Access-Control-Allow-Credentials 该字段与简单请求时的含义相同。 Access-Control-Max-Age 该字段可选，用来指定本次预检请求的有效期，单位为秒。上面结果中，有效期是20天（1728000秒），即允许缓存该条回应1728000秒（即20天），在此期间，不用发出另一条预检请求。 浏览器的正常请求和回应一旦服务器通过了\"预检\"请求，以后每次浏览器正常的CORS请求，就都跟简单请求一样，会有一个Origin头信息字段。服务器的回应，也都会有一个Access-Control-Allow-Origin头信息字段。 下面是\"预检\"请求之后，浏览器的正常CORS请求。 PUT /cors HTTP/1.1 Origin: http://api.bob.com Host: api.alice.com X-Custom-Header: value Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... 上面头信息的Origin字段是浏览器自动添加的。 下面是服务器正常的回应。 Access-Control-Allow-Origin: http://api.bob.com Content-Type: text/html; charset=utf-8 上面头信息中，Access-Control-Allow-Origin字段是每次回应都必定包含的。 ","date":"2019-03-18","objectID":"/cross-domain/:3:3","series":null,"tags":["web"],"title":"跨域问题","uri":"/cross-domain/#预检请求"},{"categories":["web"],"content":" 非简单请求 预检请求非简单请求是那种对服务器有特殊要求的请求，比如请求方法是PUT或DELETE，或者Content-Type字段的类型是application/json。 非简单请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，称为“预检\"请求preflight request。 浏览器先询问服务器，当前网页所在的域名是否在服务器的许可名单之中，以及可以使用哪些HTTP动词和头信息字段。只有得到肯定答复，浏览器才会发出正式的XMLHttpRequest请求，否则就报错。 下面是一段浏览器的JavaScript脚本。 var url = 'http://api.alice.com/cors'; var xhr = new XMLHttpRequest(); xhr.open('PUT', url, true); xhr.setRequestHeader('X-Custom-Header', 'value'); xhr.send(); 上面代码中，HTTP请求的方法是PUT，并且发送一个自定义头信息X-Custom-Header。 浏览器发现，这是一个非简单请求，就自动发出一个\"预检\"请求，要求服务器确认可以这样请求。下面是这个\"预检\"请求的HTTP头信息。 OPTIONS /cors HTTP/1.1 Origin: http://api.bob.com Access-Control-Request-Method: PUT Access-Control-Request-Headers: X-Custom-Header Host: api.alice.com Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... “预检\"请求用的请求方法是OPTIONS，表示这个请求是用来询问的。头信息里面，关键字段是Origin，表示请求来自哪个源。 除了Origin字段，“预检\"请求的头信息包括两个特殊字段。 Access-Control-Request-Method 该字段是必须的，用来列出浏览器的CORS请求会用到哪些HTTP方法，上例是PUT。 Access-Control-Request-Headers 该字段是一个逗号分隔的字符串，指定浏览器CORS请求会额外发送的头信息字段，上例是X-Custom-Header。 预检请求的回应服务器收到\"预检\"请求以后，检查了Origin、Access-Control-Request-Method和Access-Control-Request-Headers字段以后，确认允许跨源请求，就可以做出回应。 HTTP/1.1 200 OK Date: Mon, 01 Dec 2008 01:15:39 GMT Server: Apache/2.0.61 (Unix) Access-Control-Allow-Origin: http://api.bob.com Access-Control-Allow-Methods: GET, POST, PUT Access-Control-Allow-Headers: X-Custom-Header Content-Type: text/html; charset=utf-8 Content-Encoding: gzip Content-Length: 0 Keep-Alive: timeout=2, max=100 Connection: Keep-Alive Content-Type: text/plain 上面的HTTP回应中，关键的是Access-Control-Allow-Origin字段，表示http://api.bob.com可以请求数据。该字段也可以设为星号，表示同意任意跨源请求。 Access-Control-Allow-Origin: * 如果服务器否定了\"预检\"请求，会返回一个正常的HTTP回应，但是没有任何CORS相关的头信息字段。这时，浏览器就会认定，服务器不同意预检请求，因此触发一个错误，被XMLHttpRequest对象的onerror 回调函数捕获。控制台会打印出如下的报错信息。 XMLHttpRequest cannot load http://api.alice.com. Origin http://api.bob.com is not allowed by Access-Control-Allow-Origin. 服务器回应的其他CORS相关字段如下。 Access-Control-Allow-Methods: GET, POST, PUT Access-Control-Allow-Headers: X-Custom-Header Access-Control-Allow-Credentials: true Access-Control-Max-Age: 1728000 Access-Control-Allow-Methods 该字段必需，它的值是逗号分隔的一个字符串，表明服务器支持的所有跨域请求的方法。注意，返回的是所有支持的方法，而不单是浏览器请求的那个方法。这是为了避免多次\"预检\"请求。 Access-Control-Allow-Headers 如果浏览器请求包括Access-Control-Request-Headers字段，则Access-Control-Allow-Headers字段是必需的。它也是一个逗号分隔的字符串，表明服务器支持的所有头信息字段，不限于浏览器在” 预检\"中请求的字段。 Access-Control-Allow-Credentials 该字段与简单请求时的含义相同。 Access-Control-Max-Age 该字段可选，用来指定本次预检请求的有效期，单位为秒。上面结果中，有效期是20天（1728000秒），即允许缓存该条回应1728000秒（即20天），在此期间，不用发出另一条预检请求。 浏览器的正常请求和回应一旦服务器通过了\"预检\"请求，以后每次浏览器正常的CORS请求，就都跟简单请求一样，会有一个Origin头信息字段。服务器的回应，也都会有一个Access-Control-Allow-Origin头信息字段。 下面是\"预检\"请求之后，浏览器的正常CORS请求。 PUT /cors HTTP/1.1 Origin: http://api.bob.com Host: api.alice.com X-Custom-Header: value Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... 上面头信息的Origin字段是浏览器自动添加的。 下面是服务器正常的回应。 Access-Control-Allow-Origin: http://api.bob.com Content-Type: text/html; charset=utf-8 上面头信息中，Access-Control-Allow-Origin字段是每次回应都必定包含的。 ","date":"2019-03-18","objectID":"/cross-domain/:3:3","series":null,"tags":["web"],"title":"跨域问题","uri":"/cross-domain/#预检请求的回应"},{"categories":["web"],"content":" 非简单请求 预检请求非简单请求是那种对服务器有特殊要求的请求，比如请求方法是PUT或DELETE，或者Content-Type字段的类型是application/json。 非简单请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，称为“预检\"请求preflight request。 浏览器先询问服务器，当前网页所在的域名是否在服务器的许可名单之中，以及可以使用哪些HTTP动词和头信息字段。只有得到肯定答复，浏览器才会发出正式的XMLHttpRequest请求，否则就报错。 下面是一段浏览器的JavaScript脚本。 var url = 'http://api.alice.com/cors'; var xhr = new XMLHttpRequest(); xhr.open('PUT', url, true); xhr.setRequestHeader('X-Custom-Header', 'value'); xhr.send(); 上面代码中，HTTP请求的方法是PUT，并且发送一个自定义头信息X-Custom-Header。 浏览器发现，这是一个非简单请求，就自动发出一个\"预检\"请求，要求服务器确认可以这样请求。下面是这个\"预检\"请求的HTTP头信息。 OPTIONS /cors HTTP/1.1 Origin: http://api.bob.com Access-Control-Request-Method: PUT Access-Control-Request-Headers: X-Custom-Header Host: api.alice.com Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... “预检\"请求用的请求方法是OPTIONS，表示这个请求是用来询问的。头信息里面，关键字段是Origin，表示请求来自哪个源。 除了Origin字段，“预检\"请求的头信息包括两个特殊字段。 Access-Control-Request-Method 该字段是必须的，用来列出浏览器的CORS请求会用到哪些HTTP方法，上例是PUT。 Access-Control-Request-Headers 该字段是一个逗号分隔的字符串，指定浏览器CORS请求会额外发送的头信息字段，上例是X-Custom-Header。 预检请求的回应服务器收到\"预检\"请求以后，检查了Origin、Access-Control-Request-Method和Access-Control-Request-Headers字段以后，确认允许跨源请求，就可以做出回应。 HTTP/1.1 200 OK Date: Mon, 01 Dec 2008 01:15:39 GMT Server: Apache/2.0.61 (Unix) Access-Control-Allow-Origin: http://api.bob.com Access-Control-Allow-Methods: GET, POST, PUT Access-Control-Allow-Headers: X-Custom-Header Content-Type: text/html; charset=utf-8 Content-Encoding: gzip Content-Length: 0 Keep-Alive: timeout=2, max=100 Connection: Keep-Alive Content-Type: text/plain 上面的HTTP回应中，关键的是Access-Control-Allow-Origin字段，表示http://api.bob.com可以请求数据。该字段也可以设为星号，表示同意任意跨源请求。 Access-Control-Allow-Origin: * 如果服务器否定了\"预检\"请求，会返回一个正常的HTTP回应，但是没有任何CORS相关的头信息字段。这时，浏览器就会认定，服务器不同意预检请求，因此触发一个错误，被XMLHttpRequest对象的onerror 回调函数捕获。控制台会打印出如下的报错信息。 XMLHttpRequest cannot load http://api.alice.com. Origin http://api.bob.com is not allowed by Access-Control-Allow-Origin. 服务器回应的其他CORS相关字段如下。 Access-Control-Allow-Methods: GET, POST, PUT Access-Control-Allow-Headers: X-Custom-Header Access-Control-Allow-Credentials: true Access-Control-Max-Age: 1728000 Access-Control-Allow-Methods 该字段必需，它的值是逗号分隔的一个字符串，表明服务器支持的所有跨域请求的方法。注意，返回的是所有支持的方法，而不单是浏览器请求的那个方法。这是为了避免多次\"预检\"请求。 Access-Control-Allow-Headers 如果浏览器请求包括Access-Control-Request-Headers字段，则Access-Control-Allow-Headers字段是必需的。它也是一个逗号分隔的字符串，表明服务器支持的所有头信息字段，不限于浏览器在” 预检\"中请求的字段。 Access-Control-Allow-Credentials 该字段与简单请求时的含义相同。 Access-Control-Max-Age 该字段可选，用来指定本次预检请求的有效期，单位为秒。上面结果中，有效期是20天（1728000秒），即允许缓存该条回应1728000秒（即20天），在此期间，不用发出另一条预检请求。 浏览器的正常请求和回应一旦服务器通过了\"预检\"请求，以后每次浏览器正常的CORS请求，就都跟简单请求一样，会有一个Origin头信息字段。服务器的回应，也都会有一个Access-Control-Allow-Origin头信息字段。 下面是\"预检\"请求之后，浏览器的正常CORS请求。 PUT /cors HTTP/1.1 Origin: http://api.bob.com Host: api.alice.com X-Custom-Header: value Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... 上面头信息的Origin字段是浏览器自动添加的。 下面是服务器正常的回应。 Access-Control-Allow-Origin: http://api.bob.com Content-Type: text/html; charset=utf-8 上面头信息中，Access-Control-Allow-Origin字段是每次回应都必定包含的。 ","date":"2019-03-18","objectID":"/cross-domain/:3:3","series":null,"tags":["web"],"title":"跨域问题","uri":"/cross-domain/#浏览器的正常请求和回应"},{"categories":["web"],"content":" 参考 跨域及其解决方案 https://developer.mozilla.org/en-US/docs/web/http/cors#simple_requests 跨域资源共享 CORS 详解 ","date":"2019-03-18","objectID":"/cross-domain/:4:0","series":null,"tags":["web"],"title":"跨域问题","uri":"/cross-domain/#参考"},{"categories":["开发者手册"],"content":"xiaobinqt,wampserver的安装和使用,wampserver,wampserver php","date":"2018-11-12","objectID":"/wampserver-install-usage/","series":null,"tags":["wampserver","php"],"title":"wampserver 的安装和使用","uri":"/wampserver-install-usage/"},{"categories":["开发者手册"],"content":" 本地开发 php 环境推荐使用 wampserver，下载地址为 WampServer download | SourceForge.net 当然国产的 phpStudy 也可以，个人喜好问题。 ","date":"2018-11-12","objectID":"/wampserver-install-usage/:0:0","series":null,"tags":["wampserver","php"],"title":"wampserver 的安装和使用","uri":"/wampserver-install-usage/#"},{"categories":["开发者手册"],"content":" 下载安装 wampserver 之前我们需要先安装 Visual C++ Redistributable for Visual Studio，这是 visual 2015 的下载地址 Visual C++ Redistributable for Visual Studio 2015 from Official Microsoft Download Center ，根据自己的版本下载对应的版本。 Visual Studio visual c++ redistributable for visual studio 2012 在很多时候已经不能用了。 我安装的是 64 位的 wampserver，所以下载对应的 visual C++： visual C++ 图 1 visual C++ 图 2 之后安装 wampserver 安装 wampserver 在安装过程中会让你选择默认的浏览器和编辑器。 ","date":"2018-11-12","objectID":"/wampserver-install-usage/:1:0","series":null,"tags":["wampserver","php"],"title":"wampserver 的安装和使用","uri":"/wampserver-install-usage/#下载"},{"categories":["开发者手册"],"content":" 配置如果有需要可以修改 wampserver 的根目录 打开 wampserver 的安装目录，在打开里面的 script 文件夹，用记事本打开里面的 config.inc.php,找到 $wwwDir = $c_installDir.'/www' 改成希望的目录就行了。 比如改成 D:\\website，对应的代码就是 $wwwDir = 'D:/website'；然后关闭 wampserver。 （注意，windows下表示路径的 \\在这里必须改为 /） wwwDir 打开 Apach 下面的 httpd.conf 文件，路径示例 httpd.conf 寻找 DocumentRoot，把后面的值改成我们实际网站需要的路径，再寻找 \u003cDirectory \"c:/wamp/www/\"\u003e 修改成相同的路径 DocumentRoot 配置虚拟主机 VirtualHost 示例 \u003cVirtualHost *\u003e ServerAdmin root@localhost DocumentRoot \"D:\\projects\\pc-mes\\src\\public\" ServerName pc-mes.local ErrorLog \"d:\\wamp64\\www\\pc-mes.localhost-error.log\" CustomLog \"d:\\wamp64\\www\\pc-mes.localhost.log\" common \u003c/VirtualHost\u003e 访问示例 访问示例 ","date":"2018-11-12","objectID":"/wampserver-install-usage/:2:0","series":null,"tags":["wampserver","php"],"title":"wampserver 的安装和使用","uri":"/wampserver-install-usage/#配置"},{"categories":["mysql"],"content":"xiaobinqt,mysql 时间函数,mysql 时间戳转时间类型","date":"2018-11-08","objectID":"/mysql-common-func-memo/","series":null,"tags":["mysql"],"title":"Mysql 常用函数备忘","uri":"/mysql-common-func-memo/"},{"categories":["mysql"],"content":" 时间戳与日期格式转换UNIX时间戳转换为日期用函数： FROM_UNIXTIME() select FROM_UNIXTIME(1156219870); FROM_UNIXTIME 日期转换为UNIX时间戳用函数： UNIX_TIMESTAMP() UNIX_TIMESTAMP ","date":"2018-11-08","objectID":"/mysql-common-func-memo/:1:0","series":null,"tags":["mysql"],"title":"Mysql 常用函数备忘","uri":"/mysql-common-func-memo/#时间戳与日期格式转换"},{"categories":["开发者手册"],"content":"Windows,系统,重做系统,win,win10,U盘安装系统","date":"2018-10-07","objectID":"/redo-system/","series":null,"tags":["windows"],"title":"windows 重做系统","uri":"/redo-system/"},{"categories":["开发者手册"],"content":" 下载系统首先我们需要一个最小 4G 大的 U 盘 进入大白菜网站下载大白菜装机版安装到电脑 将 U 盘插到电脑上,双击打开大白菜装机版,它会自动读到我们插入的 U 盘，自动匹配默认模式,不需要手动选择。 点击开始制作 –\u003e 确认 等待写入数据包完成和格式化完成后关掉大白菜软件 制作完 U 盘启动盘后我们的 U 盘会变成这样说明启动盘已经制作成功 ! image 去itellyou下载需要安装的操作系统 ! 比如我们要安装这个版本的 win10 通过百度网盘下载 ed2k 文件资源,然后再通过百度网盘下载到电脑本地,可以放在除 C 盘外的其他盘中 下载到本地的文件是这样的光盘映像文件 ! 将我们下载到电脑本地的光盘映像文件复制到刚制作完成的 U 盘启动盘中,复制完成后我们 U 盘启动盘就全部制作完成了 ","date":"2018-10-07","objectID":"/redo-system/:1:0","series":null,"tags":["windows"],"title":"windows 重做系统","uri":"/redo-system/#下载系统"},{"categories":["开发者手册"],"content":" 重做系统将我们制作好的 U 盘启动盘插到需要重做系统的电脑上,重启电脑一直按 F12 进入 bios 界面(不同的电脑可能按键不同,可以百度) 选择 usb 方式,回车进入大白菜启动方式,选择 02 方式进入 进入 u 盘驱动界面,系统默认会选择 C 盘为系统盘,直接点击确定 确定后进入格式化 C 盘阶段,所有配置为默认配置,直接确定,等待格式化完成! 格式化完成后选择 是 重启电脑,拔出 u 盘 等待电脑重启 ","date":"2018-10-07","objectID":"/redo-system/:2:0","series":null,"tags":["windows"],"title":"windows 重做系统","uri":"/redo-system/#重做系统"},{"categories":["开发者手册"],"content":" 安装系统完成 ","date":"2018-10-07","objectID":"/redo-system/:3:0","series":null,"tags":["windows"],"title":"windows 重做系统","uri":"/redo-system/#安装系统完成"},{"categories":["mysql"],"content":"xiaobinqt,mysql where 和 having 的区别,what difference between where and having","date":"2018-07-05","objectID":"/what-difference-between-where-and-having/","series":null,"tags":["mysql"],"title":"mysql where 和 having 的区别","uri":"/what-difference-between-where-and-having/"},{"categories":["mysql"],"content":" 总览where 是一个约束声明，在查询数据库的结果返回之前对数据库中的条件进行约束，where 后面要跟的必须是数据表里真实存在的字段，即在结果返回之前起作用。 那么为什么 where 后面不能写聚合函数这个问题也好理解了，因为聚合函数的列不是数据库表里的字段。 having 是一个过滤声明，是在查询数据库结果返回之后进行过滤，即在结果返回值后起作用。 那么为什么 having 后面可以写聚合函数也好理解了，因为 having 只是根据前面查询出来的是什么就可以后面接什么。 ","date":"2018-07-05","objectID":"/what-difference-between-where-and-having/:1:0","series":null,"tags":["mysql"],"title":"mysql where 和 having 的区别","uri":"/what-difference-between-where-and-having/#总览"},{"categories":["mysql"],"content":" 示例","date":"2018-07-05","objectID":"/what-difference-between-where-and-having/:2:0","series":null,"tags":["mysql"],"title":"mysql where 和 having 的区别","uri":"/what-difference-between-where-and-having/#示例"},{"categories":["mysql"],"content":" 示例 1 select * from edge where edge_id = '0104932b-8edc-4f2d-9d51-c0450867e373'; -- sql1 正确 select * from edge having edge_id = '0104932b-8edc-4f2d-9d51-c0450867e373'; -- sql2 正确 这 2 句 sql 的效果是一样的，sql1 用 where 过滤相当于在返回结果前过滤，sql2 用 having 过滤相当于在返回结果后过滤。 ","date":"2018-07-05","objectID":"/what-difference-between-where-and-having/:2:1","series":null,"tags":["mysql"],"title":"mysql where 和 having 的区别","uri":"/what-difference-between-where-and-having/#示例-1"},{"categories":["mysql"],"content":" 示例 2 select token name from edge where edge_id = '0104932b-8edc-4f2d-9d51-c0450867e373'; -- sql1 正确 select token name from student having edge_id = '0104932b-8edc-4f2d-9d51-c0450867e373'; -- sql2 错误 sql1 是正确的，表中存在 edge_id 这个字段可以过滤。 sql2 是错误的，因为 select 后没有 edge_id 这个字段，所以不能过滤。 ","date":"2018-07-05","objectID":"/what-difference-between-where-and-having/:2:2","series":null,"tags":["mysql"],"title":"mysql where 和 having 的区别","uri":"/what-difference-between-where-and-having/#示例-2"},{"categories":["mysql"],"content":" 示例 3 select count(instance_id) as c, instance_id from edge group by instance_id having c \u003e 10; -- sql1 正确 select count(instance_id) as c, instance_id from edge where c \u003e 10 group by instance_id; -- sql2 错误 sql1 是正确的，因为 c 在 select 的字段中是存在，是 count(instance_id) 的别名，所以这里写成 having count(instance_id) \u003e 10 也是正确的。 sql2 是错误的，因为 c 这个字段在表中是不存在的，所以不能过滤，即使写成 count(instance_id) \u003e 10 也是错误的。 ","date":"2018-07-05","objectID":"/what-difference-between-where-and-having/:2:3","series":null,"tags":["mysql"],"title":"mysql where 和 having 的区别","uri":"/what-difference-between-where-and-having/#示例-3"},{"categories":["mysql"],"content":" 总结where 只能过滤数据库真实存在的字段，having 可以过滤 select 后的查询字段。 where 子句在分组前进行过滤，作用于每一条记录，where 子句过滤掉的记录将不包括在分组中。而 having 子句在数据分组后进行过滤，作用于整个分组。 ","date":"2018-07-05","objectID":"/what-difference-between-where-and-having/:3:0","series":null,"tags":["mysql"],"title":"mysql where 和 having 的区别","uri":"/what-difference-between-where-and-having/#总结"},{"categories":["开发者手册"],"content":"xiaobinqt,Docker slim、stretch、buster、jessie、alpine、busyBox、debian、Ubuntu、CentOS、Fedora、buildpack-deps","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/","series":null,"tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora、buildpack-deps","uri":"/docker-slim-stretch-buster-jessie/"},{"categories":["开发者手册"],"content":" 版本代号我太难了，搞这么多代号干啥😢 在写 Dockerfile 引用基础镜像时经常会看到这样的写法： FROM debian:buster 或是 FROM node:14.16.1-stretch-slim 那这里的 buster 和 stretch 具体是什么呢？其实 buster、stretch还有jessie针对的是不同 Debian 代号codename，除了 Debian 之外，还有 Ubuntu、CentOS、Fedora，他们每次在更新版本时都会更新代号。 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:1:0","series":null,"tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora、buildpack-deps","uri":"/docker-slim-stretch-buster-jessie/#版本代号"},{"categories":["开发者手册"],"content":" ubuntu 版本代号 ubuntu版本代号 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:1:1","series":null,"tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora、buildpack-deps","uri":"/docker-slim-stretch-buster-jessie/#ubuntu-版本代号"},{"categories":["开发者手册"],"content":" debian 版本代号 debian版本代号 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:1:2","series":null,"tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora、buildpack-deps","uri":"/docker-slim-stretch-buster-jessie/#debian-版本代号"},{"categories":["开发者手册"],"content":" slimslim 可以理解为精简版，跟 Minimal是一样的，仅安装运行特定工具所需的最少软件包。 所以FROM debian:buster 就是把 debian 10 作为基础镜像，FROM node:14.16.1-stretch-slim 就是把 debian 9 的精简版作为基础镜像。 容器的核心是应用。选择过大的父镜像（如Ubuntu系统镜像）会造成最终生成应用镜像的臃肿，所以会有这种瘦身过的应用镜像（如node:slim），或者较为小巧的系统镜像（如 alpine、busybox 或 debian） ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:2:0","series":null,"tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora、buildpack-deps","uri":"/docker-slim-stretch-buster-jessie/#slim"},{"categories":["开发者手册"],"content":" buildpack-depsbuildpack-deps 是 docker hub 官方提供的一个镜像工具包，很多 docker 官方的基础镜像都基于此基础镜像进行构建的，buildpack-deps 已经提供了很多内置好的依赖库，可以简化镜像部署，官方也提供了 debian 以及 ubuntu 等的镜像，如 debian 10 buildpack-deps:buster，ubuntu 16 buildpack-deps:xenial 等。 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:3:0","series":null,"tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora、buildpack-deps","uri":"/docker-slim-stretch-buster-jessie/#buildpack-deps"},{"categories":["开发者手册"],"content":" 操作系统alpine、debian、ubuntu、centOS、fedora 这些都是操作系统，是 Linux 的发行版。 Linux发行版 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:4:0","series":null,"tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora、buildpack-deps","uri":"/docker-slim-stretch-buster-jessie/#操作系统"},{"categories":["开发者手册"],"content":" BusyboxBusyBox 是一个集成了一百多个最常用 Linux 命令和工具（如 cat、echo、grep、mount、telnet 等）的精简工具箱，它只有几MB的大小，被誉为“Linux系统的瑞士军刀”。 busybox ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:4:1","series":null,"tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora、buildpack-deps","uri":"/docker-slim-stretch-buster-jessie/#busybox"},{"categories":["开发者手册"],"content":" AlpineAlpine 操作系统是一个面向安全的轻型 Linux 发行版。它不同于通常的 Linux 发行版，Alpine 采用了 musl libc 和 BusyBox 以减小系统的体积和运行时资源消耗，但功能上比 BusyBox 又完善得多。在保持瘦身的同时，Alpine 还提供了自己的包管理工具apk ，可以通过 https://pkgs.alpinelinux.org/packages 查询包信息，也可以通过apk命令直接查询和安装各种软件。 相比于其他 Docker 镜像，Alpine Docker 的容量非常小，仅仅只有 5MB 左右，而 Ubuntu 系列镜像接近 200MB，且拥有非常友好的包管理机制。 Docker 官方推荐使用 Alpine 作为默认的基础镜像环境，这可以带来多个优势，如镜像下载速度加快、镜像安全性提高、主机之间的切换更方便、占用更少磁盘空间等。 Alpine 镜像的缺点就在于它实在过于精简，以至于麻雀虽小，也无法做到五脏俱全了。在 Alpine 中缺少很多常见的工具和类库，以至于如果想基于 Alpine 标签的镜像进行二次构建，那搭建的过程会相当烦琐。所以如果想要对软件镜像进行改造，并基于其构建新的镜像，那么 Alpine 镜像不是一个很好的选择。 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:4:2","series":null,"tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora、buildpack-deps","uri":"/docker-slim-stretch-buster-jessie/#alpine"},{"categories":["开发者手册"],"content":" DebianDebian 是由 GPL 和其他自由软件许可协议授权的自由软件组成的操作系统，由 Debian Project 组织维护。Debian 以其坚守 Unix 和自由软件的精神，以及给予用户的众多选择而闻名。 作为一个大的系统组织框架，Debian下面有多种不同操作系统核心的分支计划，主要为采用 Linux 核心的 Debian GNU/Linux 系统，其他还有采用 GNU Hurd 核心的 Debian GNU/Hurd 系统、采用 FreeBSD 核心的 Debian GNU/kFreeBSD 系统，以及采用 NetBSD 核心的 Debian GNU/NetBSD 系统，甚至还有利用 Debian 的系统架构和工具，采用 OpenSolaris 核心构建而成的 Nexenta OS 系统。在这些 Debian 系统中，以采用 Linux 核心的 Debian GNU/Linux 最为著名。 众多的 Linux 发行版，例如 Ubuntu、Knoppix 和 Linspire 及 Xandros 等，都基于 Debian GNU/Linux。 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:4:3","series":null,"tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora、buildpack-deps","uri":"/docker-slim-stretch-buster-jessie/#debian"},{"categories":["开发者手册"],"content":" UbuntuUbuntu 是一个以桌面应用为主的 GNU/Linux 操作系统。Ubuntu 基于 Debian 发行版和 GNOME/Unity 桌面环境，与 Debian 的不同在于它每6个月会发布一个新版本，每2年会推出一个长期支持（Long Term Support，LTS）版本，一般支持3年。 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:4:4","series":null,"tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora、buildpack-deps","uri":"/docker-slim-stretch-buster-jessie/#ubuntu"},{"categories":["开发者手册"],"content":" CentOSCentOS 是基于 Redhat 的常见 Linux 分支。CentOS 是目前企业级服务器的常用操作系统。 CentOS（Community Enterprise Operating System，社区企业操作系统）是基于 Red Hat Enterprise Linux 源代码编译而成的。由于 CentOS 与 Redhat Linux 源于相同的代码基础，所以很多成本敏感且需要高稳定性的公司就使用 CentOS 来替代商业版 Red Hat Enterprise Linux。CentOS 自身不包含闭源软件。 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:4:5","series":null,"tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora、buildpack-deps","uri":"/docker-slim-stretch-buster-jessie/#centos"},{"categories":["开发者手册"],"content":" FedoraFedora 也是基于 Redhat 的常见 Linux分支。Fedora 则主要面向个人桌面用户。 Fedora 是由 Fedora Project 社区开发，红帽公司赞助的 Linux 发行版。它的目标是创建一套新颖、多功能并且自由和开源的操作系统。对用户而言，Fedora是一套功能完备的、可以更新的免费操作系统，而对赞助商 RedHat 而言，它是许多新技术的测试平台，被认为可用的技术最终会加入到 RedHat Enterprise Linux 中。 ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:4:6","series":null,"tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora、buildpack-deps","uri":"/docker-slim-stretch-buster-jessie/#fedora"},{"categories":["开发者手册"],"content":" 参考 趣谈形形色色的 Linux 发行版的代号 Docker运行操作系统环境(BusyBox\u0026Alpine\u0026Debian/Ubuntu\u0026CentOS/Fedora) ","date":"2018-05-10","objectID":"/docker-slim-stretch-buster-jessie/:5:0","series":null,"tags":["docker","操作系统"],"title":"slim、stretch、buster、jessie、alpine、debian、ubuntu、centOS、fedora、buildpack-deps","uri":"/docker-slim-stretch-buster-jessie/#参考"},{"categories":["开发者手册"],"content":"xiaobinqt,endpoint 和 cmd 的区别,docker 启动，docker 写时复制,docker 常见命令,虚拟化,docker 文件挂载,数据卷是什么,容器网络,如何写 dockerfile","date":"2018-03-18","objectID":"/docker-glance/","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/"},{"categories":["开发者手册"],"content":" 虚拟化","date":"2018-03-18","objectID":"/docker-glance/:1:0","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#虚拟化"},{"categories":["开发者手册"],"content":" 硬件虚拟化硬件虚拟化，指物理硬件本身就提供虚拟化的支持。 比如，A 平台的 CPU，能够将 B 平台的指令集转换为自身的指令集执行，并给程序完全运行在 B 平台上的感觉。又或者，CPU 能够自身模拟裂变，让程序或者操作系统认为存在多个 CPU，进而能够同时运行多个程序或者操作系统。这些都是硬件虚拟化的体现。 ","date":"2018-03-18","objectID":"/docker-glance/:1:1","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#硬件虚拟化"},{"categories":["开发者手册"],"content":" 软件虚拟化软件虚拟化指的是通过软件的方式来实现虚拟化中关键的指令转换部分。 比如，在软件虚拟化实现中，通过一层夹杂在应用程序和硬件平台上的虚拟化实现软件来进行指令的转换。也就是说，虽然应用程序向操作系统或者物理硬件发出的指令不是当前硬件平台所支持的指令，这个实现虚拟化的软件也会将之转换为当前硬件平台所能识别的。 ","date":"2018-03-18","objectID":"/docker-glance/:1:2","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#软件虚拟化"},{"categories":["开发者手册"],"content":" 浅析 Docker可以把容器看作一个简易版的 Linux 系统环境（包括 root 用户权限、进程空间、用户空间和网络空间等）以及运行在其中的应用程序打包而成的沙盒sandbox。 每个容器内运行着一个应用，不同的容器相互隔离，容器之间也可以通过网络互相通信。容器的创建和停止十分快速，几乎跟创建和终止原生应用一致；另外，容器自身对系统资源的额外需求也十分有限，远远低于传统虚拟机。很多时候，甚至直接把容器当作应用本身也没有任何问题。 Docker 并没有和虚拟机一样利用一个独立的 OS 执行环境的隔离，它利用的是目前 Linux 内核本身支持的容器方式，实现了资源和环境的隔离。 支撑 docker 的核心技术有三个：Namespace，Cgroup，UnionFS。 Namespace 提供了虚拟层面的隔离，比如文件隔离，网络隔离等等。每个命名空间中的应用看到的，都是不同的IP地址，用户空间，进程 ID 等。 Cgroup提供了物理资源的隔离，比如 CPU，内存，磁盘等等。 UnionFS 给 docker 镜像提供了技术支撑。在 Docker 中，提供了一种对 UnionFS 的改进实现，也就是 AUFSAdvanced Union File System。 AUFS 将文件的更新挂载到老的文件之上，而不去修改那些不更新的内容，这意味着即使虚拟的文件系统被反复修改，也能保证对真实文件系统的空间占用保持一个较低水平。就像在 Git 中每进行一次提交，Git 并不是将我们所有的内容打包成一个版本，而只是将修改的部分进行记录，这样即使我们提交很多次后，代码库的空间占用也不会倍数增加。 通过 AUFS，Docker 大幅减少了虚拟文件系统对物理存储空间的占用。 ","date":"2018-03-18","objectID":"/docker-glance/:1:3","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#浅析-docker"},{"categories":["开发者手册"],"content":" 虚拟机和 Docker虚拟机Virtual Machine，通常来说就是通过一个虚拟机监视器Virtual Machine Monitor 的设施来隔离操作系统与硬件或者应用程序和操作系统，以此达到虚拟化的目的。这个夹在其中的虚拟机监视器，常常被称为 Hypervisor。👇是虚拟机和 Docker 的对比： 虚拟机和容器 传统方式是在硬件层面实现虚拟化，需要有额外的虚拟机管理应用和虚拟机操作系统层。Docker容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，因此更加轻量级。 虚拟机更擅长彻底隔离整个运行环境。例如，云服务提供商通常采用虚拟机技术隔离不同的用户。而 Docker 通常用于隔离不同的应用，例如前端，后端以及数据库。 ","date":"2018-03-18","objectID":"/docker-glance/:1:4","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#虚拟机和-docker"},{"categories":["开发者手册"],"content":" 常用命令 CMD 说明 sudo docker create --name 容器名称 镜像名称 创建容器 docker start 名称 启动容器 sudo docker run --name 容器名称 -d 镜像名称 创建并启动容器且在后台运行,-d=--detach docker ps 列出运行中容器 docker ps -a 列出所有容器, -a=--all docker stop 容器名称/ID 停止容器 docker rm 容器名称/ID 删除容器 docker exec -it 容器名称 bash/sh 进入容器 docker attach --sig-proxy=false 容器名称 将容器转为了前台运行，如果不加--sig-proxy=false Ctrl + C 后会停止容器 docker logs 容器名称 查看容器日志 docker network ls/list 查看已经存在的网络 docker network create -d 网络驱动 网络名 创建新网络 docker volume ls 列出当前已创建的数据卷 docker volume create 名称 创建数据卷 docker volume rm 名称 删除数据卷 docker volume prune 删除没有被容器引用的数据卷 docker build 构建镜像 docker inspect 容器名/ID 查看容器详情 docker run --privileged 容器获取宿主机root权限 docker exec 命令能帮助我们在正在运行的容器中运行指定的命令。 docker exec [-i] 容器名 命令 docker exec --rm 选项参数，可以让容器在停止后自动删除，不需要再使用容器删除命令来删除，对临时容器友好👇。 docker run --rm --name mysql2 -e MYSQL_RANDOM_ROOT_PASSWORD=yes mysql:5.7 docker build -t webapp:latest -f ./webapp/a.Dockerfile ./webapp ☝️-t 选项，指定新生成镜像的名称。-f 指定 Dockerfile 文件所在目录，如果不写的话会从 ./webapp 目录中去找，./webapp 可以直接写成. 理解成当前目录，也是镜像构建的上下文，比如 COPY指令执行时就是从这个上下文中去找的。 如果需要禁止缓存可以加上--no-cache参数。 docker build --no-cache ..... ","date":"2018-03-18","objectID":"/docker-glance/:2:0","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#常用命令"},{"categories":["开发者手册"],"content":" images 子命令更多子命令选项还可以通过man docker-images 来查看，或者查看官方文档 docker images 。 选项 说明 -a, ‐‐all=true | false 列出所有（包括临时文件）镜像文件，默认为否 ‐‐digests=true | false 列出镜像的数字摘要值，默认为否 -f, --filter=[] 过滤列出的镜像，如dangling=true只显示没有被使用的镜像；也可指定带有特定标注的镜像等 --format=\"TEMPLATE\" 控制输出格式，如.ID代表ID信息，.Repository代表仓库信息等 ‐‐no-trunc=true | false 对输出结果中太长的部分是否进行截断，如镜像的ID信息，默认为是 -q, ‐‐quiet=true | false 仅输出ID信息，默认为否 --format --filter ","date":"2018-03-18","objectID":"/docker-glance/:2:1","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#images-子命令"},{"categories":["开发者手册"],"content":" create 子命令create 命令支持的选项都十分复杂，选项主要包括如下几大类：与容器运行模式相关、与容器环境配置相关、与容器资源限制和安全保护相关。 ⚠️容器运行模式相关的选项 容器运行模式相关的选项 ⚠️容器环境和配置相关的选项 容器环境和配置相关的选项 ⚠️容器资源限制和安全保护相关的选项 容器资源限制和安全保护相关的选项 ","date":"2018-03-18","objectID":"/docker-glance/:2:2","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#create-子命令"},{"categories":["开发者手册"],"content":" build 子命令 build 子命令 ","date":"2018-03-18","objectID":"/docker-glance/:2:3","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#build-子命令"},{"categories":["开发者手册"],"content":" 容器网络在 Docker 网络中，有三个比较核心的概念，就是：沙盒Sandbox 、网络Network、端点Endpoint。 沙盒提供了容器的虚拟网络栈，也就是端口套接字、IP 路由表、防火墙等内容。实现隔离容器网络与宿主机网络，形成了完全独立的容器网络环境。 网络可以理解为 Docker 内部的虚拟子网，网络内的参与者相互可见并能够进行通讯。Docker 的这种虚拟网络也是于宿主机网络存在隔离关系的，其目的主要是形成容器间的安全通讯环境。 端点是位于容器或网络隔离墙之上的洞，其主要目的是形成一个可以控制的突破封闭的网络环境的出入口。当容器的端点与网络的端点形成配对后，就如同在这两者之间搭建了桥梁，便能够进行数据传输了。 这三者形成了 Docker 网络的核心模型，也就是容器网络模型Container Network Model。 容器网络 Docker 官方提供了五种 Docker 网络驱动：Bridge Driver、Host Driver、Overlay Driver、MacLan Driver、None Driver。 网络驱动 Bridge 网络是 Docker 容器的默认网络驱动，通过网桥来实现网络通讯。为容器创建独立的网络命名空间，分配网卡、IP 地址等网络配置，并通过 veth 接口对将容器挂载到一个虚拟网桥（默认为docker0）上。 none为容器创建独立的网络命名空间，但不进行网络配置，即容器内没有创建网卡、IP地址等。 host不为容器创建独立的网络命名空间，容器内看到的网络配置（网卡信息、路由表、Iptables 规则等）均与主机上的保持一致。注意其他资源还是与主机隔离的。 Overlay 驱动默认采用 VXLAN 协议，在 IP 地址可以互相访问的多个主机之间搭建隧道，让容器可以互相访问，并且让这些容器感觉这个网络与其他类型的网络没有区别。 Overlay network ","date":"2018-03-18","objectID":"/docker-glance/:3:0","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#容器网络"},{"categories":["开发者手册"],"content":" 创建网络在 Docker 中，能够创建自定义网络，形成自己定义虚拟子网的目的。创建网络的命令是 docker network create。 docker network create -d bridge individual 通过 -d 选项我们可以为新的网络指定驱动的类型，其值可以默认的 bridge、host、overlay、maclan、none ，也可以是其他网络驱动插件所定义的类型。 当不指定网络驱动时，Docker 也会默认采用 Bridge Driver 作为网络驱动。 通过 docker network ls 或是 docker network list 可以查看 Docker 中已经存在的网络。 在创建容器时，可以通过 --network 来指定容器所加入的网络，一旦这个参数被指定，容器便不会默认加入到 bridge 这个网络中，但是仍然可以通过 --network bridge 让其加入。 docker run -d --name mysql -e MYSQL_RANDOM_ROOT_PASSWORD=yes --network individual mysql:5.7 可以通过 docker inspect 容器名/ID 观察此时的容器网络。 Docker 中如果两个容器处于不同的网络，之间是不能相互连接引用的。 ","date":"2018-03-18","objectID":"/docker-glance/:3:1","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#创建网络"},{"categories":["开发者手册"],"content":" 容器互联要让一个容器连接到另外一个容器，可以在容器通过 docker create 或 docker run 创建时通过 --link 选项进行配置。 例如，创建一个 MySQL 容器，将运行 Web 应用的容器连接到这个 MySQL 容器上，打通两个容器间的网络，实现它们之间的网络互通。 docker run -d --name mysql -e MYSQL_RANDOM_ROOT_PASSWORD=yes mysql docker run -d --name webapp --link mysql webapp:latest 容器间的网络已经打通，如何在 Web 应用中连接到 MySQL 数据库❓ Docker 为容器间连接提供了一种非常友好的方式，只需要将容器的网络命名（容器名）填入到连接地址中，就可以访问需要连接的容器了。 mysql:3306 在这里，连接地址中的 mysql（容器名） 好比常见的域名解析，Docker 会将其指向 MySQL 容器的 IP 地址。 Docker 在容器互通中带来的一项便利就是，不再需要真实的知道另外一个容器的 IP 地址就能进行连接。在以往的开发中，每切换一个环境（例如将程序从开发环境提交到测试环境），都需要重新配置程序中的各项连接地址等参数，而在 Docker 里，并不需要关心这个，只需要程序中配置被连接容器的名称，映射 IP 的工作就可以交给 Docker。 在 Docker 里还支持连接时使用别名来摆脱容器名的限制。 sudo docker run -d --name webapp --link mysql:database webapp:latest 在这里，使用 --link \u003cname\u003e:\u003calias\u003e 的形式，连接到 MySQL 容器，并设置它的别名为 database。当我们要在 Web 应用中使用 MySQL 连接时，我们就可以使用 database 来代替连接地址了。 database:3306 ","date":"2018-03-18","objectID":"/docker-glance/:3:2","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#容器互联"},{"categories":["开发者手册"],"content":" 端口映射容器直接通过 Docker 网络进行的互相访问，在实际使用中，我们需要在容器外通过网络访问容器中的应用。最简单的一个例子，我们提供了 Web 服务，那么我们就需要提供一种方式访问运行在容器中的 Web 应用。 端口映射 通过 Docker 端口映射功能，可以把容器的端口映射到宿主操作系统的端口上，当我们从外部访问宿主操作系统的端口时，数据请求就会自动发送给与之关联的容器端口。 要映射端口，可以在创建容器时使用 -p 或者是 –publish 选项。 docker run -d --name nginx -p 80:80 -p 443:443 nginx:1.12 使用端口映射选项的格式是 -p \u003cip\u003e:\u003chost-port\u003e:\u003ccontainer-port\u003e，其中 ip 是宿主操作系统的监听 ip，可以用来控制监听的网卡，默认为 0.0.0.0，即是监听所有网卡。host-port 和 container-port 分别表示映射到宿主操作系统的端口和容器的端口，这两者是可以不一样的，比如，可以将容器的 80 端口映射到宿主操作系统的 8080 端口，传入 -p 8080:80 即可。 ","date":"2018-03-18","objectID":"/docker-glance/:3:3","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#端口映射"},{"categories":["开发者手册"],"content":" 文件挂载","date":"2018-03-18","objectID":"/docker-glance/:4:0","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#文件挂载"},{"categories":["开发者手册"],"content":" Bind MountBind Mount能够直接将宿主操作系统中的目录和文件挂载到容器内的文件系统中，通过指定容器外的路径和容器内的路径，就可以形成挂载映射关系，在容器内外对文件的读写，都是相互可见的。 docker run -d --name nginx -v /webapp/html:/usr/share/nginx/html nginx:1.12 使用 -v 或 --volume 来挂载宿主操作系统目录的形式是 -v \u003chost-path\u003e:\u003ccontainer-path\u003e 或 --volume \u003chost-path\u003e:\u003ccontainer-path\u003e，其中 host-path 和 container-path 分别代表宿主操作系统中的目录和容器中的目录。这里需要注意的是，Docker 这里强制定义目录时必须使用绝对路径，不能使用相对路径。 Docker 还支持以只读的方式挂载，通过只读方式挂载的目录和文件，只能被容器中的程序读取，但不接受容器中程序修改它们的请求。在挂载选项 -v 后再接上 :ro 就可以只读挂载了。 docker run -d --name nginx -v /webapp/html:/usr/share/nginx/html:ro nginx:1.12 ","date":"2018-03-18","objectID":"/docker-glance/:4:1","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#bind-mount"},{"categories":["开发者手册"],"content":" Volume数据卷Volume是从宿主操作系统中挂载目录到容器内，只不过这个挂载的目录由 Docker 进行管理，只需要指定容器内的目录 ，不需要关心具体挂载到了宿主操作系统中的哪里。 可以使用 -v 或 --volume 选项来定义数据卷的挂载。 docker run -d --name webapp -v /webapp/storage webapp:latest 数据卷挂载到容器后，可以通过 docker inspect 容器名/ID 看到容器中数据卷挂载的信息👇。 [ { // ...... \"Mounts\": [ { \"Type\": \"volume\", \"Name\": \"2bbd2719b81fbe030e6f446243386d763ef25879ec82bb60c9be7ef7f3a25336\", \"Source\": \"/var/lib/docker/volumes/2bbd2719b81fbe030e6f446243386d763ef25879ec82bb60c9be7ef7f3a25336/_data\", \"Destination\": \"/webapp/storage\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" } ] // ...... } ] Source 是 Docker 为我们分配用于挂载的宿主机目录，其位于 Docker 的资源区域，一般默认为 /var/lib/docker 。一般并不需要关心这个目录，一切对它的管理都已经在 Docker 内实现了。 为了方便识别数据卷，可以像命名容器一样为数据卷命名，这里的 Name 是数据卷的命名，在未给出数据卷命名的时候，Docker 会采用数据卷的 ID 命名数据卷。可以通过 -v \u003cname\u003e:\u003ccontainer-path\u003e 这种形式来命名数据卷。 docker run -d --name webapp -v appdata:/webapp/storage webapp:latest -v 在定义Bind Mount时必须使用绝对路径，当不是绝对路径是就是Volume 的定义。 ","date":"2018-03-18","objectID":"/docker-glance/:4:2","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#volume"},{"categories":["开发者手册"],"content":" Tmpfs MountTmpfs Mount支持挂载系统内存中的一部分到容器的文件系统里，不过由于内存和容器的特征，它的存储并不是持久的，其中的内容会随着容器的停止而消失。 挂载临时文件目录要通过 --tmpfs 这个选项来完成。由于内存的具体位置不需要指定，在这个选项里只需要传递挂载到容器内的目录即可。 docker run -d --name webapp --tmpfs /webapp/cache webapp:latest ","date":"2018-03-18","objectID":"/docker-glance/:4:3","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#tmpfs-mount"},{"categories":["开发者手册"],"content":" 镜像版本管理工作中，当某个镜像不能满足我们的需求时，我们能够将容器内的修改记录下来，保存为一个新的镜像。 ","date":"2018-03-18","objectID":"/docker-glance/:5:0","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#镜像版本管理"},{"categories":["开发者手册"],"content":" 提交修改生成新镜像以下以官方的 nginx:1.12 镜像示例，修改后生成一个 nginx-v2 镜像。 先下载镜像👇 # 下载镜像 docker pull nginx:1.12 # 运行容器 docker run --name mynginx -d nginx:1.12 下载镜像并启动容器 通过 docker exec 进入容器，并在 /root 目录下新增 hw.txt 文件，文件内容为 hello world： 添加新文件 将这个改动后的容器保存为新的镜像，docker commit提交这次修改，提交容器更新后产生的镜像并没 REPOSITORY 和 TAG 的内容，也就是说，这个新的镜像还没有名字。可以使用 docker tag 给新镜像命名。 提交修改生成新镜像 ","date":"2018-03-18","objectID":"/docker-glance/:5:1","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#提交修改生成新镜像"},{"categories":["开发者手册"],"content":" 存出载入镜像对于某个镜像我们可以导出成一个 tar 包，也可以将一个 tar 镜像导入到系统中。 docker save -o 命令可以将一个镜像导出为 tar 包👇 导出镜像 可以通过docker load 导入一个 tar 包为镜像，以下删除了原有的 nginx-v2 镜像，通过 nginx-v2.tar 成功导入了 nginx-v2 镜像。 导入镜像 利用导入的 nginx-v2 镜像启动一个新容器 mynginxv2，在新容器中的 /root 就会有一个 hw.txt 新文件，内容为 hello world： 新镜像 ","date":"2018-03-18","objectID":"/docker-glance/:5:2","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#存出载入镜像"},{"categories":["开发者手册"],"content":" Dockerfile利用 Dockerfile 文件可以生成镜像，这对于自定义镜像非常优雅，也利于镜像分享，直接分享 Dockerfile 文件就可以了。 ","date":"2018-03-18","objectID":"/docker-glance/:6:0","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#dockerfile"},{"categories":["开发者手册"],"content":" 常用指令 常用指令 🏆FROM 通过 FROM 指令指定一个基础镜像，接下来所有的指令都是基于这个镜像所展开的。** 为了保证镜像精简，可以选用体积较小的镜像如Alpine或Debian作为基础镜像**。 FROM 指令支持三种形式： FROM \u003cimage\u003e [AS \u003cname\u003e] FROM \u003cimage\u003e[:\u003ctag\u003e] [AS \u003cname\u003e] FROM \u003cimage\u003e[@\u003cdigest\u003e] [AS \u003cname\u003e] Dockerfile 中的第一条指令必须是 FROM 指令，因为没有了基础镜像，一切构建过程都无法开展。 🏆RUN RUN 指令用于向控制台发送命令的指令，在 RUN 指令之后，我们直接拼接上需要执行的命令，在构建时，Docker 就会执行这些命令，并将它们对文件系统的修改记录下来，形成镜像的变化。 RUN \u003ccommand\u003e RUN [\"executable\", \"param1\", \"param2\"] RUN 指令是支持 \\换行的，如果单行的长度过长，建议对内容进行切割，方便阅读。 🏆ENTRYPOINT 和 CMD 在容器启动时会根据镜像所定义的一条命令来启动容器中进程号为 1 的进程。而这个命令的定义，就是通过 Dockerfile 中的 ENTRYPOINT 和 CMD 实现的。 ENTRYPOINT [\"executable\", \"param1\", \"param2\"] ENTRYPOINT command param1 param2 CMD [\"executable\",\"param1\",\"param2\"] CMD [\"param1\",\"param2\"] CMD command param1 param2 当 ENTRYPOINT 与 CMD 同时给出时，CMD 中的内容会作为 ENTRYPOINT 定义命令的参数，最终执行容器启动的还是 ENTRYPOINT 中给出的命令。 🏆EXPOSE 通过 EXPOSE 指令就可以为镜像指定要暴露的端口。 EXPOSE \u003cport\u003e 🏆VOLUME VOLUME [\"/data\"] 在 VOLUME 指令中定义的目录，在基于新镜像创建容器时，会自动建立为数据卷，不需要再单独使用 -v 选项来配置。 🏆LABEL LABEL指令可以为生成的镜像添加元数据标签信息。这些信息可以用来辅助过滤出特定镜像。格式为： LABEL \u003ckey\u003e=\u003cvalue\u003e LABEL version=1.2 🏆 COPY 和 ADD 在制作新的镜像的时候，可能需要将一些软件配置、程序代码、执行脚本等直接导入到镜像内的文件系统里，使用 COPY 或 ADD 指令能够帮助我们直接从宿主机的文件系统里拷贝内容到镜像里的文件系统中。 COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] \u003csrc\u003e... \u003cdest\u003e ADD [--chown=\u003cuser\u003e:\u003cgroup\u003e] \u003csrc\u003e... \u003cdest\u003e COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] [\"\u003csrc\u003e\",... \"\u003cdest\u003e\"] ADD [--chown=\u003cuser\u003e:\u003cgroup\u003e] [\"\u003csrc\u003e\",... \"\u003cdest\u003e\"] COPY 与 ADD 指令的定义方式完全一样，需要注意的仅是当我们的目录中存在空格时，可以使用后两种格式避免空格产生歧义。 ","date":"2018-03-18","objectID":"/docker-glance/:6:1","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#常用指令"},{"categories":["开发者手册"],"content":" ARG 参数在 Dockerfile 里，可以用 ARG 指令来建立一个参数变量，可以在构建时通过构建指令传入这个参数变量，并且在 Dockerfile 里使用它。 FROM debian:stretch-slim ## ...... ARG TOMCAT_MAJOR ARG TOMCAT_VERSION ## ...... RUN wget -O tomcat.tar.gz \"https://www.apache.org/dyn/closer.cgi?action=download\u0026filename=tomcat/tomcat-$TOMCAT_MAJOR/v$TOMCAT_VERSION/bin/apache-tomcat-$TOMCAT_VERSION.tar.gz\" ## ...... 在☝️这个例子里，我们将 Tomcat 的版本号通过 ARG 指令定义为参数变量，在调用下载 Tomcat 包时，使用变量替换掉下载地址中的版本号。通过这样的定义，就可以让我们在不对 Dockerfile 进行大幅修改的前提下，轻松实现对 Tomcat 版本的切换并重新构建镜像了。 如果我们需要通过这个 Dockerfile 文件构建 Tomcat 镜像，我们可以在构建时通过 docker build 的 --build-arg 选项来设置参数变量。 Docker内置了一些镜像创建变量，用户可以直接使用而无须声明，包括（不区分大小写）HTTP_PROXY、HTTPS_PROXY、FTP_PROXY 、NO_PROXY。 docker build --build-arg TOMCAT_MAJOR=8 --build-arg TOMCAT_VERSION=8.0.53 -t tomcat:8.0 ./tomcat ","date":"2018-03-18","objectID":"/docker-glance/:6:2","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#arg-参数"},{"categories":["开发者手册"],"content":" ENV 参数ENV 环境变量设置的实质，其实就是定义操作系统环境变量，所以在运行的容器里，一样拥有这些变量，而容器中运行的程序也能够得到这些变量的值。 FROM debian:stretch-slim ## ...... ENV TOMCAT_MAJOR 8 ENV TOMCAT_VERSION 8.0.53 ## ...... 环境变量的值不是在构建指令中传入的，而是在 Dockerfile 中编写。由于环境变量在容器运行时依然有效，所以运行容器时我们也可以对其进行覆盖，在创建容器时使用 -e 或是 --env 选项，可以对环境变量的值进行修改或定义新的环境变量。 docker run -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:5.7 ENV 指令所定义的变量，永远会覆盖 ARG 所定义的变量。 ","date":"2018-03-18","objectID":"/docker-glance/:6:3","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#env-参数"},{"categories":["开发者手册"],"content":" FAQ","date":"2018-03-18","objectID":"/docker-glance/:7:0","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#faq"},{"categories":["开发者手册"],"content":" CMD 指令CMD指令用来指定启动容器时默认执行的命令。支持三种格式： CMD [\"executable\", \"param1\", \"param2\"]：相当于执行 executable param1 param2，推荐方式。 CMD command param1 param2：在默认的 Shell 中执行，提供给需要交互的应用。 CMD [\"param1\", \"param2\"]：提供给 ENTRYPOINT 的默认参数。 每个 Dockerfile 只能有一条 CMD 命令。如果指定了多条命令，只有最后一条会被执行。 ","date":"2018-03-18","objectID":"/docker-glance/:7:1","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#cmd-指令"},{"categories":["开发者手册"],"content":" ENTRYPOINT 和 CMD 的区别这 2 个命令都是用来指定基于此镜像所创建容器里主进程的启动命令。 ENTRYPOINT 指令的优先级高于 CMD 指令。当 ENTRYPOINT 和 CMD 同时在镜像中被指定时，CMD 里的内容会作为 ENTRYPOINT 的参数，两者拼接之后，才是最终执行的命令。 NTRYPOINT CMD 实际执行 ENTRYPOINT [\"/bin/ep\", \"arge\"] /bin/ep arge ENTRYPOINT /bin/ep arge /bin/sh -c /bin/ep arge CMD [\"/bin/exec\", \"args\"] /bin/exec args CMD /bin/exec args /bin/sh -c /bin/exec args ENTRYPOINT [\"/bin/ep\", \"arge\"] CMD [\"/bin/exec\", \"argc\"] /bin/ep arge /bin/exec argc ENTRYPOINT [\"/bin/ep\", \"arge\"] CMD /bin/exec args /bin/ep arge /bin/sh -c /bin/exec args ENTRYPOINT /bin/ep arge CMD [\"/bin/exec\", \"argc\"] /bin/sh -c /bin/ep arge /bin/exec argc ENTRYPOINT /bin/ep arge CMD /bin/exec args /bin/sh -c /bin/ep arge /bin/sh -c /bin/exec args ENTRYPOINT 指令主要用于对容器进行一些初始化，而 CMD 指令则用于真正定义容器中主程序的启动命令。 创建容器时可以改写容器主程序的启动命令，而这个覆盖只会覆盖 CMD 中定义的内容，不会影响 ENTRYPOINT 中的内容。 每个 Dockerfile 中只能有一个ENTRYPOINT，当指定多个时，只有最后一个起效。在运行时，可以被--entrypoint 参数覆盖掉，如docker run --entrypoint。 使用脚本文件来作为 ENTRYPOINT 的内容是常见的做法，因为对容器运行初始化的命令相对较多，全部直接放置在 ENTRYPOINT 后会特别复杂。 ","date":"2018-03-18","objectID":"/docker-glance/:7:2","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#entrypoint-和-cmd-的区别"},{"categories":["开发者手册"],"content":" COPY 和 ADD 的区别 COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] \u003csrc\u003e... \u003cdest\u003e ADD [--chown=\u003cuser\u003e:\u003cgroup\u003e] \u003csrc\u003e... \u003cdest\u003e COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] [\"\u003csrc\u003e\",... \"\u003cdest\u003e\"] ADD [--chown=\u003cuser\u003e:\u003cgroup\u003e] [\"\u003csrc\u003e\",... \"\u003cdest\u003e\"] 两者的区别主要在于 ADD 能够支持使用网络端的 URL 地址作为 src 源，并且在源文件被识别为压缩包时，自动进行解压，而 COPY 没有这两个能力。 当使用本地目录为源目录时，推荐使用 COPY。 ","date":"2018-03-18","objectID":"/docker-glance/:7:3","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#copy-和-add-的区别"},{"categories":["开发者手册"],"content":" 写时复制在编程里，写时复制Copy on Write 常常用于对象或数组的拷贝中，当拷贝对象或数组时，复制的过程并不是马上发生在内存中，而只是先让两个变量同时指向同一个内存空间，并进行一些标记，当要对对象或数组进行修改时，才真正进行内存的拷贝。 Docker 的写时复制与编程中的相类似，在通过镜像运行容器时，并不是马上就把镜像里的所有内容拷贝到容器所运行的沙盒文件系统中，而是利用 UnionFS 将镜像以只读的方式挂载到沙盒文件系统中。只有在容器中发生对文件的修改时，修改才会体现到沙盒环境上。 也就是说，容器在创建和启动的过程中，不需要进行任何的文件系统复制操作，也不需要为容器单独开辟大量的硬盘空间，与其他虚拟化方式对这个过程的操作进行对比，Docker 启动的速度可见一斑。 采用写时复制机制来设计的 Docker，既保证了镜像在生成为容器时，以及容器在运行过程中，不会对自身造成修改。又借助剔除常见虚拟化在初始化时需要从镜像中拷贝整个文件系统的过程，大幅提高了容器的创建和启动速度。可以说，Docker 容器能够实现秒级启动速度，写时复制机制在其中发挥了举足轻重的作用。 ","date":"2018-03-18","objectID":"/docker-glance/:7:4","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#写时复制"},{"categories":["开发者手册"],"content":" docker save、export 区别docker save 和 docker load 是对镜像的操作，导入导出的是镜像文件。 docker export 和 docker import是对容器的操作，是导出导入容器，导出一个已经创建的容器到一个文件，不管此时这个容器是否处于运行状态，可以理解为容器快照。 容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态，如标签信息会被丢弃），而镜像存储文件将保存完整记录，体积更大。从容器快照文件导入时可以重新指定标签等元数据信息。 ","date":"2018-03-18","objectID":"/docker-glance/:7:5","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#docker-saveexport-区别"},{"categories":["开发者手册"],"content":" -v 和 –mount 的区别使用-v时，如果宿主机上没有这个文件，也会自动创建，如果使用--mount时，宿主机中没有这个文件会报错找不到这个文件，并创建失败。 --mount由多个键-值对组成，以逗号分隔，每个键-值对由一个\u003ckey\u003e=\u003cvalue\u003e元组组成。--mount语法比-v或--volume更冗长，但是键的顺序并不重要，标记的值也更容易理解。 挂载的类型type，可以是bind、volume或者tmpfs。 ","date":"2018-03-18","objectID":"/docker-glance/:7:6","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#-v-和---mount-的区别"},{"categories":["开发者手册"],"content":" 参考 Docker技术入门与实战(第三版) 对比Docker和虚拟机 开发者必备的 Docker 实践指南 Docker 基础知识 - 使用绑定挂载(bind mounts)管理应用程序数据 php 中文网 docker ","date":"2018-03-18","objectID":"/docker-glance/:8:0","series":null,"tags":["docker"],"title":"Docker 学习笔记","uri":"/docker-glance/#参考"}]